/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-02-07 11:07:13.225240: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-07 11:07:31.000088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:07:00.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-02-07 11:07:31.000138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)
/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Total number of parameters: 121
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012054949
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.01469375e-07
Norm of the params: 9.153184
Orig loss: 0.01205. Accuracy: 0.992
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08725222
Train loss (w/o reg) on all data: 0.08002844
Test loss (w/o reg) on all data: 0.04792318
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0794628e-05
Norm of the params: 12.019804
Flipped loss: 0.04792. Accuracy: 0.992
### Flips: 52, rs: 0, checks: 52
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217297
Test loss (w/o reg) on all data: 0.012054599
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7624865e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729688
Test loss (w/o reg) on all data: 0.012054757
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3662291e-07
Norm of the params: 9.153194
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079653315
Train loss (w/o reg) on all data: 0.07249861
Test loss (w/o reg) on all data: 0.049037233
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2853099e-05
Norm of the params: 11.962191
              Random: fixed   2 labels. Loss 0.04904. Accuracy 0.989.
### Flips: 52, rs: 0, checks: 104
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173023
Test loss (w/o reg) on all data: 0.0120548215
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3945055e-07
Norm of the params: 9.153136
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730214
Test loss (w/o reg) on all data: 0.01205486
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5590128e-07
Norm of the params: 9.153137
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07139544
Train loss (w/o reg) on all data: 0.063402295
Test loss (w/o reg) on all data: 0.04379407
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.393697e-06
Norm of the params: 12.643689
              Random: fixed   4 labels. Loss 0.04379. Accuracy 0.989.
### Flips: 52, rs: 0, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729541
Test loss (w/o reg) on all data: 0.012055653
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.415051e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729548
Test loss (w/o reg) on all data: 0.012055746
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5017127e-07
Norm of the params: 9.153212
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06725767
Train loss (w/o reg) on all data: 0.058860466
Test loss (w/o reg) on all data: 0.0410655
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0404383e-06
Norm of the params: 12.959325
              Random: fixed   5 labels. Loss 0.04107. Accuracy 0.989.
### Flips: 52, rs: 0, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729998
Test loss (w/o reg) on all data: 0.01205519
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2220188e-07
Norm of the params: 9.153163
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729993
Test loss (w/o reg) on all data: 0.012055268
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9498894e-07
Norm of the params: 9.153163
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059911072
Train loss (w/o reg) on all data: 0.051430106
Test loss (w/o reg) on all data: 0.039164025
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0192015e-05
Norm of the params: 13.023798
              Random: fixed   8 labels. Loss 0.03916. Accuracy 0.989.
### Flips: 52, rs: 0, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729465
Test loss (w/o reg) on all data: 0.012055433
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.630996e-07
Norm of the params: 9.153219
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729479
Test loss (w/o reg) on all data: 0.012055362
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.551889e-07
Norm of the params: 9.153219
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05991106
Train loss (w/o reg) on all data: 0.051428985
Test loss (w/o reg) on all data: 0.039165735
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.973384e-06
Norm of the params: 13.024652
              Random: fixed   8 labels. Loss 0.03917. Accuracy 0.989.
### Flips: 52, rs: 0, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729756
Test loss (w/o reg) on all data: 0.012055027
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3952162e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729749
Test loss (w/o reg) on all data: 0.012055102
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1125401e-07
Norm of the params: 9.153189
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054738425
Train loss (w/o reg) on all data: 0.046064932
Test loss (w/o reg) on all data: 0.03640971
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.6731005e-06
Norm of the params: 13.1707945
              Random: fixed  10 labels. Loss 0.03641. Accuracy 0.989.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07200119
Train loss (w/o reg) on all data: 0.060799293
Test loss (w/o reg) on all data: 0.047043025
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.7498544e-06
Norm of the params: 14.9678955
Flipped loss: 0.04704. Accuracy: 0.989
### Flips: 52, rs: 1, checks: 52
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010216442
Train loss (w/o reg) on all data: 0.0055173505
Test loss (w/o reg) on all data: 0.012119067
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.97364e-07
Norm of the params: 9.694423
     Influence (LOO): fixed  25 labels. Loss 0.01212. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.012055046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9769733e-07
Norm of the params: 9.153225
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07196042
Train loss (w/o reg) on all data: 0.0608647
Test loss (w/o reg) on all data: 0.047947563
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.137458e-06
Norm of the params: 14.8967905
              Random: fixed   1 labels. Loss 0.04795. Accuracy 0.989.
### Flips: 52, rs: 1, checks: 104
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729853
Test loss (w/o reg) on all data: 0.012055345
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5705514e-07
Norm of the params: 9.153179
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729846
Test loss (w/o reg) on all data: 0.012055268
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.410995e-07
Norm of the params: 9.15318
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0702761
Train loss (w/o reg) on all data: 0.059132453
Test loss (w/o reg) on all data: 0.04741932
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9465944e-05
Norm of the params: 14.928929
              Random: fixed   2 labels. Loss 0.04742. Accuracy 0.989.
### Flips: 52, rs: 1, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729746
Test loss (w/o reg) on all data: 0.012055587
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.904788e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729763
Test loss (w/o reg) on all data: 0.012055759
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.444024e-07
Norm of the params: 9.153188
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07027609
Train loss (w/o reg) on all data: 0.059136637
Test loss (w/o reg) on all data: 0.04742142
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8261187e-06
Norm of the params: 14.92612
              Random: fixed   2 labels. Loss 0.04742. Accuracy 0.989.
### Flips: 52, rs: 1, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729488
Test loss (w/o reg) on all data: 0.012054975
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.36364e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.0120550925
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3905154e-07
Norm of the params: 9.153217
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069444835
Train loss (w/o reg) on all data: 0.058209527
Test loss (w/o reg) on all data: 0.04514668
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3406836e-06
Norm of the params: 14.990203
              Random: fixed   3 labels. Loss 0.04515. Accuracy 0.992.
### Flips: 52, rs: 1, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172796
Test loss (w/o reg) on all data: 0.012054767
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.036001e-07
Norm of the params: 9.153386
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021727975
Test loss (w/o reg) on all data: 0.0120549
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8884543e-07
Norm of the params: 9.153383
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06539181
Train loss (w/o reg) on all data: 0.054966394
Test loss (w/o reg) on all data: 0.0447444
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.554812e-06
Norm of the params: 14.439815
              Random: fixed   5 labels. Loss 0.04474. Accuracy 0.992.
### Flips: 52, rs: 1, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730575
Test loss (w/o reg) on all data: 0.012055212
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.62726e-07
Norm of the params: 9.153099
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002173056
Test loss (w/o reg) on all data: 0.012055294
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9548173e-07
Norm of the params: 9.1531
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060915574
Train loss (w/o reg) on all data: 0.050351202
Test loss (w/o reg) on all data: 0.04206141
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.7879985e-06
Norm of the params: 14.535728
              Random: fixed   6 labels. Loss 0.04206. Accuracy 0.989.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08262793
Train loss (w/o reg) on all data: 0.07450104
Test loss (w/o reg) on all data: 0.05464516
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.4390143e-06
Norm of the params: 12.749036
Flipped loss: 0.05465. Accuracy: 0.985
### Flips: 52, rs: 2, checks: 52
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217293
Test loss (w/o reg) on all data: 0.012054983
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6568423e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729327
Test loss (w/o reg) on all data: 0.01205492
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2314379e-07
Norm of the params: 9.153237
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07762253
Train loss (w/o reg) on all data: 0.06910162
Test loss (w/o reg) on all data: 0.054546196
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.246068e-06
Norm of the params: 13.054436
              Random: fixed   1 labels. Loss 0.05455. Accuracy 0.981.
### Flips: 52, rs: 2, checks: 104
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729763
Test loss (w/o reg) on all data: 0.012054486
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1938929e-07
Norm of the params: 9.15319
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729758
Test loss (w/o reg) on all data: 0.012054563
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5906755e-07
Norm of the params: 9.15319
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077622525
Train loss (w/o reg) on all data: 0.06910257
Test loss (w/o reg) on all data: 0.054540113
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.1584614e-06
Norm of the params: 13.053702
              Random: fixed   1 labels. Loss 0.05454. Accuracy 0.981.
### Flips: 52, rs: 2, checks: 156
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012056222
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5124474e-07
Norm of the params: 9.153193
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012056091
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0889572e-07
Norm of the params: 9.153194
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07645431
Train loss (w/o reg) on all data: 0.06827107
Test loss (w/o reg) on all data: 0.05431234
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.007319e-06
Norm of the params: 12.793153
              Random: fixed   2 labels. Loss 0.05431. Accuracy 0.977.
### Flips: 52, rs: 2, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729437
Test loss (w/o reg) on all data: 0.0120542925
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7850167e-07
Norm of the params: 9.153224
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729441
Test loss (w/o reg) on all data: 0.0120544
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.887303e-07
Norm of the params: 9.153222
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0730586
Train loss (w/o reg) on all data: 0.06505725
Test loss (w/o reg) on all data: 0.055134196
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.809411e-06
Norm of the params: 12.650178
              Random: fixed   3 labels. Loss 0.05513. Accuracy 0.977.
### Flips: 52, rs: 2, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.012055546
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.992877e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729819
Test loss (w/o reg) on all data: 0.012055439
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3157614e-07
Norm of the params: 9.153182
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062552944
Train loss (w/o reg) on all data: 0.054974314
Test loss (w/o reg) on all data: 0.04836523
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0383634e-05
Norm of the params: 12.31148
              Random: fixed   7 labels. Loss 0.04837. Accuracy 0.977.
### Flips: 52, rs: 2, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173
Test loss (w/o reg) on all data: 0.012055477
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4313632e-07
Norm of the params: 9.153162
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729984
Test loss (w/o reg) on all data: 0.0120554175
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.944185e-07
Norm of the params: 9.153163
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05627917
Train loss (w/o reg) on all data: 0.048211776
Test loss (w/o reg) on all data: 0.040065445
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.307115e-06
Norm of the params: 12.702279
              Random: fixed   9 labels. Loss 0.04007. Accuracy 0.985.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088991664
Train loss (w/o reg) on all data: 0.079717085
Test loss (w/o reg) on all data: 0.03805769
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1791762e-06
Norm of the params: 13.61953
Flipped loss: 0.03806. Accuracy: 0.992
### Flips: 52, rs: 3, checks: 52
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362022
Train loss (w/o reg) on all data: 0.0021729495
Test loss (w/o reg) on all data: 0.012055584
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0821861e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172947
Test loss (w/o reg) on all data: 0.012055535
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1306801e-07
Norm of the params: 9.153219
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.087101765
Train loss (w/o reg) on all data: 0.07800168
Test loss (w/o reg) on all data: 0.037807375
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2731684e-06
Norm of the params: 13.490805
              Random: fixed   1 labels. Loss 0.03781. Accuracy 0.992.
### Flips: 52, rs: 3, checks: 104
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729446
Test loss (w/o reg) on all data: 0.012056013
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3949125e-07
Norm of the params: 9.153223
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729465
Test loss (w/o reg) on all data: 0.012055965
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.893212e-07
Norm of the params: 9.153221
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078181066
Train loss (w/o reg) on all data: 0.069318764
Test loss (w/o reg) on all data: 0.035261527
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.594248e-05
Norm of the params: 13.313376
              Random: fixed   4 labels. Loss 0.03526. Accuracy 0.992.
### Flips: 52, rs: 3, checks: 156
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012055109
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.353696e-07
Norm of the params: 9.153186
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729786
Test loss (w/o reg) on all data: 0.01205524
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.803465e-07
Norm of the params: 9.153186
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070804834
Train loss (w/o reg) on all data: 0.06264556
Test loss (w/o reg) on all data: 0.030165663
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9086873e-06
Norm of the params: 12.7744055
              Random: fixed   7 labels. Loss 0.03017. Accuracy 0.992.
### Flips: 52, rs: 3, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730554
Test loss (w/o reg) on all data: 0.012056292
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2887781e-07
Norm of the params: 9.153102
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730482
Test loss (w/o reg) on all data: 0.012056186
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8509145e-07
Norm of the params: 9.153109
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067986205
Train loss (w/o reg) on all data: 0.059628114
Test loss (w/o reg) on all data: 0.028449422
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9398793e-06
Norm of the params: 12.929108
              Random: fixed   8 labels. Loss 0.02845. Accuracy 0.992.
### Flips: 52, rs: 3, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729246
Test loss (w/o reg) on all data: 0.012054919
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5910705e-07
Norm of the params: 9.153246
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729246
Test loss (w/o reg) on all data: 0.012054883
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2918282e-07
Norm of the params: 9.153245
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06408659
Train loss (w/o reg) on all data: 0.055925578
Test loss (w/o reg) on all data: 0.030135231
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7534895e-06
Norm of the params: 12.775769
              Random: fixed   9 labels. Loss 0.03014. Accuracy 0.992.
### Flips: 52, rs: 3, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729262
Test loss (w/o reg) on all data: 0.012055188
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2967278e-07
Norm of the params: 9.153243
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172927
Test loss (w/o reg) on all data: 0.012055152
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2365228e-07
Norm of the params: 9.153243
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06310693
Train loss (w/o reg) on all data: 0.05502508
Test loss (w/o reg) on all data: 0.028754728
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3439925e-06
Norm of the params: 12.713655
              Random: fixed  10 labels. Loss 0.02875. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07887535
Train loss (w/o reg) on all data: 0.07000409
Test loss (w/o reg) on all data: 0.02699282
Train acc on all data:  0.9789875835721108
Test acc on all data:   1.0
Norm of the mean of gradients: 4.529383e-06
Norm of the params: 13.3201065
Flipped loss: 0.02699. Accuracy: 1.000
### Flips: 52, rs: 4, checks: 52
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173099
Test loss (w/o reg) on all data: 0.012056032
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6440694e-07
Norm of the params: 9.153052
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021730931
Test loss (w/o reg) on all data: 0.012056087
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.370045e-07
Norm of the params: 9.153057
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07646249
Train loss (w/o reg) on all data: 0.06768401
Test loss (w/o reg) on all data: 0.02585059
Train acc on all data:  0.9799426934097422
Test acc on all data:   1.0
Norm of the mean of gradients: 3.2126052e-06
Norm of the params: 13.250268
              Random: fixed   1 labels. Loss 0.02585. Accuracy 1.000.
### Flips: 52, rs: 4, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730058
Test loss (w/o reg) on all data: 0.012056364
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0127054e-07
Norm of the params: 9.153154
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730047
Test loss (w/o reg) on all data: 0.012056393
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1269985e-07
Norm of the params: 9.153156
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072589695
Train loss (w/o reg) on all data: 0.06317282
Test loss (w/o reg) on all data: 0.028438214
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.986956e-06
Norm of the params: 13.723614
              Random: fixed   2 labels. Loss 0.02844. Accuracy 0.996.
### Flips: 52, rs: 4, checks: 156
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729704
Test loss (w/o reg) on all data: 0.012055176
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.00986405e-07
Norm of the params: 9.153196
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729819
Test loss (w/o reg) on all data: 0.012055189
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7640533e-07
Norm of the params: 9.153183
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062054902
Train loss (w/o reg) on all data: 0.052069683
Test loss (w/o reg) on all data: 0.024631009
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.901491e-06
Norm of the params: 14.1316805
              Random: fixed   6 labels. Loss 0.02463. Accuracy 0.996.
### Flips: 52, rs: 4, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729218
Test loss (w/o reg) on all data: 0.012055422
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.1245326e-08
Norm of the params: 9.153249
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729216
Test loss (w/o reg) on all data: 0.012055447
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.136256e-07
Norm of the params: 9.153247
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06205491
Train loss (w/o reg) on all data: 0.0520716
Test loss (w/o reg) on all data: 0.024629122
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0805458e-05
Norm of the params: 14.130326
              Random: fixed   6 labels. Loss 0.02463. Accuracy 0.996.
### Flips: 52, rs: 4, checks: 260
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729777
Test loss (w/o reg) on all data: 0.0120555
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1109284e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729611
Test loss (w/o reg) on all data: 0.0120557975
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9384592e-07
Norm of the params: 9.153203
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05621303
Train loss (w/o reg) on all data: 0.046162523
Test loss (w/o reg) on all data: 0.026711924
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0094296e-05
Norm of the params: 14.177804
              Random: fixed   7 labels. Loss 0.02671. Accuracy 0.996.
### Flips: 52, rs: 4, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729388
Test loss (w/o reg) on all data: 0.012054747
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4224134e-07
Norm of the params: 9.153228
     Influence (LOO): fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729402
Test loss (w/o reg) on all data: 0.012054864
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2220549e-07
Norm of the params: 9.153227
                Loss: fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047232997
Train loss (w/o reg) on all data: 0.03670421
Test loss (w/o reg) on all data: 0.025329202
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7406679e-06
Norm of the params: 14.5112295
              Random: fixed   9 labels. Loss 0.02533. Accuracy 0.996.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08950249
Train loss (w/o reg) on all data: 0.07912155
Test loss (w/o reg) on all data: 0.037317038
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1783665e-06
Norm of the params: 14.408983
Flipped loss: 0.03732. Accuracy: 0.992
### Flips: 52, rs: 5, checks: 52
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009510402
Train loss (w/o reg) on all data: 0.0043743816
Test loss (w/o reg) on all data: 0.011669285
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.092744e-07
Norm of the params: 10.135108
     Influence (LOO): fixed  29 labels. Loss 0.01167. Accuracy 0.996.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009510402
Train loss (w/o reg) on all data: 0.0043743774
Test loss (w/o reg) on all data: 0.01166935
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.785306e-07
Norm of the params: 10.135112
                Loss: fixed  29 labels. Loss 0.01167. Accuracy 0.996.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08572607
Train loss (w/o reg) on all data: 0.075257
Test loss (w/o reg) on all data: 0.036616806
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.10266e-06
Norm of the params: 14.470016
              Random: fixed   1 labels. Loss 0.03662. Accuracy 0.996.
### Flips: 52, rs: 5, checks: 104
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009510402
Train loss (w/o reg) on all data: 0.004373996
Test loss (w/o reg) on all data: 0.0116698
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.838147e-07
Norm of the params: 10.135488
     Influence (LOO): fixed  29 labels. Loss 0.01167. Accuracy 0.996.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729579
Test loss (w/o reg) on all data: 0.012055077
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3341369e-07
Norm of the params: 9.153209
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078795135
Train loss (w/o reg) on all data: 0.06831453
Test loss (w/o reg) on all data: 0.03208299
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.0580535e-06
Norm of the params: 14.477989
              Random: fixed   4 labels. Loss 0.03208. Accuracy 0.996.
### Flips: 52, rs: 5, checks: 156
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009510403
Train loss (w/o reg) on all data: 0.004374149
Test loss (w/o reg) on all data: 0.011670138
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.0506237e-07
Norm of the params: 10.135338
     Influence (LOO): fixed  29 labels. Loss 0.01167. Accuracy 0.996.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729586
Test loss (w/o reg) on all data: 0.012055081
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3693082e-07
Norm of the params: 9.153209
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071019806
Train loss (w/o reg) on all data: 0.05984114
Test loss (w/o reg) on all data: 0.030626617
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.926158e-06
Norm of the params: 14.95237
              Random: fixed   6 labels. Loss 0.03063. Accuracy 0.992.
### Flips: 52, rs: 5, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012055774
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.45212e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012055497
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4697795e-07
Norm of the params: 9.153184
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07101981
Train loss (w/o reg) on all data: 0.059841517
Test loss (w/o reg) on all data: 0.030632103
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0428254e-06
Norm of the params: 14.952119
              Random: fixed   6 labels. Loss 0.03063. Accuracy 0.992.
### Flips: 52, rs: 5, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729697
Test loss (w/o reg) on all data: 0.012054825
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1009744e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729695
Test loss (w/o reg) on all data: 0.012055054
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.871574e-07
Norm of the params: 9.153196
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06734438
Train loss (w/o reg) on all data: 0.056381285
Test loss (w/o reg) on all data: 0.027843827
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8925266e-05
Norm of the params: 14.807496
              Random: fixed   7 labels. Loss 0.02784. Accuracy 0.992.
### Flips: 52, rs: 5, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729968
Test loss (w/o reg) on all data: 0.012055252
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9993672e-07
Norm of the params: 9.153164
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729944
Test loss (w/o reg) on all data: 0.012055333
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9085967e-07
Norm of the params: 9.153167
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06734438
Train loss (w/o reg) on all data: 0.056381512
Test loss (w/o reg) on all data: 0.02785229
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.362385e-06
Norm of the params: 14.807341
              Random: fixed   7 labels. Loss 0.02785. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07236477
Train loss (w/o reg) on all data: 0.06405494
Test loss (w/o reg) on all data: 0.031580117
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.229384e-06
Norm of the params: 12.891724
Flipped loss: 0.03158. Accuracy: 0.996
### Flips: 52, rs: 6, checks: 52
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012055201
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5757894e-07
Norm of the params: 9.153196
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172964
Test loss (w/o reg) on all data: 0.012055448
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5762985e-07
Norm of the params: 9.153203
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07236478
Train loss (w/o reg) on all data: 0.06405406
Test loss (w/o reg) on all data: 0.031581543
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7775114e-06
Norm of the params: 12.892415
              Random: fixed   0 labels. Loss 0.03158. Accuracy 0.996.
### Flips: 52, rs: 6, checks: 104
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728822
Test loss (w/o reg) on all data: 0.01205514
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9849298e-07
Norm of the params: 9.15329
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172885
Test loss (w/o reg) on all data: 0.012055088
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1365905e-07
Norm of the params: 9.15329
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07002457
Train loss (w/o reg) on all data: 0.061824773
Test loss (w/o reg) on all data: 0.029487709
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.657796e-06
Norm of the params: 12.806089
              Random: fixed   1 labels. Loss 0.02949. Accuracy 0.996.
### Flips: 52, rs: 6, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729693
Test loss (w/o reg) on all data: 0.012055332
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1955378e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729697
Test loss (w/o reg) on all data: 0.01205524
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9481643e-07
Norm of the params: 9.153194
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07002457
Train loss (w/o reg) on all data: 0.061824612
Test loss (w/o reg) on all data: 0.029486708
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.433886e-06
Norm of the params: 12.806217
              Random: fixed   1 labels. Loss 0.02949. Accuracy 0.996.
### Flips: 52, rs: 6, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172949
Test loss (w/o reg) on all data: 0.0120554175
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5453156e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.012055174
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6153102e-07
Norm of the params: 9.153215
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07002458
Train loss (w/o reg) on all data: 0.06182481
Test loss (w/o reg) on all data: 0.02948073
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.630763e-06
Norm of the params: 12.8060665
              Random: fixed   1 labels. Loss 0.02948. Accuracy 0.996.
### Flips: 52, rs: 6, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729607
Test loss (w/o reg) on all data: 0.012055603
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9973784e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729602
Test loss (w/o reg) on all data: 0.012055716
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2903339e-07
Norm of the params: 9.153204
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06519723
Train loss (w/o reg) on all data: 0.05690603
Test loss (w/o reg) on all data: 0.027823102
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.5486963e-06
Norm of the params: 12.877268
              Random: fixed   2 labels. Loss 0.02782. Accuracy 0.996.
### Flips: 52, rs: 6, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729958
Test loss (w/o reg) on all data: 0.012055834
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1683646e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729937
Test loss (w/o reg) on all data: 0.012055743
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.530277e-07
Norm of the params: 9.153168
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056067273
Train loss (w/o reg) on all data: 0.048343923
Test loss (w/o reg) on all data: 0.024238236
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.3007473e-06
Norm of the params: 12.428476
              Random: fixed   5 labels. Loss 0.02424. Accuracy 0.996.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06362461
Train loss (w/o reg) on all data: 0.053843617
Test loss (w/o reg) on all data: 0.044557158
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.0270264e-06
Norm of the params: 13.986417
Flipped loss: 0.04456. Accuracy: 0.985
### Flips: 52, rs: 7, checks: 52
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729649
Test loss (w/o reg) on all data: 0.0120557435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0848181e-07
Norm of the params: 9.153202
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012055626
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5759203e-07
Norm of the params: 9.153201
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06059411
Train loss (w/o reg) on all data: 0.050402272
Test loss (w/o reg) on all data: 0.043955218
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1031238e-05
Norm of the params: 14.277143
              Random: fixed   1 labels. Loss 0.04396. Accuracy 0.981.
### Flips: 52, rs: 7, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729516
Test loss (w/o reg) on all data: 0.012056086
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7048483e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729528
Test loss (w/o reg) on all data: 0.012056152
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8104105e-07
Norm of the params: 9.153214
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06059412
Train loss (w/o reg) on all data: 0.050401494
Test loss (w/o reg) on all data: 0.043956418
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.057265e-05
Norm of the params: 14.277692
              Random: fixed   1 labels. Loss 0.04396. Accuracy 0.981.
### Flips: 52, rs: 7, checks: 156
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728892
Test loss (w/o reg) on all data: 0.012055838
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7856641e-07
Norm of the params: 9.153283
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172907
Test loss (w/o reg) on all data: 0.0120558
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.975137e-07
Norm of the params: 9.153264
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06006228
Train loss (w/o reg) on all data: 0.05053916
Test loss (w/o reg) on all data: 0.03750057
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.13849e-05
Norm of the params: 13.800812
              Random: fixed   2 labels. Loss 0.03750. Accuracy 0.992.
### Flips: 52, rs: 7, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172866
Test loss (w/o reg) on all data: 0.012054934
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.2590475e-07
Norm of the params: 9.153307
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728696
Test loss (w/o reg) on all data: 0.0120550925
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7198763e-07
Norm of the params: 9.153305
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04868926
Train loss (w/o reg) on all data: 0.03906968
Test loss (w/o reg) on all data: 0.031750206
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9804787e-06
Norm of the params: 13.870533
              Random: fixed   6 labels. Loss 0.03175. Accuracy 0.992.
### Flips: 52, rs: 7, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172919
Test loss (w/o reg) on all data: 0.012055423
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5257082e-07
Norm of the params: 9.153251
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729185
Test loss (w/o reg) on all data: 0.012055374
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.288836e-07
Norm of the params: 9.153251
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0464482
Train loss (w/o reg) on all data: 0.036414307
Test loss (w/o reg) on all data: 0.03358393
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4362662e-06
Norm of the params: 14.166082
              Random: fixed   7 labels. Loss 0.03358. Accuracy 0.989.
### Flips: 52, rs: 7, checks: 312
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021727586
Test loss (w/o reg) on all data: 0.012055972
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4038187e-07
Norm of the params: 9.153425
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172961
Test loss (w/o reg) on all data: 0.012055311
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9570432e-07
Norm of the params: 9.153204
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04075973
Train loss (w/o reg) on all data: 0.03012121
Test loss (w/o reg) on all data: 0.037062246
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.1159443e-06
Norm of the params: 14.586653
              Random: fixed   8 labels. Loss 0.03706. Accuracy 0.981.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093761906
Train loss (w/o reg) on all data: 0.0826065
Test loss (w/o reg) on all data: 0.062836744
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.836117e-06
Norm of the params: 14.936804
Flipped loss: 0.06284. Accuracy: 0.985
### Flips: 52, rs: 8, checks: 52
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02007905
Train loss (w/o reg) on all data: 0.013315401
Test loss (w/o reg) on all data: 0.014227136
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5373073e-06
Norm of the params: 11.630691
     Influence (LOO): fixed  29 labels. Loss 0.01423. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008345254
Train loss (w/o reg) on all data: 0.0031439923
Test loss (w/o reg) on all data: 0.01132456
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5176053e-07
Norm of the params: 10.199275
                Loss: fixed  33 labels. Loss 0.01132. Accuracy 0.996.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08875706
Train loss (w/o reg) on all data: 0.0774906
Test loss (w/o reg) on all data: 0.06032689
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.4527446e-06
Norm of the params: 15.010969
              Random: fixed   2 labels. Loss 0.06033. Accuracy 0.989.
### Flips: 52, rs: 8, checks: 104
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008345253
Train loss (w/o reg) on all data: 0.0031440281
Test loss (w/o reg) on all data: 0.01132528
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4626144e-07
Norm of the params: 10.19924
     Influence (LOO): fixed  33 labels. Loss 0.01133. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012055033
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0314285e-07
Norm of the params: 9.15321
                Loss: fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086179934
Train loss (w/o reg) on all data: 0.07489285
Test loss (w/o reg) on all data: 0.060023922
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4820704e-05
Norm of the params: 15.024704
              Random: fixed   3 labels. Loss 0.06002. Accuracy 0.985.
### Flips: 52, rs: 8, checks: 156
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730126
Test loss (w/o reg) on all data: 0.012054849
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5311544e-07
Norm of the params: 9.153149
     Influence (LOO): fixed  34 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730114
Test loss (w/o reg) on all data: 0.012054914
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2419914e-07
Norm of the params: 9.153151
                Loss: fixed  34 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08375781
Train loss (w/o reg) on all data: 0.072689824
Test loss (w/o reg) on all data: 0.058644056
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7471375e-06
Norm of the params: 14.87816
              Random: fixed   5 labels. Loss 0.05864. Accuracy 0.985.
### Flips: 52, rs: 8, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729819
Test loss (w/o reg) on all data: 0.012055227
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5765353e-07
Norm of the params: 9.153182
     Influence (LOO): fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729812
Test loss (w/o reg) on all data: 0.01205517
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1986045e-07
Norm of the params: 9.153182
                Loss: fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07427459
Train loss (w/o reg) on all data: 0.06382574
Test loss (w/o reg) on all data: 0.059983306
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.243959e-05
Norm of the params: 14.45604
              Random: fixed   8 labels. Loss 0.05998. Accuracy 0.989.
### Flips: 52, rs: 8, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729993
Test loss (w/o reg) on all data: 0.012055548
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3627286e-07
Norm of the params: 9.153163
     Influence (LOO): fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172997
Test loss (w/o reg) on all data: 0.012055591
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3965217e-07
Norm of the params: 9.153164
                Loss: fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0742746
Train loss (w/o reg) on all data: 0.06382289
Test loss (w/o reg) on all data: 0.059980474
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0221289e-05
Norm of the params: 14.458015
              Random: fixed   8 labels. Loss 0.05998. Accuracy 0.989.
### Flips: 52, rs: 8, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172952
Test loss (w/o reg) on all data: 0.012055295
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.668343e-08
Norm of the params: 9.153215
     Influence (LOO): fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729537
Test loss (w/o reg) on all data: 0.012055325
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9498513e-07
Norm of the params: 9.153214
                Loss: fixed  34 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0693267
Train loss (w/o reg) on all data: 0.05929
Test loss (w/o reg) on all data: 0.05408832
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7968678e-05
Norm of the params: 14.168061
              Random: fixed  10 labels. Loss 0.05409. Accuracy 0.989.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05737938
Train loss (w/o reg) on all data: 0.04871365
Test loss (w/o reg) on all data: 0.04057678
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.012049e-06
Norm of the params: 13.164901
Flipped loss: 0.04058. Accuracy: 0.989
### Flips: 52, rs: 9, checks: 52
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172917
Test loss (w/o reg) on all data: 0.012054365
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1225803e-07
Norm of the params: 9.153251
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729195
Test loss (w/o reg) on all data: 0.012054462
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2429795e-07
Norm of the params: 9.153248
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05737938
Train loss (w/o reg) on all data: 0.04871174
Test loss (w/o reg) on all data: 0.04057045
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6833187e-06
Norm of the params: 13.166351
              Random: fixed   0 labels. Loss 0.04057. Accuracy 0.989.
### Flips: 52, rs: 9, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730713
Test loss (w/o reg) on all data: 0.01205465
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4429633e-07
Norm of the params: 9.153084
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730682
Test loss (w/o reg) on all data: 0.012054731
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.357709e-07
Norm of the params: 9.153088
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052772313
Train loss (w/o reg) on all data: 0.043952037
Test loss (w/o reg) on all data: 0.03613402
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3280525e-06
Norm of the params: 13.2817745
              Random: fixed   2 labels. Loss 0.03613. Accuracy 0.996.
### Flips: 52, rs: 9, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729667
Test loss (w/o reg) on all data: 0.012054918
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6701505e-07
Norm of the params: 9.153199
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.012054961
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.482386e-08
Norm of the params: 9.1532
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049879044
Train loss (w/o reg) on all data: 0.041132588
Test loss (w/o reg) on all data: 0.034177188
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.12274e-06
Norm of the params: 13.226078
              Random: fixed   3 labels. Loss 0.03418. Accuracy 0.996.
### Flips: 52, rs: 9, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730016
Test loss (w/o reg) on all data: 0.012055837
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8831362e-07
Norm of the params: 9.153159
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730005
Test loss (w/o reg) on all data: 0.01205591
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6344412e-07
Norm of the params: 9.15316
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044132024
Train loss (w/o reg) on all data: 0.0350377
Test loss (w/o reg) on all data: 0.03107059
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2813914e-05
Norm of the params: 13.486529
              Random: fixed   5 labels. Loss 0.03107. Accuracy 0.992.
### Flips: 52, rs: 9, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729942
Test loss (w/o reg) on all data: 0.012054619
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.9906116e-08
Norm of the params: 9.153169
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.01205467
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6985369e-07
Norm of the params: 9.15317
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037788037
Train loss (w/o reg) on all data: 0.028831244
Test loss (w/o reg) on all data: 0.023089642
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2840707e-06
Norm of the params: 13.384165
              Random: fixed   7 labels. Loss 0.02309. Accuracy 0.996.
### Flips: 52, rs: 9, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730054
Test loss (w/o reg) on all data: 0.012055028
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7472274e-07
Norm of the params: 9.153154
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730047
Test loss (w/o reg) on all data: 0.012055076
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5841457e-07
Norm of the params: 9.153155
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037180375
Train loss (w/o reg) on all data: 0.028170597
Test loss (w/o reg) on all data: 0.031837437
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2419764e-06
Norm of the params: 13.423695
              Random: fixed   8 labels. Loss 0.03184. Accuracy 0.989.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09714837
Train loss (w/o reg) on all data: 0.08999119
Test loss (w/o reg) on all data: 0.039963584
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.067526e-06
Norm of the params: 11.964266
Flipped loss: 0.03996. Accuracy: 0.992
### Flips: 52, rs: 10, checks: 52
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006318258
Train loss (w/o reg) on all data: 0.0021987215
Test loss (w/o reg) on all data: 0.006433358
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.073414e-07
Norm of the params: 9.076935
     Influence (LOO): fixed  31 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063182586
Train loss (w/o reg) on all data: 0.0021987217
Test loss (w/o reg) on all data: 0.0064333496
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.3117792e-07
Norm of the params: 9.076935
                Loss: fixed  31 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08318223
Train loss (w/o reg) on all data: 0.07526198
Test loss (w/o reg) on all data: 0.0343573
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5120165e-05
Norm of the params: 12.585902
              Random: fixed   4 labels. Loss 0.03436. Accuracy 0.992.
### Flips: 52, rs: 10, checks: 104
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729895
Test loss (w/o reg) on all data: 0.012054858
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5117661e-07
Norm of the params: 9.153173
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172989
Test loss (w/o reg) on all data: 0.012054905
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5643998e-07
Norm of the params: 9.153173
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08318221
Train loss (w/o reg) on all data: 0.075261444
Test loss (w/o reg) on all data: 0.03433976
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4502991e-06
Norm of the params: 12.586314
              Random: fixed   4 labels. Loss 0.03434. Accuracy 0.992.
### Flips: 52, rs: 10, checks: 156
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729448
Test loss (w/o reg) on all data: 0.012054939
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.9755006e-08
Norm of the params: 9.153222
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729455
Test loss (w/o reg) on all data: 0.01205498
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.900723e-07
Norm of the params: 9.153222
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08197089
Train loss (w/o reg) on all data: 0.07425568
Test loss (w/o reg) on all data: 0.030630112
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8673669e-05
Norm of the params: 12.421925
              Random: fixed   5 labels. Loss 0.03063. Accuracy 0.996.
### Flips: 52, rs: 10, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729926
Test loss (w/o reg) on all data: 0.012055185
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.861715e-07
Norm of the params: 9.153171
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729893
Test loss (w/o reg) on all data: 0.012055152
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.1403925e-07
Norm of the params: 9.1531725
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0715512
Train loss (w/o reg) on all data: 0.06396876
Test loss (w/o reg) on all data: 0.031193234
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.9425453e-06
Norm of the params: 12.314574
              Random: fixed   9 labels. Loss 0.03119. Accuracy 0.989.
### Flips: 52, rs: 10, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173135
Test loss (w/o reg) on all data: 0.012055426
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2240154e-07
Norm of the params: 9.153014
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173132
Test loss (w/o reg) on all data: 0.012055328
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4349552e-07
Norm of the params: 9.153016
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06503578
Train loss (w/o reg) on all data: 0.057530213
Test loss (w/o reg) on all data: 0.025560662
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.125324e-06
Norm of the params: 12.251995
              Random: fixed  12 labels. Loss 0.02556. Accuracy 0.992.
### Flips: 52, rs: 10, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729204
Test loss (w/o reg) on all data: 0.012055327
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3047453e-07
Norm of the params: 9.15325
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729185
Test loss (w/o reg) on all data: 0.012055265
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2745937e-07
Norm of the params: 9.15325
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05866532
Train loss (w/o reg) on all data: 0.05081447
Test loss (w/o reg) on all data: 0.020847281
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6410904e-06
Norm of the params: 12.530644
              Random: fixed  14 labels. Loss 0.02085. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098725036
Train loss (w/o reg) on all data: 0.090638764
Test loss (w/o reg) on all data: 0.050596397
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8116312e-06
Norm of the params: 12.717132
Flipped loss: 0.05060. Accuracy: 0.992
### Flips: 52, rs: 11, checks: 52
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021730307
Test loss (w/o reg) on all data: 0.012054662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.508202e-07
Norm of the params: 9.153131
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730228
Test loss (w/o reg) on all data: 0.012054666
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3025418e-07
Norm of the params: 9.153136
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09310423
Train loss (w/o reg) on all data: 0.084544666
Test loss (w/o reg) on all data: 0.049291927
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0927065e-05
Norm of the params: 13.084008
              Random: fixed   2 labels. Loss 0.04929. Accuracy 0.992.
### Flips: 52, rs: 11, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730135
Test loss (w/o reg) on all data: 0.012055381
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5908977e-07
Norm of the params: 9.153147
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730142
Test loss (w/o reg) on all data: 0.012055422
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8601405e-07
Norm of the params: 9.153148
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0905188
Train loss (w/o reg) on all data: 0.08182152
Test loss (w/o reg) on all data: 0.047427073
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.581736e-06
Norm of the params: 13.188843
              Random: fixed   3 labels. Loss 0.04743. Accuracy 0.996.
### Flips: 52, rs: 11, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729486
Test loss (w/o reg) on all data: 0.012055458
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.756366e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729483
Test loss (w/o reg) on all data: 0.012055335
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4647621e-07
Norm of the params: 9.153217
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08867226
Train loss (w/o reg) on all data: 0.07977826
Test loss (w/o reg) on all data: 0.046235252
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1178868e-05
Norm of the params: 13.337165
              Random: fixed   4 labels. Loss 0.04624. Accuracy 0.996.
### Flips: 52, rs: 11, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172948
Test loss (w/o reg) on all data: 0.012055165
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.918412e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729488
Test loss (w/o reg) on all data: 0.012055212
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.677415e-07
Norm of the params: 9.153217
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08681525
Train loss (w/o reg) on all data: 0.077777304
Test loss (w/o reg) on all data: 0.04315846
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.755112e-05
Norm of the params: 13.444666
              Random: fixed   5 labels. Loss 0.04316. Accuracy 0.996.
### Flips: 52, rs: 11, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620154
Train loss (w/o reg) on all data: 0.0021729458
Test loss (w/o reg) on all data: 0.012055756
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4088367e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729476
Test loss (w/o reg) on all data: 0.012055828
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6890862e-07
Norm of the params: 9.153218
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08424824
Train loss (w/o reg) on all data: 0.07495277
Test loss (w/o reg) on all data: 0.04046762
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1538203e-06
Norm of the params: 13.634863
              Random: fixed   6 labels. Loss 0.04047. Accuracy 0.992.
### Flips: 52, rs: 11, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172698
Test loss (w/o reg) on all data: 0.01205553
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.17387e-07
Norm of the params: 9.153492
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172701
Test loss (w/o reg) on all data: 0.012055663
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4521932e-07
Norm of the params: 9.153488
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084248245
Train loss (w/o reg) on all data: 0.07495426
Test loss (w/o reg) on all data: 0.040475953
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7210608e-05
Norm of the params: 13.633776
              Random: fixed   6 labels. Loss 0.04048. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0945739
Train loss (w/o reg) on all data: 0.086596124
Test loss (w/o reg) on all data: 0.06308223
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.263967e-06
Norm of the params: 12.631529
Flipped loss: 0.06308. Accuracy: 0.981
### Flips: 52, rs: 12, checks: 52
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729164
Test loss (w/o reg) on all data: 0.0120554175
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.702493e-07
Norm of the params: 9.153252
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729188
Test loss (w/o reg) on all data: 0.01205532
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2035909e-07
Norm of the params: 9.153252
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09120894
Train loss (w/o reg) on all data: 0.08303359
Test loss (w/o reg) on all data: 0.059074346
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.250487e-05
Norm of the params: 12.786987
              Random: fixed   1 labels. Loss 0.05907. Accuracy 0.981.
### Flips: 52, rs: 12, checks: 104
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172864
Test loss (w/o reg) on all data: 0.012056289
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6117855e-07
Norm of the params: 9.153309
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172867
Test loss (w/o reg) on all data: 0.0120564075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.329515e-07
Norm of the params: 9.153307
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08629014
Train loss (w/o reg) on all data: 0.07774023
Test loss (w/o reg) on all data: 0.058591556
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.528442e-06
Norm of the params: 13.0766325
              Random: fixed   3 labels. Loss 0.05859. Accuracy 0.977.
### Flips: 52, rs: 12, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729118
Test loss (w/o reg) on all data: 0.012054975
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.725528e-07
Norm of the params: 9.153257
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729134
Test loss (w/o reg) on all data: 0.012055095
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2209583e-07
Norm of the params: 9.153256
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07949972
Train loss (w/o reg) on all data: 0.07057473
Test loss (w/o reg) on all data: 0.055432152
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0504145e-05
Norm of the params: 13.360381
              Random: fixed   5 labels. Loss 0.05543. Accuracy 0.981.
### Flips: 52, rs: 12, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730019
Test loss (w/o reg) on all data: 0.012055723
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.380709e-07
Norm of the params: 9.153161
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.002173
Test loss (w/o reg) on all data: 0.012055682
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.09578e-07
Norm of the params: 9.153164
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0727782
Train loss (w/o reg) on all data: 0.06357548
Test loss (w/o reg) on all data: 0.049530644
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.478564e-06
Norm of the params: 13.566668
              Random: fixed   7 labels. Loss 0.04953. Accuracy 0.981.
### Flips: 52, rs: 12, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729774
Test loss (w/o reg) on all data: 0.012055409
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2795067e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729767
Test loss (w/o reg) on all data: 0.012055384
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.59147e-08
Norm of the params: 9.153188
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067399606
Train loss (w/o reg) on all data: 0.058199592
Test loss (w/o reg) on all data: 0.045582518
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.091719e-06
Norm of the params: 13.564669
              Random: fixed   9 labels. Loss 0.04558. Accuracy 0.981.
### Flips: 52, rs: 12, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729474
Test loss (w/o reg) on all data: 0.012055338
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.659515e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172948
Test loss (w/o reg) on all data: 0.01205545
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.882918e-07
Norm of the params: 9.153218
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062350012
Train loss (w/o reg) on all data: 0.052628465
Test loss (w/o reg) on all data: 0.04779715
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.309223e-06
Norm of the params: 13.9438505
              Random: fixed  11 labels. Loss 0.04780. Accuracy 0.981.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07961573
Train loss (w/o reg) on all data: 0.070993684
Test loss (w/o reg) on all data: 0.040554516
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0500516e-06
Norm of the params: 13.13167
Flipped loss: 0.04055. Accuracy: 0.992
### Flips: 52, rs: 13, checks: 52
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729271
Test loss (w/o reg) on all data: 0.012055174
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.17315516e-07
Norm of the params: 9.15324
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729309
Test loss (w/o reg) on all data: 0.012055144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6953403e-07
Norm of the params: 9.153237
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079615735
Train loss (w/o reg) on all data: 0.07099201
Test loss (w/o reg) on all data: 0.040551327
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.988992e-06
Norm of the params: 13.132958
              Random: fixed   0 labels. Loss 0.04055. Accuracy 0.992.
### Flips: 52, rs: 13, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729413
Test loss (w/o reg) on all data: 0.012055735
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9093852e-07
Norm of the params: 9.153226
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729409
Test loss (w/o reg) on all data: 0.012055848
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3679577e-07
Norm of the params: 9.153225
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07772335
Train loss (w/o reg) on all data: 0.06931299
Test loss (w/o reg) on all data: 0.04016534
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2118419e-05
Norm of the params: 12.96947
              Random: fixed   1 labels. Loss 0.04017. Accuracy 0.992.
### Flips: 52, rs: 13, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730273
Test loss (w/o reg) on all data: 0.012055629
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2159411e-07
Norm of the params: 9.153134
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730217
Test loss (w/o reg) on all data: 0.012055518
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2571828e-07
Norm of the params: 9.153139
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06921671
Train loss (w/o reg) on all data: 0.060793918
Test loss (w/o reg) on all data: 0.03765677
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.932412e-06
Norm of the params: 12.979059
              Random: fixed   4 labels. Loss 0.03766. Accuracy 0.992.
### Flips: 52, rs: 13, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730612
Test loss (w/o reg) on all data: 0.012056249
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.6929143e-07
Norm of the params: 9.153093
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620154
Train loss (w/o reg) on all data: 0.0021730599
Test loss (w/o reg) on all data: 0.012056402
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7112593e-07
Norm of the params: 9.153094
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06835373
Train loss (w/o reg) on all data: 0.059926357
Test loss (w/o reg) on all data: 0.036777906
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2596294e-05
Norm of the params: 12.982581
              Random: fixed   5 labels. Loss 0.03678. Accuracy 0.992.
### Flips: 52, rs: 13, checks: 260
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729488
Test loss (w/o reg) on all data: 0.012055944
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.11489186e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.002172954
Test loss (w/o reg) on all data: 0.012055685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.833035e-07
Norm of the params: 9.15321
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06524016
Train loss (w/o reg) on all data: 0.0569525
Test loss (w/o reg) on all data: 0.036606964
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.731451e-06
Norm of the params: 12.874518
              Random: fixed   6 labels. Loss 0.03661. Accuracy 0.996.
### Flips: 52, rs: 13, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021731132
Test loss (w/o reg) on all data: 0.012056416
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7473873e-07
Norm of the params: 9.153036
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731064
Test loss (w/o reg) on all data: 0.012056329
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.7931295e-07
Norm of the params: 9.153045
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06301925
Train loss (w/o reg) on all data: 0.054873656
Test loss (w/o reg) on all data: 0.035935745
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2482136e-06
Norm of the params: 12.763698
              Random: fixed   7 labels. Loss 0.03594. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08454633
Train loss (w/o reg) on all data: 0.07576093
Test loss (w/o reg) on all data: 0.042154398
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.803294e-06
Norm of the params: 13.255489
Flipped loss: 0.04215. Accuracy: 0.992
### Flips: 52, rs: 14, checks: 52
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172895
Test loss (w/o reg) on all data: 0.012055792
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.261444e-07
Norm of the params: 9.153277
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172895
Test loss (w/o reg) on all data: 0.012055688
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.895597e-07
Norm of the params: 9.153276
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08276438
Train loss (w/o reg) on all data: 0.0740097
Test loss (w/o reg) on all data: 0.038848445
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2286405e-06
Norm of the params: 13.232294
              Random: fixed   1 labels. Loss 0.03885. Accuracy 0.992.
### Flips: 52, rs: 14, checks: 104
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217296
Test loss (w/o reg) on all data: 0.012055639
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6843736e-07
Norm of the params: 9.153206
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729576
Test loss (w/o reg) on all data: 0.012055537
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8794784e-07
Norm of the params: 9.153207
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08276439
Train loss (w/o reg) on all data: 0.07400828
Test loss (w/o reg) on all data: 0.03884723
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1462097e-05
Norm of the params: 13.233373
              Random: fixed   1 labels. Loss 0.03885. Accuracy 0.992.
### Flips: 52, rs: 14, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729583
Test loss (w/o reg) on all data: 0.012055064
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1687575e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729579
Test loss (w/o reg) on all data: 0.012055115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3515103e-07
Norm of the params: 9.153208
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07933326
Train loss (w/o reg) on all data: 0.07103122
Test loss (w/o reg) on all data: 0.034766473
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6895995e-06
Norm of the params: 12.885687
              Random: fixed   3 labels. Loss 0.03477. Accuracy 0.996.
### Flips: 52, rs: 14, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729562
Test loss (w/o reg) on all data: 0.012055244
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0177188e-07
Norm of the params: 9.153211
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729562
Test loss (w/o reg) on all data: 0.012055197
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.40191e-08
Norm of the params: 9.153211
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07095028
Train loss (w/o reg) on all data: 0.06257584
Test loss (w/o reg) on all data: 0.031560447
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.8695183e-06
Norm of the params: 12.941744
              Random: fixed   6 labels. Loss 0.03156. Accuracy 0.996.
### Flips: 52, rs: 14, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730005
Test loss (w/o reg) on all data: 0.012055239
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6150425e-07
Norm of the params: 9.153161
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.012055312
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.665523e-07
Norm of the params: 9.153181
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070950285
Train loss (w/o reg) on all data: 0.06257743
Test loss (w/o reg) on all data: 0.031553894
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4336519e-05
Norm of the params: 12.940521
              Random: fixed   6 labels. Loss 0.03155. Accuracy 0.996.
### Flips: 52, rs: 14, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730503
Test loss (w/o reg) on all data: 0.012055262
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.475253e-07
Norm of the params: 9.153107
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173049
Test loss (w/o reg) on all data: 0.012055192
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.588355e-07
Norm of the params: 9.15311
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07095028
Train loss (w/o reg) on all data: 0.06257713
Test loss (w/o reg) on all data: 0.031559743
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4237555e-06
Norm of the params: 12.940746
              Random: fixed   6 labels. Loss 0.03156. Accuracy 0.996.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08138946
Train loss (w/o reg) on all data: 0.07260764
Test loss (w/o reg) on all data: 0.036039177
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5715186e-05
Norm of the params: 13.25279
Flipped loss: 0.03604. Accuracy: 0.996
### Flips: 52, rs: 15, checks: 52
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007404577
Train loss (w/o reg) on all data: 0.002783289
Test loss (w/o reg) on all data: 0.013697876
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1911827e-07
Norm of the params: 9.613832
     Influence (LOO): fixed  25 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007404578
Train loss (w/o reg) on all data: 0.0027832899
Test loss (w/o reg) on all data: 0.01369794
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1865812e-07
Norm of the params: 9.613832
                Loss: fixed  25 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07611612
Train loss (w/o reg) on all data: 0.067626156
Test loss (w/o reg) on all data: 0.034885187
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0168794e-05
Norm of the params: 13.030705
              Random: fixed   3 labels. Loss 0.03489. Accuracy 0.996.
### Flips: 52, rs: 15, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729704
Test loss (w/o reg) on all data: 0.012055687
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2569004e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729695
Test loss (w/o reg) on all data: 0.012055597
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.10879206e-07
Norm of the params: 9.153194
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07611614
Train loss (w/o reg) on all data: 0.0676318
Test loss (w/o reg) on all data: 0.03489944
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.923787e-06
Norm of the params: 13.026383
              Random: fixed   3 labels. Loss 0.03490. Accuracy 0.996.
### Flips: 52, rs: 15, checks: 156
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172974
Test loss (w/o reg) on all data: 0.01205619
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7372554e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729746
Test loss (w/o reg) on all data: 0.012055949
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7170623e-07
Norm of the params: 9.15319
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074200965
Train loss (w/o reg) on all data: 0.06661693
Test loss (w/o reg) on all data: 0.033353392
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.413208e-06
Norm of the params: 12.315874
              Random: fixed   5 labels. Loss 0.03335. Accuracy 0.996.
### Flips: 52, rs: 15, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.00217294
Test loss (w/o reg) on all data: 0.012055509
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.952711e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.012055621
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9982126e-07
Norm of the params: 9.153226
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074200965
Train loss (w/o reg) on all data: 0.06661662
Test loss (w/o reg) on all data: 0.033348814
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.789326e-06
Norm of the params: 12.316127
              Random: fixed   5 labels. Loss 0.03335. Accuracy 0.996.
### Flips: 52, rs: 15, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729574
Test loss (w/o reg) on all data: 0.0120549435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1528093e-07
Norm of the params: 9.153209
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729583
Test loss (w/o reg) on all data: 0.012055089
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4561025e-07
Norm of the params: 9.153209
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06914554
Train loss (w/o reg) on all data: 0.061963677
Test loss (w/o reg) on all data: 0.031756375
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.516069e-06
Norm of the params: 11.984872
              Random: fixed   6 labels. Loss 0.03176. Accuracy 0.992.
### Flips: 52, rs: 15, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728522
Test loss (w/o reg) on all data: 0.012056427
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0010606e-06
Norm of the params: 9.153325
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728508
Test loss (w/o reg) on all data: 0.01205622
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1695168e-07
Norm of the params: 9.153323
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06914553
Train loss (w/o reg) on all data: 0.06196152
Test loss (w/o reg) on all data: 0.031760883
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.230409e-06
Norm of the params: 11.986671
              Random: fixed   6 labels. Loss 0.03176. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07485262
Train loss (w/o reg) on all data: 0.06524927
Test loss (w/o reg) on all data: 0.06527653
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0108755e-05
Norm of the params: 13.858828
Flipped loss: 0.06528. Accuracy: 0.981
### Flips: 52, rs: 16, checks: 52
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729732
Test loss (w/o reg) on all data: 0.012055446
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.795623e-07
Norm of the params: 9.153191
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729632
Test loss (w/o reg) on all data: 0.012055102
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1057365e-06
Norm of the params: 9.153201
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07485262
Train loss (w/o reg) on all data: 0.06525208
Test loss (w/o reg) on all data: 0.06526471
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.7429166e-06
Norm of the params: 13.856799
              Random: fixed   0 labels. Loss 0.06526. Accuracy 0.981.
### Flips: 52, rs: 16, checks: 104
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730692
Test loss (w/o reg) on all data: 0.01205566
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4017995e-07
Norm of the params: 9.153086
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730668
Test loss (w/o reg) on all data: 0.012055573
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5252719e-07
Norm of the params: 9.153088
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06826821
Train loss (w/o reg) on all data: 0.058781046
Test loss (w/o reg) on all data: 0.06617608
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.0904715e-06
Norm of the params: 13.774735
              Random: fixed   2 labels. Loss 0.06618. Accuracy 0.973.
### Flips: 52, rs: 16, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729863
Test loss (w/o reg) on all data: 0.012055823
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.340518e-07
Norm of the params: 9.153175
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021729842
Test loss (w/o reg) on all data: 0.012055748
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2412188e-07
Norm of the params: 9.153176
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0652236
Train loss (w/o reg) on all data: 0.056123067
Test loss (w/o reg) on all data: 0.057873987
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5482638e-05
Norm of the params: 13.491133
              Random: fixed   4 labels. Loss 0.05787. Accuracy 0.977.
### Flips: 52, rs: 16, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620154
Train loss (w/o reg) on all data: 0.0021729816
Test loss (w/o reg) on all data: 0.012056625
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9628006e-07
Norm of the params: 9.153179
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012056534
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1203955e-07
Norm of the params: 9.15318
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06154378
Train loss (w/o reg) on all data: 0.051845416
Test loss (w/o reg) on all data: 0.060742617
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.858586e-06
Norm of the params: 13.927214
              Random: fixed   5 labels. Loss 0.06074. Accuracy 0.981.
### Flips: 52, rs: 16, checks: 260
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729951
Test loss (w/o reg) on all data: 0.012055165
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3151094e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729951
Test loss (w/o reg) on all data: 0.012055217
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.779269e-08
Norm of the params: 9.153168
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061543774
Train loss (w/o reg) on all data: 0.05184737
Test loss (w/o reg) on all data: 0.06073912
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.080866e-06
Norm of the params: 13.925807
              Random: fixed   5 labels. Loss 0.06074. Accuracy 0.981.
### Flips: 52, rs: 16, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730182
Test loss (w/o reg) on all data: 0.012055592
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0982866e-07
Norm of the params: 9.153143
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730165
Test loss (w/o reg) on all data: 0.012055654
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9556695e-07
Norm of the params: 9.153144
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06154378
Train loss (w/o reg) on all data: 0.05184563
Test loss (w/o reg) on all data: 0.060739476
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.1535823e-06
Norm of the params: 13.927061
              Random: fixed   5 labels. Loss 0.06074. Accuracy 0.981.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07356674
Train loss (w/o reg) on all data: 0.06314199
Test loss (w/o reg) on all data: 0.07806621
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0151919e-05
Norm of the params: 14.439363
Flipped loss: 0.07807. Accuracy: 0.973
### Flips: 52, rs: 17, checks: 52
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172922
Test loss (w/o reg) on all data: 0.01205494
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.13363505e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729434
Test loss (w/o reg) on all data: 0.012055123
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2379242e-07
Norm of the params: 9.153225
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0658425
Train loss (w/o reg) on all data: 0.056237232
Test loss (w/o reg) on all data: 0.063388236
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1644401e-05
Norm of the params: 13.86021
              Random: fixed   3 labels. Loss 0.06339. Accuracy 0.985.
### Flips: 52, rs: 17, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730275
Test loss (w/o reg) on all data: 0.0120539665
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2938243e-07
Norm of the params: 9.153133
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730266
Test loss (w/o reg) on all data: 0.012054046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5293993e-07
Norm of the params: 9.153133
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06275891
Train loss (w/o reg) on all data: 0.053130858
Test loss (w/o reg) on all data: 0.064448155
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4431474e-06
Norm of the params: 13.876634
              Random: fixed   4 labels. Loss 0.06445. Accuracy 0.981.
### Flips: 52, rs: 17, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730096
Test loss (w/o reg) on all data: 0.012053853
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.708618e-07
Norm of the params: 9.153151
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730105
Test loss (w/o reg) on all data: 0.012053959
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6651716e-07
Norm of the params: 9.1531515
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060537204
Train loss (w/o reg) on all data: 0.05083713
Test loss (w/o reg) on all data: 0.06391678
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.698006e-06
Norm of the params: 13.928444
              Random: fixed   5 labels. Loss 0.06392. Accuracy 0.977.
### Flips: 52, rs: 17, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729823
Test loss (w/o reg) on all data: 0.012054724
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6920102e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729826
Test loss (w/o reg) on all data: 0.012054699
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.3419664e-08
Norm of the params: 9.153182
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05785411
Train loss (w/o reg) on all data: 0.049227595
Test loss (w/o reg) on all data: 0.059962258
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.623315e-06
Norm of the params: 13.135078
              Random: fixed   7 labels. Loss 0.05996. Accuracy 0.985.
### Flips: 52, rs: 17, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728948
Test loss (w/o reg) on all data: 0.012054344
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4151286e-07
Norm of the params: 9.153277
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729227
Test loss (w/o reg) on all data: 0.012054378
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.279917e-07
Norm of the params: 9.153247
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056032103
Train loss (w/o reg) on all data: 0.047598794
Test loss (w/o reg) on all data: 0.049675338
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2454285e-06
Norm of the params: 12.987153
              Random: fixed   8 labels. Loss 0.04968. Accuracy 0.992.
### Flips: 52, rs: 17, checks: 312
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.012054785
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.029585e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729677
Test loss (w/o reg) on all data: 0.012054849
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2946019e-07
Norm of the params: 9.153197
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052088276
Train loss (w/o reg) on all data: 0.04335032
Test loss (w/o reg) on all data: 0.046427473
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4908658e-06
Norm of the params: 13.219649
              Random: fixed   9 labels. Loss 0.04643. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07054171
Train loss (w/o reg) on all data: 0.062607616
Test loss (w/o reg) on all data: 0.0408482
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3193529e-05
Norm of the params: 12.596897
Flipped loss: 0.04085. Accuracy: 0.989
### Flips: 52, rs: 18, checks: 52
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729707
Test loss (w/o reg) on all data: 0.012054821
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4546478e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217297
Test loss (w/o reg) on all data: 0.012054873
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4387156e-07
Norm of the params: 9.153193
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05760745
Train loss (w/o reg) on all data: 0.049157266
Test loss (w/o reg) on all data: 0.033145577
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6726587e-06
Norm of the params: 13.000143
              Random: fixed   5 labels. Loss 0.03315. Accuracy 0.989.
### Flips: 52, rs: 18, checks: 104
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728077
Test loss (w/o reg) on all data: 0.012055457
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0438619e-06
Norm of the params: 9.153372
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728096
Test loss (w/o reg) on all data: 0.012055653
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8353275e-07
Norm of the params: 9.15337
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05453308
Train loss (w/o reg) on all data: 0.046382423
Test loss (w/o reg) on all data: 0.029297372
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.485769e-06
Norm of the params: 12.767659
              Random: fixed   6 labels. Loss 0.02930. Accuracy 0.989.
### Flips: 52, rs: 18, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729085
Test loss (w/o reg) on all data: 0.012054882
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.655947e-07
Norm of the params: 9.153263
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729218
Test loss (w/o reg) on all data: 0.012055179
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.182339e-07
Norm of the params: 9.153249
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05180011
Train loss (w/o reg) on all data: 0.043749664
Test loss (w/o reg) on all data: 0.02925488
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.2277663e-06
Norm of the params: 12.688929
              Random: fixed   7 labels. Loss 0.02925. Accuracy 0.989.
### Flips: 52, rs: 18, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729993
Test loss (w/o reg) on all data: 0.012055314
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5756723e-07
Norm of the params: 9.153162
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729972
Test loss (w/o reg) on all data: 0.012055364
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.312883e-07
Norm of the params: 9.153165
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042825345
Train loss (w/o reg) on all data: 0.034478355
Test loss (w/o reg) on all data: 0.027096637
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3128692e-06
Norm of the params: 12.920519
              Random: fixed   9 labels. Loss 0.02710. Accuracy 0.996.
### Flips: 52, rs: 18, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730175
Test loss (w/o reg) on all data: 0.0120550115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1862764e-07
Norm of the params: 9.153141
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730189
Test loss (w/o reg) on all data: 0.012055109
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1115969e-07
Norm of the params: 9.153142
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04282535
Train loss (w/o reg) on all data: 0.034478538
Test loss (w/o reg) on all data: 0.027097462
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0716514e-06
Norm of the params: 12.92038
              Random: fixed   9 labels. Loss 0.02710. Accuracy 0.996.
### Flips: 52, rs: 18, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730792
Test loss (w/o reg) on all data: 0.012054688
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5916448e-07
Norm of the params: 9.153074
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173079
Test loss (w/o reg) on all data: 0.012054614
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3830972e-07
Norm of the params: 9.153076
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04100789
Train loss (w/o reg) on all data: 0.03308439
Test loss (w/o reg) on all data: 0.023857478
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.128786e-06
Norm of the params: 12.588488
              Random: fixed  10 labels. Loss 0.02386. Accuracy 0.996.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093134485
Train loss (w/o reg) on all data: 0.08258879
Test loss (w/o reg) on all data: 0.06985815
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.6464065e-06
Norm of the params: 14.522872
Flipped loss: 0.06986. Accuracy: 0.989
### Flips: 52, rs: 19, checks: 52
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729553
Test loss (w/o reg) on all data: 0.012054396
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8541216e-07
Norm of the params: 9.153211
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729553
Test loss (w/o reg) on all data: 0.01205451
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9854504e-07
Norm of the params: 9.153211
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08724585
Train loss (w/o reg) on all data: 0.07670474
Test loss (w/o reg) on all data: 0.06719583
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.402359e-06
Norm of the params: 14.519721
              Random: fixed   2 labels. Loss 0.06720. Accuracy 0.985.
### Flips: 52, rs: 19, checks: 104
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012055044
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.804743e-07
Norm of the params: 9.15318
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729819
Test loss (w/o reg) on all data: 0.012055151
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9534828e-07
Norm of the params: 9.153181
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07793773
Train loss (w/o reg) on all data: 0.067039624
Test loss (w/o reg) on all data: 0.06289345
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8979276e-06
Norm of the params: 14.763542
              Random: fixed   5 labels. Loss 0.06289. Accuracy 0.989.
### Flips: 52, rs: 19, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172933
Test loss (w/o reg) on all data: 0.012054147
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0333723e-07
Norm of the params: 9.1532345
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729337
Test loss (w/o reg) on all data: 0.012054265
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.078823e-07
Norm of the params: 9.153234
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07634512
Train loss (w/o reg) on all data: 0.06557379
Test loss (w/o reg) on all data: 0.060536332
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.33788635e-05
Norm of the params: 14.677421
              Random: fixed   6 labels. Loss 0.06054. Accuracy 0.992.
### Flips: 52, rs: 19, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730068
Test loss (w/o reg) on all data: 0.0120547665
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0927891e-07
Norm of the params: 9.153153
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730072
Test loss (w/o reg) on all data: 0.0120548
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7997474e-07
Norm of the params: 9.153154
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07484379
Train loss (w/o reg) on all data: 0.06410579
Test loss (w/o reg) on all data: 0.057877414
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.7009624e-06
Norm of the params: 14.654694
              Random: fixed   7 labels. Loss 0.05788. Accuracy 0.989.
### Flips: 52, rs: 19, checks: 260
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730233
Test loss (w/o reg) on all data: 0.012054845
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6791702e-07
Norm of the params: 9.153138
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730198
Test loss (w/o reg) on all data: 0.012054813
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.03786874e-07
Norm of the params: 9.15314
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06809806
Train loss (w/o reg) on all data: 0.056940623
Test loss (w/o reg) on all data: 0.050713684
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.738318e-06
Norm of the params: 14.938164
              Random: fixed   9 labels. Loss 0.05071. Accuracy 0.989.
### Flips: 52, rs: 19, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.012055032
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0325373e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729753
Test loss (w/o reg) on all data: 0.012054979
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4150926e-08
Norm of the params: 9.153188
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06588432
Train loss (w/o reg) on all data: 0.055221576
Test loss (w/o reg) on all data: 0.045366794
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2251468e-05
Norm of the params: 14.603253
              Random: fixed  11 labels. Loss 0.04537. Accuracy 0.989.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07368483
Train loss (w/o reg) on all data: 0.0636291
Test loss (w/o reg) on all data: 0.054163598
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.053761e-06
Norm of the params: 14.181485
Flipped loss: 0.05416. Accuracy: 0.989
### Flips: 52, rs: 20, checks: 52
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172977
Test loss (w/o reg) on all data: 0.012055217
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1123163e-07
Norm of the params: 9.153187
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729777
Test loss (w/o reg) on all data: 0.012055129
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0517804e-07
Norm of the params: 9.153187
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069951296
Train loss (w/o reg) on all data: 0.059947927
Test loss (w/o reg) on all data: 0.050870124
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.789012e-06
Norm of the params: 14.14452
              Random: fixed   2 labels. Loss 0.05087. Accuracy 0.989.
### Flips: 52, rs: 20, checks: 104
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730026
Test loss (w/o reg) on all data: 0.012055076
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.091742e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730028
Test loss (w/o reg) on all data: 0.012055211
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4967163e-07
Norm of the params: 9.153159
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06574498
Train loss (w/o reg) on all data: 0.05528014
Test loss (w/o reg) on all data: 0.05158423
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.6370795e-06
Norm of the params: 14.467094
              Random: fixed   3 labels. Loss 0.05158. Accuracy 0.989.
### Flips: 52, rs: 20, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173042
Test loss (w/o reg) on all data: 0.012055037
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.109233e-07
Norm of the params: 9.153116
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002173042
Test loss (w/o reg) on all data: 0.012055167
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7321628e-07
Norm of the params: 9.153117
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05697523
Train loss (w/o reg) on all data: 0.04641258
Test loss (w/o reg) on all data: 0.053696353
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.976313e-06
Norm of the params: 14.534545
              Random: fixed   5 labels. Loss 0.05370. Accuracy 0.989.
### Flips: 52, rs: 20, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729688
Test loss (w/o reg) on all data: 0.01205506
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5165705e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729693
Test loss (w/o reg) on all data: 0.012054949
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4604614e-08
Norm of the params: 9.153197
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05523129
Train loss (w/o reg) on all data: 0.044413656
Test loss (w/o reg) on all data: 0.057999436
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.8025958e-06
Norm of the params: 14.708934
              Random: fixed   6 labels. Loss 0.05800. Accuracy 0.985.
### Flips: 52, rs: 20, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729628
Test loss (w/o reg) on all data: 0.012055071
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3200616e-07
Norm of the params: 9.153205
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172961
Test loss (w/o reg) on all data: 0.012055033
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.5856215e-08
Norm of the params: 9.153204
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04595044
Train loss (w/o reg) on all data: 0.03568652
Test loss (w/o reg) on all data: 0.046913482
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2013603e-06
Norm of the params: 14.32754
              Random: fixed   9 labels. Loss 0.04691. Accuracy 0.992.
### Flips: 52, rs: 20, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729209
Test loss (w/o reg) on all data: 0.012054373
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.043915e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729225
Test loss (w/o reg) on all data: 0.012054545
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8081593e-07
Norm of the params: 9.153248
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042192813
Train loss (w/o reg) on all data: 0.031963337
Test loss (w/o reg) on all data: 0.048519805
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2000554e-06
Norm of the params: 14.303479
              Random: fixed  10 labels. Loss 0.04852. Accuracy 0.985.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08157077
Train loss (w/o reg) on all data: 0.07253765
Test loss (w/o reg) on all data: 0.0542129
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.099221e-06
Norm of the params: 13.441067
Flipped loss: 0.05421. Accuracy: 0.989
### Flips: 52, rs: 21, checks: 52
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729034
Test loss (w/o reg) on all data: 0.012055585
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.030349e-07
Norm of the params: 9.153267
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172904
Test loss (w/o reg) on all data: 0.012055488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5672745e-07
Norm of the params: 9.153266
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07955775
Train loss (w/o reg) on all data: 0.070477046
Test loss (w/o reg) on all data: 0.049873814
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2840421e-06
Norm of the params: 13.476427
              Random: fixed   1 labels. Loss 0.04987. Accuracy 0.992.
### Flips: 52, rs: 21, checks: 104
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730256
Test loss (w/o reg) on all data: 0.012055052
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0315051e-07
Norm of the params: 9.153133
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730089
Test loss (w/o reg) on all data: 0.012055088
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.016203e-07
Norm of the params: 9.1531515
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0790672
Train loss (w/o reg) on all data: 0.07064402
Test loss (w/o reg) on all data: 0.050103698
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1071867e-06
Norm of the params: 12.979351
              Random: fixed   2 labels. Loss 0.05010. Accuracy 0.992.
### Flips: 52, rs: 21, checks: 156
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729888
Test loss (w/o reg) on all data: 0.012055104
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2383489e-07
Norm of the params: 9.153173
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729872
Test loss (w/o reg) on all data: 0.012054944
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7373774e-07
Norm of the params: 9.153176
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06959559
Train loss (w/o reg) on all data: 0.06152198
Test loss (w/o reg) on all data: 0.042264815
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.46499e-06
Norm of the params: 12.707168
              Random: fixed   5 labels. Loss 0.04226. Accuracy 0.992.
### Flips: 52, rs: 21, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021731132
Test loss (w/o reg) on all data: 0.012055429
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.542457e-07
Norm of the params: 9.153036
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731046
Test loss (w/o reg) on all data: 0.0120551
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.196014e-07
Norm of the params: 9.153048
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067158684
Train loss (w/o reg) on all data: 0.059243765
Test loss (w/o reg) on all data: 0.041008677
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1818881e-06
Norm of the params: 12.581669
              Random: fixed   6 labels. Loss 0.04101. Accuracy 0.992.
### Flips: 52, rs: 21, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729989
Test loss (w/o reg) on all data: 0.012054976
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.976528e-08
Norm of the params: 9.153164
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729984
Test loss (w/o reg) on all data: 0.012054998
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.00879866e-07
Norm of the params: 9.153165
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064843416
Train loss (w/o reg) on all data: 0.05739658
Test loss (w/o reg) on all data: 0.04086671
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7077364e-06
Norm of the params: 12.203964
              Random: fixed   7 labels. Loss 0.04087. Accuracy 0.992.
### Flips: 52, rs: 21, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728391
Test loss (w/o reg) on all data: 0.012055297
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.03186e-07
Norm of the params: 9.153338
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728473
Test loss (w/o reg) on all data: 0.01205544
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.287014e-07
Norm of the params: 9.15333
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064843416
Train loss (w/o reg) on all data: 0.057397544
Test loss (w/o reg) on all data: 0.040862523
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.675304e-06
Norm of the params: 12.203175
              Random: fixed   7 labels. Loss 0.04086. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07095988
Train loss (w/o reg) on all data: 0.061854925
Test loss (w/o reg) on all data: 0.040808726
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7203776e-06
Norm of the params: 13.494414
Flipped loss: 0.04081. Accuracy: 0.996
### Flips: 52, rs: 22, checks: 52
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172963
Test loss (w/o reg) on all data: 0.012055591
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4780827e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.012055697
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7198835e-07
Norm of the params: 9.153204
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07095989
Train loss (w/o reg) on all data: 0.061853956
Test loss (w/o reg) on all data: 0.04080422
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.907685e-06
Norm of the params: 13.495133
              Random: fixed   0 labels. Loss 0.04080. Accuracy 0.996.
### Flips: 52, rs: 22, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728573
Test loss (w/o reg) on all data: 0.012055656
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7835574e-07
Norm of the params: 9.153317
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728687
Test loss (w/o reg) on all data: 0.012055329
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0410935e-06
Norm of the params: 9.153306
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06612305
Train loss (w/o reg) on all data: 0.056235723
Test loss (w/o reg) on all data: 0.041363277
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7824124e-06
Norm of the params: 14.062242
              Random: fixed   2 labels. Loss 0.04136. Accuracy 0.992.
### Flips: 52, rs: 22, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.01205558
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3046325e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729798
Test loss (w/o reg) on all data: 0.012055607
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4555646e-07
Norm of the params: 9.153184
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06417325
Train loss (w/o reg) on all data: 0.05386444
Test loss (w/o reg) on all data: 0.04159943
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.113155e-06
Norm of the params: 14.358839
              Random: fixed   3 labels. Loss 0.04160. Accuracy 0.992.
### Flips: 52, rs: 22, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729863
Test loss (w/o reg) on all data: 0.012055661
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.691386e-07
Norm of the params: 9.153179
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012055572
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.296496e-07
Norm of the params: 9.153178
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06164041
Train loss (w/o reg) on all data: 0.051157013
Test loss (w/o reg) on all data: 0.037314035
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6757416e-06
Norm of the params: 14.479918
              Random: fixed   5 labels. Loss 0.03731. Accuracy 0.992.
### Flips: 52, rs: 22, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.00217298
Test loss (w/o reg) on all data: 0.012055221
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4360306e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172978
Test loss (w/o reg) on all data: 0.012055024
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0934952e-07
Norm of the params: 9.153186
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05393089
Train loss (w/o reg) on all data: 0.04423525
Test loss (w/o reg) on all data: 0.039181102
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.994287e-06
Norm of the params: 13.925257
              Random: fixed  10 labels. Loss 0.03918. Accuracy 0.989.
### Flips: 52, rs: 22, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730163
Test loss (w/o reg) on all data: 0.012056045
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1663607e-07
Norm of the params: 9.153145
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730142
Test loss (w/o reg) on all data: 0.012056096
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3865786e-07
Norm of the params: 9.153146
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053930894
Train loss (w/o reg) on all data: 0.044236083
Test loss (w/o reg) on all data: 0.039181065
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.1843538e-06
Norm of the params: 13.924662
              Random: fixed  10 labels. Loss 0.03918. Accuracy 0.989.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07009508
Train loss (w/o reg) on all data: 0.061105743
Test loss (w/o reg) on all data: 0.035113487
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.068492e-06
Norm of the params: 13.408459
Flipped loss: 0.03511. Accuracy: 0.992
### Flips: 52, rs: 23, checks: 52
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729276
Test loss (w/o reg) on all data: 0.012055291
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.680197e-07
Norm of the params: 9.153241
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729297
Test loss (w/o reg) on all data: 0.012055244
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0956142e-07
Norm of the params: 9.153241
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07009508
Train loss (w/o reg) on all data: 0.061106816
Test loss (w/o reg) on all data: 0.035107378
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.983925e-06
Norm of the params: 13.407655
              Random: fixed   0 labels. Loss 0.03511. Accuracy 0.992.
### Flips: 52, rs: 23, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172997
Test loss (w/o reg) on all data: 0.012055143
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1439423e-07
Norm of the params: 9.153167
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729944
Test loss (w/o reg) on all data: 0.012055108
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7461181e-07
Norm of the params: 9.153168
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061571043
Train loss (w/o reg) on all data: 0.05226276
Test loss (w/o reg) on all data: 0.03667662
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3094649e-05
Norm of the params: 13.644253
              Random: fixed   2 labels. Loss 0.03668. Accuracy 0.989.
### Flips: 52, rs: 23, checks: 156
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728992
Test loss (w/o reg) on all data: 0.012056028
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1787454e-06
Norm of the params: 9.153272
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729004
Test loss (w/o reg) on all data: 0.012055792
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.930673e-07
Norm of the params: 9.153271
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046057254
Train loss (w/o reg) on all data: 0.037043944
Test loss (w/o reg) on all data: 0.03706471
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.4089787e-06
Norm of the params: 13.426324
              Random: fixed   6 labels. Loss 0.03706. Accuracy 0.985.
### Flips: 52, rs: 23, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729344
Test loss (w/o reg) on all data: 0.012055384
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9943206e-07
Norm of the params: 9.153234
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729353
Test loss (w/o reg) on all data: 0.012055318
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2789951e-07
Norm of the params: 9.153233
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044010624
Train loss (w/o reg) on all data: 0.03538223
Test loss (w/o reg) on all data: 0.0374077
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.978903e-06
Norm of the params: 13.13651
              Random: fixed   7 labels. Loss 0.03741. Accuracy 0.989.
### Flips: 52, rs: 23, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729474
Test loss (w/o reg) on all data: 0.012055235
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2863719e-07
Norm of the params: 9.15322
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729572
Test loss (w/o reg) on all data: 0.012055801
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.6113265e-07
Norm of the params: 9.15321
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040821366
Train loss (w/o reg) on all data: 0.0327163
Test loss (w/o reg) on all data: 0.034684174
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1338476e-06
Norm of the params: 12.731901
              Random: fixed   8 labels. Loss 0.03468. Accuracy 0.989.
### Flips: 52, rs: 23, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729087
Test loss (w/o reg) on all data: 0.012055032
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0115707e-07
Norm of the params: 9.153263
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729092
Test loss (w/o reg) on all data: 0.0120551605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1777639e-07
Norm of the params: 9.153262
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040821366
Train loss (w/o reg) on all data: 0.032716766
Test loss (w/o reg) on all data: 0.034683555
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.447696e-06
Norm of the params: 12.731534
              Random: fixed   8 labels. Loss 0.03468. Accuracy 0.989.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06857747
Train loss (w/o reg) on all data: 0.05802335
Test loss (w/o reg) on all data: 0.08231595
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.1314394e-06
Norm of the params: 14.528677
Flipped loss: 0.08232. Accuracy: 0.969
### Flips: 52, rs: 24, checks: 52
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0072474866
Train loss (w/o reg) on all data: 0.0026130257
Test loss (w/o reg) on all data: 0.011079496
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9910652e-07
Norm of the params: 9.627524
     Influence (LOO): fixed  23 labels. Loss 0.01108. Accuracy 0.996.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729325
Test loss (w/o reg) on all data: 0.012054797
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.26897e-08
Norm of the params: 9.153235
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06491591
Train loss (w/o reg) on all data: 0.05396482
Test loss (w/o reg) on all data: 0.07995961
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2297418e-05
Norm of the params: 14.799383
              Random: fixed   1 labels. Loss 0.07996. Accuracy 0.966.
### Flips: 52, rs: 24, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729397
Test loss (w/o reg) on all data: 0.012055146
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.346749e-08
Norm of the params: 9.153229
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172947
Test loss (w/o reg) on all data: 0.012054972
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0340846e-07
Norm of the params: 9.153221
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06519359
Train loss (w/o reg) on all data: 0.054825045
Test loss (w/o reg) on all data: 0.07735791
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.0169585e-06
Norm of the params: 14.400383
              Random: fixed   2 labels. Loss 0.07736. Accuracy 0.969.
### Flips: 52, rs: 24, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729465
Test loss (w/o reg) on all data: 0.012055847
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.164976e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172945
Test loss (w/o reg) on all data: 0.012055728
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2464488e-07
Norm of the params: 9.153221
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06519359
Train loss (w/o reg) on all data: 0.054825015
Test loss (w/o reg) on all data: 0.07736473
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1305467e-05
Norm of the params: 14.400403
              Random: fixed   2 labels. Loss 0.07736. Accuracy 0.969.
### Flips: 52, rs: 24, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.012054992
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.160415e-08
Norm of the params: 9.153182
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012055065
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3028797e-07
Norm of the params: 9.153183
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06519359
Train loss (w/o reg) on all data: 0.054825384
Test loss (w/o reg) on all data: 0.07736314
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.4650958e-06
Norm of the params: 14.400146
              Random: fixed   2 labels. Loss 0.07736. Accuracy 0.969.
### Flips: 52, rs: 24, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729814
Test loss (w/o reg) on all data: 0.012054785
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2738378e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729777
Test loss (w/o reg) on all data: 0.012054883
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.6249706e-07
Norm of the params: 9.153187
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062051862
Train loss (w/o reg) on all data: 0.052092098
Test loss (w/o reg) on all data: 0.07006748
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.7413128e-06
Norm of the params: 14.113658
              Random: fixed   3 labels. Loss 0.07007. Accuracy 0.981.
### Flips: 52, rs: 24, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729767
Test loss (w/o reg) on all data: 0.012055812
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.958292e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729772
Test loss (w/o reg) on all data: 0.012055713
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6655709e-07
Norm of the params: 9.153188
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06205187
Train loss (w/o reg) on all data: 0.05209098
Test loss (w/o reg) on all data: 0.07006062
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.71821e-05
Norm of the params: 14.114452
              Random: fixed   3 labels. Loss 0.07006. Accuracy 0.981.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09873342
Train loss (w/o reg) on all data: 0.09164863
Test loss (w/o reg) on all data: 0.04843174
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2494913e-05
Norm of the params: 11.903604
Flipped loss: 0.04843. Accuracy: 0.989
### Flips: 52, rs: 25, checks: 52
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173063
Test loss (w/o reg) on all data: 0.01205536
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.803383e-07
Norm of the params: 9.153091
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730617
Test loss (w/o reg) on all data: 0.012055267
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9288217e-07
Norm of the params: 9.153092
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09674991
Train loss (w/o reg) on all data: 0.08963787
Test loss (w/o reg) on all data: 0.046935562
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8435869e-05
Norm of the params: 11.926477
              Random: fixed   1 labels. Loss 0.04694. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 104
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172998
Test loss (w/o reg) on all data: 0.012055452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0566243e-06
Norm of the params: 9.153165
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729977
Test loss (w/o reg) on all data: 0.012055652
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.05753e-07
Norm of the params: 9.153165
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0967499
Train loss (w/o reg) on all data: 0.08963807
Test loss (w/o reg) on all data: 0.046944432
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.247811e-05
Norm of the params: 11.926304
              Random: fixed   1 labels. Loss 0.04694. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 156
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729902
Test loss (w/o reg) on all data: 0.012054863
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.323373e-07
Norm of the params: 9.153173
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729916
Test loss (w/o reg) on all data: 0.012054846
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7224e-08
Norm of the params: 9.153173
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09674992
Train loss (w/o reg) on all data: 0.08963914
Test loss (w/o reg) on all data: 0.046939038
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.524404e-06
Norm of the params: 11.925411
              Random: fixed   1 labels. Loss 0.04694. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.012054764
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.16770615e-07
Norm of the params: 9.153191
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172977
Test loss (w/o reg) on all data: 0.012054846
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7261063e-07
Norm of the params: 9.153188
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.085310675
Train loss (w/o reg) on all data: 0.07749951
Test loss (w/o reg) on all data: 0.048649523
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8401564e-05
Norm of the params: 12.498935
              Random: fixed   4 labels. Loss 0.04865. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 260
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730047
Test loss (w/o reg) on all data: 0.012055019
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8989759e-07
Norm of the params: 9.153157
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730056
Test loss (w/o reg) on all data: 0.012054975
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1945168e-07
Norm of the params: 9.153157
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06843868
Train loss (w/o reg) on all data: 0.060254104
Test loss (w/o reg) on all data: 0.04354104
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.681751e-06
Norm of the params: 12.794195
              Random: fixed   9 labels. Loss 0.04354. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729814
Test loss (w/o reg) on all data: 0.012055136
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2436943e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729758
Test loss (w/o reg) on all data: 0.012054797
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.550953e-07
Norm of the params: 9.153188
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06356003
Train loss (w/o reg) on all data: 0.055510048
Test loss (w/o reg) on all data: 0.041828416
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.274612e-06
Norm of the params: 12.688564
              Random: fixed  11 labels. Loss 0.04183. Accuracy 0.989.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079595946
Train loss (w/o reg) on all data: 0.06879038
Test loss (w/o reg) on all data: 0.060622003
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.81411e-06
Norm of the params: 14.700723
Flipped loss: 0.06062. Accuracy: 0.981
### Flips: 52, rs: 26, checks: 52
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728664
Test loss (w/o reg) on all data: 0.0120557295
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4349973e-07
Norm of the params: 9.153306
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728696
Test loss (w/o reg) on all data: 0.01205577
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0191353e-07
Norm of the params: 9.153303
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07959595
Train loss (w/o reg) on all data: 0.06878872
Test loss (w/o reg) on all data: 0.060621545
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6952847e-05
Norm of the params: 14.701856
              Random: fixed   0 labels. Loss 0.06062. Accuracy 0.981.
### Flips: 52, rs: 26, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012056008
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.9255494e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.012056151
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7700274e-07
Norm of the params: 9.153194
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07226576
Train loss (w/o reg) on all data: 0.061529465
Test loss (w/o reg) on all data: 0.056632385
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1849034e-06
Norm of the params: 14.653526
              Random: fixed   3 labels. Loss 0.05663. Accuracy 0.977.
### Flips: 52, rs: 26, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172938
Test loss (w/o reg) on all data: 0.01205452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9611107e-07
Norm of the params: 9.153231
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729392
Test loss (w/o reg) on all data: 0.012054655
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.883235e-07
Norm of the params: 9.15323
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07226576
Train loss (w/o reg) on all data: 0.061526928
Test loss (w/o reg) on all data: 0.056639235
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.904635e-06
Norm of the params: 14.655259
              Random: fixed   3 labels. Loss 0.05664. Accuracy 0.977.
### Flips: 52, rs: 26, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021730196
Test loss (w/o reg) on all data: 0.012055251
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4721594e-07
Norm of the params: 9.153143
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173015
Test loss (w/o reg) on all data: 0.0120552145
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5301167e-07
Norm of the params: 9.153144
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07226575
Train loss (w/o reg) on all data: 0.061527126
Test loss (w/o reg) on all data: 0.056642685
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.7398408e-06
Norm of the params: 14.655122
              Random: fixed   3 labels. Loss 0.05664. Accuracy 0.977.
### Flips: 52, rs: 26, checks: 260
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730023
Test loss (w/o reg) on all data: 0.012055257
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1435342e-07
Norm of the params: 9.153161
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730016
Test loss (w/o reg) on all data: 0.012055298
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7737369e-07
Norm of the params: 9.153161
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07022373
Train loss (w/o reg) on all data: 0.0596195
Test loss (w/o reg) on all data: 0.056210265
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.565991e-06
Norm of the params: 14.563122
              Random: fixed   4 labels. Loss 0.05621. Accuracy 0.981.
### Flips: 52, rs: 26, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021725274
Test loss (w/o reg) on all data: 0.012054282
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8997614e-06
Norm of the params: 9.153678
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [5] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021731162
Test loss (w/o reg) on all data: 0.0120554175
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7921234e-07
Norm of the params: 9.153035
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05895491
Train loss (w/o reg) on all data: 0.048273876
Test loss (w/o reg) on all data: 0.04702848
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2515929e-05
Norm of the params: 14.6157675
              Random: fixed   8 labels. Loss 0.04703. Accuracy 0.977.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07041482
Train loss (w/o reg) on all data: 0.062218267
Test loss (w/o reg) on all data: 0.032475855
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1152605e-06
Norm of the params: 12.803556
Flipped loss: 0.03248. Accuracy: 0.992
### Flips: 52, rs: 27, checks: 52
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730212
Test loss (w/o reg) on all data: 0.0120552685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.458369e-07
Norm of the params: 9.153138
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.00217302
Test loss (w/o reg) on all data: 0.012055201
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7040645e-07
Norm of the params: 9.153139
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06906163
Train loss (w/o reg) on all data: 0.061176203
Test loss (w/o reg) on all data: 0.030375434
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.561282e-06
Norm of the params: 12.558205
              Random: fixed   1 labels. Loss 0.03038. Accuracy 0.992.
### Flips: 52, rs: 27, checks: 104
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729798
Test loss (w/o reg) on all data: 0.012055113
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.21468e-07
Norm of the params: 9.153182
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729758
Test loss (w/o reg) on all data: 0.01205537
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.764551e-07
Norm of the params: 9.153188
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067413665
Train loss (w/o reg) on all data: 0.059535224
Test loss (w/o reg) on all data: 0.02898337
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2622116e-05
Norm of the params: 12.552639
              Random: fixed   2 labels. Loss 0.02898. Accuracy 0.992.
### Flips: 52, rs: 27, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728154
Test loss (w/o reg) on all data: 0.012056155
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9912428e-07
Norm of the params: 9.153363
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728168
Test loss (w/o reg) on all data: 0.012056222
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3316072e-07
Norm of the params: 9.153361
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0636372
Train loss (w/o reg) on all data: 0.055674553
Test loss (w/o reg) on all data: 0.02636275
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.199563e-06
Norm of the params: 12.619544
              Random: fixed   3 labels. Loss 0.02636. Accuracy 0.996.
### Flips: 52, rs: 27, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172917
Test loss (w/o reg) on all data: 0.012055399
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9108556e-07
Norm of the params: 9.153252
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729225
Test loss (w/o reg) on all data: 0.012055268
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.6880276e-07
Norm of the params: 9.153247
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06363719
Train loss (w/o reg) on all data: 0.055673867
Test loss (w/o reg) on all data: 0.026366577
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.398045e-06
Norm of the params: 12.620084
              Random: fixed   3 labels. Loss 0.02637. Accuracy 0.996.
### Flips: 52, rs: 27, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172945
Test loss (w/o reg) on all data: 0.012055955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.825205e-07
Norm of the params: 9.153223
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729441
Test loss (w/o reg) on all data: 0.012055876
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2619194e-07
Norm of the params: 9.153223
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055447996
Train loss (w/o reg) on all data: 0.04800143
Test loss (w/o reg) on all data: 0.02785409
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4560107e-06
Norm of the params: 12.203741
              Random: fixed   6 labels. Loss 0.02785. Accuracy 0.992.
### Flips: 52, rs: 27, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729628
Test loss (w/o reg) on all data: 0.012055122
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0565489e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.012055185
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0546734e-07
Norm of the params: 9.153203
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045717753
Train loss (w/o reg) on all data: 0.038146704
Test loss (w/o reg) on all data: 0.020963699
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.241696e-06
Norm of the params: 12.305324
              Random: fixed   9 labels. Loss 0.02096. Accuracy 0.996.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083284706
Train loss (w/o reg) on all data: 0.0749711
Test loss (w/o reg) on all data: 0.042838387
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.000029e-05
Norm of the params: 12.894653
Flipped loss: 0.04284. Accuracy: 0.992
### Flips: 52, rs: 28, checks: 52
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172905
Test loss (w/o reg) on all data: 0.012056523
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3882404e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729094
Test loss (w/o reg) on all data: 0.012056538
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.123607e-07
Norm of the params: 9.153261
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080336094
Train loss (w/o reg) on all data: 0.07189656
Test loss (w/o reg) on all data: 0.04179759
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8776547e-05
Norm of the params: 12.991943
              Random: fixed   1 labels. Loss 0.04180. Accuracy 0.992.
### Flips: 52, rs: 28, checks: 104
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.012055164
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8258206e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729525
Test loss (w/o reg) on all data: 0.012055143
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8956836e-07
Norm of the params: 9.153214
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06758141
Train loss (w/o reg) on all data: 0.059636056
Test loss (w/o reg) on all data: 0.042880878
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9311416e-06
Norm of the params: 12.605834
              Random: fixed   5 labels. Loss 0.04288. Accuracy 0.992.
### Flips: 52, rs: 28, checks: 156
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729048
Test loss (w/o reg) on all data: 0.012055233
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5993798e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172918
Test loss (w/o reg) on all data: 0.012055301
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.284091e-07
Norm of the params: 9.153253
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0675814
Train loss (w/o reg) on all data: 0.059636902
Test loss (w/o reg) on all data: 0.04287393
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5585632e-06
Norm of the params: 12.605155
              Random: fixed   5 labels. Loss 0.04287. Accuracy 0.992.
### Flips: 52, rs: 28, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729234
Test loss (w/o reg) on all data: 0.012055581
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0612573e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729236
Test loss (w/o reg) on all data: 0.012055681
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4779845e-07
Norm of the params: 9.153244
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06126626
Train loss (w/o reg) on all data: 0.053665627
Test loss (w/o reg) on all data: 0.033245973
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0640065e-06
Norm of the params: 12.329341
              Random: fixed   7 labels. Loss 0.03325. Accuracy 0.996.
### Flips: 52, rs: 28, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729616
Test loss (w/o reg) on all data: 0.012055211
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6577444e-07
Norm of the params: 9.153205
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729616
Test loss (w/o reg) on all data: 0.012055272
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.069903e-07
Norm of the params: 9.153205
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05818651
Train loss (w/o reg) on all data: 0.050489154
Test loss (w/o reg) on all data: 0.034058396
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.339004e-06
Norm of the params: 12.40754
              Random: fixed   8 labels. Loss 0.03406. Accuracy 0.996.
### Flips: 52, rs: 28, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729749
Test loss (w/o reg) on all data: 0.012055749
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.082271e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729765
Test loss (w/o reg) on all data: 0.012055854
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.536094e-07
Norm of the params: 9.153188
                Loss: fixed  24 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058186512
Train loss (w/o reg) on all data: 0.05048951
Test loss (w/o reg) on all data: 0.03405909
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.090829e-06
Norm of the params: 12.407255
              Random: fixed   8 labels. Loss 0.03406. Accuracy 0.996.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07191625
Train loss (w/o reg) on all data: 0.063397035
Test loss (w/o reg) on all data: 0.036150925
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.953462e-06
Norm of the params: 13.053135
Flipped loss: 0.03615. Accuracy: 0.996
### Flips: 52, rs: 29, checks: 52
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730084
Test loss (w/o reg) on all data: 0.012054891
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.776358e-07
Norm of the params: 9.153152
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730075
Test loss (w/o reg) on all data: 0.012054964
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.069182e-07
Norm of the params: 9.153153
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06978868
Train loss (w/o reg) on all data: 0.061002366
Test loss (w/o reg) on all data: 0.036282558
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.916609e-06
Norm of the params: 13.256177
              Random: fixed   1 labels. Loss 0.03628. Accuracy 0.996.
### Flips: 52, rs: 29, checks: 104
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172943
Test loss (w/o reg) on all data: 0.0120551335
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8437398e-07
Norm of the params: 9.153223
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729453
Test loss (w/o reg) on all data: 0.012055209
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2172188e-07
Norm of the params: 9.153222
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06978867
Train loss (w/o reg) on all data: 0.06100222
Test loss (w/o reg) on all data: 0.036287557
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.7722937e-06
Norm of the params: 13.256282
              Random: fixed   1 labels. Loss 0.03629. Accuracy 0.996.
### Flips: 52, rs: 29, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728408
Test loss (w/o reg) on all data: 0.012055818
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3841325e-07
Norm of the params: 9.153337
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172841
Test loss (w/o reg) on all data: 0.012055921
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.736854e-07
Norm of the params: 9.153336
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06669659
Train loss (w/o reg) on all data: 0.058128722
Test loss (w/o reg) on all data: 0.03389852
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9838365e-06
Norm of the params: 13.090356
              Random: fixed   2 labels. Loss 0.03390. Accuracy 0.996.
### Flips: 52, rs: 29, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729434
Test loss (w/o reg) on all data: 0.012055845
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2450182e-07
Norm of the params: 9.153224
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172942
Test loss (w/o reg) on all data: 0.012055952
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.669952e-07
Norm of the params: 9.153223
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0610857
Train loss (w/o reg) on all data: 0.052693166
Test loss (w/o reg) on all data: 0.03201448
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3284713e-06
Norm of the params: 12.955721
              Random: fixed   4 labels. Loss 0.03201. Accuracy 0.992.
### Flips: 52, rs: 29, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730284
Test loss (w/o reg) on all data: 0.012055408
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.56961e-07
Norm of the params: 9.153129
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730266
Test loss (w/o reg) on all data: 0.012055454
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.118987e-07
Norm of the params: 9.1531315
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061085697
Train loss (w/o reg) on all data: 0.052691463
Test loss (w/o reg) on all data: 0.032015245
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8663193e-06
Norm of the params: 12.957033
              Random: fixed   4 labels. Loss 0.03202. Accuracy 0.992.
### Flips: 52, rs: 29, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729795
Test loss (w/o reg) on all data: 0.012055566
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5442122e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.012055491
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.548316e-07
Norm of the params: 9.153191
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05434378
Train loss (w/o reg) on all data: 0.045590922
Test loss (w/o reg) on all data: 0.030979455
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9628182e-06
Norm of the params: 13.230915
              Random: fixed   6 labels. Loss 0.03098. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08841732
Train loss (w/o reg) on all data: 0.08005105
Test loss (w/o reg) on all data: 0.06270415
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.93054e-06
Norm of the params: 12.9354315
Flipped loss: 0.06270. Accuracy: 0.985
### Flips: 52, rs: 30, checks: 52
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489635
Train loss (w/o reg) on all data: 0.0055393334
Test loss (w/o reg) on all data: 0.013845686
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2848145e-06
Norm of the params: 9.950178
     Influence (LOO): fixed  26 labels. Loss 0.01385. Accuracy 0.989.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.012054586
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5633435e-07
Norm of the params: 9.15319
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088417314
Train loss (w/o reg) on all data: 0.08004785
Test loss (w/o reg) on all data: 0.06269426
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.7552303e-06
Norm of the params: 12.937901
              Random: fixed   0 labels. Loss 0.06269. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728987
Test loss (w/o reg) on all data: 0.012055214
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.528835e-07
Norm of the params: 9.153274
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729004
Test loss (w/o reg) on all data: 0.012055327
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.111907e-07
Norm of the params: 9.153271
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08841732
Train loss (w/o reg) on all data: 0.08004953
Test loss (w/o reg) on all data: 0.06269692
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4525189e-05
Norm of the params: 12.936609
              Random: fixed   0 labels. Loss 0.06270. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729507
Test loss (w/o reg) on all data: 0.012056131
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2437958e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729514
Test loss (w/o reg) on all data: 0.012056179
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2182555e-07
Norm of the params: 9.153214
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08704899
Train loss (w/o reg) on all data: 0.07891681
Test loss (w/o reg) on all data: 0.059600513
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.991925e-06
Norm of the params: 12.753179
              Random: fixed   1 labels. Loss 0.05960. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.002172877
Test loss (w/o reg) on all data: 0.012056802
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8110819e-07
Norm of the params: 9.153294
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [2] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729358
Test loss (w/o reg) on all data: 0.01205656
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.640751e-07
Norm of the params: 9.153232
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08704899
Train loss (w/o reg) on all data: 0.07891342
Test loss (w/o reg) on all data: 0.059594464
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.2029745e-06
Norm of the params: 12.75584
              Random: fixed   1 labels. Loss 0.05959. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729195
Test loss (w/o reg) on all data: 0.012056172
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.888574e-07
Norm of the params: 9.15325
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172919
Test loss (w/o reg) on all data: 0.012056063
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1424057e-07
Norm of the params: 9.15325
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081300125
Train loss (w/o reg) on all data: 0.07291305
Test loss (w/o reg) on all data: 0.057543226
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.5077703e-06
Norm of the params: 12.951507
              Random: fixed   3 labels. Loss 0.05754. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728156
Test loss (w/o reg) on all data: 0.012055647
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6089756e-07
Norm of the params: 9.153363
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728303
Test loss (w/o reg) on all data: 0.012055799
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.0731076e-07
Norm of the params: 9.153348
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07367305
Train loss (w/o reg) on all data: 0.06521572
Test loss (w/o reg) on all data: 0.05424718
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5272672e-05
Norm of the params: 13.005632
              Random: fixed   5 labels. Loss 0.05425. Accuracy 0.985.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066691004
Train loss (w/o reg) on all data: 0.05680201
Test loss (w/o reg) on all data: 0.03975467
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.7140338e-06
Norm of the params: 14.063424
Flipped loss: 0.03975. Accuracy: 0.989
### Flips: 52, rs: 31, checks: 52
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172949
Test loss (w/o reg) on all data: 0.012054555
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9351148e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.00217295
Test loss (w/o reg) on all data: 0.012054607
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5559881e-07
Norm of the params: 9.153217
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06669101
Train loss (w/o reg) on all data: 0.05680238
Test loss (w/o reg) on all data: 0.03975982
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4773194e-05
Norm of the params: 14.063167
              Random: fixed   0 labels. Loss 0.03976. Accuracy 0.989.
### Flips: 52, rs: 31, checks: 104
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729905
Test loss (w/o reg) on all data: 0.012054372
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9506946e-07
Norm of the params: 9.153173
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729898
Test loss (w/o reg) on all data: 0.01205447
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.907971e-07
Norm of the params: 9.153173
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065928645
Train loss (w/o reg) on all data: 0.05632951
Test loss (w/o reg) on all data: 0.035783455
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8799059e-06
Norm of the params: 13.855781
              Random: fixed   1 labels. Loss 0.03578. Accuracy 0.992.
### Flips: 52, rs: 31, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730005
Test loss (w/o reg) on all data: 0.012055212
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0615325e-07
Norm of the params: 9.153161
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730007
Test loss (w/o reg) on all data: 0.012055156
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.506756e-07
Norm of the params: 9.153161
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062511094
Train loss (w/o reg) on all data: 0.052761585
Test loss (w/o reg) on all data: 0.035942648
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.054569e-05
Norm of the params: 13.963887
              Random: fixed   2 labels. Loss 0.03594. Accuracy 0.989.
### Flips: 52, rs: 31, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729378
Test loss (w/o reg) on all data: 0.012055089
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4041704e-07
Norm of the params: 9.15323
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.002172987
Test loss (w/o reg) on all data: 0.012055624
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.540147e-07
Norm of the params: 9.153177
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060390662
Train loss (w/o reg) on all data: 0.051223032
Test loss (w/o reg) on all data: 0.03735466
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0538206e-05
Norm of the params: 13.540776
              Random: fixed   3 labels. Loss 0.03735. Accuracy 0.992.
### Flips: 52, rs: 31, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728748
Test loss (w/o reg) on all data: 0.012054362
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9861647e-07
Norm of the params: 9.153298
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728864
Test loss (w/o reg) on all data: 0.012054414
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.5365874e-07
Norm of the params: 9.153286
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05752197
Train loss (w/o reg) on all data: 0.04843108
Test loss (w/o reg) on all data: 0.035564423
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.974488e-06
Norm of the params: 13.483982
              Random: fixed   4 labels. Loss 0.03556. Accuracy 0.992.
### Flips: 52, rs: 31, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.012054943
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3492932e-07
Norm of the params: 9.153249
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.0120550515
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.387032e-07
Norm of the params: 9.153247
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04687595
Train loss (w/o reg) on all data: 0.03714049
Test loss (w/o reg) on all data: 0.027979482
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.9792415e-06
Norm of the params: 13.953824
              Random: fixed   7 labels. Loss 0.02798. Accuracy 0.996.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06893472
Train loss (w/o reg) on all data: 0.060964428
Test loss (w/o reg) on all data: 0.036062244
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.6404406e-06
Norm of the params: 12.625598
Flipped loss: 0.03606. Accuracy: 0.996
### Flips: 52, rs: 32, checks: 52
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.00217287
Test loss (w/o reg) on all data: 0.012055166
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.966829e-07
Norm of the params: 9.153304
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217287
Test loss (w/o reg) on all data: 0.012055059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9579857e-07
Norm of the params: 9.153302
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06587609
Train loss (w/o reg) on all data: 0.05785472
Test loss (w/o reg) on all data: 0.03547201
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0864697e-06
Norm of the params: 12.665992
              Random: fixed   1 labels. Loss 0.03547. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 104
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730487
Test loss (w/o reg) on all data: 0.012055317
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.793016e-07
Norm of the params: 9.153109
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730475
Test loss (w/o reg) on all data: 0.012055481
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.706383e-07
Norm of the params: 9.15311
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06250859
Train loss (w/o reg) on all data: 0.054167144
Test loss (w/o reg) on all data: 0.037067227
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.7040983e-06
Norm of the params: 12.916226
              Random: fixed   2 labels. Loss 0.03707. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 156
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729628
Test loss (w/o reg) on all data: 0.012054688
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0418685e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729635
Test loss (w/o reg) on all data: 0.012054684
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3880937e-07
Norm of the params: 9.153201
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06250858
Train loss (w/o reg) on all data: 0.05416696
Test loss (w/o reg) on all data: 0.037066285
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.611092e-06
Norm of the params: 12.916367
              Random: fixed   2 labels. Loss 0.03707. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 208
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729763
Test loss (w/o reg) on all data: 0.012054607
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.739669e-08
Norm of the params: 9.153188
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729746
Test loss (w/o reg) on all data: 0.012054799
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5547814e-07
Norm of the params: 9.153189
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06250859
Train loss (w/o reg) on all data: 0.05416756
Test loss (w/o reg) on all data: 0.03706777
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.797866e-06
Norm of the params: 12.915905
              Random: fixed   2 labels. Loss 0.03707. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.012054872
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.482588e-07
Norm of the params: 9.153206
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.01205495
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.13773154e-07
Norm of the params: 9.153206
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059900336
Train loss (w/o reg) on all data: 0.05173957
Test loss (w/o reg) on all data: 0.040283013
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.403456e-06
Norm of the params: 12.775576
              Random: fixed   3 labels. Loss 0.04028. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730098
Test loss (w/o reg) on all data: 0.012054607
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0387298e-07
Norm of the params: 9.153151
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730093
Test loss (w/o reg) on all data: 0.01205469
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3425105e-07
Norm of the params: 9.153151
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054237723
Train loss (w/o reg) on all data: 0.04598692
Test loss (w/o reg) on all data: 0.037330512
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.012195e-06
Norm of the params: 12.845857
              Random: fixed   5 labels. Loss 0.03733. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07785225
Train loss (w/o reg) on all data: 0.06820784
Test loss (w/o reg) on all data: 0.0411897
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2814576e-05
Norm of the params: 13.88842
Flipped loss: 0.04119. Accuracy: 0.996
### Flips: 52, rs: 33, checks: 52
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172944
Test loss (w/o reg) on all data: 0.012055307
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2718054e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021729432
Test loss (w/o reg) on all data: 0.012055396
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.60703e-07
Norm of the params: 9.153221
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07785225
Train loss (w/o reg) on all data: 0.068206705
Test loss (w/o reg) on all data: 0.04118993
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0968984e-05
Norm of the params: 13.8892355
              Random: fixed   0 labels. Loss 0.04119. Accuracy 0.996.
### Flips: 52, rs: 33, checks: 104
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730138
Test loss (w/o reg) on all data: 0.012055164
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7396887e-07
Norm of the params: 9.153146
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620145
Train loss (w/o reg) on all data: 0.00217301
Test loss (w/o reg) on all data: 0.012055083
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4539378e-07
Norm of the params: 9.153147
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07785225
Train loss (w/o reg) on all data: 0.06821165
Test loss (w/o reg) on all data: 0.041182104
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.4494766e-06
Norm of the params: 13.885676
              Random: fixed   0 labels. Loss 0.04118. Accuracy 0.996.
### Flips: 52, rs: 33, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729404
Test loss (w/o reg) on all data: 0.012054382
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3781953e-07
Norm of the params: 9.153229
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729376
Test loss (w/o reg) on all data: 0.012054466
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5210542e-07
Norm of the params: 9.153228
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06989098
Train loss (w/o reg) on all data: 0.059432056
Test loss (w/o reg) on all data: 0.035413772
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.483915e-06
Norm of the params: 14.463009
              Random: fixed   2 labels. Loss 0.03541. Accuracy 0.996.
### Flips: 52, rs: 33, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730345
Test loss (w/o reg) on all data: 0.012054658
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4383287e-07
Norm of the params: 9.153124
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730335
Test loss (w/o reg) on all data: 0.012054571
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8738326e-07
Norm of the params: 9.153125
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066605225
Train loss (w/o reg) on all data: 0.05639187
Test loss (w/o reg) on all data: 0.03040559
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.5264455e-06
Norm of the params: 14.292206
              Random: fixed   4 labels. Loss 0.03041. Accuracy 0.996.
### Flips: 52, rs: 33, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.012055133
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9415445e-07
Norm of the params: 9.153236
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.012055043
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8661034e-07
Norm of the params: 9.153236
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06246381
Train loss (w/o reg) on all data: 0.052273594
Test loss (w/o reg) on all data: 0.02799072
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.1189332e-06
Norm of the params: 14.276004
              Random: fixed   5 labels. Loss 0.02799. Accuracy 0.996.
### Flips: 52, rs: 33, checks: 312
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021732158
Test loss (w/o reg) on all data: 0.012056706
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0351956e-06
Norm of the params: 9.1529255
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021732147
Test loss (w/o reg) on all data: 0.012056889
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7907645e-07
Norm of the params: 9.152929
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052989975
Train loss (w/o reg) on all data: 0.042929757
Test loss (w/o reg) on all data: 0.02248436
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2833428e-06
Norm of the params: 14.18465
              Random: fixed   8 labels. Loss 0.02248. Accuracy 0.996.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08631423
Train loss (w/o reg) on all data: 0.07769474
Test loss (w/o reg) on all data: 0.043784507
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.266951e-06
Norm of the params: 13.12973
Flipped loss: 0.04378. Accuracy: 0.992
### Flips: 52, rs: 34, checks: 52
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729672
Test loss (w/o reg) on all data: 0.012054601
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.070953e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172966
Test loss (w/o reg) on all data: 0.012054686
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.972551e-07
Norm of the params: 9.153198
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08221688
Train loss (w/o reg) on all data: 0.07412729
Test loss (w/o reg) on all data: 0.042077728
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5572732e-05
Norm of the params: 12.719742
              Random: fixed   2 labels. Loss 0.04208. Accuracy 0.992.
### Flips: 52, rs: 34, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730026
Test loss (w/o reg) on all data: 0.0120558245
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3115643e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730026
Test loss (w/o reg) on all data: 0.012055743
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9409207e-07
Norm of the params: 9.153159
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08181437
Train loss (w/o reg) on all data: 0.074032426
Test loss (w/o reg) on all data: 0.04247787
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.80297e-06
Norm of the params: 12.4755335
              Random: fixed   3 labels. Loss 0.04248. Accuracy 0.992.
### Flips: 52, rs: 34, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729828
Test loss (w/o reg) on all data: 0.0120555805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6217282e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729807
Test loss (w/o reg) on all data: 0.012055411
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8405417e-07
Norm of the params: 9.153183
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075194456
Train loss (w/o reg) on all data: 0.06819302
Test loss (w/o reg) on all data: 0.03926401
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.166781e-06
Norm of the params: 11.833375
              Random: fixed   5 labels. Loss 0.03926. Accuracy 0.992.
### Flips: 52, rs: 34, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729977
Test loss (w/o reg) on all data: 0.012056755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1677538e-07
Norm of the params: 9.153163
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729972
Test loss (w/o reg) on all data: 0.012056809
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.978945e-07
Norm of the params: 9.153163
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06905204
Train loss (w/o reg) on all data: 0.06221095
Test loss (w/o reg) on all data: 0.035212558
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6326868e-06
Norm of the params: 11.697084
              Random: fixed   7 labels. Loss 0.03521. Accuracy 0.992.
### Flips: 52, rs: 34, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729753
Test loss (w/o reg) on all data: 0.012055774
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1078337e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172974
Test loss (w/o reg) on all data: 0.012055677
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4213882e-07
Norm of the params: 9.153189
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06905203
Train loss (w/o reg) on all data: 0.062209353
Test loss (w/o reg) on all data: 0.035214502
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4560616e-06
Norm of the params: 11.698446
              Random: fixed   7 labels. Loss 0.03521. Accuracy 0.992.
### Flips: 52, rs: 34, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728405
Test loss (w/o reg) on all data: 0.012055406
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2972139e-06
Norm of the params: 9.153336
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728487
Test loss (w/o reg) on all data: 0.012055725
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.555252e-07
Norm of the params: 9.153328
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062497824
Train loss (w/o reg) on all data: 0.055821117
Test loss (w/o reg) on all data: 0.03487703
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.46671e-06
Norm of the params: 11.5556965
              Random: fixed   9 labels. Loss 0.03488. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05618783
Train loss (w/o reg) on all data: 0.04508322
Test loss (w/o reg) on all data: 0.032465536
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4136184e-06
Norm of the params: 14.902759
Flipped loss: 0.03247. Accuracy: 0.989
### Flips: 52, rs: 35, checks: 52
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729004
Test loss (w/o reg) on all data: 0.012055344
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.804446e-07
Norm of the params: 9.153271
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729015
Test loss (w/o reg) on all data: 0.012055185
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6322485e-07
Norm of the params: 9.153271
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056187827
Train loss (w/o reg) on all data: 0.04508336
Test loss (w/o reg) on all data: 0.03246465
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.112881e-06
Norm of the params: 14.902663
              Random: fixed   0 labels. Loss 0.03246. Accuracy 0.989.
### Flips: 52, rs: 35, checks: 104
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172908
Test loss (w/o reg) on all data: 0.012054921
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5529268e-07
Norm of the params: 9.153263
     Influence (LOO): fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729118
Test loss (w/o reg) on all data: 0.012055033
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.149678e-07
Norm of the params: 9.153257
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053119846
Train loss (w/o reg) on all data: 0.04174089
Test loss (w/o reg) on all data: 0.03408992
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1973267e-06
Norm of the params: 15.085725
              Random: fixed   1 labels. Loss 0.03409. Accuracy 0.989.
### Flips: 52, rs: 35, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730973
Test loss (w/o reg) on all data: 0.012056192
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.930228e-07
Norm of the params: 9.153057
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730927
Test loss (w/o reg) on all data: 0.012056037
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4245132e-07
Norm of the params: 9.153059
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05311985
Train loss (w/o reg) on all data: 0.041737325
Test loss (w/o reg) on all data: 0.034090333
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.979048e-06
Norm of the params: 15.088091
              Random: fixed   1 labels. Loss 0.03409. Accuracy 0.989.
### Flips: 52, rs: 35, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728936
Test loss (w/o reg) on all data: 0.012054914
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1971508e-07
Norm of the params: 9.153278
     Influence (LOO): fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729216
Test loss (w/o reg) on all data: 0.012054766
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4705094e-07
Norm of the params: 9.153248
                Loss: fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048900872
Train loss (w/o reg) on all data: 0.037892915
Test loss (w/o reg) on all data: 0.028591301
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0061362e-06
Norm of the params: 14.837762
              Random: fixed   2 labels. Loss 0.02859. Accuracy 0.992.
### Flips: 52, rs: 35, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729276
Test loss (w/o reg) on all data: 0.012055902
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5782028e-07
Norm of the params: 9.153241
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [2] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729534
Test loss (w/o reg) on all data: 0.012055606
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3225538e-07
Norm of the params: 9.153212
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048316605
Train loss (w/o reg) on all data: 0.037452113
Test loss (w/o reg) on all data: 0.024552252
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8192435e-05
Norm of the params: 14.740755
              Random: fixed   3 labels. Loss 0.02455. Accuracy 0.992.
### Flips: 52, rs: 35, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729346
Test loss (w/o reg) on all data: 0.012055307
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1974745e-07
Norm of the params: 9.153234
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172936
Test loss (w/o reg) on all data: 0.012055366
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8263329e-07
Norm of the params: 9.153234
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048316598
Train loss (w/o reg) on all data: 0.037455957
Test loss (w/o reg) on all data: 0.02456603
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.356439e-06
Norm of the params: 14.738144
              Random: fixed   3 labels. Loss 0.02457. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101101756
Train loss (w/o reg) on all data: 0.09256871
Test loss (w/o reg) on all data: 0.052322708
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.769801e-05
Norm of the params: 13.063727
Flipped loss: 0.05232. Accuracy: 0.996
### Flips: 52, rs: 36, checks: 52
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011764763
Train loss (w/o reg) on all data: 0.0068204235
Test loss (w/o reg) on all data: 0.017269691
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3248299e-06
Norm of the params: 9.944185
     Influence (LOO): fixed  31 labels. Loss 0.01727. Accuracy 0.989.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730755
Test loss (w/o reg) on all data: 0.012055826
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4603514e-07
Norm of the params: 9.153079
                Loss: fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10110176
Train loss (w/o reg) on all data: 0.092566766
Test loss (w/o reg) on all data: 0.052327782
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.2282464e-06
Norm of the params: 13.065218
              Random: fixed   0 labels. Loss 0.05233. Accuracy 0.996.
### Flips: 52, rs: 36, checks: 104
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729327
Test loss (w/o reg) on all data: 0.012055792
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8523914e-07
Norm of the params: 9.153234
     Influence (LOO): fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729467
Test loss (w/o reg) on all data: 0.0120556075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.1628904e-07
Norm of the params: 9.15322
                Loss: fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09098628
Train loss (w/o reg) on all data: 0.08285492
Test loss (w/o reg) on all data: 0.047488596
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.964955e-06
Norm of the params: 12.752538
              Random: fixed   5 labels. Loss 0.04749. Accuracy 0.992.
### Flips: 52, rs: 36, checks: 156
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173004
Test loss (w/o reg) on all data: 0.012055386
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6152578e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730037
Test loss (w/o reg) on all data: 0.012055402
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3409837e-07
Norm of the params: 9.153158
                Loss: fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08796953
Train loss (w/o reg) on all data: 0.07980629
Test loss (w/o reg) on all data: 0.04798996
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.473122e-06
Norm of the params: 12.77751
              Random: fixed   6 labels. Loss 0.04799. Accuracy 0.992.
### Flips: 52, rs: 36, checks: 208
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729765
Test loss (w/o reg) on all data: 0.0120560955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.961457e-07
Norm of the params: 9.153187
     Influence (LOO): fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729756
Test loss (w/o reg) on all data: 0.012056233
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.718199e-07
Norm of the params: 9.153188
                Loss: fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08101119
Train loss (w/o reg) on all data: 0.072589524
Test loss (w/o reg) on all data: 0.048757568
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9640616e-06
Norm of the params: 12.978185
              Random: fixed   9 labels. Loss 0.04876. Accuracy 0.992.
### Flips: 52, rs: 36, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729565
Test loss (w/o reg) on all data: 0.012055349
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6774518e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729574
Test loss (w/o reg) on all data: 0.012055297
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0262571e-07
Norm of the params: 9.153208
                Loss: fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07296603
Train loss (w/o reg) on all data: 0.06417317
Test loss (w/o reg) on all data: 0.050450552
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.134157e-05
Norm of the params: 13.261119
              Random: fixed  13 labels. Loss 0.05045. Accuracy 0.992.
### Flips: 52, rs: 36, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729756
Test loss (w/o reg) on all data: 0.012055925
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0904368e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729753
Test loss (w/o reg) on all data: 0.012055992
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3440082e-07
Norm of the params: 9.153188
                Loss: fixed  35 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07193068
Train loss (w/o reg) on all data: 0.0632651
Test loss (w/o reg) on all data: 0.049265616
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.773021e-05
Norm of the params: 13.1647835
              Random: fixed  14 labels. Loss 0.04927. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089151934
Train loss (w/o reg) on all data: 0.0808874
Test loss (w/o reg) on all data: 0.06562153
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.8883232e-06
Norm of the params: 12.856545
Flipped loss: 0.06562. Accuracy: 0.981
### Flips: 52, rs: 37, checks: 52
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729493
Test loss (w/o reg) on all data: 0.012056891
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0078697e-06
Norm of the params: 9.153216
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217295
Test loss (w/o reg) on all data: 0.0120566655
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.241252e-07
Norm of the params: 9.153216
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082505584
Train loss (w/o reg) on all data: 0.073971435
Test loss (w/o reg) on all data: 0.06444775
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.4230866e-06
Norm of the params: 13.06457
              Random: fixed   3 labels. Loss 0.06445. Accuracy 0.981.
### Flips: 52, rs: 37, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729378
Test loss (w/o reg) on all data: 0.012055022
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5011043e-07
Norm of the params: 9.153228
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.012055109
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7377546e-07
Norm of the params: 9.153228
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08250556
Train loss (w/o reg) on all data: 0.07396779
Test loss (w/o reg) on all data: 0.06446416
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.86559e-06
Norm of the params: 13.067341
              Random: fixed   3 labels. Loss 0.06446. Accuracy 0.981.
### Flips: 52, rs: 37, checks: 156
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172978
Test loss (w/o reg) on all data: 0.012055662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1811913e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729784
Test loss (w/o reg) on all data: 0.012055714
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.516463e-07
Norm of the params: 9.153186
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07673094
Train loss (w/o reg) on all data: 0.06830937
Test loss (w/o reg) on all data: 0.06396308
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.7511625e-06
Norm of the params: 12.978112
              Random: fixed   5 labels. Loss 0.06396. Accuracy 0.977.
### Flips: 52, rs: 37, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172937
Test loss (w/o reg) on all data: 0.012054642
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1600407e-07
Norm of the params: 9.153233
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172936
Test loss (w/o reg) on all data: 0.012054702
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9515355e-07
Norm of the params: 9.153232
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076730944
Train loss (w/o reg) on all data: 0.068309754
Test loss (w/o reg) on all data: 0.06396088
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5853358e-05
Norm of the params: 12.977818
              Random: fixed   5 labels. Loss 0.06396. Accuracy 0.977.
### Flips: 52, rs: 37, checks: 260
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729749
Test loss (w/o reg) on all data: 0.012055099
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.141048e-08
Norm of the params: 9.15319
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172973
Test loss (w/o reg) on all data: 0.012055078
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9968509e-07
Norm of the params: 9.153192
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07326784
Train loss (w/o reg) on all data: 0.06466033
Test loss (w/o reg) on all data: 0.06478953
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3243501e-05
Norm of the params: 13.120599
              Random: fixed   6 labels. Loss 0.06479. Accuracy 0.977.
### Flips: 52, rs: 37, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730443
Test loss (w/o reg) on all data: 0.012054815
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0148771e-07
Norm of the params: 9.153113
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.002173031
Test loss (w/o reg) on all data: 0.012054765
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.7976395e-07
Norm of the params: 9.153131
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06317681
Train loss (w/o reg) on all data: 0.05504164
Test loss (w/o reg) on all data: 0.051391397
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3389445e-05
Norm of the params: 12.755525
              Random: fixed  10 labels. Loss 0.05139. Accuracy 0.989.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061764088
Train loss (w/o reg) on all data: 0.054141738
Test loss (w/o reg) on all data: 0.033383314
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3238439e-06
Norm of the params: 12.346943
Flipped loss: 0.03338. Accuracy: 0.992
### Flips: 52, rs: 38, checks: 52
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.012055366
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2219804e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172969
Test loss (w/o reg) on all data: 0.01205526
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7354958e-07
Norm of the params: 9.153197
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057502046
Train loss (w/o reg) on all data: 0.049439717
Test loss (w/o reg) on all data: 0.034866683
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.075497e-06
Norm of the params: 12.698292
              Random: fixed   1 labels. Loss 0.03487. Accuracy 0.992.
### Flips: 52, rs: 38, checks: 104
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729795
Test loss (w/o reg) on all data: 0.012054735
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.874091e-07
Norm of the params: 9.153186
     Influence (LOO): fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012054711
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.11337066e-07
Norm of the params: 9.153186
                Loss: fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057502046
Train loss (w/o reg) on all data: 0.049437955
Test loss (w/o reg) on all data: 0.034872863
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5767925e-06
Norm of the params: 12.699677
              Random: fixed   1 labels. Loss 0.03487. Accuracy 0.992.
### Flips: 52, rs: 38, checks: 156
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172922
Test loss (w/o reg) on all data: 0.0120544
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3959398e-07
Norm of the params: 9.153245
     Influence (LOO): fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729264
Test loss (w/o reg) on all data: 0.012054509
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7050363e-07
Norm of the params: 9.153244
                Loss: fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052921623
Train loss (w/o reg) on all data: 0.04492868
Test loss (w/o reg) on all data: 0.030297466
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1894823e-06
Norm of the params: 12.64353
              Random: fixed   3 labels. Loss 0.03030. Accuracy 0.992.
### Flips: 52, rs: 38, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172942
Test loss (w/o reg) on all data: 0.012054919
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.479155e-08
Norm of the params: 9.153226
     Influence (LOO): fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729534
Test loss (w/o reg) on all data: 0.012055142
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.92721e-08
Norm of the params: 9.153214
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04424197
Train loss (w/o reg) on all data: 0.03682568
Test loss (w/o reg) on all data: 0.02592738
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.0983363e-06
Norm of the params: 12.178907
              Random: fixed   6 labels. Loss 0.02593. Accuracy 0.996.
### Flips: 52, rs: 38, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730363
Test loss (w/o reg) on all data: 0.012055762
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3516568e-07
Norm of the params: 9.153123
     Influence (LOO): fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730217
Test loss (w/o reg) on all data: 0.012055684
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.063074e-07
Norm of the params: 9.153138
                Loss: fixed  19 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041106667
Train loss (w/o reg) on all data: 0.033622064
Test loss (w/o reg) on all data: 0.023470748
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.205878e-06
Norm of the params: 12.23487
              Random: fixed   7 labels. Loss 0.02347. Accuracy 0.996.
### Flips: 52, rs: 38, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729218
Test loss (w/o reg) on all data: 0.012054731
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2862879e-07
Norm of the params: 9.153249
     Influence (LOO): fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729222
Test loss (w/o reg) on all data: 0.012054798
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6461375e-07
Norm of the params: 9.153249
                Loss: fixed  19 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041106664
Train loss (w/o reg) on all data: 0.033622403
Test loss (w/o reg) on all data: 0.023464408
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9066085e-06
Norm of the params: 12.234592
              Random: fixed   7 labels. Loss 0.02346. Accuracy 0.996.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08165273
Train loss (w/o reg) on all data: 0.07237985
Test loss (w/o reg) on all data: 0.03916947
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5345007e-06
Norm of the params: 13.618283
Flipped loss: 0.03917. Accuracy: 0.992
### Flips: 52, rs: 39, checks: 52
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008836913
Train loss (w/o reg) on all data: 0.0034140071
Test loss (w/o reg) on all data: 0.02394408
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.782657e-07
Norm of the params: 10.414323
     Influence (LOO): fixed  28 labels. Loss 0.02394. Accuracy 0.989.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008045952
Train loss (w/o reg) on all data: 0.0029450513
Test loss (w/o reg) on all data: 0.017279882
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5636144e-07
Norm of the params: 10.100397
                Loss: fixed  29 labels. Loss 0.01728. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07934716
Train loss (w/o reg) on all data: 0.07025635
Test loss (w/o reg) on all data: 0.036304343
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3234168e-06
Norm of the params: 13.483926
              Random: fixed   2 labels. Loss 0.03630. Accuracy 0.992.
### Flips: 52, rs: 39, checks: 104
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172945
Test loss (w/o reg) on all data: 0.012055972
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8419586e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008045953
Train loss (w/o reg) on all data: 0.0029450879
Test loss (w/o reg) on all data: 0.017281061
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.409318e-07
Norm of the params: 10.100361
                Loss: fixed  29 labels. Loss 0.01728. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07674774
Train loss (w/o reg) on all data: 0.067543104
Test loss (w/o reg) on all data: 0.03859362
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.157518e-06
Norm of the params: 13.568076
              Random: fixed   3 labels. Loss 0.03859. Accuracy 0.992.
### Flips: 52, rs: 39, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021731239
Test loss (w/o reg) on all data: 0.012055376
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7221826e-07
Norm of the params: 9.153025
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731227
Test loss (w/o reg) on all data: 0.0120552955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6211998e-07
Norm of the params: 9.153027
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074696355
Train loss (w/o reg) on all data: 0.065369345
Test loss (w/o reg) on all data: 0.039946187
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4138636e-06
Norm of the params: 13.657973
              Random: fixed   4 labels. Loss 0.03995. Accuracy 0.992.
### Flips: 52, rs: 39, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728708
Test loss (w/o reg) on all data: 0.012056394
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3725124e-07
Norm of the params: 9.153303
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728727
Test loss (w/o reg) on all data: 0.01205644
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8832005e-07
Norm of the params: 9.1533
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070469074
Train loss (w/o reg) on all data: 0.06152734
Test loss (w/o reg) on all data: 0.041522082
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.9567693e-06
Norm of the params: 13.372907
              Random: fixed   6 labels. Loss 0.04152. Accuracy 0.985.
### Flips: 52, rs: 39, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728612
Test loss (w/o reg) on all data: 0.012055994
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3135694e-07
Norm of the params: 9.153314
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217289
Test loss (w/o reg) on all data: 0.012056187
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.042305e-07
Norm of the params: 9.153281
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068674706
Train loss (w/o reg) on all data: 0.05972816
Test loss (w/o reg) on all data: 0.042260487
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.2945472e-06
Norm of the params: 13.376504
              Random: fixed   7 labels. Loss 0.04226. Accuracy 0.985.
### Flips: 52, rs: 39, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729055
Test loss (w/o reg) on all data: 0.012055844
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8583994e-07
Norm of the params: 9.153265
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729073
Test loss (w/o reg) on all data: 0.012055769
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0037535e-07
Norm of the params: 9.153263
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066082336
Train loss (w/o reg) on all data: 0.057349313
Test loss (w/o reg) on all data: 0.039105598
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0100509e-05
Norm of the params: 13.215918
              Random: fixed   8 labels. Loss 0.03911. Accuracy 0.985.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14590088
Train loss (w/o reg) on all data: 0.13804826
Test loss (w/o reg) on all data: 0.08506072
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.003727e-06
Norm of the params: 12.532055
Flipped loss: 0.08506. Accuracy: 0.973
### Flips: 104, rs: 0, checks: 52
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031033743
Train loss (w/o reg) on all data: 0.023288513
Test loss (w/o reg) on all data: 0.037755363
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2984633e-06
Norm of the params: 12.446068
     Influence (LOO): fixed  43 labels. Loss 0.03776. Accuracy 0.989.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010720149
Train loss (w/o reg) on all data: 0.004341087
Test loss (w/o reg) on all data: 0.0117716845
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.7180518e-07
Norm of the params: 11.295187
                Loss: fixed  49 labels. Loss 0.01177. Accuracy 0.996.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1356608
Train loss (w/o reg) on all data: 0.12774688
Test loss (w/o reg) on all data: 0.078230195
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.9551232e-05
Norm of the params: 12.580872
              Random: fixed   5 labels. Loss 0.07823. Accuracy 0.981.
### Flips: 104, rs: 0, checks: 104
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729916
Test loss (w/o reg) on all data: 0.012055112
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3020222e-07
Norm of the params: 9.153172
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729905
Test loss (w/o reg) on all data: 0.012055172
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.76717e-07
Norm of the params: 9.1531725
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12463059
Train loss (w/o reg) on all data: 0.1166691
Test loss (w/o reg) on all data: 0.07540715
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.43712605e-05
Norm of the params: 12.6186285
              Random: fixed   9 labels. Loss 0.07541. Accuracy 0.977.
### Flips: 104, rs: 0, checks: 156
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730494
Test loss (w/o reg) on all data: 0.012055138
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.129623e-07
Norm of the params: 9.153109
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730487
Test loss (w/o reg) on all data: 0.01205509
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3426974e-07
Norm of the params: 9.153109
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11543007
Train loss (w/o reg) on all data: 0.10773919
Test loss (w/o reg) on all data: 0.06621421
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4604003e-05
Norm of the params: 12.402326
              Random: fixed  13 labels. Loss 0.06621. Accuracy 0.992.
### Flips: 104, rs: 0, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173178
Test loss (w/o reg) on all data: 0.01205584
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.231459e-06
Norm of the params: 9.152968
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731628
Test loss (w/o reg) on all data: 0.012055893
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7340786e-07
Norm of the params: 9.152983
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1154301
Train loss (w/o reg) on all data: 0.10773986
Test loss (w/o reg) on all data: 0.06620331
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.652806e-05
Norm of the params: 12.401806
              Random: fixed  13 labels. Loss 0.06620. Accuracy 0.992.
### Flips: 104, rs: 0, checks: 260
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172966
Test loss (w/o reg) on all data: 0.012054952
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.0419434e-08
Norm of the params: 9.153198
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729672
Test loss (w/o reg) on all data: 0.012054978
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.98539e-07
Norm of the params: 9.153198
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115430094
Train loss (w/o reg) on all data: 0.10773969
Test loss (w/o reg) on all data: 0.06621249
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4797955e-05
Norm of the params: 12.40194
              Random: fixed  13 labels. Loss 0.06621. Accuracy 0.992.
### Flips: 104, rs: 0, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729914
Test loss (w/o reg) on all data: 0.012055127
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.703265e-07
Norm of the params: 9.1531725
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729886
Test loss (w/o reg) on all data: 0.012055286
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.918209e-07
Norm of the params: 9.1531725
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11125165
Train loss (w/o reg) on all data: 0.10389121
Test loss (w/o reg) on all data: 0.0609821
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.089599e-06
Norm of the params: 12.132966
              Random: fixed  15 labels. Loss 0.06098. Accuracy 0.989.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12933978
Train loss (w/o reg) on all data: 0.12081827
Test loss (w/o reg) on all data: 0.06286222
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.817334e-06
Norm of the params: 13.054895
Flipped loss: 0.06286. Accuracy: 0.992
### Flips: 104, rs: 1, checks: 52
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035183147
Train loss (w/o reg) on all data: 0.026770292
Test loss (w/o reg) on all data: 0.016488591
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.2770197e-06
Norm of the params: 12.971395
     Influence (LOO): fixed  35 labels. Loss 0.01649. Accuracy 0.996.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009073757
Train loss (w/o reg) on all data: 0.0041187643
Test loss (w/o reg) on all data: 0.012108902
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.497053e-07
Norm of the params: 9.954891
                Loss: fixed  47 labels. Loss 0.01211. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12224582
Train loss (w/o reg) on all data: 0.11323244
Test loss (w/o reg) on all data: 0.06254212
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9825087e-06
Norm of the params: 13.426373
              Random: fixed   3 labels. Loss 0.06254. Accuracy 0.992.
### Flips: 104, rs: 1, checks: 104
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009073759
Train loss (w/o reg) on all data: 0.004118867
Test loss (w/o reg) on all data: 0.012107842
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5192684e-07
Norm of the params: 9.95479
     Influence (LOO): fixed  47 labels. Loss 0.01211. Accuracy 0.992.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730098
Test loss (w/o reg) on all data: 0.012054787
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.690823e-07
Norm of the params: 9.153151
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121323295
Train loss (w/o reg) on all data: 0.11241101
Test loss (w/o reg) on all data: 0.062169738
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6563055e-05
Norm of the params: 13.350873
              Random: fixed   4 labels. Loss 0.06217. Accuracy 0.992.
### Flips: 104, rs: 1, checks: 156
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173136
Test loss (w/o reg) on all data: 0.012055961
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.815975e-07
Norm of the params: 9.153012
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021731341
Test loss (w/o reg) on all data: 0.012056139
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9405925e-07
Norm of the params: 9.153014
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11906982
Train loss (w/o reg) on all data: 0.11004048
Test loss (w/o reg) on all data: 0.061300345
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.969426e-05
Norm of the params: 13.43826
              Random: fixed   5 labels. Loss 0.06130. Accuracy 0.992.
### Flips: 104, rs: 1, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729823
Test loss (w/o reg) on all data: 0.0120547945
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.525702e-08
Norm of the params: 9.153181
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729823
Test loss (w/o reg) on all data: 0.012054781
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.13526e-08
Norm of the params: 9.153182
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11458989
Train loss (w/o reg) on all data: 0.10565916
Test loss (w/o reg) on all data: 0.059181124
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.036828e-05
Norm of the params: 13.36468
              Random: fixed   7 labels. Loss 0.05918. Accuracy 0.992.
### Flips: 104, rs: 1, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729385
Test loss (w/o reg) on all data: 0.012055852
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.156179e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729402
Test loss (w/o reg) on all data: 0.01205591
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6755065e-07
Norm of the params: 9.153227
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11268003
Train loss (w/o reg) on all data: 0.10347275
Test loss (w/o reg) on all data: 0.06233679
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9833998e-05
Norm of the params: 13.570029
              Random: fixed   9 labels. Loss 0.06234. Accuracy 0.992.
### Flips: 104, rs: 1, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173082
Test loss (w/o reg) on all data: 0.012055087
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.138052e-06
Norm of the params: 9.153071
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730796
Test loss (w/o reg) on all data: 0.012054897
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.019283e-07
Norm of the params: 9.153072
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10849872
Train loss (w/o reg) on all data: 0.0988672
Test loss (w/o reg) on all data: 0.06140416
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4722854e-05
Norm of the params: 13.879139
              Random: fixed  10 labels. Loss 0.06140. Accuracy 0.989.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12262257
Train loss (w/o reg) on all data: 0.11237621
Test loss (w/o reg) on all data: 0.06762789
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.268077e-06
Norm of the params: 14.315278
Flipped loss: 0.06763. Accuracy: 0.981
### Flips: 104, rs: 2, checks: 52
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03161423
Train loss (w/o reg) on all data: 0.024412317
Test loss (w/o reg) on all data: 0.024526218
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5138456e-06
Norm of the params: 12.001594
     Influence (LOO): fixed  38 labels. Loss 0.02453. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729548
Test loss (w/o reg) on all data: 0.012054918
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4835196e-07
Norm of the params: 9.153211
                Loss: fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11918793
Train loss (w/o reg) on all data: 0.10875889
Test loss (w/o reg) on all data: 0.06289335
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.546507e-05
Norm of the params: 14.442327
              Random: fixed   1 labels. Loss 0.06289. Accuracy 0.989.
### Flips: 104, rs: 2, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729684
Test loss (w/o reg) on all data: 0.012056875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7141117e-06
Norm of the params: 9.153196
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172966
Test loss (w/o reg) on all data: 0.012056507
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.695129e-07
Norm of the params: 9.153198
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11652272
Train loss (w/o reg) on all data: 0.10611336
Test loss (w/o reg) on all data: 0.058561515
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1281248e-05
Norm of the params: 14.428698
              Random: fixed   3 labels. Loss 0.05856. Accuracy 0.992.
### Flips: 104, rs: 2, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729746
Test loss (w/o reg) on all data: 0.012054949
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4155852e-07
Norm of the params: 9.15319
     Influence (LOO): fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729742
Test loss (w/o reg) on all data: 0.012054908
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6401673e-07
Norm of the params: 9.153189
                Loss: fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11652272
Train loss (w/o reg) on all data: 0.106124654
Test loss (w/o reg) on all data: 0.058562547
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0165618e-06
Norm of the params: 14.420863
              Random: fixed   3 labels. Loss 0.05856. Accuracy 0.992.
### Flips: 104, rs: 2, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.01205515
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.82662e-07
Norm of the params: 9.15325
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729216
Test loss (w/o reg) on all data: 0.012055111
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.2997585e-07
Norm of the params: 9.153248
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109134935
Train loss (w/o reg) on all data: 0.09882318
Test loss (w/o reg) on all data: 0.05487304
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1497213e-06
Norm of the params: 14.360886
              Random: fixed   6 labels. Loss 0.05487. Accuracy 0.992.
### Flips: 104, rs: 2, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728857
Test loss (w/o reg) on all data: 0.012055893
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5044548e-07
Norm of the params: 9.153286
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728904
Test loss (w/o reg) on all data: 0.012055818
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3451328e-07
Norm of the params: 9.15328
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10398324
Train loss (w/o reg) on all data: 0.093488164
Test loss (w/o reg) on all data: 0.051784027
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.391308e-06
Norm of the params: 14.487978
              Random: fixed   8 labels. Loss 0.05178. Accuracy 0.989.
### Flips: 104, rs: 2, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362022
Train loss (w/o reg) on all data: 0.0021729986
Test loss (w/o reg) on all data: 0.0120551335
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.6698983e-08
Norm of the params: 9.153167
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729968
Test loss (w/o reg) on all data: 0.012055146
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3764782e-07
Norm of the params: 9.153167
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09973086
Train loss (w/o reg) on all data: 0.089539565
Test loss (w/o reg) on all data: 0.049223565
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.820657e-06
Norm of the params: 14.276757
              Random: fixed  10 labels. Loss 0.04922. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13628507
Train loss (w/o reg) on all data: 0.12625085
Test loss (w/o reg) on all data: 0.10321917
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.04795845e-05
Norm of the params: 14.166317
Flipped loss: 0.10322. Accuracy: 0.969
### Flips: 104, rs: 3, checks: 52
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0258965
Train loss (w/o reg) on all data: 0.018077902
Test loss (w/o reg) on all data: 0.02618813
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.936453e-06
Norm of the params: 12.504878
     Influence (LOO): fixed  43 labels. Loss 0.02619. Accuracy 0.989.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011310063
Train loss (w/o reg) on all data: 0.005112464
Test loss (w/o reg) on all data: 0.0413803
Train acc on all data:  1.0
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4181918e-07
Norm of the params: 11.133373
                Loss: fixed  47 labels. Loss 0.04138. Accuracy 0.981.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1309793
Train loss (w/o reg) on all data: 0.12135576
Test loss (w/o reg) on all data: 0.08590581
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0543766e-05
Norm of the params: 13.873384
              Random: fixed   4 labels. Loss 0.08591. Accuracy 0.977.
### Flips: 104, rs: 3, checks: 104
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007540387
Train loss (w/o reg) on all data: 0.0028312802
Test loss (w/o reg) on all data: 0.014728041
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.575343e-07
Norm of the params: 9.704748
     Influence (LOO): fixed  51 labels. Loss 0.01473. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007540386
Train loss (w/o reg) on all data: 0.0028312793
Test loss (w/o reg) on all data: 0.014727989
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.155763e-07
Norm of the params: 9.704748
                Loss: fixed  51 labels. Loss 0.01473. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1309793
Train loss (w/o reg) on all data: 0.12134952
Test loss (w/o reg) on all data: 0.085931815
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.036538e-06
Norm of the params: 13.877879
              Random: fixed   4 labels. Loss 0.08593. Accuracy 0.977.
### Flips: 104, rs: 3, checks: 156
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729046
Test loss (w/o reg) on all data: 0.012055129
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2661124e-07
Norm of the params: 9.153267
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729067
Test loss (w/o reg) on all data: 0.012055172
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1053386e-07
Norm of the params: 9.153265
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1271234
Train loss (w/o reg) on all data: 0.11722117
Test loss (w/o reg) on all data: 0.08095655
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.136935e-06
Norm of the params: 14.072831
              Random: fixed   6 labels. Loss 0.08096. Accuracy 0.977.
### Flips: 104, rs: 3, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.012056045
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4214034e-07
Norm of the params: 9.1531925
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.012055909
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2906732e-07
Norm of the params: 9.1531925
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121271454
Train loss (w/o reg) on all data: 0.11165047
Test loss (w/o reg) on all data: 0.08012869
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.814603e-05
Norm of the params: 13.871545
              Random: fixed   9 labels. Loss 0.08013. Accuracy 0.973.
### Flips: 104, rs: 3, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729392
Test loss (w/o reg) on all data: 0.01205482
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8656733e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172941
Test loss (w/o reg) on all data: 0.012054747
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.71667e-07
Norm of the params: 9.153227
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107062824
Train loss (w/o reg) on all data: 0.09674822
Test loss (w/o reg) on all data: 0.077549666
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5307334e-05
Norm of the params: 14.362874
              Random: fixed  14 labels. Loss 0.07755. Accuracy 0.973.
### Flips: 104, rs: 3, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172897
Test loss (w/o reg) on all data: 0.012056974
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.0069285e-07
Norm of the params: 9.153275
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728978
Test loss (w/o reg) on all data: 0.012056824
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.386221e-07
Norm of the params: 9.153274
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101800054
Train loss (w/o reg) on all data: 0.09235637
Test loss (w/o reg) on all data: 0.06731008
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0110682e-05
Norm of the params: 13.743136
              Random: fixed  18 labels. Loss 0.06731. Accuracy 0.981.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12103199
Train loss (w/o reg) on all data: 0.11242891
Test loss (w/o reg) on all data: 0.066124156
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.812554e-06
Norm of the params: 13.117228
Flipped loss: 0.06612. Accuracy: 0.992
### Flips: 104, rs: 4, checks: 52
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016784288
Train loss (w/o reg) on all data: 0.010866723
Test loss (w/o reg) on all data: 0.017571328
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5503022e-06
Norm of the params: 10.878938
     Influence (LOO): fixed  37 labels. Loss 0.01757. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729954
Test loss (w/o reg) on all data: 0.012055
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3397265e-07
Norm of the params: 9.153167
                Loss: fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116622865
Train loss (w/o reg) on all data: 0.1082501
Test loss (w/o reg) on all data: 0.066028275
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8184428e-05
Norm of the params: 12.940454
              Random: fixed   2 labels. Loss 0.06603. Accuracy 0.992.
### Flips: 104, rs: 4, checks: 104
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021727995
Test loss (w/o reg) on all data: 0.012054829
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.819993e-07
Norm of the params: 9.153381
     Influence (LOO): fixed  42 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728012
Test loss (w/o reg) on all data: 0.012054693
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5830842e-07
Norm of the params: 9.153379
                Loss: fixed  42 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11327222
Train loss (w/o reg) on all data: 0.10466798
Test loss (w/o reg) on all data: 0.066176996
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.3782482e-05
Norm of the params: 13.11811
              Random: fixed   3 labels. Loss 0.06618. Accuracy 0.989.
### Flips: 104, rs: 4, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.012054601
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.52386e-08
Norm of the params: 9.153214
     Influence (LOO): fixed  42 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729693
Test loss (w/o reg) on all data: 0.012054707
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8250215e-07
Norm of the params: 9.153197
                Loss: fixed  42 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110219724
Train loss (w/o reg) on all data: 0.10157096
Test loss (w/o reg) on all data: 0.06787904
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4757247e-05
Norm of the params: 13.15201
              Random: fixed   5 labels. Loss 0.06788. Accuracy 0.992.
### Flips: 104, rs: 4, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729905
Test loss (w/o reg) on all data: 0.012054667
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0677967e-07
Norm of the params: 9.1531725
     Influence (LOO): fixed  42 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729898
Test loss (w/o reg) on all data: 0.012054742
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6724354e-07
Norm of the params: 9.1531725
                Loss: fixed  42 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107485324
Train loss (w/o reg) on all data: 0.098780386
Test loss (w/o reg) on all data: 0.0681644
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.982375e-06
Norm of the params: 13.194651
              Random: fixed   6 labels. Loss 0.06816. Accuracy 0.992.
### Flips: 104, rs: 4, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729465
Test loss (w/o reg) on all data: 0.01205526
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0162794e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729479
Test loss (w/o reg) on all data: 0.012055336
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8045533e-07
Norm of the params: 9.15322
                Loss: fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09876348
Train loss (w/o reg) on all data: 0.08971154
Test loss (w/o reg) on all data: 0.06297706
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.5880745e-06
Norm of the params: 13.455065
              Random: fixed  10 labels. Loss 0.06298. Accuracy 0.989.
### Flips: 104, rs: 4, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172966
Test loss (w/o reg) on all data: 0.012055267
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5503172e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729665
Test loss (w/o reg) on all data: 0.012055197
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.15379734e-07
Norm of the params: 9.153201
                Loss: fixed  42 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093498275
Train loss (w/o reg) on all data: 0.084407866
Test loss (w/o reg) on all data: 0.059037384
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5680316e-05
Norm of the params: 13.48363
              Random: fixed  12 labels. Loss 0.05904. Accuracy 0.989.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13010761
Train loss (w/o reg) on all data: 0.121596284
Test loss (w/o reg) on all data: 0.07128272
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4387022e-05
Norm of the params: 13.047095
Flipped loss: 0.07128. Accuracy: 0.985
### Flips: 104, rs: 5, checks: 52
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020398377
Train loss (w/o reg) on all data: 0.013472613
Test loss (w/o reg) on all data: 0.016843224
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.113901e-06
Norm of the params: 11.769251
     Influence (LOO): fixed  39 labels. Loss 0.01684. Accuracy 0.996.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217296
Test loss (w/o reg) on all data: 0.012054974
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.01409384e-07
Norm of the params: 9.153206
                Loss: fixed  44 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120800234
Train loss (w/o reg) on all data: 0.11224562
Test loss (w/o reg) on all data: 0.06378176
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0289407e-05
Norm of the params: 13.080228
              Random: fixed   3 labels. Loss 0.06378. Accuracy 0.989.
### Flips: 104, rs: 5, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217286
Test loss (w/o reg) on all data: 0.012054931
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8971517e-07
Norm of the params: 9.153314
     Influence (LOO): fixed  44 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.00217287
Test loss (w/o reg) on all data: 0.012054851
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.266284e-07
Norm of the params: 9.153304
                Loss: fixed  44 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11532992
Train loss (w/o reg) on all data: 0.10671213
Test loss (w/o reg) on all data: 0.057055
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8414017e-05
Norm of the params: 13.128432
              Random: fixed   5 labels. Loss 0.05706. Accuracy 0.989.
### Flips: 104, rs: 5, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729667
Test loss (w/o reg) on all data: 0.01205525
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3556444e-07
Norm of the params: 9.153199
     Influence (LOO): fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729667
Test loss (w/o reg) on all data: 0.012055192
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.8080804e-08
Norm of the params: 9.1532
                Loss: fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11299544
Train loss (w/o reg) on all data: 0.10469228
Test loss (w/o reg) on all data: 0.055926766
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2057656e-05
Norm of the params: 12.88655
              Random: fixed   7 labels. Loss 0.05593. Accuracy 0.989.
### Flips: 104, rs: 5, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730573
Test loss (w/o reg) on all data: 0.012055132
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2965037e-07
Norm of the params: 9.1531
     Influence (LOO): fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730566
Test loss (w/o reg) on all data: 0.012055082
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8553192e-07
Norm of the params: 9.153102
                Loss: fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104981005
Train loss (w/o reg) on all data: 0.09603003
Test loss (w/o reg) on all data: 0.05337242
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.898352e-05
Norm of the params: 13.379822
              Random: fixed  10 labels. Loss 0.05337. Accuracy 0.989.
### Flips: 104, rs: 5, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729646
Test loss (w/o reg) on all data: 0.012055214
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7066863e-07
Norm of the params: 9.153199
     Influence (LOO): fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172965
Test loss (w/o reg) on all data: 0.012055127
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.110652e-07
Norm of the params: 9.153199
                Loss: fixed  44 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10324691
Train loss (w/o reg) on all data: 0.094564594
Test loss (w/o reg) on all data: 0.05468497
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.089911e-06
Norm of the params: 13.177493
              Random: fixed  11 labels. Loss 0.05468. Accuracy 0.989.
### Flips: 104, rs: 5, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729642
Test loss (w/o reg) on all data: 0.012054821
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3611306e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  44 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.012054745
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.00372375e-07
Norm of the params: 9.153201
                Loss: fixed  44 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1032469
Train loss (w/o reg) on all data: 0.09456596
Test loss (w/o reg) on all data: 0.05468412
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.3713406e-06
Norm of the params: 13.176451
              Random: fixed  11 labels. Loss 0.05468. Accuracy 0.989.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1374807
Train loss (w/o reg) on all data: 0.12922525
Test loss (w/o reg) on all data: 0.06326196
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3608812e-05
Norm of the params: 12.849478
Flipped loss: 0.06326. Accuracy: 0.992
### Flips: 104, rs: 6, checks: 52
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039617978
Train loss (w/o reg) on all data: 0.031914428
Test loss (w/o reg) on all data: 0.028909853
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3934733e-06
Norm of the params: 12.412536
     Influence (LOO): fixed  38 labels. Loss 0.02891. Accuracy 0.992.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010785196
Train loss (w/o reg) on all data: 0.0043964665
Test loss (w/o reg) on all data: 0.015504753
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.771611e-07
Norm of the params: 11.303742
                Loss: fixed  48 labels. Loss 0.01550. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13038142
Train loss (w/o reg) on all data: 0.12220512
Test loss (w/o reg) on all data: 0.062327612
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.244134e-06
Norm of the params: 12.787729
              Random: fixed   3 labels. Loss 0.06233. Accuracy 0.985.
### Flips: 104, rs: 6, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729518
Test loss (w/o reg) on all data: 0.012055801
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7324244e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729488
Test loss (w/o reg) on all data: 0.01205587
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.965721e-07
Norm of the params: 9.153216
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12703377
Train loss (w/o reg) on all data: 0.118631944
Test loss (w/o reg) on all data: 0.06046646
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.94249e-05
Norm of the params: 12.962895
              Random: fixed   4 labels. Loss 0.06047. Accuracy 0.992.
### Flips: 104, rs: 6, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172919
Test loss (w/o reg) on all data: 0.0120552275
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9871414e-07
Norm of the params: 9.153252
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729197
Test loss (w/o reg) on all data: 0.012055331
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.409652e-07
Norm of the params: 9.153251
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12550539
Train loss (w/o reg) on all data: 0.1172365
Test loss (w/o reg) on all data: 0.058998436
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.01429605e-05
Norm of the params: 12.859927
              Random: fixed   5 labels. Loss 0.05900. Accuracy 0.992.
### Flips: 104, rs: 6, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729688
Test loss (w/o reg) on all data: 0.012055013
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5058991e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729677
Test loss (w/o reg) on all data: 0.012055098
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7564736e-07
Norm of the params: 9.153197
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119721994
Train loss (w/o reg) on all data: 0.11092149
Test loss (w/o reg) on all data: 0.05754529
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.124673e-06
Norm of the params: 13.266882
              Random: fixed   7 labels. Loss 0.05755. Accuracy 0.992.
### Flips: 104, rs: 6, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172986
Test loss (w/o reg) on all data: 0.012055426
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0960364e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729867
Test loss (w/o reg) on all data: 0.012055362
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2209256e-07
Norm of the params: 9.153177
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11650498
Train loss (w/o reg) on all data: 0.10783264
Test loss (w/o reg) on all data: 0.05412497
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6097363e-05
Norm of the params: 13.169919
              Random: fixed   9 labels. Loss 0.05412. Accuracy 0.992.
### Flips: 104, rs: 6, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172952
Test loss (w/o reg) on all data: 0.012055055
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.452601e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.012055193
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.956392e-07
Norm of the params: 9.1532135
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11381681
Train loss (w/o reg) on all data: 0.10479949
Test loss (w/o reg) on all data: 0.053890154
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4916983e-05
Norm of the params: 13.429317
              Random: fixed  10 labels. Loss 0.05389. Accuracy 0.989.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13511148
Train loss (w/o reg) on all data: 0.12715122
Test loss (w/o reg) on all data: 0.08239239
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0406138e-05
Norm of the params: 12.617651
Flipped loss: 0.08239. Accuracy: 0.989
### Flips: 104, rs: 7, checks: 52
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028120767
Train loss (w/o reg) on all data: 0.020879287
Test loss (w/o reg) on all data: 0.038952053
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4913218e-06
Norm of the params: 12.034518
     Influence (LOO): fixed  42 labels. Loss 0.03895. Accuracy 0.985.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009838757
Train loss (w/o reg) on all data: 0.003881756
Test loss (w/o reg) on all data: 0.019094786
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.369917e-07
Norm of the params: 10.915129
                Loss: fixed  48 labels. Loss 0.01909. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13314931
Train loss (w/o reg) on all data: 0.12509923
Test loss (w/o reg) on all data: 0.08060751
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2089373e-05
Norm of the params: 12.68864
              Random: fixed   1 labels. Loss 0.08061. Accuracy 0.989.
### Flips: 104, rs: 7, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729814
Test loss (w/o reg) on all data: 0.0120552685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.361626e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729807
Test loss (w/o reg) on all data: 0.012055185
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2622915e-07
Norm of the params: 9.153183
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1252955
Train loss (w/o reg) on all data: 0.11692685
Test loss (w/o reg) on all data: 0.076936126
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.527074e-06
Norm of the params: 12.937283
              Random: fixed   4 labels. Loss 0.07694. Accuracy 0.985.
### Flips: 104, rs: 7, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729004
Test loss (w/o reg) on all data: 0.012055291
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4859658e-07
Norm of the params: 9.153269
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729034
Test loss (w/o reg) on all data: 0.012055325
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.467573e-07
Norm of the params: 9.153267
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12359951
Train loss (w/o reg) on all data: 0.115326494
Test loss (w/o reg) on all data: 0.07439773
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1920498e-05
Norm of the params: 12.863133
              Random: fixed   5 labels. Loss 0.07440. Accuracy 0.985.
### Flips: 104, rs: 7, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172979
Test loss (w/o reg) on all data: 0.012054695
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3863853e-07
Norm of the params: 9.153186
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012054759
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2525712e-07
Norm of the params: 9.153186
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1168706
Train loss (w/o reg) on all data: 0.10816446
Test loss (w/o reg) on all data: 0.07331189
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.141079e-06
Norm of the params: 13.1955595
              Random: fixed   8 labels. Loss 0.07331. Accuracy 0.985.
### Flips: 104, rs: 7, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173035
Test loss (w/o reg) on all data: 0.01205471
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6492509e-07
Norm of the params: 9.153124
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730333
Test loss (w/o reg) on all data: 0.012054793
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1950111e-07
Norm of the params: 9.153125
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11104183
Train loss (w/o reg) on all data: 0.10245452
Test loss (w/o reg) on all data: 0.06848661
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.830371e-06
Norm of the params: 13.105199
              Random: fixed  11 labels. Loss 0.06849. Accuracy 0.985.
### Flips: 104, rs: 7, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729134
Test loss (w/o reg) on all data: 0.012055552
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0851314e-07
Norm of the params: 9.153255
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172917
Test loss (w/o reg) on all data: 0.012055481
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9415305e-07
Norm of the params: 9.1532545
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0984637
Train loss (w/o reg) on all data: 0.08924199
Test loss (w/o reg) on all data: 0.069044046
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.958695e-06
Norm of the params: 13.580658
              Random: fixed  16 labels. Loss 0.06904. Accuracy 0.985.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1383766
Train loss (w/o reg) on all data: 0.13035013
Test loss (w/o reg) on all data: 0.06161569
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2852742e-05
Norm of the params: 12.670019
Flipped loss: 0.06162. Accuracy: 0.992
### Flips: 104, rs: 8, checks: 52
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02775018
Train loss (w/o reg) on all data: 0.021271084
Test loss (w/o reg) on all data: 0.018714663
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9911015e-06
Norm of the params: 11.383405
     Influence (LOO): fixed  42 labels. Loss 0.01871. Accuracy 0.996.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729548
Test loss (w/o reg) on all data: 0.012055064
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.407957e-07
Norm of the params: 9.153211
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13551585
Train loss (w/o reg) on all data: 0.12732555
Test loss (w/o reg) on all data: 0.062149704
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8684768e-05
Norm of the params: 12.7986765
              Random: fixed   1 labels. Loss 0.06215. Accuracy 0.992.
### Flips: 104, rs: 8, checks: 104
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172931
Test loss (w/o reg) on all data: 0.012055094
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.23674e-07
Norm of the params: 9.153238
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.012055172
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2904984e-07
Norm of the params: 9.153238
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1322639
Train loss (w/o reg) on all data: 0.12431407
Test loss (w/o reg) on all data: 0.057206906
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4145018e-05
Norm of the params: 12.609381
              Random: fixed   3 labels. Loss 0.05721. Accuracy 0.996.
### Flips: 104, rs: 8, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729711
Test loss (w/o reg) on all data: 0.012055096
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6022717e-07
Norm of the params: 9.153193
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217297
Test loss (w/o reg) on all data: 0.012055064
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.196171e-07
Norm of the params: 9.153193
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12260877
Train loss (w/o reg) on all data: 0.11432464
Test loss (w/o reg) on all data: 0.05678487
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1093585e-05
Norm of the params: 12.87178
              Random: fixed   6 labels. Loss 0.05678. Accuracy 0.992.
### Flips: 104, rs: 8, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730678
Test loss (w/o reg) on all data: 0.012055513
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0299043e-07
Norm of the params: 9.153087
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730657
Test loss (w/o reg) on all data: 0.012055446
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2448428e-07
Norm of the params: 9.15309
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120584995
Train loss (w/o reg) on all data: 0.11245932
Test loss (w/o reg) on all data: 0.056998916
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.985247e-05
Norm of the params: 12.748081
              Random: fixed   7 labels. Loss 0.05700. Accuracy 0.992.
### Flips: 104, rs: 8, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730247
Test loss (w/o reg) on all data: 0.012055234
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4581294e-07
Norm of the params: 9.153134
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730252
Test loss (w/o reg) on all data: 0.012055289
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3416212e-07
Norm of the params: 9.153135
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11202606
Train loss (w/o reg) on all data: 0.10310273
Test loss (w/o reg) on all data: 0.053433374
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4143488e-05
Norm of the params: 13.359139
              Random: fixed  10 labels. Loss 0.05343. Accuracy 0.992.
### Flips: 104, rs: 8, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172971
Test loss (w/o reg) on all data: 0.0120548755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.8226636e-08
Norm of the params: 9.153194
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729611
Test loss (w/o reg) on all data: 0.012055208
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6001674e-07
Norm of the params: 9.153205
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10004671
Train loss (w/o reg) on all data: 0.09063084
Test loss (w/o reg) on all data: 0.05144869
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.600653e-06
Norm of the params: 13.722882
              Random: fixed  14 labels. Loss 0.05145. Accuracy 0.989.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15861525
Train loss (w/o reg) on all data: 0.1514473
Test loss (w/o reg) on all data: 0.085076585
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6866e-05
Norm of the params: 11.973265
Flipped loss: 0.08508. Accuracy: 0.989
### Flips: 104, rs: 9, checks: 52
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052718792
Train loss (w/o reg) on all data: 0.04537046
Test loss (w/o reg) on all data: 0.035150755
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.057338e-06
Norm of the params: 12.122981
     Influence (LOO): fixed  44 labels. Loss 0.03515. Accuracy 0.989.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020214614
Train loss (w/o reg) on all data: 0.011257751
Test loss (w/o reg) on all data: 0.024710841
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5507005e-06
Norm of the params: 13.384217
                Loss: fixed  52 labels. Loss 0.02471. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15677755
Train loss (w/o reg) on all data: 0.14958748
Test loss (w/o reg) on all data: 0.080980904
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.257834e-05
Norm of the params: 11.99172
              Random: fixed   1 labels. Loss 0.08098. Accuracy 0.989.
### Flips: 104, rs: 9, checks: 104
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0121942125
Train loss (w/o reg) on all data: 0.006805174
Test loss (w/o reg) on all data: 0.014141403
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.2270795e-07
Norm of the params: 10.381751
     Influence (LOO): fixed  59 labels. Loss 0.01414. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730454
Test loss (w/o reg) on all data: 0.012055219
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8021954e-07
Norm of the params: 9.15311
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14766917
Train loss (w/o reg) on all data: 0.13975047
Test loss (w/o reg) on all data: 0.07713568
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.8252103e-05
Norm of the params: 12.58467
              Random: fixed   4 labels. Loss 0.07714. Accuracy 0.985.
### Flips: 104, rs: 9, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173049
Test loss (w/o reg) on all data: 0.012054983
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2168178e-07
Norm of the params: 9.153108
     Influence (LOO): fixed  61 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173003
Test loss (w/o reg) on all data: 0.01205494
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1729935e-07
Norm of the params: 9.153157
                Loss: fixed  61 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14475998
Train loss (w/o reg) on all data: 0.13683985
Test loss (w/o reg) on all data: 0.0743692
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6507757e-06
Norm of the params: 12.585815
              Random: fixed   6 labels. Loss 0.07437. Accuracy 0.992.
### Flips: 104, rs: 9, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172968
Test loss (w/o reg) on all data: 0.012055447
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6000465e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729697
Test loss (w/o reg) on all data: 0.012055559
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7893525e-07
Norm of the params: 9.153196
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13843387
Train loss (w/o reg) on all data: 0.12991332
Test loss (w/o reg) on all data: 0.07225741
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2044652e-05
Norm of the params: 13.054168
              Random: fixed   9 labels. Loss 0.07226. Accuracy 0.992.
### Flips: 104, rs: 9, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217291
Test loss (w/o reg) on all data: 0.012055009
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5435067e-07
Norm of the params: 9.153261
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172923
Test loss (w/o reg) on all data: 0.012055248
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5226845e-07
Norm of the params: 9.153247
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13702342
Train loss (w/o reg) on all data: 0.12832297
Test loss (w/o reg) on all data: 0.071605176
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6160937e-05
Norm of the params: 13.191248
              Random: fixed  10 labels. Loss 0.07161. Accuracy 0.989.
### Flips: 104, rs: 9, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729155
Test loss (w/o reg) on all data: 0.012055337
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5647466e-07
Norm of the params: 9.1532545
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729185
Test loss (w/o reg) on all data: 0.012055363
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.737483e-07
Norm of the params: 9.153253
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13164258
Train loss (w/o reg) on all data: 0.123366594
Test loss (w/o reg) on all data: 0.06940629
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7909711e-05
Norm of the params: 12.865448
              Random: fixed  13 labels. Loss 0.06941. Accuracy 0.985.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13397911
Train loss (w/o reg) on all data: 0.12505789
Test loss (w/o reg) on all data: 0.08178198
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.949115e-06
Norm of the params: 13.357559
Flipped loss: 0.08178. Accuracy: 0.977
### Flips: 104, rs: 10, checks: 52
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028101407
Train loss (w/o reg) on all data: 0.02073986
Test loss (w/o reg) on all data: 0.030975968
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.8557464e-06
Norm of the params: 12.133875
     Influence (LOO): fixed  43 labels. Loss 0.03098. Accuracy 0.985.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013029318
Train loss (w/o reg) on all data: 0.005809001
Test loss (w/o reg) on all data: 0.009395897
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.425785e-07
Norm of the params: 12.016919
                Loss: fixed  47 labels. Loss 0.00940. Accuracy 0.996.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1281621
Train loss (w/o reg) on all data: 0.11935299
Test loss (w/o reg) on all data: 0.07855876
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.828341e-06
Norm of the params: 13.273367
              Random: fixed   3 labels. Loss 0.07856. Accuracy 0.977.
### Flips: 104, rs: 10, checks: 104
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01114835
Train loss (w/o reg) on all data: 0.00574555
Test loss (w/o reg) on all data: 0.015265361
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2778721e-06
Norm of the params: 10.3949995
     Influence (LOO): fixed  51 labels. Loss 0.01527. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729472
Test loss (w/o reg) on all data: 0.012054409
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3139982e-07
Norm of the params: 9.15322
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124419704
Train loss (w/o reg) on all data: 0.115566894
Test loss (w/o reg) on all data: 0.06894325
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.4829165e-06
Norm of the params: 13.306246
              Random: fixed   6 labels. Loss 0.06894. Accuracy 0.989.
### Flips: 104, rs: 10, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729234
Test loss (w/o reg) on all data: 0.012054484
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3602433e-07
Norm of the params: 9.153243
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729262
Test loss (w/o reg) on all data: 0.012054536
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0312368e-07
Norm of the params: 9.153242
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124419704
Train loss (w/o reg) on all data: 0.11556816
Test loss (w/o reg) on all data: 0.068944946
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.2330533e-06
Norm of the params: 13.305297
              Random: fixed   6 labels. Loss 0.06894. Accuracy 0.989.
### Flips: 104, rs: 10, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172864
Test loss (w/o reg) on all data: 0.012054048
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.146362e-07
Norm of the params: 9.15331
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172866
Test loss (w/o reg) on all data: 0.0120539935
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0627331e-07
Norm of the params: 9.153308
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11814253
Train loss (w/o reg) on all data: 0.10930564
Test loss (w/o reg) on all data: 0.06201584
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.891782e-06
Norm of the params: 13.294272
              Random: fixed  10 labels. Loss 0.06202. Accuracy 0.989.
### Flips: 104, rs: 10, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729786
Test loss (w/o reg) on all data: 0.012054806
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.079248e-08
Norm of the params: 9.153186
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172977
Test loss (w/o reg) on all data: 0.012054829
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.6052275e-08
Norm of the params: 9.153186
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112558536
Train loss (w/o reg) on all data: 0.104046814
Test loss (w/o reg) on all data: 0.059642214
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.296497e-06
Norm of the params: 13.04739
              Random: fixed  13 labels. Loss 0.05964. Accuracy 0.989.
### Flips: 104, rs: 10, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172909
Test loss (w/o reg) on all data: 0.012055491
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1653725e-06
Norm of the params: 9.153263
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729069
Test loss (w/o reg) on all data: 0.012055243
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.573566e-07
Norm of the params: 9.153262
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10964698
Train loss (w/o reg) on all data: 0.10106098
Test loss (w/o reg) on all data: 0.057468668
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7391567e-05
Norm of the params: 13.104203
              Random: fixed  14 labels. Loss 0.05747. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13957998
Train loss (w/o reg) on all data: 0.13236992
Test loss (w/o reg) on all data: 0.05994463
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0361088e-05
Norm of the params: 12.008383
Flipped loss: 0.05994. Accuracy: 0.996
### Flips: 104, rs: 11, checks: 52
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033772334
Train loss (w/o reg) on all data: 0.024974244
Test loss (w/o reg) on all data: 0.014379796
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7392696e-06
Norm of the params: 13.265059
     Influence (LOO): fixed  39 labels. Loss 0.01438. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007180445
Train loss (w/o reg) on all data: 0.002611274
Test loss (w/o reg) on all data: 0.011524592
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3315371e-06
Norm of the params: 9.559467
                Loss: fixed  49 labels. Loss 0.01152. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13162783
Train loss (w/o reg) on all data: 0.124904655
Test loss (w/o reg) on all data: 0.057906892
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.044773e-05
Norm of the params: 11.595841
              Random: fixed   5 labels. Loss 0.05791. Accuracy 0.996.
### Flips: 104, rs: 11, checks: 104
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002173008
Test loss (w/o reg) on all data: 0.0120549565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.38044e-07
Norm of the params: 9.1531515
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730084
Test loss (w/o reg) on all data: 0.012054856
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5943427e-07
Norm of the params: 9.1531515
                Loss: fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12544383
Train loss (w/o reg) on all data: 0.11866524
Test loss (w/o reg) on all data: 0.055995382
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9081973e-05
Norm of the params: 11.643537
              Random: fixed   7 labels. Loss 0.05600. Accuracy 0.992.
### Flips: 104, rs: 11, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729278
Test loss (w/o reg) on all data: 0.012054609
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9195562e-07
Norm of the params: 9.153241
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729278
Test loss (w/o reg) on all data: 0.01205467
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.376483e-07
Norm of the params: 9.15324
                Loss: fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12041125
Train loss (w/o reg) on all data: 0.11354839
Test loss (w/o reg) on all data: 0.05368133
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.791911e-06
Norm of the params: 11.71568
              Random: fixed  10 labels. Loss 0.05368. Accuracy 0.992.
### Flips: 104, rs: 11, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172959
Test loss (w/o reg) on all data: 0.012054853
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2369326e-07
Norm of the params: 9.153207
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729597
Test loss (w/o reg) on all data: 0.012054917
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6432767e-07
Norm of the params: 9.153206
                Loss: fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11863165
Train loss (w/o reg) on all data: 0.11165621
Test loss (w/o reg) on all data: 0.053978384
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.729935e-06
Norm of the params: 11.811388
              Random: fixed  11 labels. Loss 0.05398. Accuracy 0.992.
### Flips: 104, rs: 11, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730224
Test loss (w/o reg) on all data: 0.012055462
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.284519e-07
Norm of the params: 9.153139
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217302
Test loss (w/o reg) on all data: 0.012055542
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0951412e-07
Norm of the params: 9.15314
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112802595
Train loss (w/o reg) on all data: 0.105747044
Test loss (w/o reg) on all data: 0.05131298
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.777171e-06
Norm of the params: 11.879017
              Random: fixed  13 labels. Loss 0.05131. Accuracy 0.992.
### Flips: 104, rs: 11, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729823
Test loss (w/o reg) on all data: 0.01205565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2459937e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012055707
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7207563e-07
Norm of the params: 9.153181
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10325202
Train loss (w/o reg) on all data: 0.09550034
Test loss (w/o reg) on all data: 0.046462003
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6019443e-05
Norm of the params: 12.451248
              Random: fixed  17 labels. Loss 0.04646. Accuracy 0.989.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14654756
Train loss (w/o reg) on all data: 0.13855895
Test loss (w/o reg) on all data: 0.09850639
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.9335962e-05
Norm of the params: 12.640092
Flipped loss: 0.09851. Accuracy: 0.969
### Flips: 104, rs: 12, checks: 52
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041961245
Train loss (w/o reg) on all data: 0.032066494
Test loss (w/o reg) on all data: 0.059092138
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4522694e-06
Norm of the params: 14.067515
     Influence (LOO): fixed  41 labels. Loss 0.05909. Accuracy 0.985.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019682031
Train loss (w/o reg) on all data: 0.010555105
Test loss (w/o reg) on all data: 0.030316027
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.7827044e-06
Norm of the params: 13.510682
                Loss: fixed  49 labels. Loss 0.03032. Accuracy 0.989.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14017016
Train loss (w/o reg) on all data: 0.13236839
Test loss (w/o reg) on all data: 0.098116755
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9366433e-05
Norm of the params: 12.491415
              Random: fixed   4 labels. Loss 0.09812. Accuracy 0.973.
### Flips: 104, rs: 12, checks: 104
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009240155
Train loss (w/o reg) on all data: 0.0036347478
Test loss (w/o reg) on all data: 0.012737907
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.263769e-07
Norm of the params: 10.588115
     Influence (LOO): fixed  56 labels. Loss 0.01274. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009240156
Train loss (w/o reg) on all data: 0.0036347474
Test loss (w/o reg) on all data: 0.01273788
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4805206e-07
Norm of the params: 10.588115
                Loss: fixed  56 labels. Loss 0.01274. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1381141
Train loss (w/o reg) on all data: 0.13063094
Test loss (w/o reg) on all data: 0.09792387
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.180553e-05
Norm of the params: 12.233683
              Random: fixed   6 labels. Loss 0.09792. Accuracy 0.977.
### Flips: 104, rs: 12, checks: 156
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008591905
Train loss (w/o reg) on all data: 0.003347612
Test loss (w/o reg) on all data: 0.011985338
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.802859e-07
Norm of the params: 10.24138
     Influence (LOO): fixed  57 labels. Loss 0.01199. Accuracy 0.996.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730235
Test loss (w/o reg) on all data: 0.01205486
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7866662e-07
Norm of the params: 9.153136
                Loss: fixed  58 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13146906
Train loss (w/o reg) on all data: 0.12364956
Test loss (w/o reg) on all data: 0.092176564
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.379613e-06
Norm of the params: 12.505596
              Random: fixed   9 labels. Loss 0.09218. Accuracy 0.977.
### Flips: 104, rs: 12, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730142
Test loss (w/o reg) on all data: 0.012055689
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3860758e-07
Norm of the params: 9.153145
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173013
Test loss (w/o reg) on all data: 0.012055426
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.3059083e-07
Norm of the params: 9.153148
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11794385
Train loss (w/o reg) on all data: 0.11012839
Test loss (w/o reg) on all data: 0.08224785
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9934949e-05
Norm of the params: 12.502371
              Random: fixed  16 labels. Loss 0.08225. Accuracy 0.981.
### Flips: 104, rs: 12, checks: 260
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173011
Test loss (w/o reg) on all data: 0.0120559065
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0431973e-07
Norm of the params: 9.153149
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730054
Test loss (w/o reg) on all data: 0.012055604
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2111255e-07
Norm of the params: 9.153154
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11037493
Train loss (w/o reg) on all data: 0.10282923
Test loss (w/o reg) on all data: 0.067036144
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1272148e-05
Norm of the params: 12.284702
              Random: fixed  20 labels. Loss 0.06704. Accuracy 0.985.
### Flips: 104, rs: 12, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730834
Test loss (w/o reg) on all data: 0.012055688
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.632271e-07
Norm of the params: 9.153071
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730822
Test loss (w/o reg) on all data: 0.012055533
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.580399e-07
Norm of the params: 9.153072
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102478325
Train loss (w/o reg) on all data: 0.0944355
Test loss (w/o reg) on all data: 0.06859669
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6170332e-05
Norm of the params: 12.682923
              Random: fixed  23 labels. Loss 0.06860. Accuracy 0.981.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11927919
Train loss (w/o reg) on all data: 0.1109403
Test loss (w/o reg) on all data: 0.052361593
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0727e-05
Norm of the params: 12.914248
Flipped loss: 0.05236. Accuracy: 0.996
### Flips: 104, rs: 13, checks: 52
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018503264
Train loss (w/o reg) on all data: 0.012363323
Test loss (w/o reg) on all data: 0.014416613
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.54653e-06
Norm of the params: 11.081462
     Influence (LOO): fixed  37 labels. Loss 0.01442. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729933
Test loss (w/o reg) on all data: 0.012055628
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2872435e-07
Norm of the params: 9.15317
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11927919
Train loss (w/o reg) on all data: 0.110941686
Test loss (w/o reg) on all data: 0.052358206
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.496582e-06
Norm of the params: 12.913174
              Random: fixed   0 labels. Loss 0.05236. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 104
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172926
Test loss (w/o reg) on all data: 0.012055187
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3376715e-06
Norm of the params: 9.153241
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729285
Test loss (w/o reg) on all data: 0.0120554045
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7076064e-07
Norm of the params: 9.15324
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11538786
Train loss (w/o reg) on all data: 0.10689311
Test loss (w/o reg) on all data: 0.05060669
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7246899e-05
Norm of the params: 13.034375
              Random: fixed   2 labels. Loss 0.05061. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 156
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002173012
Test loss (w/o reg) on all data: 0.01205494
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4125078e-07
Norm of the params: 9.153147
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729819
Test loss (w/o reg) on all data: 0.012055239
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7201356e-07
Norm of the params: 9.153182
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11468023
Train loss (w/o reg) on all data: 0.1061742
Test loss (w/o reg) on all data: 0.05228933
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0131387e-05
Norm of the params: 13.043029
              Random: fixed   3 labels. Loss 0.05229. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729295
Test loss (w/o reg) on all data: 0.012055096
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2219989e-07
Norm of the params: 9.153238
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729295
Test loss (w/o reg) on all data: 0.012055058
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.8589965e-08
Norm of the params: 9.153237
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10803388
Train loss (w/o reg) on all data: 0.10009225
Test loss (w/o reg) on all data: 0.047086358
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.850075e-06
Norm of the params: 12.602882
              Random: fixed   6 labels. Loss 0.04709. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172984
Test loss (w/o reg) on all data: 0.012055029
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7738894e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729844
Test loss (w/o reg) on all data: 0.012054959
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3745755e-07
Norm of the params: 9.153177
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105017185
Train loss (w/o reg) on all data: 0.097360864
Test loss (w/o reg) on all data: 0.046496663
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.6494542e-06
Norm of the params: 12.374427
              Random: fixed   7 labels. Loss 0.04650. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730203
Test loss (w/o reg) on all data: 0.012056799
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0423163e-06
Norm of the params: 9.153139
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021730221
Test loss (w/o reg) on all data: 0.012056567
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.834909e-07
Norm of the params: 9.15314
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09813311
Train loss (w/o reg) on all data: 0.09062971
Test loss (w/o reg) on all data: 0.043849494
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0399239e-05
Norm of the params: 12.25022
              Random: fixed   9 labels. Loss 0.04385. Accuracy 0.996.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1337365
Train loss (w/o reg) on all data: 0.12443457
Test loss (w/o reg) on all data: 0.08933218
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.704214e-05
Norm of the params: 13.639602
Flipped loss: 0.08933. Accuracy: 0.977
### Flips: 104, rs: 14, checks: 52
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024064507
Train loss (w/o reg) on all data: 0.014678703
Test loss (w/o reg) on all data: 0.026032727
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3421394e-06
Norm of the params: 13.7009535
     Influence (LOO): fixed  44 labels. Loss 0.02603. Accuracy 0.996.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014309601
Train loss (w/o reg) on all data: 0.0066179223
Test loss (w/o reg) on all data: 0.03383426
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.762117e-06
Norm of the params: 12.402966
                Loss: fixed  47 labels. Loss 0.03383. Accuracy 0.985.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12854582
Train loss (w/o reg) on all data: 0.119034484
Test loss (w/o reg) on all data: 0.08716358
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8745646e-05
Norm of the params: 13.792269
              Random: fixed   3 labels. Loss 0.08716. Accuracy 0.981.
### Flips: 104, rs: 14, checks: 104
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008839532
Train loss (w/o reg) on all data: 0.0033498898
Test loss (w/o reg) on all data: 0.023644987
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0305247e-07
Norm of the params: 10.478209
     Influence (LOO): fixed  52 labels. Loss 0.02364. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007847015
Train loss (w/o reg) on all data: 0.0030393982
Test loss (w/o reg) on all data: 0.015349605
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8085657e-07
Norm of the params: 9.805729
                Loss: fixed  52 labels. Loss 0.01535. Accuracy 0.989.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123474315
Train loss (w/o reg) on all data: 0.113712974
Test loss (w/o reg) on all data: 0.081892885
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5502705e-05
Norm of the params: 13.97236
              Random: fixed   5 labels. Loss 0.08189. Accuracy 0.981.
### Flips: 104, rs: 14, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172992
Test loss (w/o reg) on all data: 0.012055268
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7169415e-07
Norm of the params: 9.153172
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729912
Test loss (w/o reg) on all data: 0.012055184
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4584406e-07
Norm of the params: 9.1531725
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119989045
Train loss (w/o reg) on all data: 0.11071093
Test loss (w/o reg) on all data: 0.07802541
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.9479921e-05
Norm of the params: 13.62213
              Random: fixed   7 labels. Loss 0.07803. Accuracy 0.981.
### Flips: 104, rs: 14, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728983
Test loss (w/o reg) on all data: 0.0120556075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2285253e-07
Norm of the params: 9.153274
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172901
Test loss (w/o reg) on all data: 0.012055642
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6474592e-07
Norm of the params: 9.153272
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11280659
Train loss (w/o reg) on all data: 0.1032307
Test loss (w/o reg) on all data: 0.082152605
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.102472e-06
Norm of the params: 13.838994
              Random: fixed  10 labels. Loss 0.08215. Accuracy 0.981.
### Flips: 104, rs: 14, checks: 260
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729576
Test loss (w/o reg) on all data: 0.012056134
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8429279e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729597
Test loss (w/o reg) on all data: 0.012055977
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.115889e-07
Norm of the params: 9.153205
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105958365
Train loss (w/o reg) on all data: 0.096238814
Test loss (w/o reg) on all data: 0.07562757
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.425627e-06
Norm of the params: 13.942416
              Random: fixed  14 labels. Loss 0.07563. Accuracy 0.969.
### Flips: 104, rs: 14, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729115
Test loss (w/o reg) on all data: 0.012055302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8122576e-07
Norm of the params: 9.153258
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172912
Test loss (w/o reg) on all data: 0.012055397
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5438728e-07
Norm of the params: 9.153257
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09819245
Train loss (w/o reg) on all data: 0.08848244
Test loss (w/o reg) on all data: 0.074895784
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.1356907e-05
Norm of the params: 13.935575
              Random: fixed  17 labels. Loss 0.07490. Accuracy 0.977.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12628537
Train loss (w/o reg) on all data: 0.11774988
Test loss (w/o reg) on all data: 0.06324538
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0176315e-05
Norm of the params: 13.065601
Flipped loss: 0.06325. Accuracy: 0.992
### Flips: 104, rs: 15, checks: 52
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02779667
Train loss (w/o reg) on all data: 0.019849218
Test loss (w/o reg) on all data: 0.022475656
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6988321e-06
Norm of the params: 12.6075
     Influence (LOO): fixed  40 labels. Loss 0.02248. Accuracy 0.992.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011403561
Train loss (w/o reg) on all data: 0.0047795796
Test loss (w/o reg) on all data: 0.010835999
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8604366e-06
Norm of the params: 11.509979
                Loss: fixed  43 labels. Loss 0.01084. Accuracy 1.000.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11401571
Train loss (w/o reg) on all data: 0.10523281
Test loss (w/o reg) on all data: 0.061127387
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.654552e-05
Norm of the params: 13.253606
              Random: fixed   4 labels. Loss 0.06113. Accuracy 0.985.
### Flips: 104, rs: 15, checks: 104
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730033
Test loss (w/o reg) on all data: 0.012055584
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.633396e-07
Norm of the params: 9.153156
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173004
Test loss (w/o reg) on all data: 0.012055613
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.426413e-07
Norm of the params: 9.153158
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10933198
Train loss (w/o reg) on all data: 0.10047295
Test loss (w/o reg) on all data: 0.059968084
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.835299e-06
Norm of the params: 13.310919
              Random: fixed   7 labels. Loss 0.05997. Accuracy 0.985.
### Flips: 104, rs: 15, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730405
Test loss (w/o reg) on all data: 0.012055572
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.8636723e-07
Norm of the params: 9.153116
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730391
Test loss (w/o reg) on all data: 0.012055432
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3331403e-07
Norm of the params: 9.153118
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09730604
Train loss (w/o reg) on all data: 0.08734888
Test loss (w/o reg) on all data: 0.062343467
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1307646e-05
Norm of the params: 14.111812
              Random: fixed  11 labels. Loss 0.06234. Accuracy 0.981.
### Flips: 104, rs: 15, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728342
Test loss (w/o reg) on all data: 0.012057791
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0940208e-06
Norm of the params: 9.153342
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172843
Test loss (w/o reg) on all data: 0.012057247
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6091035e-07
Norm of the params: 9.153333
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096324995
Train loss (w/o reg) on all data: 0.08649925
Test loss (w/o reg) on all data: 0.058478538
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.0138204e-06
Norm of the params: 14.018379
              Random: fixed  12 labels. Loss 0.05848. Accuracy 0.981.
### Flips: 104, rs: 15, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012055394
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.89408e-07
Norm of the params: 9.153211
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729562
Test loss (w/o reg) on all data: 0.012055308
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.53816e-08
Norm of the params: 9.153211
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08951874
Train loss (w/o reg) on all data: 0.07906017
Test loss (w/o reg) on all data: 0.050145894
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1567478e-05
Norm of the params: 14.462764
              Random: fixed  16 labels. Loss 0.05015. Accuracy 0.989.
### Flips: 104, rs: 15, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729586
Test loss (w/o reg) on all data: 0.012055397
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.985096e-08
Norm of the params: 9.153207
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729597
Test loss (w/o reg) on all data: 0.012055423
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6649294e-07
Norm of the params: 9.153207
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086577736
Train loss (w/o reg) on all data: 0.07620928
Test loss (w/o reg) on all data: 0.04724526
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.785801e-06
Norm of the params: 14.400322
              Random: fixed  17 labels. Loss 0.04725. Accuracy 0.985.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13543591
Train loss (w/o reg) on all data: 0.12680228
Test loss (w/o reg) on all data: 0.06940352
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0363684e-05
Norm of the params: 13.140498
Flipped loss: 0.06940. Accuracy: 0.989
### Flips: 104, rs: 16, checks: 52
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023224663
Train loss (w/o reg) on all data: 0.016694857
Test loss (w/o reg) on all data: 0.014290042
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.245999e-06
Norm of the params: 11.427866
     Influence (LOO): fixed  41 labels. Loss 0.01429. Accuracy 0.996.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730568
Test loss (w/o reg) on all data: 0.01205614
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5179272e-07
Norm of the params: 9.153099
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13053982
Train loss (w/o reg) on all data: 0.121581584
Test loss (w/o reg) on all data: 0.06646057
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.5286335e-06
Norm of the params: 13.385244
              Random: fixed   2 labels. Loss 0.06646. Accuracy 0.989.
### Flips: 104, rs: 16, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729046
Test loss (w/o reg) on all data: 0.012054896
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.579189e-07
Norm of the params: 9.153266
     Influence (LOO): fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729062
Test loss (w/o reg) on all data: 0.012054847
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4887075e-07
Norm of the params: 9.153265
                Loss: fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12098209
Train loss (w/o reg) on all data: 0.11157984
Test loss (w/o reg) on all data: 0.061301764
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.663236e-06
Norm of the params: 13.712946
              Random: fixed   6 labels. Loss 0.06130. Accuracy 0.989.
### Flips: 104, rs: 16, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729092
Test loss (w/o reg) on all data: 0.012055225
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1211778e-07
Norm of the params: 9.153261
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729087
Test loss (w/o reg) on all data: 0.012055287
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3569584e-07
Norm of the params: 9.15326
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11463094
Train loss (w/o reg) on all data: 0.10581496
Test loss (w/o reg) on all data: 0.05578396
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4456918e-05
Norm of the params: 13.278533
              Random: fixed   9 labels. Loss 0.05578. Accuracy 0.989.
### Flips: 104, rs: 16, checks: 208
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620214
Train loss (w/o reg) on all data: 0.0021730107
Test loss (w/o reg) on all data: 0.0120552825
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.583077e-07
Norm of the params: 9.153153
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730077
Test loss (w/o reg) on all data: 0.012055153
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.13515156e-07
Norm of the params: 9.153153
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10795151
Train loss (w/o reg) on all data: 0.09835721
Test loss (w/o reg) on all data: 0.051213272
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3278792e-05
Norm of the params: 13.852291
              Random: fixed  12 labels. Loss 0.05121. Accuracy 0.989.
### Flips: 104, rs: 16, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729942
Test loss (w/o reg) on all data: 0.012055872
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2714984e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012055717
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1798973e-07
Norm of the params: 9.153168
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103048176
Train loss (w/o reg) on all data: 0.09308237
Test loss (w/o reg) on all data: 0.049215287
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3732384e-05
Norm of the params: 14.11794
              Random: fixed  14 labels. Loss 0.04922. Accuracy 0.989.
### Flips: 104, rs: 16, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730233
Test loss (w/o reg) on all data: 0.012056246
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8816893e-07
Norm of the params: 9.153135
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730233
Test loss (w/o reg) on all data: 0.0120562995
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.647381e-07
Norm of the params: 9.153137
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093945816
Train loss (w/o reg) on all data: 0.08288765
Test loss (w/o reg) on all data: 0.049615163
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4124507e-05
Norm of the params: 14.871561
              Random: fixed  18 labels. Loss 0.04962. Accuracy 0.989.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14674772
Train loss (w/o reg) on all data: 0.13723277
Test loss (w/o reg) on all data: 0.09526653
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6672904e-05
Norm of the params: 13.7949
Flipped loss: 0.09527. Accuracy: 0.977
### Flips: 104, rs: 17, checks: 52
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05305353
Train loss (w/o reg) on all data: 0.04141391
Test loss (w/o reg) on all data: 0.045018416
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.213224e-06
Norm of the params: 15.257537
     Influence (LOO): fixed  39 labels. Loss 0.04502. Accuracy 0.989.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019311827
Train loss (w/o reg) on all data: 0.009761759
Test loss (w/o reg) on all data: 0.06607519
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.380458e-07
Norm of the params: 13.820324
                Loss: fixed  49 labels. Loss 0.06608. Accuracy 0.981.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14454702
Train loss (w/o reg) on all data: 0.13465162
Test loss (w/o reg) on all data: 0.096515514
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.800825e-06
Norm of the params: 14.067972
              Random: fixed   1 labels. Loss 0.09652. Accuracy 0.977.
### Flips: 104, rs: 17, checks: 104
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00902817
Train loss (w/o reg) on all data: 0.0033739842
Test loss (w/o reg) on all data: 0.027832687
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.197213e-07
Norm of the params: 10.634083
     Influence (LOO): fixed  56 labels. Loss 0.02783. Accuracy 0.989.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008839531
Train loss (w/o reg) on all data: 0.0033499557
Test loss (w/o reg) on all data: 0.023645695
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.736238e-07
Norm of the params: 10.478144
                Loss: fixed  57 labels. Loss 0.02365. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14327581
Train loss (w/o reg) on all data: 0.1335734
Test loss (w/o reg) on all data: 0.09368165
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.6434127e-06
Norm of the params: 13.930125
              Random: fixed   2 labels. Loss 0.09368. Accuracy 0.977.
### Flips: 104, rs: 17, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729667
Test loss (w/o reg) on all data: 0.012055457
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.308755e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172967
Test loss (w/o reg) on all data: 0.012055315
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4229896e-07
Norm of the params: 9.153197
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13528998
Train loss (w/o reg) on all data: 0.1260454
Test loss (w/o reg) on all data: 0.08878631
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.460406e-06
Norm of the params: 13.597482
              Random: fixed   7 labels. Loss 0.08879. Accuracy 0.981.
### Flips: 104, rs: 17, checks: 208
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172994
Test loss (w/o reg) on all data: 0.012055636
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3211591e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729944
Test loss (w/o reg) on all data: 0.012055674
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6471068e-07
Norm of the params: 9.153168
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13232039
Train loss (w/o reg) on all data: 0.123111404
Test loss (w/o reg) on all data: 0.08930144
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4325247e-05
Norm of the params: 13.571278
              Random: fixed   8 labels. Loss 0.08930. Accuracy 0.981.
### Flips: 104, rs: 17, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729772
Test loss (w/o reg) on all data: 0.012055701
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.1215266e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172975
Test loss (w/o reg) on all data: 0.012055576
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0374208e-07
Norm of the params: 9.153188
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1248653
Train loss (w/o reg) on all data: 0.11519889
Test loss (w/o reg) on all data: 0.08970404
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8608763e-05
Norm of the params: 13.904256
              Random: fixed  11 labels. Loss 0.08970. Accuracy 0.981.
### Flips: 104, rs: 17, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729872
Test loss (w/o reg) on all data: 0.012055719
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8765192e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172986
Test loss (w/o reg) on all data: 0.012055461
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.6890805e-07
Norm of the params: 9.153176
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117351405
Train loss (w/o reg) on all data: 0.10833144
Test loss (w/o reg) on all data: 0.085797995
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.3586914e-06
Norm of the params: 13.431279
              Random: fixed  16 labels. Loss 0.08580. Accuracy 0.981.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12506832
Train loss (w/o reg) on all data: 0.11533173
Test loss (w/o reg) on all data: 0.06427721
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0535535e-05
Norm of the params: 13.954636
Flipped loss: 0.06428. Accuracy: 0.989
### Flips: 104, rs: 18, checks: 52
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01898186
Train loss (w/o reg) on all data: 0.011007931
Test loss (w/o reg) on all data: 0.01605925
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0590342e-06
Norm of the params: 12.628483
     Influence (LOO): fixed  40 labels. Loss 0.01606. Accuracy 0.996.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012058621
Train loss (w/o reg) on all data: 0.0053688097
Test loss (w/o reg) on all data: 0.009086571
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.8278646e-07
Norm of the params: 11.567032
                Loss: fixed  44 labels. Loss 0.00909. Accuracy 0.996.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12166388
Train loss (w/o reg) on all data: 0.11195956
Test loss (w/o reg) on all data: 0.062557906
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.757149e-06
Norm of the params: 13.931489
              Random: fixed   2 labels. Loss 0.06256. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 104
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008034244
Train loss (w/o reg) on all data: 0.003051449
Test loss (w/o reg) on all data: 0.008115847
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.7341543e-07
Norm of the params: 9.98278
     Influence (LOO): fixed  46 labels. Loss 0.00812. Accuracy 0.996.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173014
Test loss (w/o reg) on all data: 0.012055166
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.422975e-08
Norm of the params: 9.153147
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1216639
Train loss (w/o reg) on all data: 0.11196381
Test loss (w/o reg) on all data: 0.06254834
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6952783e-05
Norm of the params: 13.9284525
              Random: fixed   2 labels. Loss 0.06255. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729243
Test loss (w/o reg) on all data: 0.01205464
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.634306e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729264
Test loss (w/o reg) on all data: 0.012054839
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.388726e-07
Norm of the params: 9.153244
                Loss: fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11620465
Train loss (w/o reg) on all data: 0.106977694
Test loss (w/o reg) on all data: 0.060196657
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7054e-05
Norm of the params: 13.584517
              Random: fixed   4 labels. Loss 0.06020. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730238
Test loss (w/o reg) on all data: 0.012055645
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9423592e-07
Norm of the params: 9.153136
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730212
Test loss (w/o reg) on all data: 0.012055676
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2213233e-07
Norm of the params: 9.153138
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1140309
Train loss (w/o reg) on all data: 0.10495518
Test loss (w/o reg) on all data: 0.060496755
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.237614e-05
Norm of the params: 13.472724
              Random: fixed   5 labels. Loss 0.06050. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 260
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.01205503
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1309995e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729919
Test loss (w/o reg) on all data: 0.012055092
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.247836e-07
Norm of the params: 9.15317
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106450096
Train loss (w/o reg) on all data: 0.097573735
Test loss (w/o reg) on all data: 0.055768706
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9763292e-05
Norm of the params: 13.3239355
              Random: fixed  10 labels. Loss 0.05577. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729209
Test loss (w/o reg) on all data: 0.012055099
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5515232e-07
Norm of the params: 9.15325
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729213
Test loss (w/o reg) on all data: 0.012055169
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7846129e-07
Norm of the params: 9.153249
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104591906
Train loss (w/o reg) on all data: 0.09576196
Test loss (w/o reg) on all data: 0.05531602
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.8833986e-06
Norm of the params: 13.289053
              Random: fixed  11 labels. Loss 0.05532. Accuracy 0.989.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11767772
Train loss (w/o reg) on all data: 0.10817391
Test loss (w/o reg) on all data: 0.057848044
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9879519e-05
Norm of the params: 13.786812
Flipped loss: 0.05785. Accuracy: 0.989
### Flips: 104, rs: 19, checks: 52
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014649479
Train loss (w/o reg) on all data: 0.008820311
Test loss (w/o reg) on all data: 0.014596619
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3433978e-07
Norm of the params: 10.797377
     Influence (LOO): fixed  37 labels. Loss 0.01460. Accuracy 0.996.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008973834
Train loss (w/o reg) on all data: 0.0033668282
Test loss (w/o reg) on all data: 0.0131901065
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.3270115e-07
Norm of the params: 10.5896225
                Loss: fixed  39 labels. Loss 0.01319. Accuracy 0.996.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115819775
Train loss (w/o reg) on all data: 0.10644714
Test loss (w/o reg) on all data: 0.058826078
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.558919e-06
Norm of the params: 13.691338
              Random: fixed   1 labels. Loss 0.05883. Accuracy 0.989.
### Flips: 104, rs: 19, checks: 104
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730629
Test loss (w/o reg) on all data: 0.012054799
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.298615e-07
Norm of the params: 9.153092
     Influence (LOO): fixed  40 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008973836
Train loss (w/o reg) on all data: 0.0033665062
Test loss (w/o reg) on all data: 0.013190749
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.1542638e-07
Norm of the params: 10.58993
                Loss: fixed  39 labels. Loss 0.01319. Accuracy 0.996.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11299625
Train loss (w/o reg) on all data: 0.10363835
Test loss (w/o reg) on all data: 0.056662194
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8035895e-06
Norm of the params: 13.680572
              Random: fixed   3 labels. Loss 0.05666. Accuracy 0.989.
### Flips: 104, rs: 19, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002173018
Test loss (w/o reg) on all data: 0.012055751
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9794014e-07
Norm of the params: 9.15314
     Influence (LOO): fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008973834
Train loss (w/o reg) on all data: 0.003366663
Test loss (w/o reg) on all data: 0.013190858
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3495991e-07
Norm of the params: 10.589779
                Loss: fixed  39 labels. Loss 0.01319. Accuracy 0.996.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105560035
Train loss (w/o reg) on all data: 0.09650854
Test loss (w/o reg) on all data: 0.059908334
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.731556e-06
Norm of the params: 13.454735
              Random: fixed   7 labels. Loss 0.05991. Accuracy 0.989.
### Flips: 104, rs: 19, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730158
Test loss (w/o reg) on all data: 0.012056029
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.082866e-07
Norm of the params: 9.153144
     Influence (LOO): fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730154
Test loss (w/o reg) on all data: 0.01205587
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8737347e-07
Norm of the params: 9.153145
                Loss: fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09757053
Train loss (w/o reg) on all data: 0.08808465
Test loss (w/o reg) on all data: 0.0525143
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.369825e-06
Norm of the params: 13.773799
              Random: fixed  10 labels. Loss 0.05251. Accuracy 0.989.
### Flips: 104, rs: 19, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728757
Test loss (w/o reg) on all data: 0.0120556485
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.96243e-07
Norm of the params: 9.153298
     Influence (LOO): fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728752
Test loss (w/o reg) on all data: 0.012055816
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3327782e-07
Norm of the params: 9.153297
                Loss: fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08926335
Train loss (w/o reg) on all data: 0.0802668
Test loss (w/o reg) on all data: 0.04939875
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3708794e-05
Norm of the params: 13.413831
              Random: fixed  14 labels. Loss 0.04940. Accuracy 0.989.
### Flips: 104, rs: 19, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172972
Test loss (w/o reg) on all data: 0.012055995
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6815699e-07
Norm of the params: 9.153191
     Influence (LOO): fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012055922
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0198696e-07
Norm of the params: 9.153192
                Loss: fixed  40 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08065119
Train loss (w/o reg) on all data: 0.07221133
Test loss (w/o reg) on all data: 0.047445506
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.502801e-06
Norm of the params: 12.992193
              Random: fixed  17 labels. Loss 0.04745. Accuracy 0.989.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14146331
Train loss (w/o reg) on all data: 0.13264254
Test loss (w/o reg) on all data: 0.07077769
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.16184e-05
Norm of the params: 13.282146
Flipped loss: 0.07078. Accuracy: 0.992
### Flips: 104, rs: 20, checks: 52
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03958009
Train loss (w/o reg) on all data: 0.032520466
Test loss (w/o reg) on all data: 0.021836754
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.783003e-06
Norm of the params: 11.882447
     Influence (LOO): fixed  41 labels. Loss 0.02184. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0132966675
Train loss (w/o reg) on all data: 0.0060032057
Test loss (w/o reg) on all data: 0.0167713
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3767786e-06
Norm of the params: 12.077633
                Loss: fixed  50 labels. Loss 0.01677. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13442047
Train loss (w/o reg) on all data: 0.12566547
Test loss (w/o reg) on all data: 0.06372833
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0253229e-05
Norm of the params: 13.232537
              Random: fixed   4 labels. Loss 0.06373. Accuracy 0.992.
### Flips: 104, rs: 20, checks: 104
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01122447
Train loss (w/o reg) on all data: 0.0062451167
Test loss (w/o reg) on all data: 0.012817221
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.6503095e-07
Norm of the params: 9.979332
     Influence (LOO): fixed  52 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729008
Test loss (w/o reg) on all data: 0.012055332
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7709262e-07
Norm of the params: 9.153269
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13442047
Train loss (w/o reg) on all data: 0.12567003
Test loss (w/o reg) on all data: 0.06373092
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1042298e-05
Norm of the params: 13.229091
              Random: fixed   4 labels. Loss 0.06373. Accuracy 0.992.
### Flips: 104, rs: 20, checks: 156
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729232
Test loss (w/o reg) on all data: 0.012054712
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4657115e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172926
Test loss (w/o reg) on all data: 0.0120547945
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.748441e-07
Norm of the params: 9.153243
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13266288
Train loss (w/o reg) on all data: 0.12400943
Test loss (w/o reg) on all data: 0.062441323
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0717724e-05
Norm of the params: 13.155567
              Random: fixed   5 labels. Loss 0.06244. Accuracy 0.992.
### Flips: 104, rs: 20, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729136
Test loss (w/o reg) on all data: 0.012055323
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4569553e-07
Norm of the params: 9.153257
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172914
Test loss (w/o reg) on all data: 0.012055392
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5969532e-07
Norm of the params: 9.153256
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12765059
Train loss (w/o reg) on all data: 0.11873687
Test loss (w/o reg) on all data: 0.06090218
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1573879e-05
Norm of the params: 13.351938
              Random: fixed   7 labels. Loss 0.06090. Accuracy 0.996.
### Flips: 104, rs: 20, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729427
Test loss (w/o reg) on all data: 0.012055299
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7335161e-07
Norm of the params: 9.153223
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729486
Test loss (w/o reg) on all data: 0.012055104
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.06165e-07
Norm of the params: 9.153218
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122778915
Train loss (w/o reg) on all data: 0.113695875
Test loss (w/o reg) on all data: 0.05930941
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0491202e-05
Norm of the params: 13.478161
              Random: fixed   9 labels. Loss 0.05931. Accuracy 0.996.
### Flips: 104, rs: 20, checks: 312
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172935
Test loss (w/o reg) on all data: 0.012054829
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.262225e-07
Norm of the params: 9.153233
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729374
Test loss (w/o reg) on all data: 0.012054782
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2360487e-07
Norm of the params: 9.15323
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117628664
Train loss (w/o reg) on all data: 0.10856195
Test loss (w/o reg) on all data: 0.057682354
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4625296e-05
Norm of the params: 13.466045
              Random: fixed  12 labels. Loss 0.05768. Accuracy 0.996.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13253565
Train loss (w/o reg) on all data: 0.12499675
Test loss (w/o reg) on all data: 0.07341222
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.699113e-06
Norm of the params: 12.279166
Flipped loss: 0.07341. Accuracy: 0.992
### Flips: 104, rs: 21, checks: 52
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027249072
Train loss (w/o reg) on all data: 0.019016536
Test loss (w/o reg) on all data: 0.025130617
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7582553e-06
Norm of the params: 12.831628
     Influence (LOO): fixed  40 labels. Loss 0.02513. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007547393
Train loss (w/o reg) on all data: 0.002901888
Test loss (w/o reg) on all data: 0.014047075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4735474e-07
Norm of the params: 9.6389885
                Loss: fixed  46 labels. Loss 0.01405. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12422011
Train loss (w/o reg) on all data: 0.1164701
Test loss (w/o reg) on all data: 0.06772639
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.626197e-05
Norm of the params: 12.449909
              Random: fixed   3 labels. Loss 0.06773. Accuracy 0.992.
### Flips: 104, rs: 21, checks: 104
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729448
Test loss (w/o reg) on all data: 0.012055029
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0125366e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729462
Test loss (w/o reg) on all data: 0.012055059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2801625e-07
Norm of the params: 9.153221
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108992085
Train loss (w/o reg) on all data: 0.101310834
Test loss (w/o reg) on all data: 0.05887163
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.721009e-06
Norm of the params: 12.39456
              Random: fixed   9 labels. Loss 0.05887. Accuracy 0.989.
### Flips: 104, rs: 21, checks: 156
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.0120552685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0204446e-07
Norm of the params: 9.153187
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729707
Test loss (w/o reg) on all data: 0.012055313
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6576602e-07
Norm of the params: 9.153192
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10550344
Train loss (w/o reg) on all data: 0.097902395
Test loss (w/o reg) on all data: 0.06024561
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.812765e-06
Norm of the params: 12.329676
              Random: fixed  11 labels. Loss 0.06025. Accuracy 0.989.
### Flips: 104, rs: 21, checks: 208
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729872
Test loss (w/o reg) on all data: 0.012055228
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8567644e-07
Norm of the params: 9.153175
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729865
Test loss (w/o reg) on all data: 0.012055291
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3511701e-07
Norm of the params: 9.153177
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09829446
Train loss (w/o reg) on all data: 0.09071676
Test loss (w/o reg) on all data: 0.054781526
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.75273e-06
Norm of the params: 12.310729
              Random: fixed  14 labels. Loss 0.05478. Accuracy 0.989.
### Flips: 104, rs: 21, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730016
Test loss (w/o reg) on all data: 0.0120552955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.48581e-07
Norm of the params: 9.15316
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173001
Test loss (w/o reg) on all data: 0.012055368
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3263715e-07
Norm of the params: 9.153161
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09002436
Train loss (w/o reg) on all data: 0.08284898
Test loss (w/o reg) on all data: 0.04892008
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.7094227e-06
Norm of the params: 11.979464
              Random: fixed  18 labels. Loss 0.04892. Accuracy 0.989.
### Flips: 104, rs: 21, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729039
Test loss (w/o reg) on all data: 0.012055394
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2474412e-07
Norm of the params: 9.153267
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729232
Test loss (w/o reg) on all data: 0.012055394
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.755488e-07
Norm of the params: 9.153244
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08755436
Train loss (w/o reg) on all data: 0.08131185
Test loss (w/o reg) on all data: 0.04161714
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.814288e-06
Norm of the params: 11.173636
              Random: fixed  20 labels. Loss 0.04162. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13846017
Train loss (w/o reg) on all data: 0.12964492
Test loss (w/o reg) on all data: 0.084580824
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4070676e-05
Norm of the params: 13.277999
Flipped loss: 0.08458. Accuracy: 0.989
### Flips: 104, rs: 22, checks: 52
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040309317
Train loss (w/o reg) on all data: 0.0312607
Test loss (w/o reg) on all data: 0.045367405
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.172648e-06
Norm of the params: 13.452599
     Influence (LOO): fixed  38 labels. Loss 0.04537. Accuracy 0.973.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016966853
Train loss (w/o reg) on all data: 0.008050504
Test loss (w/o reg) on all data: 0.02599765
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3220736e-06
Norm of the params: 13.353912
                Loss: fixed  48 labels. Loss 0.02600. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13780583
Train loss (w/o reg) on all data: 0.12903187
Test loss (w/o reg) on all data: 0.082881294
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.229465e-05
Norm of the params: 13.246863
              Random: fixed   1 labels. Loss 0.08288. Accuracy 0.989.
### Flips: 104, rs: 22, checks: 104
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00908159
Train loss (w/o reg) on all data: 0.003667622
Test loss (w/o reg) on all data: 0.011725406
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.223798e-07
Norm of the params: 10.405737
     Influence (LOO): fixed  54 labels. Loss 0.01173. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077833836
Train loss (w/o reg) on all data: 0.0028981725
Test loss (w/o reg) on all data: 0.012001557
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1582422e-07
Norm of the params: 9.884544
                Loss: fixed  54 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12725942
Train loss (w/o reg) on all data: 0.11819402
Test loss (w/o reg) on all data: 0.07394552
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.5903627e-05
Norm of the params: 13.465059
              Random: fixed   6 labels. Loss 0.07395. Accuracy 0.989.
### Flips: 104, rs: 22, checks: 156
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729788
Test loss (w/o reg) on all data: 0.012055513
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6527155e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172979
Test loss (w/o reg) on all data: 0.012055778
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2979424e-07
Norm of the params: 9.153184
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119267836
Train loss (w/o reg) on all data: 0.10997014
Test loss (w/o reg) on all data: 0.07115156
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6848906e-05
Norm of the params: 13.6364975
              Random: fixed   8 labels. Loss 0.07115. Accuracy 0.989.
### Flips: 104, rs: 22, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730028
Test loss (w/o reg) on all data: 0.0120551605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2746126e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730026
Test loss (w/o reg) on all data: 0.012055129
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.7728666e-08
Norm of the params: 9.153158
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11360186
Train loss (w/o reg) on all data: 0.1040608
Test loss (w/o reg) on all data: 0.066784576
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9292083e-05
Norm of the params: 13.813806
              Random: fixed  12 labels. Loss 0.06678. Accuracy 0.985.
### Flips: 104, rs: 22, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172937
Test loss (w/o reg) on all data: 0.012054872
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4061365e-07
Norm of the params: 9.153231
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172952
Test loss (w/o reg) on all data: 0.012055222
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9168145e-07
Norm of the params: 9.153214
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11042006
Train loss (w/o reg) on all data: 0.1008262
Test loss (w/o reg) on all data: 0.0651568
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2423714e-05
Norm of the params: 13.851977
              Random: fixed  14 labels. Loss 0.06516. Accuracy 0.989.
### Flips: 104, rs: 22, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728885
Test loss (w/o reg) on all data: 0.012055741
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.512162e-07
Norm of the params: 9.153283
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021728873
Test loss (w/o reg) on all data: 0.012055819
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7517315e-07
Norm of the params: 9.153282
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11103687
Train loss (w/o reg) on all data: 0.10165446
Test loss (w/o reg) on all data: 0.063740894
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9700997e-05
Norm of the params: 13.698468
              Random: fixed  15 labels. Loss 0.06374. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13234998
Train loss (w/o reg) on all data: 0.12338388
Test loss (w/o reg) on all data: 0.06909134
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0520035e-05
Norm of the params: 13.391114
Flipped loss: 0.06909. Accuracy: 0.989
### Flips: 104, rs: 23, checks: 52
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034344815
Train loss (w/o reg) on all data: 0.024009777
Test loss (w/o reg) on all data: 0.027181681
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.292522e-06
Norm of the params: 14.377091
     Influence (LOO): fixed  40 labels. Loss 0.02718. Accuracy 0.985.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01997848
Train loss (w/o reg) on all data: 0.010271278
Test loss (w/o reg) on all data: 0.018626872
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3488215e-06
Norm of the params: 13.933558
                Loss: fixed  45 labels. Loss 0.01863. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12123775
Train loss (w/o reg) on all data: 0.11273756
Test loss (w/o reg) on all data: 0.06362668
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1805832e-05
Norm of the params: 13.038547
              Random: fixed   5 labels. Loss 0.06363. Accuracy 0.985.
### Flips: 104, rs: 23, checks: 104
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014170965
Train loss (w/o reg) on all data: 0.007280176
Test loss (w/o reg) on all data: 0.008346022
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.906625e-07
Norm of the params: 11.739496
     Influence (LOO): fixed  52 labels. Loss 0.00835. Accuracy 0.996.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00873954
Train loss (w/o reg) on all data: 0.0032572465
Test loss (w/o reg) on all data: 0.009238488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.256477e-07
Norm of the params: 10.471192
                Loss: fixed  53 labels. Loss 0.00924. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12063603
Train loss (w/o reg) on all data: 0.11239927
Test loss (w/o reg) on all data: 0.06141715
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1531156e-05
Norm of the params: 12.834918
              Random: fixed   7 labels. Loss 0.06142. Accuracy 0.985.
### Flips: 104, rs: 23, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007623194
Train loss (w/o reg) on all data: 0.0027304643
Test loss (w/o reg) on all data: 0.009578233
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7298877e-07
Norm of the params: 9.892148
     Influence (LOO): fixed  55 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0087395385
Train loss (w/o reg) on all data: 0.0032571051
Test loss (w/o reg) on all data: 0.00923755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6966265e-07
Norm of the params: 10.471326
                Loss: fixed  53 labels. Loss 0.00924. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119160995
Train loss (w/o reg) on all data: 0.11099295
Test loss (w/o reg) on all data: 0.061152596
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.5351052e-06
Norm of the params: 12.781272
              Random: fixed   8 labels. Loss 0.06115. Accuracy 0.985.
### Flips: 104, rs: 23, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730803
Test loss (w/o reg) on all data: 0.012055825
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5351517e-07
Norm of the params: 9.153073
     Influence (LOO): fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076231956
Train loss (w/o reg) on all data: 0.0027304492
Test loss (w/o reg) on all data: 0.009577595
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7269505e-07
Norm of the params: 9.892165
                Loss: fixed  55 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11037027
Train loss (w/o reg) on all data: 0.102582216
Test loss (w/o reg) on all data: 0.05255332
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.522633e-06
Norm of the params: 12.480427
              Random: fixed  15 labels. Loss 0.05255. Accuracy 0.985.
### Flips: 104, rs: 23, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217302
Test loss (w/o reg) on all data: 0.012055645
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9476766e-07
Norm of the params: 9.153139
     Influence (LOO): fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076231956
Train loss (w/o reg) on all data: 0.0027303586
Test loss (w/o reg) on all data: 0.009576835
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.422456e-07
Norm of the params: 9.892257
                Loss: fixed  55 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10395788
Train loss (w/o reg) on all data: 0.09574571
Test loss (w/o reg) on all data: 0.043907963
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.120176e-05
Norm of the params: 12.815742
              Random: fixed  18 labels. Loss 0.04391. Accuracy 0.989.
### Flips: 104, rs: 23, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729257
Test loss (w/o reg) on all data: 0.012054907
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8354797e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729276
Test loss (w/o reg) on all data: 0.012054984
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3118768e-07
Norm of the params: 9.153243
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10126226
Train loss (w/o reg) on all data: 0.0930364
Test loss (w/o reg) on all data: 0.04315715
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.9260134e-06
Norm of the params: 12.826428
              Random: fixed  19 labels. Loss 0.04316. Accuracy 0.989.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13817264
Train loss (w/o reg) on all data: 0.12973626
Test loss (w/o reg) on all data: 0.08455616
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.969298e-06
Norm of the params: 12.989519
Flipped loss: 0.08456. Accuracy: 0.985
### Flips: 104, rs: 24, checks: 52
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050856665
Train loss (w/o reg) on all data: 0.042149387
Test loss (w/o reg) on all data: 0.040283423
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.5416189e-06
Norm of the params: 13.196423
     Influence (LOO): fixed  36 labels. Loss 0.04028. Accuracy 0.981.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013151582
Train loss (w/o reg) on all data: 0.006112109
Test loss (w/o reg) on all data: 0.02373328
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6599923e-06
Norm of the params: 11.865474
                Loss: fixed  48 labels. Loss 0.02373. Accuracy 0.992.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13268788
Train loss (w/o reg) on all data: 0.12428639
Test loss (w/o reg) on all data: 0.077530205
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.117422e-06
Norm of the params: 12.962629
              Random: fixed   3 labels. Loss 0.07753. Accuracy 0.985.
### Flips: 104, rs: 24, checks: 104
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01363422
Train loss (w/o reg) on all data: 0.0074069337
Test loss (w/o reg) on all data: 0.010800884
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3849916e-06
Norm of the params: 11.160005
     Influence (LOO): fixed  51 labels. Loss 0.01080. Accuracy 0.996.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729576
Test loss (w/o reg) on all data: 0.012054738
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0537318e-07
Norm of the params: 9.153209
                Loss: fixed  54 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13184172
Train loss (w/o reg) on all data: 0.123668276
Test loss (w/o reg) on all data: 0.07689622
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.819753e-06
Norm of the params: 12.785493
              Random: fixed   4 labels. Loss 0.07690. Accuracy 0.985.
### Flips: 104, rs: 24, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731032
Test loss (w/o reg) on all data: 0.012055765
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1107354e-07
Norm of the params: 9.1530485
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729937
Test loss (w/o reg) on all data: 0.012056008
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6213404e-07
Norm of the params: 9.15317
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13016377
Train loss (w/o reg) on all data: 0.12211551
Test loss (w/o reg) on all data: 0.075745665
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8265504e-05
Norm of the params: 12.687209
              Random: fixed   5 labels. Loss 0.07575. Accuracy 0.985.
### Flips: 104, rs: 24, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729737
Test loss (w/o reg) on all data: 0.012055607
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.300649e-07
Norm of the params: 9.153191
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172974
Test loss (w/o reg) on all data: 0.012055497
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3529757e-07
Norm of the params: 9.153191
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1248628
Train loss (w/o reg) on all data: 0.11651867
Test loss (w/o reg) on all data: 0.074272044
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.85394e-06
Norm of the params: 12.918304
              Random: fixed   7 labels. Loss 0.07427. Accuracy 0.992.
### Flips: 104, rs: 24, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729853
Test loss (w/o reg) on all data: 0.012055692
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5930205e-07
Norm of the params: 9.153176
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729844
Test loss (w/o reg) on all data: 0.01205575
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1065446e-07
Norm of the params: 9.153178
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12170122
Train loss (w/o reg) on all data: 0.113357194
Test loss (w/o reg) on all data: 0.07418096
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.612043e-06
Norm of the params: 12.918225
              Random: fixed   9 labels. Loss 0.07418. Accuracy 0.992.
### Flips: 104, rs: 24, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729632
Test loss (w/o reg) on all data: 0.012055226
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8679163e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729649
Test loss (w/o reg) on all data: 0.012055154
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4933797e-07
Norm of the params: 9.153201
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12077699
Train loss (w/o reg) on all data: 0.11245945
Test loss (w/o reg) on all data: 0.07361586
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.816699e-05
Norm of the params: 12.897703
              Random: fixed  10 labels. Loss 0.07362. Accuracy 0.989.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13880987
Train loss (w/o reg) on all data: 0.13013744
Test loss (w/o reg) on all data: 0.06986712
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.495736e-05
Norm of the params: 13.1699915
Flipped loss: 0.06987. Accuracy: 0.989
### Flips: 104, rs: 25, checks: 52
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034547605
Train loss (w/o reg) on all data: 0.026618935
Test loss (w/o reg) on all data: 0.025286118
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6888914e-06
Norm of the params: 12.592593
     Influence (LOO): fixed  44 labels. Loss 0.02529. Accuracy 0.989.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013811536
Train loss (w/o reg) on all data: 0.0066894544
Test loss (w/o reg) on all data: 0.0179359
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.031664e-07
Norm of the params: 11.934892
                Loss: fixed  49 labels. Loss 0.01794. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13536821
Train loss (w/o reg) on all data: 0.1262871
Test loss (w/o reg) on all data: 0.06763796
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5456584e-05
Norm of the params: 13.476727
              Random: fixed   2 labels. Loss 0.06764. Accuracy 0.985.
### Flips: 104, rs: 25, checks: 104
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011929337
Train loss (w/o reg) on all data: 0.005835956
Test loss (w/o reg) on all data: 0.019533847
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.736996e-07
Norm of the params: 11.039368
     Influence (LOO): fixed  53 labels. Loss 0.01953. Accuracy 0.989.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729192
Test loss (w/o reg) on all data: 0.012055285
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3885872e-07
Norm of the params: 9.153249
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12940593
Train loss (w/o reg) on all data: 0.12024717
Test loss (w/o reg) on all data: 0.06651583
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.607481e-05
Norm of the params: 13.534223
              Random: fixed   5 labels. Loss 0.06652. Accuracy 0.985.
### Flips: 104, rs: 25, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.012054701
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0275535e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729516
Test loss (w/o reg) on all data: 0.012054632
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2456898e-07
Norm of the params: 9.153216
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12271401
Train loss (w/o reg) on all data: 0.11357088
Test loss (w/o reg) on all data: 0.060634095
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.800209e-05
Norm of the params: 13.5226755
              Random: fixed   9 labels. Loss 0.06063. Accuracy 0.985.
### Flips: 104, rs: 25, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172906
Test loss (w/o reg) on all data: 0.012054731
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2198955e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729085
Test loss (w/o reg) on all data: 0.012054664
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8458143e-07
Norm of the params: 9.153261
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11716121
Train loss (w/o reg) on all data: 0.10751593
Test loss (w/o reg) on all data: 0.05306604
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2105465e-05
Norm of the params: 13.889042
              Random: fixed  11 labels. Loss 0.05307. Accuracy 0.989.
### Flips: 104, rs: 25, checks: 260
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729853
Test loss (w/o reg) on all data: 0.012054565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4235081e-07
Norm of the params: 9.153178
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729853
Test loss (w/o reg) on all data: 0.012054552
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8092491e-07
Norm of the params: 9.153178
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09506188
Train loss (w/o reg) on all data: 0.08432396
Test loss (w/o reg) on all data: 0.050578415
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.301714e-06
Norm of the params: 14.65464
              Random: fixed  19 labels. Loss 0.05058. Accuracy 0.985.
### Flips: 104, rs: 25, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730112
Test loss (w/o reg) on all data: 0.012055579
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2433329e-07
Norm of the params: 9.15315
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730098
Test loss (w/o reg) on all data: 0.012055479
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5271916e-07
Norm of the params: 9.153151
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089846045
Train loss (w/o reg) on all data: 0.078966156
Test loss (w/o reg) on all data: 0.05080371
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3574435e-06
Norm of the params: 14.751197
              Random: fixed  21 labels. Loss 0.05080. Accuracy 0.985.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14429182
Train loss (w/o reg) on all data: 0.13600457
Test loss (w/o reg) on all data: 0.07084016
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.5261641e-05
Norm of the params: 12.874199
Flipped loss: 0.07084. Accuracy: 0.981
### Flips: 104, rs: 26, checks: 52
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04056885
Train loss (w/o reg) on all data: 0.032422446
Test loss (w/o reg) on all data: 0.022054095
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6824476e-06
Norm of the params: 12.764328
     Influence (LOO): fixed  42 labels. Loss 0.02205. Accuracy 0.992.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012880968
Train loss (w/o reg) on all data: 0.0059235943
Test loss (w/o reg) on all data: 0.014363009
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.1170316e-07
Norm of the params: 11.796079
                Loss: fixed  52 labels. Loss 0.01436. Accuracy 0.996.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13966833
Train loss (w/o reg) on all data: 0.13112834
Test loss (w/o reg) on all data: 0.06772227
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.552326e-06
Norm of the params: 13.069042
              Random: fixed   3 labels. Loss 0.06772. Accuracy 0.985.
### Flips: 104, rs: 26, checks: 104
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021108821
Train loss (w/o reg) on all data: 0.015563353
Test loss (w/o reg) on all data: 0.014832159
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.610113e-06
Norm of the params: 10.531352
     Influence (LOO): fixed  52 labels. Loss 0.01483. Accuracy 0.996.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729302
Test loss (w/o reg) on all data: 0.012055961
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.2837575e-07
Norm of the params: 9.153239
                Loss: fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13231774
Train loss (w/o reg) on all data: 0.12373533
Test loss (w/o reg) on all data: 0.065168574
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.1954423e-06
Norm of the params: 13.101459
              Random: fixed   6 labels. Loss 0.06517. Accuracy 0.985.
### Flips: 104, rs: 26, checks: 156
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729043
Test loss (w/o reg) on all data: 0.012054878
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2733877e-06
Norm of the params: 9.153267
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729067
Test loss (w/o reg) on all data: 0.012055025
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0792208e-07
Norm of the params: 9.153264
                Loss: fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12989971
Train loss (w/o reg) on all data: 0.12149403
Test loss (w/o reg) on all data: 0.058924522
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0768155e-05
Norm of the params: 12.965866
              Random: fixed   9 labels. Loss 0.05892. Accuracy 0.985.
### Flips: 104, rs: 26, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729397
Test loss (w/o reg) on all data: 0.012054464
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.60785e-07
Norm of the params: 9.153228
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729406
Test loss (w/o reg) on all data: 0.012054342
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0472396e-07
Norm of the params: 9.153228
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12599152
Train loss (w/o reg) on all data: 0.116979815
Test loss (w/o reg) on all data: 0.058410347
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.528378e-06
Norm of the params: 13.42513
              Random: fixed  11 labels. Loss 0.05841. Accuracy 0.985.
### Flips: 104, rs: 26, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728543
Test loss (w/o reg) on all data: 0.012054782
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6984485e-07
Norm of the params: 9.15332
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728575
Test loss (w/o reg) on all data: 0.012054844
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7192405e-07
Norm of the params: 9.153318
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11763933
Train loss (w/o reg) on all data: 0.10822788
Test loss (w/o reg) on all data: 0.056759935
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.307779e-06
Norm of the params: 13.71966
              Random: fixed  14 labels. Loss 0.05676. Accuracy 0.989.
### Flips: 104, rs: 26, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730284
Test loss (w/o reg) on all data: 0.012054573
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.8489584e-07
Norm of the params: 9.153131
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730268
Test loss (w/o reg) on all data: 0.012054743
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.728847e-07
Norm of the params: 9.153131
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11037102
Train loss (w/o reg) on all data: 0.100821935
Test loss (w/o reg) on all data: 0.058181282
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.1690768e-05
Norm of the params: 13.819615
              Random: fixed  18 labels. Loss 0.05818. Accuracy 0.985.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11674595
Train loss (w/o reg) on all data: 0.108836375
Test loss (w/o reg) on all data: 0.06813019
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6161837e-05
Norm of the params: 12.577416
Flipped loss: 0.06813. Accuracy: 0.985
### Flips: 104, rs: 27, checks: 52
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02553416
Train loss (w/o reg) on all data: 0.017934367
Test loss (w/o reg) on all data: 0.023717126
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.0266377e-07
Norm of the params: 12.328662
     Influence (LOO): fixed  37 labels. Loss 0.02372. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00988051
Train loss (w/o reg) on all data: 0.00410406
Test loss (w/o reg) on all data: 0.016362181
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.396161e-06
Norm of the params: 10.748442
                Loss: fixed  43 labels. Loss 0.01636. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11559222
Train loss (w/o reg) on all data: 0.10746093
Test loss (w/o reg) on all data: 0.069154315
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.6759354e-06
Norm of the params: 12.752481
              Random: fixed   1 labels. Loss 0.06915. Accuracy 0.981.
### Flips: 104, rs: 27, checks: 104
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729504
Test loss (w/o reg) on all data: 0.012057026
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4390138e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729502
Test loss (w/o reg) on all data: 0.012056805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.990119e-07
Norm of the params: 9.153215
                Loss: fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11331358
Train loss (w/o reg) on all data: 0.10566594
Test loss (w/o reg) on all data: 0.063663684
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1274967e-05
Norm of the params: 12.367408
              Random: fixed   4 labels. Loss 0.06366. Accuracy 0.992.
### Flips: 104, rs: 27, checks: 156
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730459
Test loss (w/o reg) on all data: 0.01205725
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1682184e-07
Norm of the params: 9.153112
     Influence (LOO): fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730438
Test loss (w/o reg) on all data: 0.012057126
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9300014e-07
Norm of the params: 9.153114
                Loss: fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109554835
Train loss (w/o reg) on all data: 0.101833746
Test loss (w/o reg) on all data: 0.058450505
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.4312534e-06
Norm of the params: 12.426658
              Random: fixed   6 labels. Loss 0.05845. Accuracy 0.989.
### Flips: 104, rs: 27, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729805
Test loss (w/o reg) on all data: 0.012054818
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5881595e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  45 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729781
Test loss (w/o reg) on all data: 0.012054875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.505082e-07
Norm of the params: 9.153185
                Loss: fixed  45 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10493779
Train loss (w/o reg) on all data: 0.09727066
Test loss (w/o reg) on all data: 0.052347723
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.723238e-06
Norm of the params: 12.38316
              Random: fixed   8 labels. Loss 0.05235. Accuracy 0.989.
### Flips: 104, rs: 27, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729209
Test loss (w/o reg) on all data: 0.01205606
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5913245e-07
Norm of the params: 9.153249
     Influence (LOO): fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.012056164
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6419895e-07
Norm of the params: 9.153247
                Loss: fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09414584
Train loss (w/o reg) on all data: 0.08569757
Test loss (w/o reg) on all data: 0.053630292
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2390182e-05
Norm of the params: 12.998672
              Random: fixed  12 labels. Loss 0.05363. Accuracy 0.989.
### Flips: 104, rs: 27, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.002172938
Test loss (w/o reg) on all data: 0.012055252
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8587272e-07
Norm of the params: 9.153231
     Influence (LOO): fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729374
Test loss (w/o reg) on all data: 0.012055216
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0151934e-07
Norm of the params: 9.153231
                Loss: fixed  45 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09414587
Train loss (w/o reg) on all data: 0.085696705
Test loss (w/o reg) on all data: 0.053619966
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0882686e-05
Norm of the params: 12.999361
              Random: fixed  12 labels. Loss 0.05362. Accuracy 0.989.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1460803
Train loss (w/o reg) on all data: 0.13737388
Test loss (w/o reg) on all data: 0.085554324
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6381284e-05
Norm of the params: 13.195778
Flipped loss: 0.08555. Accuracy: 0.985
### Flips: 104, rs: 28, checks: 52
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040700454
Train loss (w/o reg) on all data: 0.03127503
Test loss (w/o reg) on all data: 0.036279373
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1549165e-05
Norm of the params: 13.729839
     Influence (LOO): fixed  41 labels. Loss 0.03628. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01632199
Train loss (w/o reg) on all data: 0.009176445
Test loss (w/o reg) on all data: 0.015804766
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3180442e-06
Norm of the params: 11.9545355
                Loss: fixed  49 labels. Loss 0.01580. Accuracy 0.996.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14430058
Train loss (w/o reg) on all data: 0.13537523
Test loss (w/o reg) on all data: 0.08423904
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.10873625e-05
Norm of the params: 13.360656
              Random: fixed   1 labels. Loss 0.08424. Accuracy 0.985.
### Flips: 104, rs: 28, checks: 104
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018661935
Train loss (w/o reg) on all data: 0.011595168
Test loss (w/o reg) on all data: 0.013787636
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.6911308e-07
Norm of the params: 11.888454
     Influence (LOO): fixed  51 labels. Loss 0.01379. Accuracy 0.996.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056516
Train loss (w/o reg) on all data: 0.0028168845
Test loss (w/o reg) on all data: 0.012755599
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2856291e-07
Norm of the params: 9.683766
                Loss: fixed  54 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1393368
Train loss (w/o reg) on all data: 0.13007522
Test loss (w/o reg) on all data: 0.08465489
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.21772e-06
Norm of the params: 13.609984
              Random: fixed   3 labels. Loss 0.08465. Accuracy 0.989.
### Flips: 104, rs: 28, checks: 156
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013159687
Train loss (w/o reg) on all data: 0.0075151646
Test loss (w/o reg) on all data: 0.013905554
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1063126e-07
Norm of the params: 10.624991
     Influence (LOO): fixed  53 labels. Loss 0.01391. Accuracy 0.996.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729262
Test loss (w/o reg) on all data: 0.012054871
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8195777e-07
Norm of the params: 9.153242
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12805428
Train loss (w/o reg) on all data: 0.118156016
Test loss (w/o reg) on all data: 0.087823555
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7994233e-05
Norm of the params: 14.070013
              Random: fixed   6 labels. Loss 0.08782. Accuracy 0.981.
### Flips: 104, rs: 28, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729502
Test loss (w/o reg) on all data: 0.012054895
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.787702e-07
Norm of the params: 9.153216
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729514
Test loss (w/o reg) on all data: 0.012054946
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7586672e-07
Norm of the params: 9.153215
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12166399
Train loss (w/o reg) on all data: 0.111594126
Test loss (w/o reg) on all data: 0.0833603
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.0087478e-05
Norm of the params: 14.191452
              Random: fixed   9 labels. Loss 0.08336. Accuracy 0.985.
### Flips: 104, rs: 28, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729183
Test loss (w/o reg) on all data: 0.012055627
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9007012e-07
Norm of the params: 9.153251
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729213
Test loss (w/o reg) on all data: 0.012055729
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.832664e-07
Norm of the params: 9.153248
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121664
Train loss (w/o reg) on all data: 0.11159091
Test loss (w/o reg) on all data: 0.0833578
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.390072e-06
Norm of the params: 14.193727
              Random: fixed   9 labels. Loss 0.08336. Accuracy 0.985.
### Flips: 104, rs: 28, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728002
Test loss (w/o reg) on all data: 0.012056796
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.8647637e-07
Norm of the params: 9.153381
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021727988
Test loss (w/o reg) on all data: 0.012056649
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4341483e-07
Norm of the params: 9.15338
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11884473
Train loss (w/o reg) on all data: 0.10904553
Test loss (w/o reg) on all data: 0.07286265
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1263255e-05
Norm of the params: 13.9994335
              Random: fixed  11 labels. Loss 0.07286. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13792199
Train loss (w/o reg) on all data: 0.12962054
Test loss (w/o reg) on all data: 0.087384135
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.816774e-05
Norm of the params: 12.88523
Flipped loss: 0.08738. Accuracy: 0.981
### Flips: 104, rs: 29, checks: 52
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030474847
Train loss (w/o reg) on all data: 0.022413235
Test loss (w/o reg) on all data: 0.01808868
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2872914e-07
Norm of the params: 12.697725
     Influence (LOO): fixed  42 labels. Loss 0.01809. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010875924
Train loss (w/o reg) on all data: 0.0047140056
Test loss (w/o reg) on all data: 0.014647192
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1393679e-06
Norm of the params: 11.101278
                Loss: fixed  47 labels. Loss 0.01465. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13284947
Train loss (w/o reg) on all data: 0.12404925
Test loss (w/o reg) on all data: 0.08915837
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.1856418e-05
Norm of the params: 13.266672
              Random: fixed   2 labels. Loss 0.08916. Accuracy 0.981.
### Flips: 104, rs: 29, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173004
Test loss (w/o reg) on all data: 0.012055442
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.95272e-08
Norm of the params: 9.153158
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729765
Test loss (w/o reg) on all data: 0.012055413
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.91927e-07
Norm of the params: 9.153189
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12815978
Train loss (w/o reg) on all data: 0.11961269
Test loss (w/o reg) on all data: 0.08016368
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.4546604e-06
Norm of the params: 13.074476
              Random: fixed   4 labels. Loss 0.08016. Accuracy 0.977.
### Flips: 104, rs: 29, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730233
Test loss (w/o reg) on all data: 0.0120549835
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4449392e-07
Norm of the params: 9.153136
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730198
Test loss (w/o reg) on all data: 0.012055116
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3936194e-07
Norm of the params: 9.153141
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12142527
Train loss (w/o reg) on all data: 0.112574816
Test loss (w/o reg) on all data: 0.07694388
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.1491632e-05
Norm of the params: 13.304474
              Random: fixed   8 labels. Loss 0.07694. Accuracy 0.985.
### Flips: 104, rs: 29, checks: 208
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730147
Test loss (w/o reg) on all data: 0.012055098
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5866868e-07
Norm of the params: 9.153145
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730142
Test loss (w/o reg) on all data: 0.012055162
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5691542e-07
Norm of the params: 9.153146
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11357418
Train loss (w/o reg) on all data: 0.1048049
Test loss (w/o reg) on all data: 0.0748153
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4414116e-05
Norm of the params: 13.24332
              Random: fixed  11 labels. Loss 0.07482. Accuracy 0.985.
### Flips: 104, rs: 29, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172911
Test loss (w/o reg) on all data: 0.012054616
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.398891e-06
Norm of the params: 9.15326
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729143
Test loss (w/o reg) on all data: 0.012054921
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6092385e-07
Norm of the params: 9.153255
                Loss: fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10247968
Train loss (w/o reg) on all data: 0.09310424
Test loss (w/o reg) on all data: 0.058050077
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.281727e-05
Norm of the params: 13.693385
              Random: fixed  17 labels. Loss 0.05805. Accuracy 0.985.
### Flips: 104, rs: 29, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172962
Test loss (w/o reg) on all data: 0.012055142
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.4322694e-08
Norm of the params: 9.153204
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012055178
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4044457e-07
Norm of the params: 9.153204
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096528955
Train loss (w/o reg) on all data: 0.08736451
Test loss (w/o reg) on all data: 0.051390298
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6509683e-05
Norm of the params: 13.538424
              Random: fixed  20 labels. Loss 0.05139. Accuracy 0.985.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13960384
Train loss (w/o reg) on all data: 0.1305849
Test loss (w/o reg) on all data: 0.082794815
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.713152e-05
Norm of the params: 13.430521
Flipped loss: 0.08279. Accuracy: 0.981
### Flips: 104, rs: 30, checks: 52
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032388154
Train loss (w/o reg) on all data: 0.024602123
Test loss (w/o reg) on all data: 0.03396678
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1235962e-05
Norm of the params: 12.478809
     Influence (LOO): fixed  40 labels. Loss 0.03397. Accuracy 0.981.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01250305
Train loss (w/o reg) on all data: 0.005277501
Test loss (w/o reg) on all data: 0.026511896
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2775664e-07
Norm of the params: 12.021273
                Loss: fixed  47 labels. Loss 0.02651. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13455674
Train loss (w/o reg) on all data: 0.12582079
Test loss (w/o reg) on all data: 0.08046571
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.283661e-05
Norm of the params: 13.218134
              Random: fixed   3 labels. Loss 0.08047. Accuracy 0.981.
### Flips: 104, rs: 30, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728936
Test loss (w/o reg) on all data: 0.01205378
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8740485e-07
Norm of the params: 9.153278
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728976
Test loss (w/o reg) on all data: 0.012053896
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3333006e-07
Norm of the params: 9.1532755
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12731548
Train loss (w/o reg) on all data: 0.118522696
Test loss (w/o reg) on all data: 0.07559137
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.615256e-06
Norm of the params: 13.261058
              Random: fixed   6 labels. Loss 0.07559. Accuracy 0.981.
### Flips: 104, rs: 30, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729486
Test loss (w/o reg) on all data: 0.012053912
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9516787e-07
Norm of the params: 9.153219
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729483
Test loss (w/o reg) on all data: 0.012053985
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.072325e-07
Norm of the params: 9.153219
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113960035
Train loss (w/o reg) on all data: 0.10509519
Test loss (w/o reg) on all data: 0.055972904
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8983512e-05
Norm of the params: 13.315285
              Random: fixed  12 labels. Loss 0.05597. Accuracy 0.989.
### Flips: 104, rs: 30, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729604
Test loss (w/o reg) on all data: 0.012054543
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1561336e-07
Norm of the params: 9.153206
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729618
Test loss (w/o reg) on all data: 0.01205449
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.15454e-08
Norm of the params: 9.153206
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108381785
Train loss (w/o reg) on all data: 0.098652296
Test loss (w/o reg) on all data: 0.054508492
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8559767e-06
Norm of the params: 13.949547
              Random: fixed  13 labels. Loss 0.05451. Accuracy 0.992.
### Flips: 104, rs: 30, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729646
Test loss (w/o reg) on all data: 0.012054361
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.854625e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.012054517
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0975864e-07
Norm of the params: 9.153202
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10014427
Train loss (w/o reg) on all data: 0.089832895
Test loss (w/o reg) on all data: 0.055175506
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6431872e-05
Norm of the params: 14.360623
              Random: fixed  17 labels. Loss 0.05518. Accuracy 0.992.
### Flips: 104, rs: 30, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728245
Test loss (w/o reg) on all data: 0.0120549835
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.44366e-07
Norm of the params: 9.153354
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728305
Test loss (w/o reg) on all data: 0.012054895
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1243118e-07
Norm of the params: 9.153349
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090871625
Train loss (w/o reg) on all data: 0.08068675
Test loss (w/o reg) on all data: 0.05529576
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.741797e-06
Norm of the params: 14.2722645
              Random: fixed  21 labels. Loss 0.05530. Accuracy 0.985.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13258733
Train loss (w/o reg) on all data: 0.1236145
Test loss (w/o reg) on all data: 0.077040575
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.6153917e-05
Norm of the params: 13.396141
Flipped loss: 0.07704. Accuracy: 0.981
### Flips: 104, rs: 31, checks: 52
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032605886
Train loss (w/o reg) on all data: 0.02492436
Test loss (w/o reg) on all data: 0.034989964
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.3695057e-06
Norm of the params: 12.394778
     Influence (LOO): fixed  39 labels. Loss 0.03499. Accuracy 0.981.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073833745
Train loss (w/o reg) on all data: 0.0026673495
Test loss (w/o reg) on all data: 0.021343768
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0937526e-06
Norm of the params: 9.711875
                Loss: fixed  47 labels. Loss 0.02134. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120105706
Train loss (w/o reg) on all data: 0.11114257
Test loss (w/o reg) on all data: 0.0703375
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.245628e-05
Norm of the params: 13.388904
              Random: fixed   5 labels. Loss 0.07034. Accuracy 0.985.
### Flips: 104, rs: 31, checks: 104
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728857
Test loss (w/o reg) on all data: 0.01205543
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.293064e-07
Norm of the params: 9.153287
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [3] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729143
Test loss (w/o reg) on all data: 0.01205598
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.232589e-07
Norm of the params: 9.153255
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11792893
Train loss (w/o reg) on all data: 0.109016575
Test loss (w/o reg) on all data: 0.069838084
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.3652326e-06
Norm of the params: 13.3509245
              Random: fixed   6 labels. Loss 0.06984. Accuracy 0.985.
### Flips: 104, rs: 31, checks: 156
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.00217298
Test loss (w/o reg) on all data: 0.012054441
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8206711e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.012054605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.8050496e-07
Norm of the params: 9.153188
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11142019
Train loss (w/o reg) on all data: 0.102146216
Test loss (w/o reg) on all data: 0.06530716
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9993232e-06
Norm of the params: 13.619085
              Random: fixed   8 labels. Loss 0.06531. Accuracy 0.985.
### Flips: 104, rs: 31, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729607
Test loss (w/o reg) on all data: 0.012054189
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0161139e-07
Norm of the params: 9.153205
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729579
Test loss (w/o reg) on all data: 0.012054492
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.3131735e-07
Norm of the params: 9.153206
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1090967
Train loss (w/o reg) on all data: 0.09965604
Test loss (w/o reg) on all data: 0.06419268
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2133732e-05
Norm of the params: 13.7409315
              Random: fixed   9 labels. Loss 0.06419. Accuracy 0.989.
### Flips: 104, rs: 31, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729488
Test loss (w/o reg) on all data: 0.012054042
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.400524e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172949
Test loss (w/o reg) on all data: 0.012054155
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1331935e-07
Norm of the params: 9.153217
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10909668
Train loss (w/o reg) on all data: 0.09965658
Test loss (w/o reg) on all data: 0.064184956
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0280256e-06
Norm of the params: 13.740526
              Random: fixed   9 labels. Loss 0.06418. Accuracy 0.989.
### Flips: 104, rs: 31, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729588
Test loss (w/o reg) on all data: 0.012054875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.358175e-07
Norm of the params: 9.153206
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729604
Test loss (w/o reg) on all data: 0.012054786
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4041096e-07
Norm of the params: 9.153207
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1090967
Train loss (w/o reg) on all data: 0.09965801
Test loss (w/o reg) on all data: 0.06417796
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0288514e-05
Norm of the params: 13.739494
              Random: fixed   9 labels. Loss 0.06418. Accuracy 0.989.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15588245
Train loss (w/o reg) on all data: 0.14883794
Test loss (w/o reg) on all data: 0.07248879
Train acc on all data:  0.9474689589302769
Test acc on all data:   1.0
Norm of the mean of gradients: 1.1637548e-05
Norm of the params: 11.869712
Flipped loss: 0.07249. Accuracy: 1.000
### Flips: 104, rs: 32, checks: 52
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05305042
Train loss (w/o reg) on all data: 0.04492439
Test loss (w/o reg) on all data: 0.041140016
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.675161e-06
Norm of the params: 12.748358
     Influence (LOO): fixed  42 labels. Loss 0.04114. Accuracy 0.989.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018982198
Train loss (w/o reg) on all data: 0.010183761
Test loss (w/o reg) on all data: 0.013298049
Train acc on all data:  0.9980897803247374
Test acc on all data:   1.0
Norm of the mean of gradients: 2.6703435e-06
Norm of the params: 13.265322
                Loss: fixed  52 labels. Loss 0.01330. Accuracy 1.000.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14833793
Train loss (w/o reg) on all data: 0.14131913
Test loss (w/o reg) on all data: 0.07542162
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.4446602e-05
Norm of the params: 11.848048
              Random: fixed   4 labels. Loss 0.07542. Accuracy 0.996.
### Flips: 104, rs: 32, checks: 104
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013749357
Train loss (w/o reg) on all data: 0.008246733
Test loss (w/o reg) on all data: 0.0107435845
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.292013e-07
Norm of the params: 10.49059
     Influence (LOO): fixed  59 labels. Loss 0.01074. Accuracy 0.996.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009718804
Train loss (w/o reg) on all data: 0.0038489385
Test loss (w/o reg) on all data: 0.012000183
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.858654e-07
Norm of the params: 10.835004
                Loss: fixed  59 labels. Loss 0.01200. Accuracy 0.996.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14456075
Train loss (w/o reg) on all data: 0.13763374
Test loss (w/o reg) on all data: 0.0737339
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.872251e-05
Norm of the params: 11.770315
              Random: fixed   6 labels. Loss 0.07373. Accuracy 0.996.
### Flips: 104, rs: 32, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729635
Test loss (w/o reg) on all data: 0.012055165
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5536524e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00740458
Train loss (w/o reg) on all data: 0.0027832964
Test loss (w/o reg) on all data: 0.0136983525
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.843029e-07
Norm of the params: 9.613828
                Loss: fixed  60 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13837536
Train loss (w/o reg) on all data: 0.13177782
Test loss (w/o reg) on all data: 0.07101206
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1712676e-06
Norm of the params: 11.486983
              Random: fixed  10 labels. Loss 0.07101. Accuracy 0.992.
### Flips: 104, rs: 32, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173001
Test loss (w/o reg) on all data: 0.012055164
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2164359e-07
Norm of the params: 9.15316
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730012
Test loss (w/o reg) on all data: 0.012055134
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7830929e-07
Norm of the params: 9.153161
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13514914
Train loss (w/o reg) on all data: 0.12848246
Test loss (w/o reg) on all data: 0.06573816
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2886562e-05
Norm of the params: 11.547011
              Random: fixed  13 labels. Loss 0.06574. Accuracy 0.996.
### Flips: 104, rs: 32, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728342
Test loss (w/o reg) on all data: 0.012055252
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.090173e-07
Norm of the params: 9.153343
     Influence (LOO): fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172854
Test loss (w/o reg) on all data: 0.012055212
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0523246e-07
Norm of the params: 9.153322
                Loss: fixed  61 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1288429
Train loss (w/o reg) on all data: 0.1218495
Test loss (w/o reg) on all data: 0.06236094
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.577902e-06
Norm of the params: 11.826582
              Random: fixed  16 labels. Loss 0.06236. Accuracy 0.996.
### Flips: 104, rs: 32, checks: 312
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012054726
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2850913e-06
Norm of the params: 9.153183
     Influence (LOO): fixed  61 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [5] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729546
Test loss (w/o reg) on all data: 0.012054646
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1261546e-07
Norm of the params: 9.15321
                Loss: fixed  61 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11609433
Train loss (w/o reg) on all data: 0.10900133
Test loss (w/o reg) on all data: 0.05846664
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.8414901e-05
Norm of the params: 11.910499
              Random: fixed  21 labels. Loss 0.05847. Accuracy 0.996.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115416825
Train loss (w/o reg) on all data: 0.10710378
Test loss (w/o reg) on all data: 0.069117874
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7282133e-05
Norm of the params: 12.894219
Flipped loss: 0.06912. Accuracy: 0.992
### Flips: 104, rs: 33, checks: 52
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017913006
Train loss (w/o reg) on all data: 0.01204263
Test loss (w/o reg) on all data: 0.017496979
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4437005e-07
Norm of the params: 10.835476
     Influence (LOO): fixed  38 labels. Loss 0.01750. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007380169
Train loss (w/o reg) on all data: 0.002746342
Test loss (w/o reg) on all data: 0.0139379995
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7886806e-06
Norm of the params: 9.626865
                Loss: fixed  40 labels. Loss 0.01394. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11360857
Train loss (w/o reg) on all data: 0.10541084
Test loss (w/o reg) on all data: 0.068992294
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1335376e-05
Norm of the params: 12.804481
              Random: fixed   1 labels. Loss 0.06899. Accuracy 0.989.
### Flips: 104, rs: 33, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729846
Test loss (w/o reg) on all data: 0.012054714
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3933633e-07
Norm of the params: 9.15318
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172983
Test loss (w/o reg) on all data: 0.012054806
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2448364e-07
Norm of the params: 9.15318
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10948463
Train loss (w/o reg) on all data: 0.10124191
Test loss (w/o reg) on all data: 0.06596569
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4719233e-05
Norm of the params: 12.839562
              Random: fixed   3 labels. Loss 0.06597. Accuracy 0.989.
### Flips: 104, rs: 33, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172975
Test loss (w/o reg) on all data: 0.012054452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6609336e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172974
Test loss (w/o reg) on all data: 0.012054401
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7303e-07
Norm of the params: 9.153189
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109484635
Train loss (w/o reg) on all data: 0.101242214
Test loss (w/o reg) on all data: 0.06597138
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7404576e-05
Norm of the params: 12.839331
              Random: fixed   3 labels. Loss 0.06597. Accuracy 0.989.
### Flips: 104, rs: 33, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172937
Test loss (w/o reg) on all data: 0.012054127
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.955683e-07
Norm of the params: 9.153231
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729378
Test loss (w/o reg) on all data: 0.012054311
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5678158e-07
Norm of the params: 9.15323
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10137642
Train loss (w/o reg) on all data: 0.0933216
Test loss (w/o reg) on all data: 0.0641283
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.064224e-06
Norm of the params: 12.692379
              Random: fixed   6 labels. Loss 0.06413. Accuracy 0.985.
### Flips: 104, rs: 33, checks: 260
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729919
Test loss (w/o reg) on all data: 0.01205471
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0599126e-07
Norm of the params: 9.153171
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172992
Test loss (w/o reg) on all data: 0.0120546995
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7890123e-08
Norm of the params: 9.153171
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09841186
Train loss (w/o reg) on all data: 0.090583965
Test loss (w/o reg) on all data: 0.054232035
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7146906e-06
Norm of the params: 12.51231
              Random: fixed   9 labels. Loss 0.05423. Accuracy 0.992.
### Flips: 104, rs: 33, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729495
Test loss (w/o reg) on all data: 0.012054571
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8523195e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729511
Test loss (w/o reg) on all data: 0.012054546
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.649177e-08
Norm of the params: 9.153217
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093664244
Train loss (w/o reg) on all data: 0.0860475
Test loss (w/o reg) on all data: 0.05270832
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.1551027e-06
Norm of the params: 12.3424
              Random: fixed  11 labels. Loss 0.05271. Accuracy 0.989.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1275824
Train loss (w/o reg) on all data: 0.11891775
Test loss (w/o reg) on all data: 0.08706976
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1998506e-05
Norm of the params: 13.1640835
Flipped loss: 0.08707. Accuracy: 0.981
### Flips: 104, rs: 34, checks: 52
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027994137
Train loss (w/o reg) on all data: 0.020429498
Test loss (w/o reg) on all data: 0.028271046
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.952002e-06
Norm of the params: 12.300115
     Influence (LOO): fixed  38 labels. Loss 0.02827. Accuracy 0.996.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009473184
Train loss (w/o reg) on all data: 0.0043806783
Test loss (w/o reg) on all data: 0.009146458
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.4554398e-07
Norm of the params: 10.092082
                Loss: fixed  45 labels. Loss 0.00915. Accuracy 0.996.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1275824
Train loss (w/o reg) on all data: 0.11892216
Test loss (w/o reg) on all data: 0.08706353
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.449244e-05
Norm of the params: 13.160735
              Random: fixed   0 labels. Loss 0.08706. Accuracy 0.981.
### Flips: 104, rs: 34, checks: 104
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021731309
Test loss (w/o reg) on all data: 0.012057157
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.563131e-07
Norm of the params: 9.153019
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731283
Test loss (w/o reg) on all data: 0.01205697
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.237442e-07
Norm of the params: 9.153021
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12516794
Train loss (w/o reg) on all data: 0.11621027
Test loss (w/o reg) on all data: 0.08750738
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.1719624e-06
Norm of the params: 13.384819
              Random: fixed   1 labels. Loss 0.08751. Accuracy 0.985.
### Flips: 104, rs: 34, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729437
Test loss (w/o reg) on all data: 0.012055081
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5156479e-07
Norm of the params: 9.153223
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729455
Test loss (w/o reg) on all data: 0.012055144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.538834e-07
Norm of the params: 9.153223
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11238356
Train loss (w/o reg) on all data: 0.10357265
Test loss (w/o reg) on all data: 0.08103864
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.9092853e-06
Norm of the params: 13.274717
              Random: fixed   7 labels. Loss 0.08104. Accuracy 0.973.
### Flips: 104, rs: 34, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730072
Test loss (w/o reg) on all data: 0.012055348
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7385647e-07
Norm of the params: 9.153154
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730056
Test loss (w/o reg) on all data: 0.012055427
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5010718e-07
Norm of the params: 9.153155
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10802825
Train loss (w/o reg) on all data: 0.09926616
Test loss (w/o reg) on all data: 0.07414882
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1650705e-05
Norm of the params: 13.237896
              Random: fixed  10 labels. Loss 0.07415. Accuracy 0.977.
### Flips: 104, rs: 34, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.0120552415
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7403644e-07
Norm of the params: 9.15318
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729849
Test loss (w/o reg) on all data: 0.012055208
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.444739e-08
Norm of the params: 9.15318
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09972503
Train loss (w/o reg) on all data: 0.09073128
Test loss (w/o reg) on all data: 0.06989788
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.357892e-06
Norm of the params: 13.411749
              Random: fixed  13 labels. Loss 0.06990. Accuracy 0.977.
### Flips: 104, rs: 34, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728866
Test loss (w/o reg) on all data: 0.012055457
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.8445896e-07
Norm of the params: 9.153287
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728869
Test loss (w/o reg) on all data: 0.012055313
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2824744e-07
Norm of the params: 9.153287
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09114598
Train loss (w/o reg) on all data: 0.0814313
Test loss (w/o reg) on all data: 0.06927214
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.37833185e-05
Norm of the params: 13.938926
              Random: fixed  16 labels. Loss 0.06927. Accuracy 0.981.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14201878
Train loss (w/o reg) on all data: 0.13409819
Test loss (w/o reg) on all data: 0.08635053
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8012868e-05
Norm of the params: 12.586182
Flipped loss: 0.08635. Accuracy: 0.989
### Flips: 104, rs: 35, checks: 52
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042503364
Train loss (w/o reg) on all data: 0.03360615
Test loss (w/o reg) on all data: 0.0480782
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.5106484e-06
Norm of the params: 13.339575
     Influence (LOO): fixed  38 labels. Loss 0.04808. Accuracy 0.985.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010500582
Train loss (w/o reg) on all data: 0.0042596697
Test loss (w/o reg) on all data: 0.008186263
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3441335e-06
Norm of the params: 11.17221
                Loss: fixed  51 labels. Loss 0.00819. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13623776
Train loss (w/o reg) on all data: 0.12771773
Test loss (w/o reg) on all data: 0.0869447
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2500602e-05
Norm of the params: 13.053748
              Random: fixed   1 labels. Loss 0.08694. Accuracy 0.989.
### Flips: 104, rs: 35, checks: 104
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015878158
Train loss (w/o reg) on all data: 0.010455352
Test loss (w/o reg) on all data: 0.012671919
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.4896433e-06
Norm of the params: 10.414226
     Influence (LOO): fixed  50 labels. Loss 0.01267. Accuracy 0.996.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008034243
Train loss (w/o reg) on all data: 0.0030516677
Test loss (w/o reg) on all data: 0.008116259
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8426276e-07
Norm of the params: 9.982561
                Loss: fixed  52 labels. Loss 0.00812. Accuracy 0.996.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13249099
Train loss (w/o reg) on all data: 0.12378245
Test loss (w/o reg) on all data: 0.083709136
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.226209e-05
Norm of the params: 13.197382
              Random: fixed   4 labels. Loss 0.08371. Accuracy 0.989.
### Flips: 104, rs: 35, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173033
Test loss (w/o reg) on all data: 0.01205475
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2445409e-07
Norm of the params: 9.153125
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730252
Test loss (w/o reg) on all data: 0.012054727
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9768636e-07
Norm of the params: 9.153134
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13249098
Train loss (w/o reg) on all data: 0.12378278
Test loss (w/o reg) on all data: 0.083707705
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0249335e-05
Norm of the params: 13.197121
              Random: fixed   4 labels. Loss 0.08371. Accuracy 0.989.
### Flips: 104, rs: 35, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021727935
Test loss (w/o reg) on all data: 0.012056553
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.292765e-07
Norm of the params: 9.153386
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021728037
Test loss (w/o reg) on all data: 0.012056639
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.708727e-07
Norm of the params: 9.153374
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12576127
Train loss (w/o reg) on all data: 0.11678025
Test loss (w/o reg) on all data: 0.0798369
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7359556e-05
Norm of the params: 13.402255
              Random: fixed   8 labels. Loss 0.07984. Accuracy 0.989.
### Flips: 104, rs: 35, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729546
Test loss (w/o reg) on all data: 0.012055954
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1962079e-07
Norm of the params: 9.153213
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729544
Test loss (w/o reg) on all data: 0.012056006
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.390162e-07
Norm of the params: 9.153212
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105027884
Train loss (w/o reg) on all data: 0.09641655
Test loss (w/o reg) on all data: 0.07675981
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.21721e-06
Norm of the params: 13.123515
              Random: fixed  15 labels. Loss 0.07676. Accuracy 0.969.
### Flips: 104, rs: 35, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021727926
Test loss (w/o reg) on all data: 0.012054986
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.107556e-07
Norm of the params: 9.153388
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021727947
Test loss (w/o reg) on all data: 0.012054895
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4617373e-07
Norm of the params: 9.153385
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100255504
Train loss (w/o reg) on all data: 0.09119129
Test loss (w/o reg) on all data: 0.07709639
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.4105373e-06
Norm of the params: 13.464185
              Random: fixed  17 labels. Loss 0.07710. Accuracy 0.973.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14465529
Train loss (w/o reg) on all data: 0.1370683
Test loss (w/o reg) on all data: 0.07140224
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4358504e-05
Norm of the params: 12.318267
Flipped loss: 0.07140. Accuracy: 0.989
### Flips: 104, rs: 36, checks: 52
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027654976
Train loss (w/o reg) on all data: 0.0202722
Test loss (w/o reg) on all data: 0.017050652
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.8390293e-06
Norm of the params: 12.15136
     Influence (LOO): fixed  44 labels. Loss 0.01705. Accuracy 0.996.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011417238
Train loss (w/o reg) on all data: 0.004780127
Test loss (w/o reg) on all data: 0.013669319
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.391327e-07
Norm of the params: 11.521381
                Loss: fixed  49 labels. Loss 0.01367. Accuracy 0.996.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14059891
Train loss (w/o reg) on all data: 0.13289179
Test loss (w/o reg) on all data: 0.07243202
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.4985518e-05
Norm of the params: 12.415404
              Random: fixed   1 labels. Loss 0.07243. Accuracy 0.989.
### Flips: 104, rs: 36, checks: 104
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006971534
Train loss (w/o reg) on all data: 0.0025172066
Test loss (w/o reg) on all data: 0.011188403
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8383764e-07
Norm of the params: 9.438567
     Influence (LOO): fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715357
Train loss (w/o reg) on all data: 0.0025172085
Test loss (w/o reg) on all data: 0.011188281
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7136945e-07
Norm of the params: 9.438567
                Loss: fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14059892
Train loss (w/o reg) on all data: 0.1328874
Test loss (w/o reg) on all data: 0.07244667
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.1402698e-06
Norm of the params: 12.418961
              Random: fixed   1 labels. Loss 0.07245. Accuracy 0.989.
### Flips: 104, rs: 36, checks: 156
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715325
Train loss (w/o reg) on all data: 0.002517158
Test loss (w/o reg) on all data: 0.011188334
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1727811e-06
Norm of the params: 9.438618
     Influence (LOO): fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715334
Train loss (w/o reg) on all data: 0.0025171586
Test loss (w/o reg) on all data: 0.011188503
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8704096e-07
Norm of the params: 9.438618
                Loss: fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13449217
Train loss (w/o reg) on all data: 0.12714095
Test loss (w/o reg) on all data: 0.067756385
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1313392e-05
Norm of the params: 12.125363
              Random: fixed   4 labels. Loss 0.06776. Accuracy 0.989.
### Flips: 104, rs: 36, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730522
Test loss (w/o reg) on all data: 0.012054973
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5774763e-07
Norm of the params: 9.153105
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715353
Train loss (w/o reg) on all data: 0.0025172175
Test loss (w/o reg) on all data: 0.011186857
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3172983e-07
Norm of the params: 9.438557
                Loss: fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12394376
Train loss (w/o reg) on all data: 0.116598874
Test loss (w/o reg) on all data: 0.066852964
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.1061535e-06
Norm of the params: 12.120134
              Random: fixed   8 labels. Loss 0.06685. Accuracy 0.985.
### Flips: 104, rs: 36, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729893
Test loss (w/o reg) on all data: 0.012054885
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.956475e-07
Norm of the params: 9.1531725
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715353
Train loss (w/o reg) on all data: 0.0025172348
Test loss (w/o reg) on all data: 0.01118739
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.304588e-07
Norm of the params: 9.438539
                Loss: fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121188655
Train loss (w/o reg) on all data: 0.11391336
Test loss (w/o reg) on all data: 0.06314857
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.259208e-05
Norm of the params: 12.062586
              Random: fixed  10 labels. Loss 0.06315. Accuracy 0.985.
### Flips: 104, rs: 36, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730072
Test loss (w/o reg) on all data: 0.012055466
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.808213e-07
Norm of the params: 9.153153
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715353
Train loss (w/o reg) on all data: 0.0025170862
Test loss (w/o reg) on all data: 0.011186717
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.525815e-07
Norm of the params: 9.438696
                Loss: fixed  51 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120115176
Train loss (w/o reg) on all data: 0.11298964
Test loss (w/o reg) on all data: 0.061218232
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.571486e-06
Norm of the params: 11.937785
              Random: fixed  12 labels. Loss 0.06122. Accuracy 0.992.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101031065
Train loss (w/o reg) on all data: 0.09253042
Test loss (w/o reg) on all data: 0.06341359
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7235538e-05
Norm of the params: 13.0389
Flipped loss: 0.06341. Accuracy: 0.977
### Flips: 104, rs: 37, checks: 52
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00959684
Train loss (w/o reg) on all data: 0.0039680162
Test loss (w/o reg) on all data: 0.015908882
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6859647e-07
Norm of the params: 10.610206
     Influence (LOO): fixed  35 labels. Loss 0.01591. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009789286
Train loss (w/o reg) on all data: 0.0037673088
Test loss (w/o reg) on all data: 0.018437425
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.558401e-07
Norm of the params: 10.974495
                Loss: fixed  34 labels. Loss 0.01844. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09967744
Train loss (w/o reg) on all data: 0.09105488
Test loss (w/o reg) on all data: 0.060337517
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0473142e-05
Norm of the params: 13.1320715
              Random: fixed   1 labels. Loss 0.06034. Accuracy 0.985.
### Flips: 104, rs: 37, checks: 104
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172958
Test loss (w/o reg) on all data: 0.012055617
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2748135e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007702246
Train loss (w/o reg) on all data: 0.0027691135
Test loss (w/o reg) on all data: 0.0123668015
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.264756e-07
Norm of the params: 9.932907
                Loss: fixed  36 labels. Loss 0.01237. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096876755
Train loss (w/o reg) on all data: 0.08829933
Test loss (w/o reg) on all data: 0.056375798
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1156655e-05
Norm of the params: 13.097657
              Random: fixed   3 labels. Loss 0.05638. Accuracy 0.985.
### Flips: 104, rs: 37, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173027
Test loss (w/o reg) on all data: 0.012055057
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7834358e-07
Norm of the params: 9.1531315
     Influence (LOO): fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002173026
Test loss (w/o reg) on all data: 0.012055187
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2036524e-07
Norm of the params: 9.153132
                Loss: fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09472235
Train loss (w/o reg) on all data: 0.0864872
Test loss (w/o reg) on all data: 0.05751757
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.26542e-06
Norm of the params: 12.8336735
              Random: fixed   4 labels. Loss 0.05752. Accuracy 0.977.
### Flips: 104, rs: 37, checks: 208
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172925
Test loss (w/o reg) on all data: 0.012055407
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1659388e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729271
Test loss (w/o reg) on all data: 0.012055493
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8201249e-07
Norm of the params: 9.153244
                Loss: fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08793248
Train loss (w/o reg) on all data: 0.07977591
Test loss (w/o reg) on all data: 0.053341206
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4257651e-05
Norm of the params: 12.772295
              Random: fixed   7 labels. Loss 0.05334. Accuracy 0.989.
### Flips: 104, rs: 37, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729164
Test loss (w/o reg) on all data: 0.012054531
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2519416e-06
Norm of the params: 9.153254
     Influence (LOO): fixed  37 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [3] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729567
Test loss (w/o reg) on all data: 0.012055223
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5746429e-07
Norm of the params: 9.153211
                Loss: fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080257155
Train loss (w/o reg) on all data: 0.07166087
Test loss (w/o reg) on all data: 0.04467442
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.876002e-06
Norm of the params: 13.112045
              Random: fixed   9 labels. Loss 0.04467. Accuracy 0.989.
### Flips: 104, rs: 37, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021728827
Test loss (w/o reg) on all data: 0.012056384
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.463482e-07
Norm of the params: 9.153289
     Influence (LOO): fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728866
Test loss (w/o reg) on all data: 0.012056139
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5461834e-07
Norm of the params: 9.153286
                Loss: fixed  37 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075464204
Train loss (w/o reg) on all data: 0.0657844
Test loss (w/o reg) on all data: 0.043033272
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.2623625e-06
Norm of the params: 13.913882
              Random: fixed  10 labels. Loss 0.04303. Accuracy 0.989.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12960415
Train loss (w/o reg) on all data: 0.12200845
Test loss (w/o reg) on all data: 0.06798884
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.9155377e-06
Norm of the params: 12.325341
Flipped loss: 0.06799. Accuracy: 0.989
### Flips: 104, rs: 38, checks: 52
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025478235
Train loss (w/o reg) on all data: 0.017807467
Test loss (w/o reg) on all data: 0.019434614
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.033784e-06
Norm of the params: 12.386095
     Influence (LOO): fixed  41 labels. Loss 0.01943. Accuracy 0.996.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010861201
Train loss (w/o reg) on all data: 0.004541117
Test loss (w/o reg) on all data: 0.014587417
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8922344e-07
Norm of the params: 11.242851
                Loss: fixed  44 labels. Loss 0.01459. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124116205
Train loss (w/o reg) on all data: 0.11622477
Test loss (w/o reg) on all data: 0.064493395
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.9638736e-06
Norm of the params: 12.562987
              Random: fixed   2 labels. Loss 0.06449. Accuracy 0.989.
### Flips: 104, rs: 38, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217286
Test loss (w/o reg) on all data: 0.012054197
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4244304e-07
Norm of the params: 9.153314
     Influence (LOO): fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728885
Test loss (w/o reg) on all data: 0.012054709
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.841367e-07
Norm of the params: 9.153283
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11818334
Train loss (w/o reg) on all data: 0.1104265
Test loss (w/o reg) on all data: 0.06027372
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6257603e-05
Norm of the params: 12.455388
              Random: fixed   5 labels. Loss 0.06027. Accuracy 0.989.
### Flips: 104, rs: 38, checks: 156
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172988
Test loss (w/o reg) on all data: 0.012055181
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9500025e-08
Norm of the params: 9.153175
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172988
Test loss (w/o reg) on all data: 0.012055195
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4872911e-07
Norm of the params: 9.153175
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11280761
Train loss (w/o reg) on all data: 0.10509234
Test loss (w/o reg) on all data: 0.060239904
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1262522e-05
Norm of the params: 12.421971
              Random: fixed   8 labels. Loss 0.06024. Accuracy 0.985.
### Flips: 104, rs: 38, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729092
Test loss (w/o reg) on all data: 0.012055012
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1474365e-07
Norm of the params: 9.15326
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729115
Test loss (w/o reg) on all data: 0.012054936
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4047713e-07
Norm of the params: 9.15326
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10729666
Train loss (w/o reg) on all data: 0.09930635
Test loss (w/o reg) on all data: 0.060578804
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.37483785e-05
Norm of the params: 12.641448
              Random: fixed  10 labels. Loss 0.06058. Accuracy 0.985.
### Flips: 104, rs: 38, checks: 260
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729565
Test loss (w/o reg) on all data: 0.01205504
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.790642e-07
Norm of the params: 9.153209
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172957
Test loss (w/o reg) on all data: 0.0120550785
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3283761e-07
Norm of the params: 9.153209
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098394446
Train loss (w/o reg) on all data: 0.09068832
Test loss (w/o reg) on all data: 0.060264803
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7142072e-05
Norm of the params: 12.414612
              Random: fixed  14 labels. Loss 0.06026. Accuracy 0.985.
### Flips: 104, rs: 38, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172957
Test loss (w/o reg) on all data: 0.012054796
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.061139e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172958
Test loss (w/o reg) on all data: 0.012054876
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9742073e-07
Norm of the params: 9.153207
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09222511
Train loss (w/o reg) on all data: 0.084735684
Test loss (w/o reg) on all data: 0.0606304
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.7776174e-06
Norm of the params: 12.238816
              Random: fixed  17 labels. Loss 0.06063. Accuracy 0.981.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11586003
Train loss (w/o reg) on all data: 0.10806238
Test loss (w/o reg) on all data: 0.077551275
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1345047e-05
Norm of the params: 12.488117
Flipped loss: 0.07755. Accuracy: 0.977
### Flips: 104, rs: 39, checks: 52
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016599093
Train loss (w/o reg) on all data: 0.010987943
Test loss (w/o reg) on all data: 0.01616818
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.127978e-07
Norm of the params: 10.593536
     Influence (LOO): fixed  38 labels. Loss 0.01617. Accuracy 0.996.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011979143
Train loss (w/o reg) on all data: 0.0052507315
Test loss (w/o reg) on all data: 0.01983091
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.474998e-07
Norm of the params: 11.600356
                Loss: fixed  38 labels. Loss 0.01983. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11472028
Train loss (w/o reg) on all data: 0.10704539
Test loss (w/o reg) on all data: 0.07769902
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7344695e-05
Norm of the params: 12.38942
              Random: fixed   1 labels. Loss 0.07770. Accuracy 0.977.
### Flips: 104, rs: 39, checks: 104
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008979259
Train loss (w/o reg) on all data: 0.0037581807
Test loss (w/o reg) on all data: 0.013042723
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.4351125e-07
Norm of the params: 10.218686
     Influence (LOO): fixed  40 labels. Loss 0.01304. Accuracy 0.996.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730561
Test loss (w/o reg) on all data: 0.01205659
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.858492e-07
Norm of the params: 9.1531
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11360637
Train loss (w/o reg) on all data: 0.10623198
Test loss (w/o reg) on all data: 0.07563768
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.722236e-05
Norm of the params: 12.144456
              Random: fixed   3 labels. Loss 0.07564. Accuracy 0.981.
### Flips: 104, rs: 39, checks: 156
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012055302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.426482e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.012055086
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1066625e-07
Norm of the params: 9.153188
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10895488
Train loss (w/o reg) on all data: 0.10150772
Test loss (w/o reg) on all data: 0.07334598
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.747384e-06
Norm of the params: 12.204221
              Random: fixed   4 labels. Loss 0.07335. Accuracy 0.981.
### Flips: 104, rs: 39, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730615
Test loss (w/o reg) on all data: 0.012054719
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0479967e-07
Norm of the params: 9.153095
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730557
Test loss (w/o reg) on all data: 0.0120548615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.869727e-07
Norm of the params: 9.153102
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102450974
Train loss (w/o reg) on all data: 0.0949419
Test loss (w/o reg) on all data: 0.07055282
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7989372e-05
Norm of the params: 12.254859
              Random: fixed   7 labels. Loss 0.07055. Accuracy 0.985.
### Flips: 104, rs: 39, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729697
Test loss (w/o reg) on all data: 0.012054598
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9935833e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217297
Test loss (w/o reg) on all data: 0.012054648
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5218204e-07
Norm of the params: 9.153194
                Loss: fixed  41 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09211656
Train loss (w/o reg) on all data: 0.08437245
Test loss (w/o reg) on all data: 0.065222256
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.5170491e-05
Norm of the params: 12.445161
              Random: fixed  11 labels. Loss 0.06522. Accuracy 0.977.
### Flips: 104, rs: 39, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729327
Test loss (w/o reg) on all data: 0.012055477
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.370862e-07
Norm of the params: 9.1532345
     Influence (LOO): fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729337
Test loss (w/o reg) on all data: 0.012055383
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9050912e-07
Norm of the params: 9.153234
                Loss: fixed  41 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091304824
Train loss (w/o reg) on all data: 0.08375073
Test loss (w/o reg) on all data: 0.060259607
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3545853e-05
Norm of the params: 12.291536
              Random: fixed  12 labels. Loss 0.06026. Accuracy 0.985.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17835599
Train loss (w/o reg) on all data: 0.1724271
Test loss (w/o reg) on all data: 0.11580685
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.7491037e-05
Norm of the params: 10.889338
Flipped loss: 0.11581. Accuracy: 0.973
### Flips: 156, rs: 0, checks: 52
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075954415
Train loss (w/o reg) on all data: 0.06769103
Test loss (w/o reg) on all data: 0.07994349
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.3352595e-06
Norm of the params: 12.855648
     Influence (LOO): fixed  43 labels. Loss 0.07994. Accuracy 0.962.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038306978
Train loss (w/o reg) on all data: 0.026801212
Test loss (w/o reg) on all data: 0.048829813
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.4602402e-06
Norm of the params: 15.169554
                Loss: fixed  52 labels. Loss 0.04883. Accuracy 0.977.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17057799
Train loss (w/o reg) on all data: 0.16447887
Test loss (w/o reg) on all data: 0.111872785
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.137075e-05
Norm of the params: 11.044565
              Random: fixed   5 labels. Loss 0.11187. Accuracy 0.973.
### Flips: 156, rs: 0, checks: 104
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029334608
Train loss (w/o reg) on all data: 0.02209054
Test loss (w/o reg) on all data: 0.03711692
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.3227815e-07
Norm of the params: 12.036668
     Influence (LOO): fixed  65 labels. Loss 0.03712. Accuracy 0.981.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006971534
Train loss (w/o reg) on all data: 0.002517216
Test loss (w/o reg) on all data: 0.011187647
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.389879e-07
Norm of the params: 9.438557
                Loss: fixed  74 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15291594
Train loss (w/o reg) on all data: 0.14652702
Test loss (w/o reg) on all data: 0.11010039
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.5421841e-05
Norm of the params: 11.303907
              Random: fixed  14 labels. Loss 0.11010. Accuracy 0.962.
### Flips: 156, rs: 0, checks: 156
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729742
Test loss (w/o reg) on all data: 0.012054751
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6189256e-07
Norm of the params: 9.15319
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729723
Test loss (w/o reg) on all data: 0.012055063
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.105006e-07
Norm of the params: 9.153192
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14595826
Train loss (w/o reg) on all data: 0.1387154
Test loss (w/o reg) on all data: 0.10765865
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.9676465e-05
Norm of the params: 12.035664
              Random: fixed  17 labels. Loss 0.10766. Accuracy 0.958.
### Flips: 156, rs: 0, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729516
Test loss (w/o reg) on all data: 0.0120549835
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4705174e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012055135
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2810896e-07
Norm of the params: 9.15321
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13949353
Train loss (w/o reg) on all data: 0.13212268
Test loss (w/o reg) on all data: 0.09966658
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3384588e-05
Norm of the params: 12.141536
              Random: fixed  21 labels. Loss 0.09967. Accuracy 0.969.
### Flips: 156, rs: 0, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730473
Test loss (w/o reg) on all data: 0.012055351
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0280563e-07
Norm of the params: 9.15311
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730463
Test loss (w/o reg) on all data: 0.012055433
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1621142e-07
Norm of the params: 9.1531105
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13612594
Train loss (w/o reg) on all data: 0.12884988
Test loss (w/o reg) on all data: 0.097865984
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.990394e-06
Norm of the params: 12.0632105
              Random: fixed  22 labels. Loss 0.09787. Accuracy 0.973.
### Flips: 156, rs: 0, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729171
Test loss (w/o reg) on all data: 0.012054963
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.397843e-07
Norm of the params: 9.153254
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729146
Test loss (w/o reg) on all data: 0.01205503
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7881861e-07
Norm of the params: 9.153254
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13612595
Train loss (w/o reg) on all data: 0.12885083
Test loss (w/o reg) on all data: 0.097871624
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0272222e-05
Norm of the params: 12.062441
              Random: fixed  22 labels. Loss 0.09787. Accuracy 0.973.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17488225
Train loss (w/o reg) on all data: 0.16710149
Test loss (w/o reg) on all data: 0.07697846
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6081874e-05
Norm of the params: 12.474578
Flipped loss: 0.07698. Accuracy: 0.992
### Flips: 156, rs: 1, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07612991
Train loss (w/o reg) on all data: 0.06672162
Test loss (w/o reg) on all data: 0.035301108
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0296622e-05
Norm of the params: 13.717357
     Influence (LOO): fixed  44 labels. Loss 0.03530. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040485248
Train loss (w/o reg) on all data: 0.027697867
Test loss (w/o reg) on all data: 0.038412113
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7937013e-06
Norm of the params: 15.992112
                Loss: fixed  52 labels. Loss 0.03841. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17177568
Train loss (w/o reg) on all data: 0.16395265
Test loss (w/o reg) on all data: 0.07271107
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.704492e-06
Norm of the params: 12.508422
              Random: fixed   2 labels. Loss 0.07271. Accuracy 0.992.
### Flips: 156, rs: 1, checks: 104
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034939848
Train loss (w/o reg) on all data: 0.027463038
Test loss (w/o reg) on all data: 0.018032312
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.388603e-06
Norm of the params: 12.228498
     Influence (LOO): fixed  62 labels. Loss 0.01803. Accuracy 0.992.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008888101
Train loss (w/o reg) on all data: 0.0035021836
Test loss (w/o reg) on all data: 0.012814582
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6706388e-07
Norm of the params: 10.378746
                Loss: fixed  71 labels. Loss 0.01281. Accuracy 0.996.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16919689
Train loss (w/o reg) on all data: 0.16141047
Test loss (w/o reg) on all data: 0.07075467
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4877306e-05
Norm of the params: 12.479119
              Random: fixed   4 labels. Loss 0.07075. Accuracy 0.992.
### Flips: 156, rs: 1, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172915
Test loss (w/o reg) on all data: 0.012055822
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1456427e-07
Norm of the params: 9.153255
     Influence (LOO): fixed  73 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172923
Test loss (w/o reg) on all data: 0.012055428
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.724657e-07
Norm of the params: 9.153244
                Loss: fixed  73 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15931734
Train loss (w/o reg) on all data: 0.15111433
Test loss (w/o reg) on all data: 0.06665921
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2506023e-05
Norm of the params: 12.808599
              Random: fixed   9 labels. Loss 0.06666. Accuracy 0.992.
### Flips: 156, rs: 1, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172969
Test loss (w/o reg) on all data: 0.012054861
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3111427e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729688
Test loss (w/o reg) on all data: 0.012054947
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.1979614e-08
Norm of the params: 9.153197
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15130053
Train loss (w/o reg) on all data: 0.14306709
Test loss (w/o reg) on all data: 0.06521041
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6989233e-05
Norm of the params: 12.832334
              Random: fixed  14 labels. Loss 0.06521. Accuracy 0.992.
### Flips: 156, rs: 1, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021728657
Test loss (w/o reg) on all data: 0.012054123
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7684243e-07
Norm of the params: 9.153311
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021728673
Test loss (w/o reg) on all data: 0.012054224
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1526574e-07
Norm of the params: 9.15331
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1438519
Train loss (w/o reg) on all data: 0.1355821
Test loss (w/o reg) on all data: 0.0633307
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9878653e-06
Norm of the params: 12.860639
              Random: fixed  17 labels. Loss 0.06333. Accuracy 0.992.
### Flips: 156, rs: 1, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730107
Test loss (w/o reg) on all data: 0.012055372
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6986857e-07
Norm of the params: 9.1531515
     Influence (LOO): fixed  73 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730084
Test loss (w/o reg) on all data: 0.012055409
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3951104e-07
Norm of the params: 9.1531515
                Loss: fixed  73 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13066673
Train loss (w/o reg) on all data: 0.12245145
Test loss (w/o reg) on all data: 0.054492753
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6589223e-05
Norm of the params: 12.818176
              Random: fixed  24 labels. Loss 0.05449. Accuracy 0.992.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16469675
Train loss (w/o reg) on all data: 0.15678483
Test loss (w/o reg) on all data: 0.0839456
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.4240027e-05
Norm of the params: 12.57929
Flipped loss: 0.08395. Accuracy: 0.977
### Flips: 156, rs: 2, checks: 52
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071691014
Train loss (w/o reg) on all data: 0.062613666
Test loss (w/o reg) on all data: 0.050006486
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4695773e-05
Norm of the params: 13.473936
     Influence (LOO): fixed  42 labels. Loss 0.05001. Accuracy 0.977.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034745418
Train loss (w/o reg) on all data: 0.023519084
Test loss (w/o reg) on all data: 0.040136583
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.6428654e-06
Norm of the params: 14.984215
                Loss: fixed  52 labels. Loss 0.04014. Accuracy 0.981.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15740725
Train loss (w/o reg) on all data: 0.14952624
Test loss (w/o reg) on all data: 0.075790256
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2107535e-05
Norm of the params: 12.554698
              Random: fixed   5 labels. Loss 0.07579. Accuracy 0.985.
### Flips: 156, rs: 2, checks: 104
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02469572
Train loss (w/o reg) on all data: 0.015783925
Test loss (w/o reg) on all data: 0.024139835
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2057482e-06
Norm of the params: 13.350504
     Influence (LOO): fixed  63 labels. Loss 0.02414. Accuracy 0.989.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077833813
Train loss (w/o reg) on all data: 0.002898265
Test loss (w/o reg) on all data: 0.012001185
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3383924e-07
Norm of the params: 9.884449
                Loss: fixed  70 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15091434
Train loss (w/o reg) on all data: 0.14284499
Test loss (w/o reg) on all data: 0.07571669
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8673825e-05
Norm of the params: 12.70382
              Random: fixed   9 labels. Loss 0.07572. Accuracy 0.985.
### Flips: 156, rs: 2, checks: 156
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01197348
Train loss (w/o reg) on all data: 0.0064860145
Test loss (w/o reg) on all data: 0.013353763
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.274365e-07
Norm of the params: 10.476131
     Influence (LOO): fixed  69 labels. Loss 0.01335. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.012055046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3761685e-07
Norm of the params: 9.153183
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14270742
Train loss (w/o reg) on all data: 0.13501792
Test loss (w/o reg) on all data: 0.07292117
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8920616e-05
Norm of the params: 12.40122
              Random: fixed  14 labels. Loss 0.07292. Accuracy 0.981.
### Flips: 156, rs: 2, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729283
Test loss (w/o reg) on all data: 0.012055234
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7617887e-07
Norm of the params: 9.15324
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729304
Test loss (w/o reg) on all data: 0.012055295
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3301248e-07
Norm of the params: 9.15324
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13980994
Train loss (w/o reg) on all data: 0.1319119
Test loss (w/o reg) on all data: 0.06719805
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.36302e-05
Norm of the params: 12.568235
              Random: fixed  16 labels. Loss 0.06720. Accuracy 0.989.
### Flips: 156, rs: 2, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729518
Test loss (w/o reg) on all data: 0.012054773
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8164072e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729541
Test loss (w/o reg) on all data: 0.012054792
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.191989e-07
Norm of the params: 9.153212
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12329303
Train loss (w/o reg) on all data: 0.115357235
Test loss (w/o reg) on all data: 0.05980697
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0120622e-05
Norm of the params: 12.598248
              Random: fixed  24 labels. Loss 0.05981. Accuracy 0.985.
### Flips: 156, rs: 2, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012055243
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5190204e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729628
Test loss (w/o reg) on all data: 0.012055286
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7010292e-07
Norm of the params: 9.153203
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11791287
Train loss (w/o reg) on all data: 0.10992568
Test loss (w/o reg) on all data: 0.055102576
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.8168592e-06
Norm of the params: 12.63898
              Random: fixed  27 labels. Loss 0.05510. Accuracy 0.989.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18960543
Train loss (w/o reg) on all data: 0.18181327
Test loss (w/o reg) on all data: 0.12296797
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5582747e-05
Norm of the params: 12.483721
Flipped loss: 0.12297. Accuracy: 0.973
### Flips: 156, rs: 3, checks: 52
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10025122
Train loss (w/o reg) on all data: 0.090480514
Test loss (w/o reg) on all data: 0.08101979
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.920749e-06
Norm of the params: 13.979059
     Influence (LOO): fixed  41 labels. Loss 0.08102. Accuracy 0.973.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06589173
Train loss (w/o reg) on all data: 0.052882083
Test loss (w/o reg) on all data: 0.07793583
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.8579676e-06
Norm of the params: 16.130495
                Loss: fixed  51 labels. Loss 0.07794. Accuracy 0.977.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18581547
Train loss (w/o reg) on all data: 0.17791064
Test loss (w/o reg) on all data: 0.11734316
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4935272e-05
Norm of the params: 12.573644
              Random: fixed   3 labels. Loss 0.11734. Accuracy 0.981.
### Flips: 156, rs: 3, checks: 104
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05065917
Train loss (w/o reg) on all data: 0.042691775
Test loss (w/o reg) on all data: 0.028661931
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9762433e-06
Norm of the params: 12.623307
     Influence (LOO): fixed  66 labels. Loss 0.02866. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009946374
Train loss (w/o reg) on all data: 0.0041290387
Test loss (w/o reg) on all data: 0.025591843
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.600421e-07
Norm of the params: 10.786412
                Loss: fixed  79 labels. Loss 0.02559. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18054771
Train loss (w/o reg) on all data: 0.17325194
Test loss (w/o reg) on all data: 0.11154942
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.7360513e-05
Norm of the params: 12.079553
              Random: fixed   8 labels. Loss 0.11155. Accuracy 0.981.
### Flips: 156, rs: 3, checks: 156
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016517134
Train loss (w/o reg) on all data: 0.010256884
Test loss (w/o reg) on all data: 0.009907381
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.699162e-07
Norm of the params: 11.189506
     Influence (LOO): fixed  80 labels. Loss 0.00991. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008392008
Train loss (w/o reg) on all data: 0.003321861
Test loss (w/o reg) on all data: 0.014457081
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6171556e-07
Norm of the params: 10.069901
                Loss: fixed  81 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17042609
Train loss (w/o reg) on all data: 0.16309975
Test loss (w/o reg) on all data: 0.10265038
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.6711306e-05
Norm of the params: 12.104817
              Random: fixed  14 labels. Loss 0.10265. Accuracy 0.981.
### Flips: 156, rs: 3, checks: 208
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172968
Test loss (w/o reg) on all data: 0.012055162
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7228464e-07
Norm of the params: 9.153196
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.012055077
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.057947e-07
Norm of the params: 9.153196
                Loss: fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1647762
Train loss (w/o reg) on all data: 0.15734826
Test loss (w/o reg) on all data: 0.09994092
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1201316e-05
Norm of the params: 12.188477
              Random: fixed  18 labels. Loss 0.09994. Accuracy 0.977.
### Flips: 156, rs: 3, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172856
Test loss (w/o reg) on all data: 0.012055722
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6347462e-07
Norm of the params: 9.153318
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728787
Test loss (w/o reg) on all data: 0.0120557295
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.498105e-07
Norm of the params: 9.153293
                Loss: fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1573412
Train loss (w/o reg) on all data: 0.1500571
Test loss (w/o reg) on all data: 0.095742084
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.9854327e-05
Norm of the params: 12.069867
              Random: fixed  22 labels. Loss 0.09574. Accuracy 0.981.
### Flips: 156, rs: 3, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729649
Test loss (w/o reg) on all data: 0.012055292
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2732613e-07
Norm of the params: 9.1532
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729565
Test loss (w/o reg) on all data: 0.012054861
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2982643e-07
Norm of the params: 9.15321
                Loss: fixed  83 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14532264
Train loss (w/o reg) on all data: 0.13796137
Test loss (w/o reg) on all data: 0.080368
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3522454e-05
Norm of the params: 12.133636
              Random: fixed  29 labels. Loss 0.08037. Accuracy 0.985.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15575309
Train loss (w/o reg) on all data: 0.14831442
Test loss (w/o reg) on all data: 0.06782521
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9856727e-05
Norm of the params: 12.19727
Flipped loss: 0.06783. Accuracy: 0.996
### Flips: 156, rs: 4, checks: 52
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062258426
Train loss (w/o reg) on all data: 0.05408686
Test loss (w/o reg) on all data: 0.03585966
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.0300137e-06
Norm of the params: 12.784026
     Influence (LOO): fixed  42 labels. Loss 0.03586. Accuracy 0.996.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019146267
Train loss (w/o reg) on all data: 0.010526653
Test loss (w/o reg) on all data: 0.01476881
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8371054e-06
Norm of the params: 13.129825
                Loss: fixed  51 labels. Loss 0.01477. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15028867
Train loss (w/o reg) on all data: 0.14214289
Test loss (w/o reg) on all data: 0.06360023
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.644797e-06
Norm of the params: 12.763839
              Random: fixed   3 labels. Loss 0.06360. Accuracy 0.996.
### Flips: 156, rs: 4, checks: 104
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011234207
Train loss (w/o reg) on all data: 0.005528465
Test loss (w/o reg) on all data: 0.014958946
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.7158014e-07
Norm of the params: 10.682455
     Influence (LOO): fixed  60 labels. Loss 0.01496. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056506
Train loss (w/o reg) on all data: 0.002817071
Test loss (w/o reg) on all data: 0.012755207
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7892668e-07
Norm of the params: 9.683574
                Loss: fixed  61 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14720379
Train loss (w/o reg) on all data: 0.1390774
Test loss (w/o reg) on all data: 0.063908674
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.918335e-06
Norm of the params: 12.748638
              Random: fixed   5 labels. Loss 0.06391. Accuracy 0.992.
### Flips: 156, rs: 4, checks: 156
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729327
Test loss (w/o reg) on all data: 0.0120552
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.217874e-07
Norm of the params: 9.153235
     Influence (LOO): fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729355
Test loss (w/o reg) on all data: 0.012055029
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.868175e-08
Norm of the params: 9.153235
                Loss: fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14666109
Train loss (w/o reg) on all data: 0.13826256
Test loss (w/o reg) on all data: 0.068063416
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0269694e-05
Norm of the params: 12.960355
              Random: fixed   6 labels. Loss 0.06806. Accuracy 0.989.
### Flips: 156, rs: 4, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172973
Test loss (w/o reg) on all data: 0.012054596
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.879933e-08
Norm of the params: 9.1531925
     Influence (LOO): fixed  62 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729728
Test loss (w/o reg) on all data: 0.012054642
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5073508e-07
Norm of the params: 9.1531925
                Loss: fixed  62 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13333438
Train loss (w/o reg) on all data: 0.12476593
Test loss (w/o reg) on all data: 0.06395771
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.56902e-06
Norm of the params: 13.090801
              Random: fixed  12 labels. Loss 0.06396. Accuracy 0.992.
### Flips: 156, rs: 4, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729565
Test loss (w/o reg) on all data: 0.012055046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.641238e-07
Norm of the params: 9.153211
     Influence (LOO): fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.002172958
Test loss (w/o reg) on all data: 0.012055107
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0032318e-07
Norm of the params: 9.15321
                Loss: fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13333437
Train loss (w/o reg) on all data: 0.124766834
Test loss (w/o reg) on all data: 0.06395118
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.877417e-06
Norm of the params: 13.090099
              Random: fixed  12 labels. Loss 0.06395. Accuracy 0.992.
### Flips: 156, rs: 4, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728526
Test loss (w/o reg) on all data: 0.0120552825
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.449529e-07
Norm of the params: 9.153321
     Influence (LOO): fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728543
Test loss (w/o reg) on all data: 0.012055167
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9960436e-07
Norm of the params: 9.15332
                Loss: fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1285942
Train loss (w/o reg) on all data: 0.1197934
Test loss (w/o reg) on all data: 0.061159935
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5976322e-05
Norm of the params: 13.267099
              Random: fixed  15 labels. Loss 0.06116. Accuracy 0.996.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18540369
Train loss (w/o reg) on all data: 0.17706676
Test loss (w/o reg) on all data: 0.11296789
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.686584e-05
Norm of the params: 12.912737
Flipped loss: 0.11297. Accuracy: 0.981
### Flips: 156, rs: 5, checks: 52
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096782334
Train loss (w/o reg) on all data: 0.08559003
Test loss (w/o reg) on all data: 0.06371923
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.932618e-06
Norm of the params: 14.961487
     Influence (LOO): fixed  40 labels. Loss 0.06372. Accuracy 0.981.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05234956
Train loss (w/o reg) on all data: 0.038679685
Test loss (w/o reg) on all data: 0.07436
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.68483e-06
Norm of the params: 16.534737
                Loss: fixed  51 labels. Loss 0.07436. Accuracy 0.969.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1779057
Train loss (w/o reg) on all data: 0.17000218
Test loss (w/o reg) on all data: 0.10785419
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2313011e-05
Norm of the params: 12.572605
              Random: fixed   5 labels. Loss 0.10785. Accuracy 0.985.
### Flips: 156, rs: 5, checks: 104
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03999704
Train loss (w/o reg) on all data: 0.03134801
Test loss (w/o reg) on all data: 0.02676278
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.8448505e-06
Norm of the params: 13.15221
     Influence (LOO): fixed  68 labels. Loss 0.02676. Accuracy 0.989.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008422848
Train loss (w/o reg) on all data: 0.0032732254
Test loss (w/o reg) on all data: 0.019713754
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.32964e-07
Norm of the params: 10.1485195
                Loss: fixed  79 labels. Loss 0.01971. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1662629
Train loss (w/o reg) on all data: 0.15838715
Test loss (w/o reg) on all data: 0.10433657
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0338745e-05
Norm of the params: 12.550493
              Random: fixed  11 labels. Loss 0.10434. Accuracy 0.981.
### Flips: 156, rs: 5, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730084
Test loss (w/o reg) on all data: 0.0120547665
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.819222e-07
Norm of the params: 9.1531515
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073833745
Train loss (w/o reg) on all data: 0.0026668229
Test loss (w/o reg) on all data: 0.021342428
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0748376e-07
Norm of the params: 9.712418
                Loss: fixed  80 labels. Loss 0.02134. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15850307
Train loss (w/o reg) on all data: 0.15066266
Test loss (w/o reg) on all data: 0.09378842
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1863178e-05
Norm of the params: 12.522303
              Random: fixed  16 labels. Loss 0.09379. Accuracy 0.985.
### Flips: 156, rs: 5, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173
Test loss (w/o reg) on all data: 0.012054895
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4979257e-07
Norm of the params: 9.153161
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007383373
Train loss (w/o reg) on all data: 0.0026668669
Test loss (w/o reg) on all data: 0.021343576
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1375026e-07
Norm of the params: 9.712371
                Loss: fixed  80 labels. Loss 0.02134. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15685399
Train loss (w/o reg) on all data: 0.14918047
Test loss (w/o reg) on all data: 0.08094151
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.3516912e-06
Norm of the params: 12.388309
              Random: fixed  19 labels. Loss 0.08094. Accuracy 0.992.
### Flips: 156, rs: 5, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730184
Test loss (w/o reg) on all data: 0.012055087
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6587613e-07
Norm of the params: 9.153141
     Influence (LOO): fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730168
Test loss (w/o reg) on all data: 0.012054996
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2852625e-07
Norm of the params: 9.153142
                Loss: fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15130949
Train loss (w/o reg) on all data: 0.14389683
Test loss (w/o reg) on all data: 0.08172681
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.3059837e-06
Norm of the params: 12.175931
              Random: fixed  21 labels. Loss 0.08173. Accuracy 0.985.
### Flips: 156, rs: 5, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729937
Test loss (w/o reg) on all data: 0.012054909
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2974737e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012054842
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8014998e-07
Norm of the params: 9.153169
                Loss: fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14795499
Train loss (w/o reg) on all data: 0.14095993
Test loss (w/o reg) on all data: 0.0738559
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3225504e-06
Norm of the params: 11.827983
              Random: fixed  25 labels. Loss 0.07386. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17814074
Train loss (w/o reg) on all data: 0.16935797
Test loss (w/o reg) on all data: 0.084313415
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.346186e-06
Norm of the params: 13.253507
Flipped loss: 0.08431. Accuracy: 0.989
### Flips: 156, rs: 6, checks: 52
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096611805
Train loss (w/o reg) on all data: 0.08501849
Test loss (w/o reg) on all data: 0.05424669
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2302958e-05
Norm of the params: 15.227155
     Influence (LOO): fixed  35 labels. Loss 0.05425. Accuracy 0.989.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04822512
Train loss (w/o reg) on all data: 0.033245854
Test loss (w/o reg) on all data: 0.03013839
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6875534e-05
Norm of the params: 17.308533
                Loss: fixed  51 labels. Loss 0.03014. Accuracy 0.996.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17415747
Train loss (w/o reg) on all data: 0.16579886
Test loss (w/o reg) on all data: 0.08145073
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0980662e-05
Norm of the params: 12.92951
              Random: fixed   4 labels. Loss 0.08145. Accuracy 0.989.
### Flips: 156, rs: 6, checks: 104
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02940197
Train loss (w/o reg) on all data: 0.021105722
Test loss (w/o reg) on all data: 0.017840726
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1587676e-06
Norm of the params: 12.8811865
     Influence (LOO): fixed  67 labels. Loss 0.01784. Accuracy 0.996.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009749221
Train loss (w/o reg) on all data: 0.0040926426
Test loss (w/o reg) on all data: 0.011582478
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.2083886e-07
Norm of the params: 10.6363325
                Loss: fixed  75 labels. Loss 0.01158. Accuracy 0.996.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16992502
Train loss (w/o reg) on all data: 0.16166095
Test loss (w/o reg) on all data: 0.07462249
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7850902e-05
Norm of the params: 12.856181
              Random: fixed   7 labels. Loss 0.07462. Accuracy 0.992.
### Flips: 156, rs: 6, checks: 156
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01063355
Train loss (w/o reg) on all data: 0.0051621944
Test loss (w/o reg) on all data: 0.011485262
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.0246977e-07
Norm of the params: 10.460742
     Influence (LOO): fixed  75 labels. Loss 0.01149. Accuracy 0.996.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075563425
Train loss (w/o reg) on all data: 0.002756789
Test loss (w/o reg) on all data: 0.010965655
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2525263e-07
Norm of the params: 9.797503
                Loss: fixed  76 labels. Loss 0.01097. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15881582
Train loss (w/o reg) on all data: 0.15034625
Test loss (w/o reg) on all data: 0.06780236
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.92123e-05
Norm of the params: 13.015043
              Random: fixed  13 labels. Loss 0.06780. Accuracy 0.989.
### Flips: 156, rs: 6, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730089
Test loss (w/o reg) on all data: 0.012054961
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1222785e-07
Norm of the params: 9.153151
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075563435
Train loss (w/o reg) on all data: 0.0027568021
Test loss (w/o reg) on all data: 0.010965245
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.903059e-08
Norm of the params: 9.797491
                Loss: fixed  76 labels. Loss 0.01097. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15191972
Train loss (w/o reg) on all data: 0.14304686
Test loss (w/o reg) on all data: 0.06536395
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0016608e-05
Norm of the params: 13.321307
              Random: fixed  16 labels. Loss 0.06536. Accuracy 0.989.
### Flips: 156, rs: 6, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729588
Test loss (w/o reg) on all data: 0.012055213
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7779552e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075563444
Train loss (w/o reg) on all data: 0.002756708
Test loss (w/o reg) on all data: 0.010964901
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5164105e-07
Norm of the params: 9.797587
                Loss: fixed  76 labels. Loss 0.01096. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14416434
Train loss (w/o reg) on all data: 0.13530792
Test loss (w/o reg) on all data: 0.0632102
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1758515e-05
Norm of the params: 13.308955
              Random: fixed  21 labels. Loss 0.06321. Accuracy 0.996.
### Flips: 156, rs: 6, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730775
Test loss (w/o reg) on all data: 0.0120552145
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.691213e-07
Norm of the params: 9.153077
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730764
Test loss (w/o reg) on all data: 0.012055041
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3925645e-07
Norm of the params: 9.153079
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14215462
Train loss (w/o reg) on all data: 0.13338026
Test loss (w/o reg) on all data: 0.061320364
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6155694e-05
Norm of the params: 13.247153
              Random: fixed  22 labels. Loss 0.06132. Accuracy 0.996.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18527338
Train loss (w/o reg) on all data: 0.17907245
Test loss (w/o reg) on all data: 0.12737514
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5008357e-05
Norm of the params: 11.136366
Flipped loss: 0.12738. Accuracy: 0.992
### Flips: 156, rs: 7, checks: 52
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089535125
Train loss (w/o reg) on all data: 0.07990918
Test loss (w/o reg) on all data: 0.08592103
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5992858e-05
Norm of the params: 13.875114
     Influence (LOO): fixed  39 labels. Loss 0.08592. Accuracy 0.973.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047949273
Train loss (w/o reg) on all data: 0.03463776
Test loss (w/o reg) on all data: 0.06678968
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.3462934e-06
Norm of the params: 16.316565
                Loss: fixed  52 labels. Loss 0.06679. Accuracy 0.981.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17451803
Train loss (w/o reg) on all data: 0.16817674
Test loss (w/o reg) on all data: 0.11471211
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3539929e-05
Norm of the params: 11.261693
              Random: fixed   7 labels. Loss 0.11471. Accuracy 0.985.
### Flips: 156, rs: 7, checks: 104
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04568878
Train loss (w/o reg) on all data: 0.03755174
Test loss (w/o reg) on all data: 0.039363343
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.8275883e-06
Norm of the params: 12.756992
     Influence (LOO): fixed  61 labels. Loss 0.03936. Accuracy 0.996.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729204
Test loss (w/o reg) on all data: 0.0120548755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3758706e-07
Norm of the params: 9.153249
                Loss: fixed  78 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16951382
Train loss (w/o reg) on all data: 0.16319951
Test loss (w/o reg) on all data: 0.10987045
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1142627e-05
Norm of the params: 11.23771
              Random: fixed  10 labels. Loss 0.10987. Accuracy 0.989.
### Flips: 156, rs: 7, checks: 156
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013341175
Train loss (w/o reg) on all data: 0.0071081263
Test loss (w/o reg) on all data: 0.013268516
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.9005744e-07
Norm of the params: 11.165169
     Influence (LOO): fixed  75 labels. Loss 0.01327. Accuracy 0.996.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729253
Test loss (w/o reg) on all data: 0.0120552145
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2070126e-07
Norm of the params: 9.153244
                Loss: fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16485685
Train loss (w/o reg) on all data: 0.1584247
Test loss (w/o reg) on all data: 0.10537162
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2832253e-05
Norm of the params: 11.342083
              Random: fixed  13 labels. Loss 0.10537. Accuracy 0.989.
### Flips: 156, rs: 7, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730391
Test loss (w/o reg) on all data: 0.012055361
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.976729e-07
Norm of the params: 9.153118
     Influence (LOO): fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730298
Test loss (w/o reg) on all data: 0.01205529
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3688449e-06
Norm of the params: 9.153128
                Loss: fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15876688
Train loss (w/o reg) on all data: 0.15221518
Test loss (w/o reg) on all data: 0.106567144
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9172797e-05
Norm of the params: 11.447009
              Random: fixed  17 labels. Loss 0.10657. Accuracy 0.985.
### Flips: 156, rs: 7, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730189
Test loss (w/o reg) on all data: 0.012055759
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.168683e-07
Norm of the params: 9.153142
     Influence (LOO): fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730175
Test loss (w/o reg) on all data: 0.012055576
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.394601e-07
Norm of the params: 9.153143
                Loss: fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15349899
Train loss (w/o reg) on all data: 0.14701013
Test loss (w/o reg) on all data: 0.09975327
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.867157e-05
Norm of the params: 11.391977
              Random: fixed  20 labels. Loss 0.09975. Accuracy 0.985.
### Flips: 156, rs: 7, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728391
Test loss (w/o reg) on all data: 0.012055393
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6932974e-07
Norm of the params: 9.153337
     Influence (LOO): fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728696
Test loss (w/o reg) on all data: 0.012055598
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.891968e-07
Norm of the params: 9.153303
                Loss: fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14998445
Train loss (w/o reg) on all data: 0.14367306
Test loss (w/o reg) on all data: 0.09238225
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8088112e-06
Norm of the params: 11.235115
              Random: fixed  23 labels. Loss 0.09238. Accuracy 0.989.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18594533
Train loss (w/o reg) on all data: 0.17637776
Test loss (w/o reg) on all data: 0.09877592
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1600452e-05
Norm of the params: 13.832987
Flipped loss: 0.09878. Accuracy: 0.985
### Flips: 156, rs: 8, checks: 52
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10098819
Train loss (w/o reg) on all data: 0.09033521
Test loss (w/o reg) on all data: 0.06532571
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.071576e-06
Norm of the params: 14.596559
     Influence (LOO): fixed  41 labels. Loss 0.06533. Accuracy 0.989.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059439614
Train loss (w/o reg) on all data: 0.04418209
Test loss (w/o reg) on all data: 0.044431005
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.038269e-06
Norm of the params: 17.46856
                Loss: fixed  51 labels. Loss 0.04443. Accuracy 0.989.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18448247
Train loss (w/o reg) on all data: 0.17487282
Test loss (w/o reg) on all data: 0.095151596
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9145486e-05
Norm of the params: 13.863376
              Random: fixed   2 labels. Loss 0.09515. Accuracy 0.989.
### Flips: 156, rs: 8, checks: 104
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042142227
Train loss (w/o reg) on all data: 0.032327604
Test loss (w/o reg) on all data: 0.03405603
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.792198e-06
Norm of the params: 14.01044
     Influence (LOO): fixed  66 labels. Loss 0.03406. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007783384
Train loss (w/o reg) on all data: 0.0028981902
Test loss (w/o reg) on all data: 0.0120013375
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4467096e-07
Norm of the params: 9.884528
                Loss: fixed  78 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17651924
Train loss (w/o reg) on all data: 0.16733494
Test loss (w/o reg) on all data: 0.09026307
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.813732e-06
Norm of the params: 13.553077
              Random: fixed   8 labels. Loss 0.09026. Accuracy 0.992.
### Flips: 156, rs: 8, checks: 156
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013553418
Train loss (w/o reg) on all data: 0.0077901166
Test loss (w/o reg) on all data: 0.014716265
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.377593e-07
Norm of the params: 10.736201
     Influence (LOO): fixed  77 labels. Loss 0.01472. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007783383
Train loss (w/o reg) on all data: 0.00289827
Test loss (w/o reg) on all data: 0.012001372
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7210629e-07
Norm of the params: 9.884446
                Loss: fixed  78 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17515732
Train loss (w/o reg) on all data: 0.16610481
Test loss (w/o reg) on all data: 0.08872785
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.554989e-06
Norm of the params: 13.455492
              Random: fixed   9 labels. Loss 0.08873. Accuracy 0.992.
### Flips: 156, rs: 8, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172938
Test loss (w/o reg) on all data: 0.012054491
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.38223e-07
Norm of the params: 9.15323
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172939
Test loss (w/o reg) on all data: 0.012054595
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.18860726e-07
Norm of the params: 9.15323
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16406278
Train loss (w/o reg) on all data: 0.15434873
Test loss (w/o reg) on all data: 0.086182766
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.6166107e-05
Norm of the params: 13.938475
              Random: fixed  14 labels. Loss 0.08618. Accuracy 0.989.
### Flips: 156, rs: 8, checks: 260
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729798
Test loss (w/o reg) on all data: 0.012055089
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0671347e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012055149
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8002194e-07
Norm of the params: 9.153183
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15518092
Train loss (w/o reg) on all data: 0.1452009
Test loss (w/o reg) on all data: 0.077381946
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5899575e-06
Norm of the params: 14.128002
              Random: fixed  19 labels. Loss 0.07738. Accuracy 0.992.
### Flips: 156, rs: 8, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729395
Test loss (w/o reg) on all data: 0.012055055
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1660105e-07
Norm of the params: 9.153229
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729458
Test loss (w/o reg) on all data: 0.012055042
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7675e-07
Norm of the params: 9.153221
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15058558
Train loss (w/o reg) on all data: 0.14072247
Test loss (w/o reg) on all data: 0.07373784
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4824906e-05
Norm of the params: 14.04501
              Random: fixed  21 labels. Loss 0.07374. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16755094
Train loss (w/o reg) on all data: 0.158251
Test loss (w/o reg) on all data: 0.10552354
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1050113e-05
Norm of the params: 13.638134
Flipped loss: 0.10552. Accuracy: 0.989
### Flips: 156, rs: 9, checks: 52
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07927943
Train loss (w/o reg) on all data: 0.06941694
Test loss (w/o reg) on all data: 0.04926217
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7280927e-06
Norm of the params: 14.044566
     Influence (LOO): fixed  41 labels. Loss 0.04926. Accuracy 0.985.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04196901
Train loss (w/o reg) on all data: 0.029341893
Test loss (w/o reg) on all data: 0.05530234
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.677058e-06
Norm of the params: 15.89158
                Loss: fixed  50 labels. Loss 0.05530. Accuracy 0.981.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16520664
Train loss (w/o reg) on all data: 0.15593742
Test loss (w/o reg) on all data: 0.10267441
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.205217e-05
Norm of the params: 13.615594
              Random: fixed   2 labels. Loss 0.10267. Accuracy 0.989.
### Flips: 156, rs: 9, checks: 104
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040549576
Train loss (w/o reg) on all data: 0.031506706
Test loss (w/o reg) on all data: 0.031949777
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6422364e-06
Norm of the params: 13.448323
     Influence (LOO): fixed  59 labels. Loss 0.03195. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007505652
Train loss (w/o reg) on all data: 0.0028170412
Test loss (w/o reg) on all data: 0.012755942
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.217628e-07
Norm of the params: 9.683605
                Loss: fixed  70 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1580139
Train loss (w/o reg) on all data: 0.14779404
Test loss (w/o reg) on all data: 0.09681401
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0439065e-05
Norm of the params: 14.296755
              Random: fixed   6 labels. Loss 0.09681. Accuracy 0.981.
### Flips: 156, rs: 9, checks: 156
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013526844
Train loss (w/o reg) on all data: 0.0077727665
Test loss (w/o reg) on all data: 0.010997348
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4241444e-07
Norm of the params: 10.727608
     Influence (LOO): fixed  69 labels. Loss 0.01100. Accuracy 0.996.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173008
Test loss (w/o reg) on all data: 0.012055104
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2050453e-07
Norm of the params: 9.153154
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15494788
Train loss (w/o reg) on all data: 0.14431542
Test loss (w/o reg) on all data: 0.09653626
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6183578e-05
Norm of the params: 14.582488
              Random: fixed   8 labels. Loss 0.09654. Accuracy 0.985.
### Flips: 156, rs: 9, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729853
Test loss (w/o reg) on all data: 0.012055237
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4299964e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729849
Test loss (w/o reg) on all data: 0.012055277
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4815905e-07
Norm of the params: 9.153177
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15074715
Train loss (w/o reg) on all data: 0.14072782
Test loss (w/o reg) on all data: 0.09190331
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.425912e-05
Norm of the params: 14.155794
              Random: fixed  11 labels. Loss 0.09190. Accuracy 0.989.
### Flips: 156, rs: 9, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730838
Test loss (w/o reg) on all data: 0.012054641
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.730832e-07
Norm of the params: 9.15307
     Influence (LOO): fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730824
Test loss (w/o reg) on all data: 0.012054783
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.132731e-07
Norm of the params: 9.153071
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14063644
Train loss (w/o reg) on all data: 0.13110135
Test loss (w/o reg) on all data: 0.08576311
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5562407e-06
Norm of the params: 13.809483
              Random: fixed  17 labels. Loss 0.08576. Accuracy 0.992.
### Flips: 156, rs: 9, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172997
Test loss (w/o reg) on all data: 0.012055298
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.590378e-07
Norm of the params: 9.153164
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729963
Test loss (w/o reg) on all data: 0.012055244
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3813838e-07
Norm of the params: 9.153164
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13497104
Train loss (w/o reg) on all data: 0.12545572
Test loss (w/o reg) on all data: 0.07738013
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.860742e-06
Norm of the params: 13.795157
              Random: fixed  22 labels. Loss 0.07738. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17049296
Train loss (w/o reg) on all data: 0.16196248
Test loss (w/o reg) on all data: 0.11804381
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.473041e-06
Norm of the params: 13.06176
Flipped loss: 0.11804. Accuracy: 0.989
### Flips: 156, rs: 10, checks: 52
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08161649
Train loss (w/o reg) on all data: 0.07159959
Test loss (w/o reg) on all data: 0.06527305
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1988024e-05
Norm of the params: 14.15408
     Influence (LOO): fixed  41 labels. Loss 0.06527. Accuracy 0.989.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037015654
Train loss (w/o reg) on all data: 0.02486693
Test loss (w/o reg) on all data: 0.045944512
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.6459077e-06
Norm of the params: 15.58764
                Loss: fixed  52 labels. Loss 0.04594. Accuracy 0.989.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16795114
Train loss (w/o reg) on all data: 0.15933602
Test loss (w/o reg) on all data: 0.11495602
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.0766394e-05
Norm of the params: 13.126401
              Random: fixed   2 labels. Loss 0.11496. Accuracy 0.981.
### Flips: 156, rs: 10, checks: 104
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02191766
Train loss (w/o reg) on all data: 0.015348511
Test loss (w/o reg) on all data: 0.019126717
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6560539e-06
Norm of the params: 11.462242
     Influence (LOO): fixed  64 labels. Loss 0.01913. Accuracy 0.996.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008086494
Train loss (w/o reg) on all data: 0.0030052362
Test loss (w/o reg) on all data: 0.015876798
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.476029e-07
Norm of the params: 10.080931
                Loss: fixed  69 labels. Loss 0.01588. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16172071
Train loss (w/o reg) on all data: 0.15308279
Test loss (w/o reg) on all data: 0.11235145
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.4510995e-06
Norm of the params: 13.143757
              Random: fixed   6 labels. Loss 0.11235. Accuracy 0.981.
### Flips: 156, rs: 10, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002354
Train loss (w/o reg) on all data: 0.005447747
Test loss (w/o reg) on all data: 0.01224497
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1010777e-07
Norm of the params: 9.544221
     Influence (LOO): fixed  69 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.012054947
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3024653e-07
Norm of the params: 9.153235
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15438366
Train loss (w/o reg) on all data: 0.1456536
Test loss (w/o reg) on all data: 0.10091834
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3505457e-05
Norm of the params: 13.213673
              Random: fixed  10 labels. Loss 0.10092. Accuracy 0.992.
### Flips: 156, rs: 10, checks: 208
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002354
Train loss (w/o reg) on all data: 0.005447847
Test loss (w/o reg) on all data: 0.012245782
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7014391e-07
Norm of the params: 9.544116
     Influence (LOO): fixed  69 labels. Loss 0.01225. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729842
Test loss (w/o reg) on all data: 0.012054862
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5975456e-07
Norm of the params: 9.153178
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1495183
Train loss (w/o reg) on all data: 0.14016284
Test loss (w/o reg) on all data: 0.097376235
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.137555e-06
Norm of the params: 13.678782
              Random: fixed  12 labels. Loss 0.09738. Accuracy 0.989.
### Flips: 156, rs: 10, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729993
Test loss (w/o reg) on all data: 0.012055199
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4562907e-07
Norm of the params: 9.153163
     Influence (LOO): fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729958
Test loss (w/o reg) on all data: 0.012055219
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3663997e-07
Norm of the params: 9.153167
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14277974
Train loss (w/o reg) on all data: 0.13308969
Test loss (w/o reg) on all data: 0.09239686
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8976254e-05
Norm of the params: 13.921239
              Random: fixed  15 labels. Loss 0.09240. Accuracy 0.992.
### Flips: 156, rs: 10, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173063
Test loss (w/o reg) on all data: 0.012053975
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7163836e-07
Norm of the params: 9.153094
     Influence (LOO): fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730615
Test loss (w/o reg) on all data: 0.012054141
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9552186e-07
Norm of the params: 9.153094
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13738501
Train loss (w/o reg) on all data: 0.12760265
Test loss (w/o reg) on all data: 0.09048785
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7128808e-05
Norm of the params: 13.987396
              Random: fixed  18 labels. Loss 0.09049. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17175524
Train loss (w/o reg) on all data: 0.16438471
Test loss (w/o reg) on all data: 0.094025955
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2761998e-05
Norm of the params: 12.14128
Flipped loss: 0.09403. Accuracy: 0.992
### Flips: 156, rs: 11, checks: 52
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07130033
Train loss (w/o reg) on all data: 0.06257282
Test loss (w/o reg) on all data: 0.04537342
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.700489e-06
Norm of the params: 13.211743
     Influence (LOO): fixed  42 labels. Loss 0.04537. Accuracy 0.985.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031009056
Train loss (w/o reg) on all data: 0.020835515
Test loss (w/o reg) on all data: 0.037669513
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3266864e-06
Norm of the params: 14.264318
                Loss: fixed  52 labels. Loss 0.03767. Accuracy 0.989.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16942658
Train loss (w/o reg) on all data: 0.16208044
Test loss (w/o reg) on all data: 0.093764834
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.665237e-06
Norm of the params: 12.121165
              Random: fixed   2 labels. Loss 0.09376. Accuracy 0.989.
### Flips: 156, rs: 11, checks: 104
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020382077
Train loss (w/o reg) on all data: 0.01405861
Test loss (w/o reg) on all data: 0.01671728
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.5801505e-06
Norm of the params: 11.245859
     Influence (LOO): fixed  63 labels. Loss 0.01672. Accuracy 0.996.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729728
Test loss (w/o reg) on all data: 0.012055152
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.8989084e-08
Norm of the params: 9.1531925
                Loss: fixed  69 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16379344
Train loss (w/o reg) on all data: 0.15691347
Test loss (w/o reg) on all data: 0.08895761
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2388012e-05
Norm of the params: 11.730273
              Random: fixed   5 labels. Loss 0.08896. Accuracy 0.989.
### Flips: 156, rs: 11, checks: 156
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012285951
Train loss (w/o reg) on all data: 0.0071040723
Test loss (w/o reg) on all data: 0.01461614
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6001913e-07
Norm of the params: 10.180254
     Influence (LOO): fixed  67 labels. Loss 0.01462. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730496
Test loss (w/o reg) on all data: 0.012054879
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1574236e-07
Norm of the params: 9.153108
                Loss: fixed  69 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15625298
Train loss (w/o reg) on all data: 0.14932735
Test loss (w/o reg) on all data: 0.08181517
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.376817e-06
Norm of the params: 11.769131
              Random: fixed  10 labels. Loss 0.08182. Accuracy 0.989.
### Flips: 156, rs: 11, checks: 208
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010038311
Train loss (w/o reg) on all data: 0.0052869082
Test loss (w/o reg) on all data: 0.012296569
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0542072e-07
Norm of the params: 9.748234
     Influence (LOO): fixed  68 labels. Loss 0.01230. Accuracy 0.992.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172857
Test loss (w/o reg) on all data: 0.012055162
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5071745e-07
Norm of the params: 9.153318
                Loss: fixed  69 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14600904
Train loss (w/o reg) on all data: 0.13898446
Test loss (w/o reg) on all data: 0.07514543
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7319795e-05
Norm of the params: 11.852925
              Random: fixed  15 labels. Loss 0.07515. Accuracy 0.992.
### Flips: 156, rs: 11, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172997
Test loss (w/o reg) on all data: 0.012054919
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6261225e-07
Norm of the params: 9.153166
     Influence (LOO): fixed  69 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729956
Test loss (w/o reg) on all data: 0.012054873
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1814438e-07
Norm of the params: 9.153166
                Loss: fixed  69 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13693325
Train loss (w/o reg) on all data: 0.13005474
Test loss (w/o reg) on all data: 0.06833584
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0958757e-05
Norm of the params: 11.729038
              Random: fixed  19 labels. Loss 0.06834. Accuracy 0.992.
### Flips: 156, rs: 11, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.012054733
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0660272e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  69 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729756
Test loss (w/o reg) on all data: 0.012054727
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0503819e-07
Norm of the params: 9.153189
                Loss: fixed  69 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13167833
Train loss (w/o reg) on all data: 0.12491457
Test loss (w/o reg) on all data: 0.066033766
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5510805e-05
Norm of the params: 11.630778
              Random: fixed  21 labels. Loss 0.06603. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1788469
Train loss (w/o reg) on all data: 0.17180091
Test loss (w/o reg) on all data: 0.098582745
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2684513e-06
Norm of the params: 11.870965
Flipped loss: 0.09858. Accuracy: 0.992
### Flips: 156, rs: 12, checks: 52
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093248665
Train loss (w/o reg) on all data: 0.084094465
Test loss (w/o reg) on all data: 0.049888555
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0612887e-05
Norm of the params: 13.530857
     Influence (LOO): fixed  39 labels. Loss 0.04989. Accuracy 0.989.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039826576
Train loss (w/o reg) on all data: 0.026374083
Test loss (w/o reg) on all data: 0.059390306
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.123886e-06
Norm of the params: 16.402739
                Loss: fixed  52 labels. Loss 0.05939. Accuracy 0.977.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17666861
Train loss (w/o reg) on all data: 0.16968326
Test loss (w/o reg) on all data: 0.096224256
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0325738e-05
Norm of the params: 11.819767
              Random: fixed   1 labels. Loss 0.09622. Accuracy 0.992.
### Flips: 156, rs: 12, checks: 104
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03531181
Train loss (w/o reg) on all data: 0.027232826
Test loss (w/o reg) on all data: 0.021761112
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0483683e-06
Norm of the params: 12.7114
     Influence (LOO): fixed  63 labels. Loss 0.02176. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0070519275
Train loss (w/o reg) on all data: 0.0024648802
Test loss (w/o reg) on all data: 0.017303783
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4123596e-07
Norm of the params: 9.57815
                Loss: fixed  74 labels. Loss 0.01730. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16796418
Train loss (w/o reg) on all data: 0.16067
Test loss (w/o reg) on all data: 0.09358454
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.212273e-06
Norm of the params: 12.078222
              Random: fixed   6 labels. Loss 0.09358. Accuracy 0.985.
### Flips: 156, rs: 12, checks: 156
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01611236
Train loss (w/o reg) on all data: 0.010083617
Test loss (w/o reg) on all data: 0.014926164
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8769791e-06
Norm of the params: 10.980659
     Influence (LOO): fixed  72 labels. Loss 0.01493. Accuracy 0.989.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012054172
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3428395e-07
Norm of the params: 9.153193
                Loss: fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16240664
Train loss (w/o reg) on all data: 0.15518296
Test loss (w/o reg) on all data: 0.08793515
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.0838335e-05
Norm of the params: 12.019712
              Random: fixed   9 labels. Loss 0.08794. Accuracy 0.985.
### Flips: 156, rs: 12, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172993
Test loss (w/o reg) on all data: 0.012055389
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.7892745e-08
Norm of the params: 9.15317
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729926
Test loss (w/o reg) on all data: 0.012055409
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2183294e-07
Norm of the params: 9.15317
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15893982
Train loss (w/o reg) on all data: 0.15181834
Test loss (w/o reg) on all data: 0.08863429
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4100257e-05
Norm of the params: 11.934393
              Random: fixed  11 labels. Loss 0.08863. Accuracy 0.981.
### Flips: 156, rs: 12, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729937
Test loss (w/o reg) on all data: 0.012055459
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1744877e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172994
Test loss (w/o reg) on all data: 0.012055394
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1255117e-07
Norm of the params: 9.15317
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15098277
Train loss (w/o reg) on all data: 0.14385474
Test loss (w/o reg) on all data: 0.08673163
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8255494e-05
Norm of the params: 11.939876
              Random: fixed  17 labels. Loss 0.08673. Accuracy 0.985.
### Flips: 156, rs: 12, checks: 312
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729749
Test loss (w/o reg) on all data: 0.012055059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1640426e-07
Norm of the params: 9.15319
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729744
Test loss (w/o reg) on all data: 0.012054998
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7374952e-07
Norm of the params: 9.15319
                Loss: fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14220145
Train loss (w/o reg) on all data: 0.1348173
Test loss (w/o reg) on all data: 0.08201204
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3212877e-05
Norm of the params: 12.152487
              Random: fixed  22 labels. Loss 0.08201. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17042176
Train loss (w/o reg) on all data: 0.1619549
Test loss (w/o reg) on all data: 0.08919568
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2296452e-05
Norm of the params: 13.012966
Flipped loss: 0.08920. Accuracy: 0.996
### Flips: 156, rs: 13, checks: 52
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06785491
Train loss (w/o reg) on all data: 0.05936174
Test loss (w/o reg) on all data: 0.045354232
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.6289844e-06
Norm of the params: 13.033166
     Influence (LOO): fixed  45 labels. Loss 0.04535. Accuracy 0.981.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036805857
Train loss (w/o reg) on all data: 0.024675103
Test loss (w/o reg) on all data: 0.028544793
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.0584013e-06
Norm of the params: 15.576106
                Loss: fixed  52 labels. Loss 0.02854. Accuracy 0.989.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16530347
Train loss (w/o reg) on all data: 0.15714279
Test loss (w/o reg) on all data: 0.0815417
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7325164e-05
Norm of the params: 12.775503
              Random: fixed   3 labels. Loss 0.08154. Accuracy 0.996.
### Flips: 156, rs: 13, checks: 104
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032399084
Train loss (w/o reg) on all data: 0.025058022
Test loss (w/o reg) on all data: 0.020959416
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.505999e-06
Norm of the params: 12.116984
     Influence (LOO): fixed  61 labels. Loss 0.02096. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069715334
Train loss (w/o reg) on all data: 0.002517115
Test loss (w/o reg) on all data: 0.011187632
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1722897e-07
Norm of the params: 9.4386635
                Loss: fixed  69 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16175601
Train loss (w/o reg) on all data: 0.15332457
Test loss (w/o reg) on all data: 0.079660624
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.117688e-05
Norm of the params: 12.985715
              Random: fixed   5 labels. Loss 0.07966. Accuracy 0.996.
### Flips: 156, rs: 13, checks: 156
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010038311
Train loss (w/o reg) on all data: 0.0052867355
Test loss (w/o reg) on all data: 0.012297024
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0995997e-06
Norm of the params: 9.74841
     Influence (LOO): fixed  69 labels. Loss 0.01230. Accuracy 0.992.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172859
Test loss (w/o reg) on all data: 0.01205517
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3378e-07
Norm of the params: 9.153317
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15123272
Train loss (w/o reg) on all data: 0.14328533
Test loss (w/o reg) on all data: 0.06902593
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.3077402e-05
Norm of the params: 12.6074505
              Random: fixed  12 labels. Loss 0.06903. Accuracy 0.996.
### Flips: 156, rs: 13, checks: 208
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01003831
Train loss (w/o reg) on all data: 0.005286634
Test loss (w/o reg) on all data: 0.012296251
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0288938e-07
Norm of the params: 9.748514
     Influence (LOO): fixed  69 labels. Loss 0.01230. Accuracy 0.992.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728594
Test loss (w/o reg) on all data: 0.012055169
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3329324e-07
Norm of the params: 9.153317
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13802211
Train loss (w/o reg) on all data: 0.13011822
Test loss (w/o reg) on all data: 0.06356927
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.317644e-05
Norm of the params: 12.572898
              Random: fixed  17 labels. Loss 0.06357. Accuracy 0.996.
### Flips: 156, rs: 13, checks: 260
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172906
Test loss (w/o reg) on all data: 0.012055014
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.863981e-07
Norm of the params: 9.153263
     Influence (LOO): fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729092
Test loss (w/o reg) on all data: 0.01205489
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8330664e-07
Norm of the params: 9.153262
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1365881
Train loss (w/o reg) on all data: 0.12865017
Test loss (w/o reg) on all data: 0.061999556
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.7115456e-05
Norm of the params: 12.5999365
              Random: fixed  19 labels. Loss 0.06200. Accuracy 0.996.
### Flips: 156, rs: 13, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172986
Test loss (w/o reg) on all data: 0.012055032
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2060748e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729849
Test loss (w/o reg) on all data: 0.012055067
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5911862e-07
Norm of the params: 9.153177
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13021113
Train loss (w/o reg) on all data: 0.12230265
Test loss (w/o reg) on all data: 0.057292245
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4423876e-05
Norm of the params: 12.576545
              Random: fixed  22 labels. Loss 0.05729. Accuracy 0.996.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18877433
Train loss (w/o reg) on all data: 0.18114386
Test loss (w/o reg) on all data: 0.1236785
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6174112e-05
Norm of the params: 12.353516
Flipped loss: 0.12368. Accuracy: 0.977
### Flips: 156, rs: 14, checks: 52
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101501256
Train loss (w/o reg) on all data: 0.09027825
Test loss (w/o reg) on all data: 0.07409043
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9354569e-05
Norm of the params: 14.98199
     Influence (LOO): fixed  39 labels. Loss 0.07409. Accuracy 0.985.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056227073
Train loss (w/o reg) on all data: 0.044440784
Test loss (w/o reg) on all data: 0.07583371
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.692896e-06
Norm of the params: 15.353364
                Loss: fixed  52 labels. Loss 0.07583. Accuracy 0.969.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18515943
Train loss (w/o reg) on all data: 0.1773545
Test loss (w/o reg) on all data: 0.11991906
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.585499e-06
Norm of the params: 12.493939
              Random: fixed   1 labels. Loss 0.11992. Accuracy 0.973.
### Flips: 156, rs: 14, checks: 104
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050001208
Train loss (w/o reg) on all data: 0.039215446
Test loss (w/o reg) on all data: 0.038058616
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1669765e-06
Norm of the params: 14.687249
     Influence (LOO): fixed  62 labels. Loss 0.03806. Accuracy 0.992.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008345253
Train loss (w/o reg) on all data: 0.0031439199
Test loss (w/o reg) on all data: 0.011323849
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0214561e-06
Norm of the params: 10.199346
                Loss: fixed  78 labels. Loss 0.01132. Accuracy 0.996.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18113163
Train loss (w/o reg) on all data: 0.17310144
Test loss (w/o reg) on all data: 0.11482438
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0466896e-05
Norm of the params: 12.672955
              Random: fixed   4 labels. Loss 0.11482. Accuracy 0.985.
### Flips: 156, rs: 14, checks: 156
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018437915
Train loss (w/o reg) on all data: 0.012403194
Test loss (w/o reg) on all data: 0.016238833
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.177591e-07
Norm of the params: 10.986101
     Influence (LOO): fixed  75 labels. Loss 0.01624. Accuracy 0.996.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008345252
Train loss (w/o reg) on all data: 0.003144023
Test loss (w/o reg) on all data: 0.0113245845
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2244296e-07
Norm of the params: 10.1992445
                Loss: fixed  78 labels. Loss 0.01132. Accuracy 0.996.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17246714
Train loss (w/o reg) on all data: 0.16457006
Test loss (w/o reg) on all data: 0.10345143
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2957673e-05
Norm of the params: 12.567488
              Random: fixed   9 labels. Loss 0.10345. Accuracy 0.989.
### Flips: 156, rs: 14, checks: 208
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016410844
Train loss (w/o reg) on all data: 0.011155713
Test loss (w/o reg) on all data: 0.016495446
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.1680654e-07
Norm of the params: 10.251957
     Influence (LOO): fixed  76 labels. Loss 0.01650. Accuracy 0.996.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730373
Test loss (w/o reg) on all data: 0.0120560145
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3875457e-07
Norm of the params: 9.153119
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16820857
Train loss (w/o reg) on all data: 0.16016668
Test loss (w/o reg) on all data: 0.10156295
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.1885324e-06
Norm of the params: 12.682182
              Random: fixed  11 labels. Loss 0.10156. Accuracy 0.989.
### Flips: 156, rs: 14, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729704
Test loss (w/o reg) on all data: 0.012054753
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.562472e-08
Norm of the params: 9.153195
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172975
Test loss (w/o reg) on all data: 0.01205471
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5363389e-07
Norm of the params: 9.153191
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16123787
Train loss (w/o reg) on all data: 0.1534404
Test loss (w/o reg) on all data: 0.096202016
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2632548e-05
Norm of the params: 12.487968
              Random: fixed  15 labels. Loss 0.09620. Accuracy 0.989.
### Flips: 156, rs: 14, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729218
Test loss (w/o reg) on all data: 0.012054922
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2813182e-07
Norm of the params: 9.153247
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729222
Test loss (w/o reg) on all data: 0.012054976
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2665772e-07
Norm of the params: 9.153247
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16011043
Train loss (w/o reg) on all data: 0.15261588
Test loss (w/o reg) on all data: 0.091210775
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7311004e-05
Norm of the params: 12.243007
              Random: fixed  17 labels. Loss 0.09121. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19191225
Train loss (w/o reg) on all data: 0.18472181
Test loss (w/o reg) on all data: 0.11318227
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.747713e-06
Norm of the params: 11.992023
Flipped loss: 0.11318. Accuracy: 0.985
### Flips: 156, rs: 15, checks: 52
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089548446
Train loss (w/o reg) on all data: 0.08039611
Test loss (w/o reg) on all data: 0.06785331
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.279039e-06
Norm of the params: 13.529479
     Influence (LOO): fixed  43 labels. Loss 0.06785. Accuracy 0.989.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05053038
Train loss (w/o reg) on all data: 0.037257083
Test loss (w/o reg) on all data: 0.06859394
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.2727028e-06
Norm of the params: 16.293129
                Loss: fixed  52 labels. Loss 0.06859. Accuracy 0.969.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18628244
Train loss (w/o reg) on all data: 0.17893852
Test loss (w/o reg) on all data: 0.10481705
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1669093e-05
Norm of the params: 12.119339
              Random: fixed   4 labels. Loss 0.10482. Accuracy 0.977.
### Flips: 156, rs: 15, checks: 104
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0400817
Train loss (w/o reg) on all data: 0.032545198
Test loss (w/o reg) on all data: 0.028724989
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6434627e-06
Norm of the params: 12.277216
     Influence (LOO): fixed  65 labels. Loss 0.02872. Accuracy 0.996.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076085236
Train loss (w/o reg) on all data: 0.002844557
Test loss (w/o reg) on all data: 0.014218702
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6892586e-07
Norm of the params: 9.761114
                Loss: fixed  76 labels. Loss 0.01422. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18207234
Train loss (w/o reg) on all data: 0.1747059
Test loss (w/o reg) on all data: 0.104577236
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1407839e-05
Norm of the params: 12.137917
              Random: fixed   6 labels. Loss 0.10458. Accuracy 0.981.
### Flips: 156, rs: 15, checks: 156
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008985575
Train loss (w/o reg) on all data: 0.004210274
Test loss (w/o reg) on all data: 0.014261738
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2033135e-07
Norm of the params: 9.772717
     Influence (LOO): fixed  76 labels. Loss 0.01426. Accuracy 0.992.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729073
Test loss (w/o reg) on all data: 0.012055207
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5861863e-07
Norm of the params: 9.153261
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1727184
Train loss (w/o reg) on all data: 0.16514885
Test loss (w/o reg) on all data: 0.10333917
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3739938e-05
Norm of the params: 12.304109
              Random: fixed  11 labels. Loss 0.10334. Accuracy 0.977.
### Flips: 156, rs: 15, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728603
Test loss (w/o reg) on all data: 0.012055592
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9507834e-07
Norm of the params: 9.153314
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [2] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729218
Test loss (w/o reg) on all data: 0.012055626
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0715665e-07
Norm of the params: 9.153246
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17190537
Train loss (w/o reg) on all data: 0.16426045
Test loss (w/o reg) on all data: 0.101785585
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.871555e-06
Norm of the params: 12.365213
              Random: fixed  12 labels. Loss 0.10179. Accuracy 0.977.
### Flips: 156, rs: 15, checks: 260
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729185
Test loss (w/o reg) on all data: 0.012055421
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.148692e-07
Norm of the params: 9.15325
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729204
Test loss (w/o reg) on all data: 0.012055316
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.261072e-07
Norm of the params: 9.153248
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16279216
Train loss (w/o reg) on all data: 0.15481019
Test loss (w/o reg) on all data: 0.09267086
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.466045e-06
Norm of the params: 12.634853
              Random: fixed  17 labels. Loss 0.09267. Accuracy 0.981.
### Flips: 156, rs: 15, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730252
Test loss (w/o reg) on all data: 0.012055212
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7054494e-07
Norm of the params: 9.153132
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730247
Test loss (w/o reg) on all data: 0.012055175
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4458787e-07
Norm of the params: 9.153133
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16168772
Train loss (w/o reg) on all data: 0.15384564
Test loss (w/o reg) on all data: 0.09074691
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.3759577e-05
Norm of the params: 12.523641
              Random: fixed  18 labels. Loss 0.09075. Accuracy 0.985.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17177057
Train loss (w/o reg) on all data: 0.16302292
Test loss (w/o reg) on all data: 0.13662148
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.469296e-05
Norm of the params: 13.226982
Flipped loss: 0.13662. Accuracy: 0.962
### Flips: 156, rs: 16, checks: 52
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09821266
Train loss (w/o reg) on all data: 0.08871286
Test loss (w/o reg) on all data: 0.091452196
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.4768624e-06
Norm of the params: 13.7839
     Influence (LOO): fixed  39 labels. Loss 0.09145. Accuracy 0.977.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04896827
Train loss (w/o reg) on all data: 0.035024088
Test loss (w/o reg) on all data: 0.07782499
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.4771045e-06
Norm of the params: 16.69981
                Loss: fixed  52 labels. Loss 0.07782. Accuracy 0.977.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16292053
Train loss (w/o reg) on all data: 0.15418401
Test loss (w/o reg) on all data: 0.1299679
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4226355e-05
Norm of the params: 13.218562
              Random: fixed   5 labels. Loss 0.12997. Accuracy 0.966.
### Flips: 156, rs: 16, checks: 104
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046454214
Train loss (w/o reg) on all data: 0.038717523
Test loss (w/o reg) on all data: 0.035016954
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2112956e-06
Norm of the params: 12.439205
     Influence (LOO): fixed  64 labels. Loss 0.03502. Accuracy 0.992.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013514563
Train loss (w/o reg) on all data: 0.0059754015
Test loss (w/o reg) on all data: 0.034420226
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2042843e-07
Norm of the params: 12.279383
                Loss: fixed  71 labels. Loss 0.03442. Accuracy 0.989.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15934707
Train loss (w/o reg) on all data: 0.15054843
Test loss (w/o reg) on all data: 0.12826145
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.5061703e-06
Norm of the params: 13.265479
              Random: fixed   8 labels. Loss 0.12826. Accuracy 0.969.
### Flips: 156, rs: 16, checks: 156
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0134315435
Train loss (w/o reg) on all data: 0.008517811
Test loss (w/o reg) on all data: 0.012956703
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9743898e-06
Norm of the params: 9.913358
     Influence (LOO): fixed  76 labels. Loss 0.01296. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008026177
Train loss (w/o reg) on all data: 0.0029985206
Test loss (w/o reg) on all data: 0.015570204
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.83479e-07
Norm of the params: 10.027618
                Loss: fixed  76 labels. Loss 0.01557. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14452887
Train loss (w/o reg) on all data: 0.13597189
Test loss (w/o reg) on all data: 0.123923205
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.1455615e-05
Norm of the params: 13.082032
              Random: fixed  17 labels. Loss 0.12392. Accuracy 0.973.
### Flips: 156, rs: 16, checks: 208
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00926006
Train loss (w/o reg) on all data: 0.004402132
Test loss (w/o reg) on all data: 0.012444412
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1615367e-07
Norm of the params: 9.856905
     Influence (LOO): fixed  77 labels. Loss 0.01244. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074045793
Train loss (w/o reg) on all data: 0.0027832561
Test loss (w/o reg) on all data: 0.013697494
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5693305e-07
Norm of the params: 9.613869
                Loss: fixed  77 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14374389
Train loss (w/o reg) on all data: 0.13517158
Test loss (w/o reg) on all data: 0.12378668
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.248426e-06
Norm of the params: 13.093741
              Random: fixed  18 labels. Loss 0.12379. Accuracy 0.973.
### Flips: 156, rs: 16, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172952
Test loss (w/o reg) on all data: 0.012055195
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6211844e-07
Norm of the params: 9.153216
     Influence (LOO): fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074045807
Train loss (w/o reg) on all data: 0.0027833306
Test loss (w/o reg) on all data: 0.013697692
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3359829e-07
Norm of the params: 9.613792
                Loss: fixed  77 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13678956
Train loss (w/o reg) on all data: 0.12779543
Test loss (w/o reg) on all data: 0.12236974
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1773633e-05
Norm of the params: 13.412039
              Random: fixed  21 labels. Loss 0.12237. Accuracy 0.969.
### Flips: 156, rs: 16, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730147
Test loss (w/o reg) on all data: 0.012057046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6409568e-07
Norm of the params: 9.153145
     Influence (LOO): fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730112
Test loss (w/o reg) on all data: 0.012056921
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6101136e-07
Norm of the params: 9.153148
                Loss: fixed  78 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12982428
Train loss (w/o reg) on all data: 0.12047043
Test loss (w/o reg) on all data: 0.117829345
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.70932e-06
Norm of the params: 13.677609
              Random: fixed  25 labels. Loss 0.11783. Accuracy 0.969.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18480505
Train loss (w/o reg) on all data: 0.17721014
Test loss (w/o reg) on all data: 0.097499415
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.4877726e-06
Norm of the params: 12.324706
Flipped loss: 0.09750. Accuracy: 0.985
### Flips: 156, rs: 17, checks: 52
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105020255
Train loss (w/o reg) on all data: 0.095101565
Test loss (w/o reg) on all data: 0.06777006
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.7544695e-06
Norm of the params: 14.084523
     Influence (LOO): fixed  38 labels. Loss 0.06777. Accuracy 0.985.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049765363
Train loss (w/o reg) on all data: 0.035398167
Test loss (w/o reg) on all data: 0.027414337
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.333888e-06
Norm of the params: 16.951223
                Loss: fixed  52 labels. Loss 0.02741. Accuracy 0.996.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17573963
Train loss (w/o reg) on all data: 0.16852847
Test loss (w/o reg) on all data: 0.08579691
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.843949e-05
Norm of the params: 12.009298
              Random: fixed   8 labels. Loss 0.08580. Accuracy 0.989.
### Flips: 156, rs: 17, checks: 104
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04055551
Train loss (w/o reg) on all data: 0.03100301
Test loss (w/o reg) on all data: 0.030483745
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.059089e-06
Norm of the params: 13.822083
     Influence (LOO): fixed  67 labels. Loss 0.03048. Accuracy 0.989.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009899871
Train loss (w/o reg) on all data: 0.003996047
Test loss (w/o reg) on all data: 0.020515846
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6937762e-06
Norm of the params: 10.8663
                Loss: fixed  80 labels. Loss 0.02052. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16345252
Train loss (w/o reg) on all data: 0.15549636
Test loss (w/o reg) on all data: 0.089136064
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.102577e-05
Norm of the params: 12.6144085
              Random: fixed  15 labels. Loss 0.08914. Accuracy 0.989.
### Flips: 156, rs: 17, checks: 156
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0128962
Train loss (w/o reg) on all data: 0.006909335
Test loss (w/o reg) on all data: 0.016698435
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.450403e-06
Norm of the params: 10.942454
     Influence (LOO): fixed  79 labels. Loss 0.01670. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730042
Test loss (w/o reg) on all data: 0.012054882
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7674446e-07
Norm of the params: 9.153158
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16046071
Train loss (w/o reg) on all data: 0.15275383
Test loss (w/o reg) on all data: 0.085624926
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.8810354e-05
Norm of the params: 12.415215
              Random: fixed  17 labels. Loss 0.08562. Accuracy 0.985.
### Flips: 156, rs: 17, checks: 208
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01184174
Train loss (w/o reg) on all data: 0.0059794304
Test loss (w/o reg) on all data: 0.016267668
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4361113e-06
Norm of the params: 10.828029
     Influence (LOO): fixed  80 labels. Loss 0.01627. Accuracy 0.989.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.01205479
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2955122e-07
Norm of the params: 9.153205
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14786455
Train loss (w/o reg) on all data: 0.13969162
Test loss (w/o reg) on all data: 0.08084322
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.7451e-06
Norm of the params: 12.785091
              Random: fixed  22 labels. Loss 0.08084. Accuracy 0.985.
### Flips: 156, rs: 17, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021728752
Test loss (w/o reg) on all data: 0.012055302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0123859e-07
Norm of the params: 9.153296
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728843
Test loss (w/o reg) on all data: 0.012055344
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2362638e-07
Norm of the params: 9.153288
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1395653
Train loss (w/o reg) on all data: 0.13101493
Test loss (w/o reg) on all data: 0.06993368
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.1462747e-06
Norm of the params: 13.076989
              Random: fixed  28 labels. Loss 0.06993. Accuracy 0.989.
### Flips: 156, rs: 17, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729209
Test loss (w/o reg) on all data: 0.012055144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.267164e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729253
Test loss (w/o reg) on all data: 0.012054998
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.9532316e-07
Norm of the params: 9.153244
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13726635
Train loss (w/o reg) on all data: 0.12888095
Test loss (w/o reg) on all data: 0.06467635
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.06204925e-05
Norm of the params: 12.950215
              Random: fixed  30 labels. Loss 0.06468. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17898613
Train loss (w/o reg) on all data: 0.17157967
Test loss (w/o reg) on all data: 0.09471852
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9872665e-05
Norm of the params: 12.170828
Flipped loss: 0.09472. Accuracy: 0.989
### Flips: 156, rs: 18, checks: 52
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090521365
Train loss (w/o reg) on all data: 0.08227978
Test loss (w/o reg) on all data: 0.055610776
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5325832e-05
Norm of the params: 12.83868
     Influence (LOO): fixed  42 labels. Loss 0.05561. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04277167
Train loss (w/o reg) on all data: 0.029541941
Test loss (w/o reg) on all data: 0.05766032
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6076846e-06
Norm of the params: 16.266363
                Loss: fixed  52 labels. Loss 0.05766. Accuracy 0.981.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17898613
Train loss (w/o reg) on all data: 0.17158155
Test loss (w/o reg) on all data: 0.094722465
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9621255e-05
Norm of the params: 12.169288
              Random: fixed   0 labels. Loss 0.09472. Accuracy 0.989.
### Flips: 156, rs: 18, checks: 104
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029477915
Train loss (w/o reg) on all data: 0.022524282
Test loss (w/o reg) on all data: 0.025940897
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1501673e-06
Norm of the params: 11.792908
     Influence (LOO): fixed  64 labels. Loss 0.02594. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006971534
Train loss (w/o reg) on all data: 0.0025172597
Test loss (w/o reg) on all data: 0.011187446
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.961382e-07
Norm of the params: 9.438511
                Loss: fixed  71 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16873169
Train loss (w/o reg) on all data: 0.16147573
Test loss (w/o reg) on all data: 0.09060101
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2498582e-05
Norm of the params: 12.046536
              Random: fixed   6 labels. Loss 0.09060. Accuracy 0.989.
### Flips: 156, rs: 18, checks: 156
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014017833
Train loss (w/o reg) on all data: 0.007836476
Test loss (w/o reg) on all data: 0.026239907
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1247896e-06
Norm of the params: 11.118774
     Influence (LOO): fixed  69 labels. Loss 0.02624. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729993
Test loss (w/o reg) on all data: 0.012055269
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.363822e-08
Norm of the params: 9.153162
                Loss: fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15822707
Train loss (w/o reg) on all data: 0.15111646
Test loss (w/o reg) on all data: 0.08575474
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8858134e-05
Norm of the params: 11.9252825
              Random: fixed  12 labels. Loss 0.08575. Accuracy 0.989.
### Flips: 156, rs: 18, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728766
Test loss (w/o reg) on all data: 0.01205615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6782878e-07
Norm of the params: 9.153296
     Influence (LOO): fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728768
Test loss (w/o reg) on all data: 0.012056039
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1164144e-07
Norm of the params: 9.1532955
                Loss: fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15562376
Train loss (w/o reg) on all data: 0.1485765
Test loss (w/o reg) on all data: 0.078935646
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8483617e-05
Norm of the params: 11.872044
              Random: fixed  14 labels. Loss 0.07894. Accuracy 0.992.
### Flips: 156, rs: 18, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728943
Test loss (w/o reg) on all data: 0.012055155
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.2533236e-07
Norm of the params: 9.153276
     Influence (LOO): fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728952
Test loss (w/o reg) on all data: 0.012055006
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9727979e-07
Norm of the params: 9.1532755
                Loss: fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14973244
Train loss (w/o reg) on all data: 0.14250092
Test loss (w/o reg) on all data: 0.077241585
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3420263e-05
Norm of the params: 12.02624
              Random: fixed  17 labels. Loss 0.07724. Accuracy 0.992.
### Flips: 156, rs: 18, checks: 312
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730089
Test loss (w/o reg) on all data: 0.012055574
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.893431e-08
Norm of the params: 9.153153
     Influence (LOO): fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730072
Test loss (w/o reg) on all data: 0.012055584
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8970715e-07
Norm of the params: 9.153155
                Loss: fixed  72 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14226623
Train loss (w/o reg) on all data: 0.13508543
Test loss (w/o reg) on all data: 0.072619975
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0032477e-06
Norm of the params: 11.983984
              Random: fixed  20 labels. Loss 0.07262. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19524634
Train loss (w/o reg) on all data: 0.1879508
Test loss (w/o reg) on all data: 0.11502662
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.6324e-05
Norm of the params: 12.079344
Flipped loss: 0.11503. Accuracy: 0.969
### Flips: 156, rs: 19, checks: 52
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106609076
Train loss (w/o reg) on all data: 0.09669044
Test loss (w/o reg) on all data: 0.07682599
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.459876e-06
Norm of the params: 14.084484
     Influence (LOO): fixed  42 labels. Loss 0.07683. Accuracy 0.977.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06191521
Train loss (w/o reg) on all data: 0.048301987
Test loss (w/o reg) on all data: 0.051113684
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.924574e-06
Norm of the params: 16.50044
                Loss: fixed  52 labels. Loss 0.05111. Accuracy 0.981.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1877533
Train loss (w/o reg) on all data: 0.1805099
Test loss (w/o reg) on all data: 0.11019561
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.4708528e-05
Norm of the params: 12.0361185
              Random: fixed   4 labels. Loss 0.11020. Accuracy 0.973.
### Flips: 156, rs: 19, checks: 104
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04865873
Train loss (w/o reg) on all data: 0.039503742
Test loss (w/o reg) on all data: 0.032513063
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9644408e-06
Norm of the params: 13.531434
     Influence (LOO): fixed  66 labels. Loss 0.03251. Accuracy 0.989.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00807604
Train loss (w/o reg) on all data: 0.0030638413
Test loss (w/o reg) on all data: 0.014257233
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1437894e-07
Norm of the params: 10.012193
                Loss: fixed  81 labels. Loss 0.01426. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18033999
Train loss (w/o reg) on all data: 0.1732925
Test loss (w/o reg) on all data: 0.10144433
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.6823454e-05
Norm of the params: 11.87223
              Random: fixed   8 labels. Loss 0.10144. Accuracy 0.973.
### Flips: 156, rs: 19, checks: 156
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010216441
Train loss (w/o reg) on all data: 0.0055171014
Test loss (w/o reg) on all data: 0.012119456
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0534337e-06
Norm of the params: 9.694678
     Influence (LOO): fixed  81 labels. Loss 0.01212. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729947
Test loss (w/o reg) on all data: 0.0120550115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8300652e-07
Norm of the params: 9.153166
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16985506
Train loss (w/o reg) on all data: 0.1628047
Test loss (w/o reg) on all data: 0.09936645
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.7845784e-05
Norm of the params: 11.874646
              Random: fixed  13 labels. Loss 0.09937. Accuracy 0.977.
### Flips: 156, rs: 19, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729067
Test loss (w/o reg) on all data: 0.012055532
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.088109e-07
Norm of the params: 9.153263
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729076
Test loss (w/o reg) on all data: 0.012055476
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1101064e-07
Norm of the params: 9.153262
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16807978
Train loss (w/o reg) on all data: 0.1612805
Test loss (w/o reg) on all data: 0.09670645
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3665783e-05
Norm of the params: 11.661285
              Random: fixed  15 labels. Loss 0.09671. Accuracy 0.985.
### Flips: 156, rs: 19, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730603
Test loss (w/o reg) on all data: 0.012054615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.2304504e-07
Norm of the params: 9.153095
     Influence (LOO): fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173061
Test loss (w/o reg) on all data: 0.012054805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.059999e-07
Norm of the params: 9.153095
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15941903
Train loss (w/o reg) on all data: 0.15216586
Test loss (w/o reg) on all data: 0.09387298
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.20357e-05
Norm of the params: 12.044229
              Random: fixed  19 labels. Loss 0.09387. Accuracy 0.977.
### Flips: 156, rs: 19, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730624
Test loss (w/o reg) on all data: 0.012055996
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.63921e-07
Norm of the params: 9.153094
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173061
Test loss (w/o reg) on all data: 0.012056123
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7683589e-07
Norm of the params: 9.153095
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1536294
Train loss (w/o reg) on all data: 0.14645365
Test loss (w/o reg) on all data: 0.08642271
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.23961145e-05
Norm of the params: 11.979785
              Random: fixed  22 labels. Loss 0.08642. Accuracy 0.985.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15640973
Train loss (w/o reg) on all data: 0.14747082
Test loss (w/o reg) on all data: 0.08632587
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7042168e-05
Norm of the params: 13.3708
Flipped loss: 0.08633. Accuracy: 0.981
### Flips: 156, rs: 20, checks: 52
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072145596
Train loss (w/o reg) on all data: 0.060527205
Test loss (w/o reg) on all data: 0.057790518
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.727647e-06
Norm of the params: 15.243614
     Influence (LOO): fixed  41 labels. Loss 0.05779. Accuracy 0.969.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036670648
Train loss (w/o reg) on all data: 0.023655448
Test loss (w/o reg) on all data: 0.021548375
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1825517e-06
Norm of the params: 16.133938
                Loss: fixed  49 labels. Loss 0.02155. Accuracy 0.996.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1474602
Train loss (w/o reg) on all data: 0.13755144
Test loss (w/o reg) on all data: 0.075802155
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.748706e-06
Norm of the params: 14.07748
              Random: fixed   5 labels. Loss 0.07580. Accuracy 0.989.
### Flips: 156, rs: 20, checks: 104
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024786301
Train loss (w/o reg) on all data: 0.01671072
Test loss (w/o reg) on all data: 0.01667027
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4698379e-06
Norm of the params: 12.708724
     Influence (LOO): fixed  64 labels. Loss 0.01667. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007247486
Train loss (w/o reg) on all data: 0.0026132048
Test loss (w/o reg) on all data: 0.0110788345
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.530141e-07
Norm of the params: 9.627337
                Loss: fixed  70 labels. Loss 0.01108. Accuracy 0.996.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14500444
Train loss (w/o reg) on all data: 0.13567102
Test loss (w/o reg) on all data: 0.0748029
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.318548e-06
Norm of the params: 13.662661
              Random: fixed   8 labels. Loss 0.07480. Accuracy 0.977.
### Flips: 156, rs: 20, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.00217294
Test loss (w/o reg) on all data: 0.012054716
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0370721e-07
Norm of the params: 9.153229
     Influence (LOO): fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729406
Test loss (w/o reg) on all data: 0.012054773
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0704876e-07
Norm of the params: 9.153228
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1406361
Train loss (w/o reg) on all data: 0.13124974
Test loss (w/o reg) on all data: 0.07369386
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1840699e-05
Norm of the params: 13.701364
              Random: fixed  10 labels. Loss 0.07369. Accuracy 0.977.
### Flips: 156, rs: 20, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729285
Test loss (w/o reg) on all data: 0.012055396
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7023947e-07
Norm of the params: 9.15324
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729302
Test loss (w/o reg) on all data: 0.012055325
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8595108e-07
Norm of the params: 9.153239
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13945454
Train loss (w/o reg) on all data: 0.13014694
Test loss (w/o reg) on all data: 0.072286606
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.27879e-06
Norm of the params: 13.643758
              Random: fixed  11 labels. Loss 0.07229. Accuracy 0.985.
### Flips: 156, rs: 20, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172894
Test loss (w/o reg) on all data: 0.012055406
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.838785e-07
Norm of the params: 9.153276
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.012054709
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2028932e-07
Norm of the params: 9.153216
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124982424
Train loss (w/o reg) on all data: 0.11505262
Test loss (w/o reg) on all data: 0.066780955
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.08048e-06
Norm of the params: 14.092416
              Random: fixed  18 labels. Loss 0.06678. Accuracy 0.992.
### Flips: 156, rs: 20, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012054777
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.758779e-08
Norm of the params: 9.153198
     Influence (LOO): fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729684
Test loss (w/o reg) on all data: 0.012054837
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6481395e-07
Norm of the params: 9.153198
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120368525
Train loss (w/o reg) on all data: 0.11105384
Test loss (w/o reg) on all data: 0.06871685
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.864595e-06
Norm of the params: 13.648949
              Random: fixed  22 labels. Loss 0.06872. Accuracy 0.985.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18565327
Train loss (w/o reg) on all data: 0.17792018
Test loss (w/o reg) on all data: 0.13066973
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0817418e-05
Norm of the params: 12.436308
Flipped loss: 0.13067. Accuracy: 0.977
### Flips: 156, rs: 21, checks: 52
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08601512
Train loss (w/o reg) on all data: 0.07441585
Test loss (w/o reg) on all data: 0.08340752
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.4758243e-06
Norm of the params: 15.231067
     Influence (LOO): fixed  43 labels. Loss 0.08341. Accuracy 0.973.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05605906
Train loss (w/o reg) on all data: 0.043198135
Test loss (w/o reg) on all data: 0.084436156
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.1425615e-06
Norm of the params: 16.038033
                Loss: fixed  51 labels. Loss 0.08444. Accuracy 0.962.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17384934
Train loss (w/o reg) on all data: 0.16606182
Test loss (w/o reg) on all data: 0.11446619
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.7041053e-05
Norm of the params: 12.480008
              Random: fixed   8 labels. Loss 0.11447. Accuracy 0.981.
### Flips: 156, rs: 21, checks: 104
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05212052
Train loss (w/o reg) on all data: 0.043095347
Test loss (w/o reg) on all data: 0.04694936
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.6112874e-07
Norm of the params: 13.435158
     Influence (LOO): fixed  60 labels. Loss 0.04695. Accuracy 0.981.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009706305
Train loss (w/o reg) on all data: 0.0038575889
Test loss (w/o reg) on all data: 0.010545542
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.207644e-06
Norm of the params: 10.815467
                Loss: fixed  77 labels. Loss 0.01055. Accuracy 0.996.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1660009
Train loss (w/o reg) on all data: 0.1583291
Test loss (w/o reg) on all data: 0.11127207
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.9412766e-05
Norm of the params: 12.386927
              Random: fixed  12 labels. Loss 0.11127. Accuracy 0.973.
### Flips: 156, rs: 21, checks: 156
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019521499
Train loss (w/o reg) on all data: 0.012593289
Test loss (w/o reg) on all data: 0.015920347
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9717487e-06
Norm of the params: 11.77133
     Influence (LOO): fixed  75 labels. Loss 0.01592. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008018333
Train loss (w/o reg) on all data: 0.0030733766
Test loss (w/o reg) on all data: 0.012502803
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.330363e-07
Norm of the params: 9.944804
                Loss: fixed  78 labels. Loss 0.01250. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15882301
Train loss (w/o reg) on all data: 0.15095669
Test loss (w/o reg) on all data: 0.10680363
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.873628e-06
Norm of the params: 12.542989
              Random: fixed  16 labels. Loss 0.10680. Accuracy 0.981.
### Flips: 156, rs: 21, checks: 208
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011224471
Train loss (w/o reg) on all data: 0.006245202
Test loss (w/o reg) on all data: 0.01281858
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.1556146e-07
Norm of the params: 9.979248
     Influence (LOO): fixed  78 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172861
Test loss (w/o reg) on all data: 0.012054971
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.265911e-07
Norm of the params: 9.153314
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14670743
Train loss (w/o reg) on all data: 0.13912426
Test loss (w/o reg) on all data: 0.103303716
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.034805e-05
Norm of the params: 12.315166
              Random: fixed  22 labels. Loss 0.10330. Accuracy 0.977.
### Flips: 156, rs: 21, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729532
Test loss (w/o reg) on all data: 0.012054914
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2643538e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172964
Test loss (w/o reg) on all data: 0.012055202
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9970206e-07
Norm of the params: 9.153202
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14313078
Train loss (w/o reg) on all data: 0.13541444
Test loss (w/o reg) on all data: 0.10088588
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.3236506e-06
Norm of the params: 12.422835
              Random: fixed  25 labels. Loss 0.10089. Accuracy 0.981.
### Flips: 156, rs: 21, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730182
Test loss (w/o reg) on all data: 0.012055275
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5484843e-07
Norm of the params: 9.153141
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730163
Test loss (w/o reg) on all data: 0.012055144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1399713e-07
Norm of the params: 9.153142
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12843123
Train loss (w/o reg) on all data: 0.12005417
Test loss (w/o reg) on all data: 0.0981061
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.381312e-05
Norm of the params: 12.943776
              Random: fixed  30 labels. Loss 0.09811. Accuracy 0.973.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1753715
Train loss (w/o reg) on all data: 0.16728011
Test loss (w/o reg) on all data: 0.10467958
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.256774e-05
Norm of the params: 12.721161
Flipped loss: 0.10468. Accuracy: 0.977
### Flips: 156, rs: 22, checks: 52
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093783095
Train loss (w/o reg) on all data: 0.08355849
Test loss (w/o reg) on all data: 0.07428237
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9486591e-05
Norm of the params: 14.30007
     Influence (LOO): fixed  37 labels. Loss 0.07428. Accuracy 0.977.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048782334
Train loss (w/o reg) on all data: 0.035406772
Test loss (w/o reg) on all data: 0.04768203
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.3127507e-06
Norm of the params: 16.355772
                Loss: fixed  52 labels. Loss 0.04768. Accuracy 0.981.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17188543
Train loss (w/o reg) on all data: 0.16420023
Test loss (w/o reg) on all data: 0.10009975
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1357724e-05
Norm of the params: 12.397741
              Random: fixed   4 labels. Loss 0.10010. Accuracy 0.977.
### Flips: 156, rs: 22, checks: 104
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026550356
Train loss (w/o reg) on all data: 0.018955847
Test loss (w/o reg) on all data: 0.024449976
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0755818e-06
Norm of the params: 12.324374
     Influence (LOO): fixed  66 labels. Loss 0.02445. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007597209
Train loss (w/o reg) on all data: 0.002920646
Test loss (w/o reg) on all data: 0.013377612
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.8849777e-07
Norm of the params: 9.671156
                Loss: fixed  73 labels. Loss 0.01338. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16833337
Train loss (w/o reg) on all data: 0.16065648
Test loss (w/o reg) on all data: 0.09758075
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.678139e-06
Norm of the params: 12.391032
              Random: fixed   6 labels. Loss 0.09758. Accuracy 0.977.
### Flips: 156, rs: 22, checks: 156
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015606975
Train loss (w/o reg) on all data: 0.009940567
Test loss (w/o reg) on all data: 0.015073338
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.5521196e-07
Norm of the params: 10.645571
     Influence (LOO): fixed  71 labels. Loss 0.01507. Accuracy 0.996.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730987
Test loss (w/o reg) on all data: 0.012055285
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.019129e-07
Norm of the params: 9.153055
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1620769
Train loss (w/o reg) on all data: 0.1546231
Test loss (w/o reg) on all data: 0.08945295
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.053736e-06
Norm of the params: 12.209666
              Random: fixed  10 labels. Loss 0.08945. Accuracy 0.985.
### Flips: 156, rs: 22, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729858
Test loss (w/o reg) on all data: 0.012055774
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7506285e-07
Norm of the params: 9.153176
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172987
Test loss (w/o reg) on all data: 0.012055644
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.700808e-07
Norm of the params: 9.153177
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16023672
Train loss (w/o reg) on all data: 0.15288426
Test loss (w/o reg) on all data: 0.085479535
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.9372804e-05
Norm of the params: 12.126386
              Random: fixed  11 labels. Loss 0.08548. Accuracy 0.989.
### Flips: 156, rs: 22, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729874
Test loss (w/o reg) on all data: 0.012055106
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.438503e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729865
Test loss (w/o reg) on all data: 0.012055074
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.995253e-08
Norm of the params: 9.153178
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15012106
Train loss (w/o reg) on all data: 0.14208034
Test loss (w/o reg) on all data: 0.081287086
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.555351e-06
Norm of the params: 12.68127
              Random: fixed  15 labels. Loss 0.08129. Accuracy 0.981.
### Flips: 156, rs: 22, checks: 312
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729476
Test loss (w/o reg) on all data: 0.01205503
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.552293e-07
Norm of the params: 9.153219
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729486
Test loss (w/o reg) on all data: 0.012054964
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.722377e-07
Norm of the params: 9.153218
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14906731
Train loss (w/o reg) on all data: 0.14099231
Test loss (w/o reg) on all data: 0.07958668
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.5046023e-05
Norm of the params: 12.70827
              Random: fixed  16 labels. Loss 0.07959. Accuracy 0.981.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1733159
Train loss (w/o reg) on all data: 0.1646796
Test loss (w/o reg) on all data: 0.108015195
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7549322e-05
Norm of the params: 13.14252
Flipped loss: 0.10802. Accuracy: 0.981
### Flips: 156, rs: 23, checks: 52
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09553839
Train loss (w/o reg) on all data: 0.08444226
Test loss (w/o reg) on all data: 0.07081024
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2639969e-05
Norm of the params: 14.897068
     Influence (LOO): fixed  39 labels. Loss 0.07081. Accuracy 0.977.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052312568
Train loss (w/o reg) on all data: 0.037417836
Test loss (w/o reg) on all data: 0.049235463
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5894024e-06
Norm of the params: 17.259624
                Loss: fixed  48 labels. Loss 0.04924. Accuracy 0.985.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16554143
Train loss (w/o reg) on all data: 0.15664198
Test loss (w/o reg) on all data: 0.09867558
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.6410762e-05
Norm of the params: 13.3412485
              Random: fixed   5 labels. Loss 0.09868. Accuracy 0.981.
### Flips: 156, rs: 23, checks: 104
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022856066
Train loss (w/o reg) on all data: 0.015791403
Test loss (w/o reg) on all data: 0.02057413
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.049755e-07
Norm of the params: 11.886684
     Influence (LOO): fixed  71 labels. Loss 0.02057. Accuracy 0.992.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011212135
Train loss (w/o reg) on all data: 0.005091908
Test loss (w/o reg) on all data: 0.024339715
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.3436854e-07
Norm of the params: 11.063658
                Loss: fixed  72 labels. Loss 0.02434. Accuracy 0.989.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16242227
Train loss (w/o reg) on all data: 0.15367289
Test loss (w/o reg) on all data: 0.09671527
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0360008e-05
Norm of the params: 13.228287
              Random: fixed   7 labels. Loss 0.09672. Accuracy 0.981.
### Flips: 156, rs: 23, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173003
Test loss (w/o reg) on all data: 0.012054928
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6875218e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021730037
Test loss (w/o reg) on all data: 0.012054862
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5574482e-07
Norm of the params: 9.153159
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15182067
Train loss (w/o reg) on all data: 0.14268509
Test loss (w/o reg) on all data: 0.086703524
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.452719e-05
Norm of the params: 13.517097
              Random: fixed  12 labels. Loss 0.08670. Accuracy 0.985.
### Flips: 156, rs: 23, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730133
Test loss (w/o reg) on all data: 0.012054674
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.369539e-08
Norm of the params: 9.153148
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172994
Test loss (w/o reg) on all data: 0.012054756
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.056447e-07
Norm of the params: 9.153169
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14574793
Train loss (w/o reg) on all data: 0.13637254
Test loss (w/o reg) on all data: 0.088134274
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.5516173e-06
Norm of the params: 13.693348
              Random: fixed  15 labels. Loss 0.08813. Accuracy 0.985.
### Flips: 156, rs: 23, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730186
Test loss (w/o reg) on all data: 0.012054645
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.129968e-07
Norm of the params: 9.153141
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.00217302
Test loss (w/o reg) on all data: 0.012054753
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2286267e-07
Norm of the params: 9.153142
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14107293
Train loss (w/o reg) on all data: 0.13160823
Test loss (w/o reg) on all data: 0.084102556
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.084432e-06
Norm of the params: 13.758411
              Random: fixed  18 labels. Loss 0.08410. Accuracy 0.981.
### Flips: 156, rs: 23, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012054875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1015202e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729805
Test loss (w/o reg) on all data: 0.012054944
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9255158e-07
Norm of the params: 9.153183
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14083022
Train loss (w/o reg) on all data: 0.13168924
Test loss (w/o reg) on all data: 0.083182104
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6741538e-05
Norm of the params: 13.521083
              Random: fixed  19 labels. Loss 0.08318. Accuracy 0.985.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18920392
Train loss (w/o reg) on all data: 0.18126562
Test loss (w/o reg) on all data: 0.117894195
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0845036e-05
Norm of the params: 12.600236
Flipped loss: 0.11789. Accuracy: 0.985
### Flips: 156, rs: 24, checks: 52
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10784101
Train loss (w/o reg) on all data: 0.09672576
Test loss (w/o reg) on all data: 0.08018738
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.454637e-06
Norm of the params: 14.909895
     Influence (LOO): fixed  37 labels. Loss 0.08019. Accuracy 0.973.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06678852
Train loss (w/o reg) on all data: 0.053158026
Test loss (w/o reg) on all data: 0.059457358
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.165712e-06
Norm of the params: 16.510899
                Loss: fixed  52 labels. Loss 0.05946. Accuracy 0.973.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18792082
Train loss (w/o reg) on all data: 0.18005754
Test loss (w/o reg) on all data: 0.117607914
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.596217e-06
Norm of the params: 12.540557
              Random: fixed   2 labels. Loss 0.11761. Accuracy 0.985.
### Flips: 156, rs: 24, checks: 104
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04330981
Train loss (w/o reg) on all data: 0.034041755
Test loss (w/o reg) on all data: 0.03670201
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.4783306e-06
Norm of the params: 13.61474
     Influence (LOO): fixed  65 labels. Loss 0.03670. Accuracy 0.985.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011734415
Train loss (w/o reg) on all data: 0.0050181556
Test loss (w/o reg) on all data: 0.01409201
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.894952e-07
Norm of the params: 11.589875
                Loss: fixed  78 labels. Loss 0.01409. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18516463
Train loss (w/o reg) on all data: 0.17718284
Test loss (w/o reg) on all data: 0.114588946
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.571866e-06
Norm of the params: 12.634711
              Random: fixed   5 labels. Loss 0.11459. Accuracy 0.985.
### Flips: 156, rs: 24, checks: 156
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01690799
Train loss (w/o reg) on all data: 0.010424982
Test loss (w/o reg) on all data: 0.015116861
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.892491e-07
Norm of the params: 11.386842
     Influence (LOO): fixed  78 labels. Loss 0.01512. Accuracy 0.989.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056534
Train loss (w/o reg) on all data: 0.0028169132
Test loss (w/o reg) on all data: 0.0127548035
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5304341e-06
Norm of the params: 9.683739
                Loss: fixed  81 labels. Loss 0.01275. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17632478
Train loss (w/o reg) on all data: 0.16801089
Test loss (w/o reg) on all data: 0.11389047
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.970427e-06
Norm of the params: 12.894884
              Random: fixed  10 labels. Loss 0.11389. Accuracy 0.981.
### Flips: 156, rs: 24, checks: 208
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0103666205
Train loss (w/o reg) on all data: 0.0053550964
Test loss (w/o reg) on all data: 0.015477796
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.8382205e-07
Norm of the params: 10.0115185
     Influence (LOO): fixed  81 labels. Loss 0.01548. Accuracy 0.992.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729302
Test loss (w/o reg) on all data: 0.012054227
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.566173e-07
Norm of the params: 9.153238
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1663304
Train loss (w/o reg) on all data: 0.15731043
Test loss (w/o reg) on all data: 0.10466299
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.5676667e-06
Norm of the params: 13.431283
              Random: fixed  16 labels. Loss 0.10466. Accuracy 0.985.
### Flips: 156, rs: 24, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729758
Test loss (w/o reg) on all data: 0.012054695
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.331199e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.012054665
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9302726e-07
Norm of the params: 9.153188
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15572436
Train loss (w/o reg) on all data: 0.14641362
Test loss (w/o reg) on all data: 0.1038775
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.845946e-06
Norm of the params: 13.6460495
              Random: fixed  20 labels. Loss 0.10388. Accuracy 0.981.
### Flips: 156, rs: 24, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730615
Test loss (w/o reg) on all data: 0.012056013
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4212042e-07
Norm of the params: 9.153092
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730599
Test loss (w/o reg) on all data: 0.012056089
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5381923e-07
Norm of the params: 9.153097
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15090775
Train loss (w/o reg) on all data: 0.14157525
Test loss (w/o reg) on all data: 0.10344575
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.041232e-05
Norm of the params: 13.662002
              Random: fixed  23 labels. Loss 0.10345. Accuracy 0.973.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17030407
Train loss (w/o reg) on all data: 0.16206321
Test loss (w/o reg) on all data: 0.08504813
Train acc on all data:  0.9369627507163324
Test acc on all data:   1.0
Norm of the mean of gradients: 4.2432574e-05
Norm of the params: 12.838119
Flipped loss: 0.08505. Accuracy: 1.000
### Flips: 156, rs: 25, checks: 52
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07529503
Train loss (w/o reg) on all data: 0.067275025
Test loss (w/o reg) on all data: 0.042692706
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0366872e-05
Norm of the params: 12.664915
     Influence (LOO): fixed  42 labels. Loss 0.04269. Accuracy 0.989.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034072995
Train loss (w/o reg) on all data: 0.021497704
Test loss (w/o reg) on all data: 0.04026465
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.854713e-06
Norm of the params: 15.858937
                Loss: fixed  52 labels. Loss 0.04026. Accuracy 0.985.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16375841
Train loss (w/o reg) on all data: 0.15553838
Test loss (w/o reg) on all data: 0.08203074
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4318731e-05
Norm of the params: 12.821882
              Random: fixed   4 labels. Loss 0.08203. Accuracy 0.996.
### Flips: 156, rs: 25, checks: 104
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029816965
Train loss (w/o reg) on all data: 0.023317944
Test loss (w/o reg) on all data: 0.019369584
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3591714e-06
Norm of the params: 11.400895
     Influence (LOO): fixed  62 labels. Loss 0.01937. Accuracy 0.992.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173042
Test loss (w/o reg) on all data: 0.012055224
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4317254e-07
Norm of the params: 9.153116
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16187173
Train loss (w/o reg) on all data: 0.1537
Test loss (w/o reg) on all data: 0.078303814
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1965387e-05
Norm of the params: 12.784156
              Random: fixed   6 labels. Loss 0.07830. Accuracy 0.996.
### Flips: 156, rs: 25, checks: 156
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002355
Train loss (w/o reg) on all data: 0.005447908
Test loss (w/o reg) on all data: 0.012245149
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3785289e-07
Norm of the params: 9.544053
     Influence (LOO): fixed  69 labels. Loss 0.01225. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172923
Test loss (w/o reg) on all data: 0.012055157
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6001808e-07
Norm of the params: 9.153244
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15757506
Train loss (w/o reg) on all data: 0.15015197
Test loss (w/o reg) on all data: 0.07676018
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.2039916e-05
Norm of the params: 12.184493
              Random: fixed   9 labels. Loss 0.07676. Accuracy 0.989.
### Flips: 156, rs: 25, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.012054752
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5579015e-07
Norm of the params: 9.153224
     Influence (LOO): fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729427
Test loss (w/o reg) on all data: 0.012054626
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9854437e-07
Norm of the params: 9.153225
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15256424
Train loss (w/o reg) on all data: 0.14523555
Test loss (w/o reg) on all data: 0.07458494
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6099195e-05
Norm of the params: 12.106765
              Random: fixed  12 labels. Loss 0.07458. Accuracy 0.992.
### Flips: 156, rs: 25, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729532
Test loss (w/o reg) on all data: 0.012054986
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.1272604e-08
Norm of the params: 9.153213
     Influence (LOO): fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729604
Test loss (w/o reg) on all data: 0.012054964
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5174594e-07
Norm of the params: 9.153207
                Loss: fixed  70 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13858818
Train loss (w/o reg) on all data: 0.13075279
Test loss (w/o reg) on all data: 0.069693714
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.4030046e-06
Norm of the params: 12.518293
              Random: fixed  18 labels. Loss 0.06969. Accuracy 0.989.
### Flips: 156, rs: 25, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730338
Test loss (w/o reg) on all data: 0.012055626
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9328257e-07
Norm of the params: 9.153126
     Influence (LOO): fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730312
Test loss (w/o reg) on all data: 0.01205569
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8218952e-07
Norm of the params: 9.153128
                Loss: fixed  70 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13545404
Train loss (w/o reg) on all data: 0.12798987
Test loss (w/o reg) on all data: 0.06834045
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5778216e-05
Norm of the params: 12.218164
              Random: fixed  20 labels. Loss 0.06834. Accuracy 0.985.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16943891
Train loss (w/o reg) on all data: 0.16035798
Test loss (w/o reg) on all data: 0.112094805
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5669139e-05
Norm of the params: 13.476597
Flipped loss: 0.11209. Accuracy: 0.977
### Flips: 156, rs: 26, checks: 52
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07909611
Train loss (w/o reg) on all data: 0.070231706
Test loss (w/o reg) on all data: 0.055452935
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.9197381e-05
Norm of the params: 13.314953
     Influence (LOO): fixed  41 labels. Loss 0.05545. Accuracy 0.981.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035196837
Train loss (w/o reg) on all data: 0.023059001
Test loss (w/o reg) on all data: 0.06494904
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.7794947e-06
Norm of the params: 15.580652
                Loss: fixed  52 labels. Loss 0.06495. Accuracy 0.973.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15719
Train loss (w/o reg) on all data: 0.14847624
Test loss (w/o reg) on all data: 0.09402766
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9093848e-05
Norm of the params: 13.201324
              Random: fixed   8 labels. Loss 0.09403. Accuracy 0.985.
### Flips: 156, rs: 26, checks: 104
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02695571
Train loss (w/o reg) on all data: 0.020139595
Test loss (w/o reg) on all data: 0.016445633
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4097267e-06
Norm of the params: 11.675714
     Influence (LOO): fixed  63 labels. Loss 0.01645. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008879047
Train loss (w/o reg) on all data: 0.0034203078
Test loss (w/o reg) on all data: 0.013982261
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.5207714e-07
Norm of the params: 10.448673
                Loss: fixed  70 labels. Loss 0.01398. Accuracy 0.996.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15584625
Train loss (w/o reg) on all data: 0.14721909
Test loss (w/o reg) on all data: 0.091303974
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4617259e-05
Norm of the params: 13.135568
              Random: fixed  10 labels. Loss 0.09130. Accuracy 0.985.
### Flips: 156, rs: 26, checks: 156
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009260063
Train loss (w/o reg) on all data: 0.0044020507
Test loss (w/o reg) on all data: 0.012441768
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3907868e-06
Norm of the params: 9.85699
     Influence (LOO): fixed  70 labels. Loss 0.01244. Accuracy 0.992.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729085
Test loss (w/o reg) on all data: 0.01205488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3953255e-07
Norm of the params: 9.153261
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14466846
Train loss (w/o reg) on all data: 0.13520147
Test loss (w/o reg) on all data: 0.09292323
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.536609e-05
Norm of the params: 13.76008
              Random: fixed  14 labels. Loss 0.09292. Accuracy 0.969.
### Flips: 156, rs: 26, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729278
Test loss (w/o reg) on all data: 0.012055775
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3543771e-07
Norm of the params: 9.153242
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172935
Test loss (w/o reg) on all data: 0.0120559465
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0853756e-07
Norm of the params: 9.1532345
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1411788
Train loss (w/o reg) on all data: 0.1317067
Test loss (w/o reg) on all data: 0.09490335
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.8857273e-05
Norm of the params: 13.763797
              Random: fixed  17 labels. Loss 0.09490. Accuracy 0.973.
### Flips: 156, rs: 26, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172919
Test loss (w/o reg) on all data: 0.01205567
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.1192925e-07
Norm of the params: 9.153249
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.012055847
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1474198e-07
Norm of the params: 9.153249
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13671568
Train loss (w/o reg) on all data: 0.12679146
Test loss (w/o reg) on all data: 0.09427136
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.8742124e-05
Norm of the params: 14.088452
              Random: fixed  19 labels. Loss 0.09427. Accuracy 0.977.
### Flips: 156, rs: 26, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021727863
Test loss (w/o reg) on all data: 0.01205517
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9390094e-07
Norm of the params: 9.153395
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021727907
Test loss (w/o reg) on all data: 0.012055116
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7723177e-07
Norm of the params: 9.153391
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12782548
Train loss (w/o reg) on all data: 0.11698631
Test loss (w/o reg) on all data: 0.09094317
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.7987568e-05
Norm of the params: 14.723572
              Random: fixed  23 labels. Loss 0.09094. Accuracy 0.977.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18353708
Train loss (w/o reg) on all data: 0.17654419
Test loss (w/o reg) on all data: 0.10668625
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.662717e-06
Norm of the params: 11.826152
Flipped loss: 0.10669. Accuracy: 0.989
### Flips: 156, rs: 27, checks: 52
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10210137
Train loss (w/o reg) on all data: 0.093958795
Test loss (w/o reg) on all data: 0.073395126
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.1066715e-06
Norm of the params: 12.76133
     Influence (LOO): fixed  38 labels. Loss 0.07340. Accuracy 0.977.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04750024
Train loss (w/o reg) on all data: 0.03509173
Test loss (w/o reg) on all data: 0.059426866
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.162399e-06
Norm of the params: 15.75342
                Loss: fixed  52 labels. Loss 0.05943. Accuracy 0.985.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17244157
Train loss (w/o reg) on all data: 0.165129
Test loss (w/o reg) on all data: 0.095528685
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.149953e-05
Norm of the params: 12.093436
              Random: fixed   6 labels. Loss 0.09553. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 104
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03386049
Train loss (w/o reg) on all data: 0.026052395
Test loss (w/o reg) on all data: 0.046729714
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.188948e-06
Norm of the params: 12.496476
     Influence (LOO): fixed  69 labels. Loss 0.04673. Accuracy 0.985.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009203455
Train loss (w/o reg) on all data: 0.0036351315
Test loss (w/o reg) on all data: 0.011850202
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0524861e-06
Norm of the params: 10.553032
                Loss: fixed  77 labels. Loss 0.01185. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16838759
Train loss (w/o reg) on all data: 0.16091013
Test loss (w/o reg) on all data: 0.09282721
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.4548634e-06
Norm of the params: 12.229033
              Random: fixed   8 labels. Loss 0.09283. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728626
Test loss (w/o reg) on all data: 0.012055138
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3220623e-07
Norm of the params: 9.153311
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076085245
Train loss (w/o reg) on all data: 0.0028444203
Test loss (w/o reg) on all data: 0.014220191
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.441008e-07
Norm of the params: 9.761253
                Loss: fixed  78 labels. Loss 0.01422. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16248563
Train loss (w/o reg) on all data: 0.15446928
Test loss (w/o reg) on all data: 0.089879654
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.206179e-06
Norm of the params: 12.662023
              Random: fixed  12 labels. Loss 0.08988. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729607
Test loss (w/o reg) on all data: 0.0120550115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1778536e-07
Norm of the params: 9.153206
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076085227
Train loss (w/o reg) on all data: 0.0028445919
Test loss (w/o reg) on all data: 0.014218654
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6219914e-07
Norm of the params: 9.761077
                Loss: fixed  78 labels. Loss 0.01422. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14830376
Train loss (w/o reg) on all data: 0.14007762
Test loss (w/o reg) on all data: 0.07915119
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1767823e-05
Norm of the params: 12.826646
              Random: fixed  20 labels. Loss 0.07915. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172974
Test loss (w/o reg) on all data: 0.012055076
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7465281e-07
Norm of the params: 9.153191
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729732
Test loss (w/o reg) on all data: 0.0120551
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.402813e-07
Norm of the params: 9.153192
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14465663
Train loss (w/o reg) on all data: 0.13617106
Test loss (w/o reg) on all data: 0.07652618
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1921701e-05
Norm of the params: 13.027334
              Random: fixed  22 labels. Loss 0.07653. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.012055582
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9287795e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.01205566
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6933195e-07
Norm of the params: 9.153211
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13931201
Train loss (w/o reg) on all data: 0.13151747
Test loss (w/o reg) on all data: 0.069002345
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.2507216e-06
Norm of the params: 12.485622
              Random: fixed  26 labels. Loss 0.06900. Accuracy 0.989.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19577166
Train loss (w/o reg) on all data: 0.18796794
Test loss (w/o reg) on all data: 0.113601916
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.2164036e-05
Norm of the params: 12.492974
Flipped loss: 0.11360. Accuracy: 0.985
### Flips: 156, rs: 28, checks: 52
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10902781
Train loss (w/o reg) on all data: 0.09707053
Test loss (w/o reg) on all data: 0.06743909
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9515132e-05
Norm of the params: 15.464336
     Influence (LOO): fixed  39 labels. Loss 0.06744. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06892985
Train loss (w/o reg) on all data: 0.054607205
Test loss (w/o reg) on all data: 0.052799426
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.067518e-06
Norm of the params: 16.924923
                Loss: fixed  52 labels. Loss 0.05280. Accuracy 0.985.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19304441
Train loss (w/o reg) on all data: 0.18573004
Test loss (w/o reg) on all data: 0.10742294
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0749278e-05
Norm of the params: 12.094933
              Random: fixed   3 labels. Loss 0.10742. Accuracy 0.989.
### Flips: 156, rs: 28, checks: 104
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05860848
Train loss (w/o reg) on all data: 0.047110163
Test loss (w/o reg) on all data: 0.052212935
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2139515e-06
Norm of the params: 15.16464
     Influence (LOO): fixed  63 labels. Loss 0.05221. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730713
Test loss (w/o reg) on all data: 0.012055681
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2648403e-07
Norm of the params: 9.153083
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1884237
Train loss (w/o reg) on all data: 0.18123133
Test loss (w/o reg) on all data: 0.10480505
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7933183e-05
Norm of the params: 11.993637
              Random: fixed   6 labels. Loss 0.10481. Accuracy 0.985.
### Flips: 156, rs: 28, checks: 156
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017325753
Train loss (w/o reg) on all data: 0.011976943
Test loss (w/o reg) on all data: 0.014870924
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.552474e-07
Norm of the params: 10.34293
     Influence (LOO): fixed  79 labels. Loss 0.01487. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729446
Test loss (w/o reg) on all data: 0.012054047
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.064767e-07
Norm of the params: 9.153223
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18510374
Train loss (w/o reg) on all data: 0.17795552
Test loss (w/o reg) on all data: 0.10412856
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.792846e-06
Norm of the params: 11.956776
              Random: fixed   8 labels. Loss 0.10413. Accuracy 0.985.
### Flips: 156, rs: 28, checks: 208
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010038309
Train loss (w/o reg) on all data: 0.005286724
Test loss (w/o reg) on all data: 0.012297493
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9450575e-07
Norm of the params: 9.748421
     Influence (LOO): fixed  81 labels. Loss 0.01230. Accuracy 0.992.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728757
Test loss (w/o reg) on all data: 0.012055139
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0397862e-07
Norm of the params: 9.153298
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1763782
Train loss (w/o reg) on all data: 0.16914426
Test loss (w/o reg) on all data: 0.10397084
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.6267476e-05
Norm of the params: 12.028257
              Random: fixed  12 labels. Loss 0.10397. Accuracy 0.977.
### Flips: 156, rs: 28, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729884
Test loss (w/o reg) on all data: 0.012055087
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7007542e-07
Norm of the params: 9.153175
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729893
Test loss (w/o reg) on all data: 0.012055041
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.802157e-08
Norm of the params: 9.153175
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17394416
Train loss (w/o reg) on all data: 0.16683581
Test loss (w/o reg) on all data: 0.102009006
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.1634594e-05
Norm of the params: 11.923378
              Random: fixed  13 labels. Loss 0.10201. Accuracy 0.973.
### Flips: 156, rs: 28, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.012055337
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.075452e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729516
Test loss (w/o reg) on all data: 0.012055411
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5185477e-07
Norm of the params: 9.153214
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16708294
Train loss (w/o reg) on all data: 0.1603618
Test loss (w/o reg) on all data: 0.09546815
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.091546e-06
Norm of the params: 11.594086
              Random: fixed  18 labels. Loss 0.09547. Accuracy 0.981.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18440148
Train loss (w/o reg) on all data: 0.177412
Test loss (w/o reg) on all data: 0.10544424
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0668972e-05
Norm of the params: 11.823265
Flipped loss: 0.10544. Accuracy: 0.989
### Flips: 156, rs: 29, checks: 52
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08861031
Train loss (w/o reg) on all data: 0.07997273
Test loss (w/o reg) on all data: 0.065650254
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.688622e-05
Norm of the params: 13.143501
     Influence (LOO): fixed  41 labels. Loss 0.06565. Accuracy 0.989.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046974897
Train loss (w/o reg) on all data: 0.03444098
Test loss (w/o reg) on all data: 0.051400207
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.0013088e-06
Norm of the params: 15.832826
                Loss: fixed  52 labels. Loss 0.05140. Accuracy 0.981.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17246582
Train loss (w/o reg) on all data: 0.16542947
Test loss (w/o reg) on all data: 0.09247847
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8213279e-05
Norm of the params: 11.862837
              Random: fixed   7 labels. Loss 0.09248. Accuracy 0.992.
### Flips: 156, rs: 29, checks: 104
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054052003
Train loss (w/o reg) on all data: 0.04587191
Test loss (w/o reg) on all data: 0.035968035
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.7918214e-06
Norm of the params: 12.790695
     Influence (LOO): fixed  58 labels. Loss 0.03597. Accuracy 0.996.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01058973
Train loss (w/o reg) on all data: 0.0041356585
Test loss (w/o reg) on all data: 0.012077069
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.7351406e-07
Norm of the params: 11.361402
                Loss: fixed  74 labels. Loss 0.01208. Accuracy 0.996.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16308129
Train loss (w/o reg) on all data: 0.15633936
Test loss (w/o reg) on all data: 0.08716953
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.052316e-05
Norm of the params: 11.611995
              Random: fixed  13 labels. Loss 0.08717. Accuracy 0.989.
### Flips: 156, rs: 29, checks: 156
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012269591
Train loss (w/o reg) on all data: 0.0068252385
Test loss (w/o reg) on all data: 0.014960452
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.495155e-07
Norm of the params: 10.434896
     Influence (LOO): fixed  75 labels. Loss 0.01496. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076231947
Train loss (w/o reg) on all data: 0.0027304676
Test loss (w/o reg) on all data: 0.009578335
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3364698e-06
Norm of the params: 9.892146
                Loss: fixed  76 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15765496
Train loss (w/o reg) on all data: 0.1507713
Test loss (w/o reg) on all data: 0.08589325
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 0.00010797544
Norm of the params: 11.733409
              Random: fixed  16 labels. Loss 0.08589. Accuracy 0.981.
### Flips: 156, rs: 29, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172962
Test loss (w/o reg) on all data: 0.012056774
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9066868e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076231947
Train loss (w/o reg) on all data: 0.0027303554
Test loss (w/o reg) on all data: 0.009578107
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5040253e-07
Norm of the params: 9.892259
                Loss: fixed  76 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14656346
Train loss (w/o reg) on all data: 0.13893211
Test loss (w/o reg) on all data: 0.08073785
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0133978e-05
Norm of the params: 12.354225
              Random: fixed  20 labels. Loss 0.08074. Accuracy 0.985.
### Flips: 156, rs: 29, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729216
Test loss (w/o reg) on all data: 0.012055382
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.260904e-07
Norm of the params: 9.153249
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.012055277
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7960589e-07
Norm of the params: 9.153248
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13698193
Train loss (w/o reg) on all data: 0.12906574
Test loss (w/o reg) on all data: 0.075939775
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.44222195e-05
Norm of the params: 12.582682
              Random: fixed  25 labels. Loss 0.07594. Accuracy 0.985.
### Flips: 156, rs: 29, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728626
Test loss (w/o reg) on all data: 0.012055616
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4340684e-07
Norm of the params: 9.153313
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021728668
Test loss (w/o reg) on all data: 0.012055781
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.113179e-07
Norm of the params: 9.15331
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12574644
Train loss (w/o reg) on all data: 0.118058756
Test loss (w/o reg) on all data: 0.07146422
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.418213e-06
Norm of the params: 12.399748
              Random: fixed  29 labels. Loss 0.07146. Accuracy 0.989.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1906969
Train loss (w/o reg) on all data: 0.18327604
Test loss (w/o reg) on all data: 0.10049713
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.1758977e-05
Norm of the params: 12.182653
Flipped loss: 0.10050. Accuracy: 0.985
### Flips: 156, rs: 30, checks: 52
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1110649
Train loss (w/o reg) on all data: 0.10055719
Test loss (w/o reg) on all data: 0.062071387
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0073457e-06
Norm of the params: 14.4966955
     Influence (LOO): fixed  37 labels. Loss 0.06207. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055288725
Train loss (w/o reg) on all data: 0.04291855
Test loss (w/o reg) on all data: 0.065050185
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6521908e-05
Norm of the params: 15.729065
                Loss: fixed  52 labels. Loss 0.06505. Accuracy 0.985.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18173397
Train loss (w/o reg) on all data: 0.174165
Test loss (w/o reg) on all data: 0.09396045
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.8169385e-05
Norm of the params: 12.303637
              Random: fixed   6 labels. Loss 0.09396. Accuracy 0.985.
### Flips: 156, rs: 30, checks: 104
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048784886
Train loss (w/o reg) on all data: 0.040692728
Test loss (w/o reg) on all data: 0.035397086
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.6790433e-06
Norm of the params: 12.721759
     Influence (LOO): fixed  66 labels. Loss 0.03540. Accuracy 0.989.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009626353
Train loss (w/o reg) on all data: 0.0036695325
Test loss (w/o reg) on all data: 0.01345585
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1689932e-06
Norm of the params: 10.914964
                Loss: fixed  78 labels. Loss 0.01346. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17499577
Train loss (w/o reg) on all data: 0.16708018
Test loss (w/o reg) on all data: 0.08907178
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4605491e-05
Norm of the params: 12.582194
              Random: fixed   9 labels. Loss 0.08907. Accuracy 0.989.
### Flips: 156, rs: 30, checks: 156
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010217052
Train loss (w/o reg) on all data: 0.005085032
Test loss (w/o reg) on all data: 0.022661123
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0201356e-07
Norm of the params: 10.131161
     Influence (LOO): fixed  80 labels. Loss 0.02266. Accuracy 0.992.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729846
Test loss (w/o reg) on all data: 0.012053982
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9247241e-07
Norm of the params: 9.153178
                Loss: fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1679719
Train loss (w/o reg) on all data: 0.15951394
Test loss (w/o reg) on all data: 0.08350051
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9356053e-05
Norm of the params: 13.006124
              Random: fixed  12 labels. Loss 0.08350. Accuracy 0.992.
### Flips: 156, rs: 30, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172939
Test loss (w/o reg) on all data: 0.012055371
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1103608e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729409
Test loss (w/o reg) on all data: 0.012055431
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0418746e-07
Norm of the params: 9.153227
                Loss: fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15916832
Train loss (w/o reg) on all data: 0.15040013
Test loss (w/o reg) on all data: 0.07984843
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2923285e-05
Norm of the params: 13.242501
              Random: fixed  16 labels. Loss 0.07985. Accuracy 0.989.
### Flips: 156, rs: 30, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002173016
Test loss (w/o reg) on all data: 0.012054893
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3717423e-07
Norm of the params: 9.153146
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217301
Test loss (w/o reg) on all data: 0.0120548755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0027638e-07
Norm of the params: 9.153151
                Loss: fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15730385
Train loss (w/o reg) on all data: 0.14831321
Test loss (w/o reg) on all data: 0.07883187
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6797683e-05
Norm of the params: 13.409438
              Random: fixed  17 labels. Loss 0.07883. Accuracy 0.989.
### Flips: 156, rs: 30, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729597
Test loss (w/o reg) on all data: 0.012055054
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.499918e-07
Norm of the params: 9.153207
     Influence (LOO): fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729616
Test loss (w/o reg) on all data: 0.012055134
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8768098e-07
Norm of the params: 9.153207
                Loss: fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15332054
Train loss (w/o reg) on all data: 0.1444964
Test loss (w/o reg) on all data: 0.07955663
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.979723e-06
Norm of the params: 13.284686
              Random: fixed  19 labels. Loss 0.07956. Accuracy 0.989.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17769282
Train loss (w/o reg) on all data: 0.16977036
Test loss (w/o reg) on all data: 0.11285669
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.2229607e-05
Norm of the params: 12.587658
Flipped loss: 0.11286. Accuracy: 0.973
### Flips: 156, rs: 31, checks: 52
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07847092
Train loss (w/o reg) on all data: 0.068568215
Test loss (w/o reg) on all data: 0.071208775
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.31497345e-05
Norm of the params: 14.073171
     Influence (LOO): fixed  43 labels. Loss 0.07121. Accuracy 0.977.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039168306
Train loss (w/o reg) on all data: 0.026966093
Test loss (w/o reg) on all data: 0.053427923
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.283503e-06
Norm of the params: 15.621916
                Loss: fixed  52 labels. Loss 0.05343. Accuracy 0.977.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1643865
Train loss (w/o reg) on all data: 0.1563454
Test loss (w/o reg) on all data: 0.102956556
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4320467e-05
Norm of the params: 12.68156
              Random: fixed   8 labels. Loss 0.10296. Accuracy 0.981.
### Flips: 156, rs: 31, checks: 104
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032612342
Train loss (w/o reg) on all data: 0.024496218
Test loss (w/o reg) on all data: 0.039800745
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0995725e-06
Norm of the params: 12.740585
     Influence (LOO): fixed  62 labels. Loss 0.03980. Accuracy 0.985.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012716407
Train loss (w/o reg) on all data: 0.005711642
Test loss (w/o reg) on all data: 0.01913463
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1720726e-06
Norm of the params: 11.836185
                Loss: fixed  71 labels. Loss 0.01913. Accuracy 0.989.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15661126
Train loss (w/o reg) on all data: 0.1481366
Test loss (w/o reg) on all data: 0.09676347
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1034417e-05
Norm of the params: 13.018958
              Random: fixed  12 labels. Loss 0.09676. Accuracy 0.981.
### Flips: 156, rs: 31, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730375
Test loss (w/o reg) on all data: 0.012055381
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0918093e-07
Norm of the params: 9.153122
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008336663
Train loss (w/o reg) on all data: 0.0032726647
Test loss (w/o reg) on all data: 0.01687158
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3782464e-07
Norm of the params: 10.063795
                Loss: fixed  73 labels. Loss 0.01687. Accuracy 0.989.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14927477
Train loss (w/o reg) on all data: 0.14044619
Test loss (w/o reg) on all data: 0.090070985
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.4098093e-06
Norm of the params: 13.288024
              Random: fixed  16 labels. Loss 0.09007. Accuracy 0.989.
### Flips: 156, rs: 31, checks: 208
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729537
Test loss (w/o reg) on all data: 0.012054782
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.45844e-08
Norm of the params: 9.1532135
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729646
Test loss (w/o reg) on all data: 0.01205492
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.190598e-07
Norm of the params: 9.153201
                Loss: fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14093141
Train loss (w/o reg) on all data: 0.13199174
Test loss (w/o reg) on all data: 0.08502794
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.76804e-05
Norm of the params: 13.371367
              Random: fixed  20 labels. Loss 0.08503. Accuracy 0.985.
### Flips: 156, rs: 31, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729125
Test loss (w/o reg) on all data: 0.01205509
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8800866e-07
Norm of the params: 9.153257
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172913
Test loss (w/o reg) on all data: 0.012055165
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2217193e-07
Norm of the params: 9.153257
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13347773
Train loss (w/o reg) on all data: 0.123767145
Test loss (w/o reg) on all data: 0.07939094
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.737757e-06
Norm of the params: 13.935986
              Random: fixed  24 labels. Loss 0.07939. Accuracy 0.985.
### Flips: 156, rs: 31, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729846
Test loss (w/o reg) on all data: 0.01205532
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1266737e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172983
Test loss (w/o reg) on all data: 0.012055249
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3144472e-07
Norm of the params: 9.153181
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12738156
Train loss (w/o reg) on all data: 0.11852794
Test loss (w/o reg) on all data: 0.07368242
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.251752e-06
Norm of the params: 13.306861
              Random: fixed  28 labels. Loss 0.07368. Accuracy 0.985.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17779735
Train loss (w/o reg) on all data: 0.17098767
Test loss (w/o reg) on all data: 0.121959664
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.05901e-05
Norm of the params: 11.670207
Flipped loss: 0.12196. Accuracy: 0.954
### Flips: 156, rs: 32, checks: 52
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.085168734
Train loss (w/o reg) on all data: 0.07326039
Test loss (w/o reg) on all data: 0.08639624
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.939871e-06
Norm of the params: 15.432657
     Influence (LOO): fixed  38 labels. Loss 0.08640. Accuracy 0.966.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05041205
Train loss (w/o reg) on all data: 0.037720162
Test loss (w/o reg) on all data: 0.10394038
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.3419059e-06
Norm of the params: 15.932287
                Loss: fixed  51 labels. Loss 0.10394. Accuracy 0.969.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17205223
Train loss (w/o reg) on all data: 0.16521496
Test loss (w/o reg) on all data: 0.10993668
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4405744e-05
Norm of the params: 11.6938305
              Random: fixed   4 labels. Loss 0.10994. Accuracy 0.969.
### Flips: 156, rs: 32, checks: 104
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038406014
Train loss (w/o reg) on all data: 0.027793596
Test loss (w/o reg) on all data: 0.054650906
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8138155e-06
Norm of the params: 14.568745
     Influence (LOO): fixed  59 labels. Loss 0.05465. Accuracy 0.981.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010115586
Train loss (w/o reg) on all data: 0.004161014
Test loss (w/o reg) on all data: 0.016151058
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.787109e-07
Norm of the params: 10.912902
                Loss: fixed  72 labels. Loss 0.01615. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16749151
Train loss (w/o reg) on all data: 0.16063087
Test loss (w/o reg) on all data: 0.10573416
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.020462e-05
Norm of the params: 11.713785
              Random: fixed   8 labels. Loss 0.10573. Accuracy 0.966.
### Flips: 156, rs: 32, checks: 156
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730326
Test loss (w/o reg) on all data: 0.012055622
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.154404e-07
Norm of the params: 9.153128
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217303
Test loss (w/o reg) on all data: 0.012055697
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7499174e-07
Norm of the params: 9.15313
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16051017
Train loss (w/o reg) on all data: 0.15359741
Test loss (w/o reg) on all data: 0.10445521
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.8467664e-05
Norm of the params: 11.758193
              Random: fixed  12 labels. Loss 0.10446. Accuracy 0.966.
### Flips: 156, rs: 32, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.0120550655
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.757979e-07
Norm of the params: 9.153182
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729814
Test loss (w/o reg) on all data: 0.012055126
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1736131e-07
Norm of the params: 9.153182
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15810418
Train loss (w/o reg) on all data: 0.15115961
Test loss (w/o reg) on all data: 0.100130424
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.6286554e-05
Norm of the params: 11.785222
              Random: fixed  14 labels. Loss 0.10013. Accuracy 0.969.
### Flips: 156, rs: 32, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730338
Test loss (w/o reg) on all data: 0.012055795
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.862689e-07
Norm of the params: 9.153124
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730317
Test loss (w/o reg) on all data: 0.012055677
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.466393e-07
Norm of the params: 9.153126
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14762002
Train loss (w/o reg) on all data: 0.14058872
Test loss (w/o reg) on all data: 0.087400004
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.5272635e-05
Norm of the params: 11.858583
              Random: fixed  20 labels. Loss 0.08740. Accuracy 0.977.
### Flips: 156, rs: 32, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728645
Test loss (w/o reg) on all data: 0.01205514
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3136164e-07
Norm of the params: 9.153309
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172883
Test loss (w/o reg) on all data: 0.012055151
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0504476e-06
Norm of the params: 9.153288
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14108181
Train loss (w/o reg) on all data: 0.13424733
Test loss (w/o reg) on all data: 0.07719291
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3532605e-05
Norm of the params: 11.691433
              Random: fixed  25 labels. Loss 0.07719. Accuracy 0.985.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1830977
Train loss (w/o reg) on all data: 0.17574643
Test loss (w/o reg) on all data: 0.12233319
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6619362e-05
Norm of the params: 12.125408
Flipped loss: 0.12233. Accuracy: 0.977
### Flips: 156, rs: 33, checks: 52
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09832766
Train loss (w/o reg) on all data: 0.08956207
Test loss (w/o reg) on all data: 0.057834253
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3890372e-05
Norm of the params: 13.240532
     Influence (LOO): fixed  41 labels. Loss 0.05783. Accuracy 0.985.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048605163
Train loss (w/o reg) on all data: 0.035144925
Test loss (w/o reg) on all data: 0.07550078
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.997375e-06
Norm of the params: 16.407461
                Loss: fixed  51 labels. Loss 0.07550. Accuracy 0.981.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17965797
Train loss (w/o reg) on all data: 0.17213562
Test loss (w/o reg) on all data: 0.11935987
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.37464685e-05
Norm of the params: 12.265683
              Random: fixed   3 labels. Loss 0.11936. Accuracy 0.977.
### Flips: 156, rs: 33, checks: 104
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04440046
Train loss (w/o reg) on all data: 0.036833778
Test loss (w/o reg) on all data: 0.034242388
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9142059e-06
Norm of the params: 12.301775
     Influence (LOO): fixed  67 labels. Loss 0.03424. Accuracy 0.992.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010698449
Train loss (w/o reg) on all data: 0.0044477847
Test loss (w/o reg) on all data: 0.022231976
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.968855e-07
Norm of the params: 11.180934
                Loss: fixed  75 labels. Loss 0.02223. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17589895
Train loss (w/o reg) on all data: 0.16842599
Test loss (w/o reg) on all data: 0.11695451
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.0896947e-05
Norm of the params: 12.225357
              Random: fixed   5 labels. Loss 0.11695. Accuracy 0.981.
### Flips: 156, rs: 33, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729511
Test loss (w/o reg) on all data: 0.012054435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.544825e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  80 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009315196
Train loss (w/o reg) on all data: 0.0038443126
Test loss (w/o reg) on all data: 0.023222407
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3591567e-06
Norm of the params: 10.460291
                Loss: fixed  77 labels. Loss 0.02322. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15462308
Train loss (w/o reg) on all data: 0.1469225
Test loss (w/o reg) on all data: 0.10491426
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9558612e-05
Norm of the params: 12.410134
              Random: fixed  14 labels. Loss 0.10491. Accuracy 0.977.
### Flips: 156, rs: 33, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730557
Test loss (w/o reg) on all data: 0.012055251
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1289247e-07
Norm of the params: 9.153102
     Influence (LOO): fixed  80 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075899092
Train loss (w/o reg) on all data: 0.0028805109
Test loss (w/o reg) on all data: 0.022446826
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.770291e-07
Norm of the params: 9.705048
                Loss: fixed  78 labels. Loss 0.02245. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15180092
Train loss (w/o reg) on all data: 0.1442556
Test loss (w/o reg) on all data: 0.100467555
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9878784e-05
Norm of the params: 12.284404
              Random: fixed  16 labels. Loss 0.10047. Accuracy 0.985.
### Flips: 156, rs: 33, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730368
Test loss (w/o reg) on all data: 0.012055261
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9779488e-07
Norm of the params: 9.153121
     Influence (LOO): fixed  80 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00656092
Train loss (w/o reg) on all data: 0.0023782535
Test loss (w/o reg) on all data: 0.012779953
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3673849e-07
Norm of the params: 9.14622
                Loss: fixed  79 labels. Loss 0.01278. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14490435
Train loss (w/o reg) on all data: 0.13670292
Test loss (w/o reg) on all data: 0.09308853
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5396345e-05
Norm of the params: 12.807359
              Random: fixed  20 labels. Loss 0.09309. Accuracy 0.985.
### Flips: 156, rs: 33, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730245
Test loss (w/o reg) on all data: 0.012054755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9353043e-07
Norm of the params: 9.153133
     Influence (LOO): fixed  80 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0065609217
Train loss (w/o reg) on all data: 0.002378225
Test loss (w/o reg) on all data: 0.01278007
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.832769e-07
Norm of the params: 9.146253
                Loss: fixed  79 labels. Loss 0.01278. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12968455
Train loss (w/o reg) on all data: 0.12101207
Test loss (w/o reg) on all data: 0.08039766
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.534647e-05
Norm of the params: 13.170029
              Random: fixed  29 labels. Loss 0.08040. Accuracy 0.985.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17705019
Train loss (w/o reg) on all data: 0.16941504
Test loss (w/o reg) on all data: 0.09688816
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.508284e-06
Norm of the params: 12.357296
Flipped loss: 0.09689. Accuracy: 0.985
### Flips: 156, rs: 34, checks: 52
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072802044
Train loss (w/o reg) on all data: 0.062133905
Test loss (w/o reg) on all data: 0.061358046
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.974768e-06
Norm of the params: 14.606943
     Influence (LOO): fixed  42 labels. Loss 0.06136. Accuracy 0.989.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038618863
Train loss (w/o reg) on all data: 0.026391203
Test loss (w/o reg) on all data: 0.040412825
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7037386e-05
Norm of the params: 15.638197
                Loss: fixed  52 labels. Loss 0.04041. Accuracy 0.985.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17012303
Train loss (w/o reg) on all data: 0.16246767
Test loss (w/o reg) on all data: 0.09164261
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5872418e-05
Norm of the params: 12.373647
              Random: fixed   4 labels. Loss 0.09164. Accuracy 0.985.
### Flips: 156, rs: 34, checks: 104
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028628703
Train loss (w/o reg) on all data: 0.021905368
Test loss (w/o reg) on all data: 0.027954813
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5047037e-06
Norm of the params: 11.595979
     Influence (LOO): fixed  63 labels. Loss 0.02795. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730855
Test loss (w/o reg) on all data: 0.012054243
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6077185e-07
Norm of the params: 9.153069
                Loss: fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16467461
Train loss (w/o reg) on all data: 0.15650849
Test loss (w/o reg) on all data: 0.08676102
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3851249e-05
Norm of the params: 12.779763
              Random: fixed   6 labels. Loss 0.08676. Accuracy 0.989.
### Flips: 156, rs: 34, checks: 156
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728904
Test loss (w/o reg) on all data: 0.012056522
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.721887e-07
Norm of the params: 9.153281
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728913
Test loss (w/o reg) on all data: 0.012056638
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9331872e-07
Norm of the params: 9.15328
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15183283
Train loss (w/o reg) on all data: 0.14364302
Test loss (w/o reg) on all data: 0.079644084
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5167912e-05
Norm of the params: 12.798294
              Random: fixed  12 labels. Loss 0.07964. Accuracy 0.985.
### Flips: 156, rs: 34, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172926
Test loss (w/o reg) on all data: 0.012057423
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5765786e-07
Norm of the params: 9.153242
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729274
Test loss (w/o reg) on all data: 0.012057253
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.919715e-07
Norm of the params: 9.153243
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14551654
Train loss (w/o reg) on all data: 0.13703015
Test loss (w/o reg) on all data: 0.07644422
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.796869e-06
Norm of the params: 13.027958
              Random: fixed  15 labels. Loss 0.07644. Accuracy 0.985.
### Flips: 156, rs: 34, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217295
Test loss (w/o reg) on all data: 0.012055199
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.06716165e-07
Norm of the params: 9.153216
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.002172952
Test loss (w/o reg) on all data: 0.012055223
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.725258e-07
Norm of the params: 9.153216
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.143868
Train loss (w/o reg) on all data: 0.13552369
Test loss (w/o reg) on all data: 0.068781525
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2660838e-05
Norm of the params: 12.918444
              Random: fixed  17 labels. Loss 0.06878. Accuracy 0.992.
### Flips: 156, rs: 34, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729146
Test loss (w/o reg) on all data: 0.01205583
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9851126e-07
Norm of the params: 9.153255
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729136
Test loss (w/o reg) on all data: 0.01205598
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4312308e-07
Norm of the params: 9.153255
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1359715
Train loss (w/o reg) on all data: 0.12723313
Test loss (w/o reg) on all data: 0.06224759
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2663597e-05
Norm of the params: 13.219965
              Random: fixed  20 labels. Loss 0.06225. Accuracy 0.992.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1775532
Train loss (w/o reg) on all data: 0.16834636
Test loss (w/o reg) on all data: 0.111129984
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.118508e-05
Norm of the params: 13.569705
Flipped loss: 0.11113. Accuracy: 0.985
### Flips: 156, rs: 35, checks: 52
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09572603
Train loss (w/o reg) on all data: 0.08618322
Test loss (w/o reg) on all data: 0.08646803
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.087238e-06
Norm of the params: 13.815071
     Influence (LOO): fixed  41 labels. Loss 0.08647. Accuracy 0.985.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04699579
Train loss (w/o reg) on all data: 0.033038974
Test loss (w/o reg) on all data: 0.06929494
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.224656e-06
Norm of the params: 16.707373
                Loss: fixed  52 labels. Loss 0.06929. Accuracy 0.954.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17242828
Train loss (w/o reg) on all data: 0.16309865
Test loss (w/o reg) on all data: 0.10926903
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0918651e-05
Norm of the params: 13.659894
              Random: fixed   4 labels. Loss 0.10927. Accuracy 0.985.
### Flips: 156, rs: 35, checks: 104
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035432518
Train loss (w/o reg) on all data: 0.02795105
Test loss (w/o reg) on all data: 0.038950965
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.4259004e-06
Norm of the params: 12.232307
     Influence (LOO): fixed  65 labels. Loss 0.03895. Accuracy 0.989.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010410203
Train loss (w/o reg) on all data: 0.0043645035
Test loss (w/o reg) on all data: 0.018408582
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.3433964e-07
Norm of the params: 10.99609
                Loss: fixed  71 labels. Loss 0.01841. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1646702
Train loss (w/o reg) on all data: 0.15577585
Test loss (w/o reg) on all data: 0.09837536
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.788595e-05
Norm of the params: 13.337435
              Random: fixed  10 labels. Loss 0.09838. Accuracy 0.981.
### Flips: 156, rs: 35, checks: 156
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012026919
Train loss (w/o reg) on all data: 0.0068831146
Test loss (w/o reg) on all data: 0.016408848
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9126485e-07
Norm of the params: 10.142786
     Influence (LOO): fixed  74 labels. Loss 0.01641. Accuracy 0.992.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008584277
Train loss (w/o reg) on all data: 0.003419324
Test loss (w/o reg) on all data: 0.015040048
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.2457717e-07
Norm of the params: 10.163614
                Loss: fixed  74 labels. Loss 0.01504. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15809885
Train loss (w/o reg) on all data: 0.14929804
Test loss (w/o reg) on all data: 0.09920549
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.209566e-06
Norm of the params: 13.267102
              Random: fixed  13 labels. Loss 0.09921. Accuracy 0.981.
### Flips: 156, rs: 35, checks: 208
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002354
Train loss (w/o reg) on all data: 0.005447759
Test loss (w/o reg) on all data: 0.012244677
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.359441e-07
Norm of the params: 9.544209
     Influence (LOO): fixed  76 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730766
Test loss (w/o reg) on all data: 0.012055137
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.255431e-07
Norm of the params: 9.153077
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14909711
Train loss (w/o reg) on all data: 0.14066191
Test loss (w/o reg) on all data: 0.09126563
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.961034e-05
Norm of the params: 12.988608
              Random: fixed  18 labels. Loss 0.09127. Accuracy 0.985.
### Flips: 156, rs: 35, checks: 260
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002354
Train loss (w/o reg) on all data: 0.005447762
Test loss (w/o reg) on all data: 0.012245069
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.236497e-07
Norm of the params: 9.544205
     Influence (LOO): fixed  76 labels. Loss 0.01225. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172986
Test loss (w/o reg) on all data: 0.012054603
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3024996e-07
Norm of the params: 9.153178
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14588556
Train loss (w/o reg) on all data: 0.13796765
Test loss (w/o reg) on all data: 0.08622539
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9966601e-05
Norm of the params: 12.584051
              Random: fixed  20 labels. Loss 0.08623. Accuracy 0.985.
### Flips: 156, rs: 35, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729816
Test loss (w/o reg) on all data: 0.01205498
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3888206e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729807
Test loss (w/o reg) on all data: 0.01205502
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6571968e-07
Norm of the params: 9.153184
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13642158
Train loss (w/o reg) on all data: 0.12846647
Test loss (w/o reg) on all data: 0.08171113
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4211453e-05
Norm of the params: 12.6135645
              Random: fixed  25 labels. Loss 0.08171. Accuracy 0.985.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.169116
Train loss (w/o reg) on all data: 0.16006982
Test loss (w/o reg) on all data: 0.11693426
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.612244e-06
Norm of the params: 13.45079
Flipped loss: 0.11693. Accuracy: 0.977
### Flips: 156, rs: 36, checks: 52
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07940149
Train loss (w/o reg) on all data: 0.06934815
Test loss (w/o reg) on all data: 0.080032215
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.152425e-06
Norm of the params: 14.179808
     Influence (LOO): fixed  40 labels. Loss 0.08003. Accuracy 0.977.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037653513
Train loss (w/o reg) on all data: 0.025012428
Test loss (w/o reg) on all data: 0.052638162
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5333842e-06
Norm of the params: 15.90037
                Loss: fixed  52 labels. Loss 0.05264. Accuracy 0.985.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16288295
Train loss (w/o reg) on all data: 0.15346898
Test loss (w/o reg) on all data: 0.11249899
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.735451e-05
Norm of the params: 13.721499
              Random: fixed   3 labels. Loss 0.11250. Accuracy 0.977.
### Flips: 156, rs: 36, checks: 104
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019094093
Train loss (w/o reg) on all data: 0.012262369
Test loss (w/o reg) on all data: 0.019695649
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1580983e-06
Norm of the params: 11.689075
     Influence (LOO): fixed  65 labels. Loss 0.01970. Accuracy 0.992.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011227811
Train loss (w/o reg) on all data: 0.004567106
Test loss (w/o reg) on all data: 0.01431102
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9170584e-07
Norm of the params: 11.5418415
                Loss: fixed  68 labels. Loss 0.01431. Accuracy 0.996.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15500133
Train loss (w/o reg) on all data: 0.14537157
Test loss (w/o reg) on all data: 0.11145218
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.929957e-06
Norm of the params: 13.877861
              Random: fixed   7 labels. Loss 0.11145. Accuracy 0.977.
### Flips: 156, rs: 36, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.012055452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5266448e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729325
Test loss (w/o reg) on all data: 0.012055501
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.092443e-07
Norm of the params: 9.153236
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15003143
Train loss (w/o reg) on all data: 0.14073883
Test loss (w/o reg) on all data: 0.1007123
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.914825e-06
Norm of the params: 13.632755
              Random: fixed  11 labels. Loss 0.10071. Accuracy 0.977.
### Flips: 156, rs: 36, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729206
Test loss (w/o reg) on all data: 0.012055082
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4360599e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729376
Test loss (w/o reg) on all data: 0.012055355
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6615434e-07
Norm of the params: 9.15323
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1408275
Train loss (w/o reg) on all data: 0.1309627
Test loss (w/o reg) on all data: 0.09971257
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.859529e-05
Norm of the params: 14.046219
              Random: fixed  15 labels. Loss 0.09971. Accuracy 0.973.
### Flips: 156, rs: 36, checks: 260
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729
Test loss (w/o reg) on all data: 0.012055994
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.2796337e-07
Norm of the params: 9.153271
     Influence (LOO): fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172902
Test loss (w/o reg) on all data: 0.012055846
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6342044e-07
Norm of the params: 9.153271
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13962549
Train loss (w/o reg) on all data: 0.13005406
Test loss (w/o reg) on all data: 0.09732533
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.4873116e-05
Norm of the params: 13.835774
              Random: fixed  17 labels. Loss 0.09733. Accuracy 0.977.
### Flips: 156, rs: 36, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729956
Test loss (w/o reg) on all data: 0.012054862
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0607048e-07
Norm of the params: 9.153166
     Influence (LOO): fixed  71 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.012055044
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.276657e-07
Norm of the params: 9.153182
                Loss: fixed  71 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13455312
Train loss (w/o reg) on all data: 0.12477354
Test loss (w/o reg) on all data: 0.09309427
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.8875743e-06
Norm of the params: 13.985407
              Random: fixed  20 labels. Loss 0.09309. Accuracy 0.981.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17251807
Train loss (w/o reg) on all data: 0.16254981
Test loss (w/o reg) on all data: 0.09009994
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.105208e-05
Norm of the params: 14.119675
Flipped loss: 0.09010. Accuracy: 0.989
### Flips: 156, rs: 37, checks: 52
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07670052
Train loss (w/o reg) on all data: 0.066416204
Test loss (w/o reg) on all data: 0.053877696
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.643342e-06
Norm of the params: 14.341769
     Influence (LOO): fixed  45 labels. Loss 0.05388. Accuracy 0.985.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04992449
Train loss (w/o reg) on all data: 0.035426147
Test loss (w/o reg) on all data: 0.03809444
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1087676e-05
Norm of the params: 17.028414
                Loss: fixed  51 labels. Loss 0.03809. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16633816
Train loss (w/o reg) on all data: 0.15587568
Test loss (w/o reg) on all data: 0.09472972
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8571227e-05
Norm of the params: 14.465465
              Random: fixed   4 labels. Loss 0.09473. Accuracy 0.985.
### Flips: 156, rs: 37, checks: 104
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03295247
Train loss (w/o reg) on all data: 0.02353595
Test loss (w/o reg) on all data: 0.029340217
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5336487e-06
Norm of the params: 13.7233515
     Influence (LOO): fixed  64 labels. Loss 0.02934. Accuracy 0.989.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007512572
Train loss (w/o reg) on all data: 0.0028915617
Test loss (w/o reg) on all data: 0.0064195595
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.0375804e-07
Norm of the params: 9.613543
                Loss: fixed  75 labels. Loss 0.00642. Accuracy 0.996.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16374832
Train loss (w/o reg) on all data: 0.15314022
Test loss (w/o reg) on all data: 0.09174011
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3700082e-05
Norm of the params: 14.565785
              Random: fixed   5 labels. Loss 0.09174. Accuracy 0.977.
### Flips: 156, rs: 37, checks: 156
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026764877
Train loss (w/o reg) on all data: 0.018330535
Test loss (w/o reg) on all data: 0.024746457
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2391449e-06
Norm of the params: 12.987951
     Influence (LOO): fixed  68 labels. Loss 0.02475. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002173014
Test loss (w/o reg) on all data: 0.012057266
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5085362e-07
Norm of the params: 9.153146
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16007584
Train loss (w/o reg) on all data: 0.14923254
Test loss (w/o reg) on all data: 0.09183851
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.6024736e-06
Norm of the params: 14.726376
              Random: fixed   7 labels. Loss 0.09184. Accuracy 0.981.
### Flips: 156, rs: 37, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729895
Test loss (w/o reg) on all data: 0.012054529
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4281464e-07
Norm of the params: 9.153174
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729863
Test loss (w/o reg) on all data: 0.012054863
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6292843e-07
Norm of the params: 9.153177
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1546712
Train loss (w/o reg) on all data: 0.14420882
Test loss (w/o reg) on all data: 0.088853054
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.039435e-05
Norm of the params: 14.465399
              Random: fixed  11 labels. Loss 0.08885. Accuracy 0.977.
### Flips: 156, rs: 37, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.012055076
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7646275e-07
Norm of the params: 9.153224
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729441
Test loss (w/o reg) on all data: 0.012055122
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.872785e-07
Norm of the params: 9.153224
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14908153
Train loss (w/o reg) on all data: 0.13856544
Test loss (w/o reg) on all data: 0.08357689
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.692211e-06
Norm of the params: 14.502481
              Random: fixed  14 labels. Loss 0.08358. Accuracy 0.977.
### Flips: 156, rs: 37, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172958
Test loss (w/o reg) on all data: 0.012055128
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7137295e-07
Norm of the params: 9.153207
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729604
Test loss (w/o reg) on all data: 0.0120550925
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.42035e-07
Norm of the params: 9.153207
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1398008
Train loss (w/o reg) on all data: 0.12967746
Test loss (w/o reg) on all data: 0.08233386
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1790375e-05
Norm of the params: 14.229084
              Random: fixed  19 labels. Loss 0.08233. Accuracy 0.977.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1946381
Train loss (w/o reg) on all data: 0.18921481
Test loss (w/o reg) on all data: 0.111251034
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1098369e-05
Norm of the params: 10.414695
Flipped loss: 0.11125. Accuracy: 0.981
### Flips: 156, rs: 38, checks: 52
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10469795
Train loss (w/o reg) on all data: 0.09513099
Test loss (w/o reg) on all data: 0.06427879
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.618997e-06
Norm of the params: 13.832542
     Influence (LOO): fixed  42 labels. Loss 0.06428. Accuracy 0.985.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061576124
Train loss (w/o reg) on all data: 0.050501183
Test loss (w/o reg) on all data: 0.05750298
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.3162975e-06
Norm of the params: 14.882837
                Loss: fixed  51 labels. Loss 0.05750. Accuracy 0.977.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18184257
Train loss (w/o reg) on all data: 0.1764281
Test loss (w/o reg) on all data: 0.10353473
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.190875e-05
Norm of the params: 10.406226
              Random: fixed   6 labels. Loss 0.10353. Accuracy 0.981.
### Flips: 156, rs: 38, checks: 104
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044674095
Train loss (w/o reg) on all data: 0.036788702
Test loss (w/o reg) on all data: 0.032964095
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0541816e-06
Norm of the params: 12.558179
     Influence (LOO): fixed  67 labels. Loss 0.03296. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0084424345
Train loss (w/o reg) on all data: 0.0031760337
Test loss (w/o reg) on all data: 0.016536575
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7905121e-07
Norm of the params: 10.262943
                Loss: fixed  80 labels. Loss 0.01654. Accuracy 0.996.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17793953
Train loss (w/o reg) on all data: 0.17233972
Test loss (w/o reg) on all data: 0.09980812
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0781677e-05
Norm of the params: 10.582826
              Random: fixed   8 labels. Loss 0.09981. Accuracy 0.977.
### Flips: 156, rs: 38, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011089494
Train loss (w/o reg) on all data: 0.005329003
Test loss (w/o reg) on all data: 0.015011609
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.173727e-06
Norm of the params: 10.733585
     Influence (LOO): fixed  80 labels. Loss 0.01501. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007355176
Train loss (w/o reg) on all data: 0.0026893264
Test loss (w/o reg) on all data: 0.015943388
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9193958e-07
Norm of the params: 9.660071
                Loss: fixed  81 labels. Loss 0.01594. Accuracy 0.996.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16845986
Train loss (w/o reg) on all data: 0.1626766
Test loss (w/o reg) on all data: 0.09563743
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5855641e-05
Norm of the params: 10.754783
              Random: fixed  13 labels. Loss 0.09564. Accuracy 0.985.
### Flips: 156, rs: 38, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728459
Test loss (w/o reg) on all data: 0.012055068
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2428633e-07
Norm of the params: 9.153331
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007383374
Train loss (w/o reg) on all data: 0.0026668436
Test loss (w/o reg) on all data: 0.021342691
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6693096e-07
Norm of the params: 9.712395
                Loss: fixed  82 labels. Loss 0.02134. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1615393
Train loss (w/o reg) on all data: 0.15570915
Test loss (w/o reg) on all data: 0.091154546
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2748367e-05
Norm of the params: 10.798287
              Random: fixed  18 labels. Loss 0.09115. Accuracy 0.985.
### Flips: 156, rs: 38, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173044
Test loss (w/o reg) on all data: 0.012055258
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0183847e-07
Norm of the params: 9.153113
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730382
Test loss (w/o reg) on all data: 0.012055147
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.287657e-07
Norm of the params: 9.153121
                Loss: fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158311
Train loss (w/o reg) on all data: 0.15259893
Test loss (w/o reg) on all data: 0.087470844
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.410529e-05
Norm of the params: 10.688375
              Random: fixed  21 labels. Loss 0.08747. Accuracy 0.977.
### Flips: 156, rs: 38, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730629
Test loss (w/o reg) on all data: 0.0120560825
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.088406e-07
Norm of the params: 9.153091
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173058
Test loss (w/o reg) on all data: 0.012055928
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4984682e-07
Norm of the params: 9.153097
                Loss: fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14145093
Train loss (w/o reg) on all data: 0.13556165
Test loss (w/o reg) on all data: 0.080180705
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2640054e-05
Norm of the params: 10.852911
              Random: fixed  29 labels. Loss 0.08018. Accuracy 0.973.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17149802
Train loss (w/o reg) on all data: 0.16322419
Test loss (w/o reg) on all data: 0.13367751
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.940082e-05
Norm of the params: 12.863773
Flipped loss: 0.13368. Accuracy: 0.977
### Flips: 156, rs: 39, checks: 52
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07214415
Train loss (w/o reg) on all data: 0.06157951
Test loss (w/o reg) on all data: 0.076115705
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.294333e-06
Norm of the params: 14.535916
     Influence (LOO): fixed  40 labels. Loss 0.07612. Accuracy 0.973.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04501847
Train loss (w/o reg) on all data: 0.032620553
Test loss (w/o reg) on all data: 0.06576802
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6443527e-06
Norm of the params: 15.746695
                Loss: fixed  49 labels. Loss 0.06577. Accuracy 0.977.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16490446
Train loss (w/o reg) on all data: 0.15641254
Test loss (w/o reg) on all data: 0.12846711
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.8563247e-05
Norm of the params: 13.032201
              Random: fixed   3 labels. Loss 0.12847. Accuracy 0.969.
### Flips: 156, rs: 39, checks: 104
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033858284
Train loss (w/o reg) on all data: 0.02637763
Test loss (w/o reg) on all data: 0.02780337
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2386422e-06
Norm of the params: 12.231642
     Influence (LOO): fixed  63 labels. Loss 0.02780. Accuracy 0.992.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012166401
Train loss (w/o reg) on all data: 0.0053448454
Test loss (w/o reg) on all data: 0.018400868
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.1617984e-07
Norm of the params: 11.680374
                Loss: fixed  71 labels. Loss 0.01840. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158449
Train loss (w/o reg) on all data: 0.15001385
Test loss (w/o reg) on all data: 0.11982603
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4373402e-05
Norm of the params: 12.988564
              Random: fixed   7 labels. Loss 0.11983. Accuracy 0.985.
### Flips: 156, rs: 39, checks: 156
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012227301
Train loss (w/o reg) on all data: 0.0065066316
Test loss (w/o reg) on all data: 0.015425256
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0838903e-07
Norm of the params: 10.696419
     Influence (LOO): fixed  72 labels. Loss 0.01543. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008664978
Train loss (w/o reg) on all data: 0.003318245
Test loss (w/o reg) on all data: 0.013527659
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.640572e-07
Norm of the params: 10.340922
                Loss: fixed  73 labels. Loss 0.01353. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14895646
Train loss (w/o reg) on all data: 0.14037432
Test loss (w/o reg) on all data: 0.113886036
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1474583e-05
Norm of the params: 13.101259
              Random: fixed  12 labels. Loss 0.11389. Accuracy 0.985.
### Flips: 156, rs: 39, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172891
Test loss (w/o reg) on all data: 0.012054289
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.725287e-07
Norm of the params: 9.153281
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172894
Test loss (w/o reg) on all data: 0.012054358
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0768155e-07
Norm of the params: 9.153279
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14283767
Train loss (w/o reg) on all data: 0.13424969
Test loss (w/o reg) on all data: 0.10523305
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.9596056e-06
Norm of the params: 13.105718
              Random: fixed  16 labels. Loss 0.10523. Accuracy 0.985.
### Flips: 156, rs: 39, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729993
Test loss (w/o reg) on all data: 0.012054885
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9280205e-06
Norm of the params: 9.153163
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172994
Test loss (w/o reg) on all data: 0.012055146
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9008848e-07
Norm of the params: 9.153167
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1428377
Train loss (w/o reg) on all data: 0.13424873
Test loss (w/o reg) on all data: 0.105228305
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6923437e-05
Norm of the params: 13.106467
              Random: fixed  16 labels. Loss 0.10523. Accuracy 0.985.
### Flips: 156, rs: 39, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730189
Test loss (w/o reg) on all data: 0.012055071
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.128793e-07
Norm of the params: 9.153141
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730184
Test loss (w/o reg) on all data: 0.0120549835
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5253252e-07
Norm of the params: 9.153141
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13410342
Train loss (w/o reg) on all data: 0.12554048
Test loss (w/o reg) on all data: 0.10277539
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.100617e-06
Norm of the params: 13.086582
              Random: fixed  19 labels. Loss 0.10278. Accuracy 0.985.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21264197
Train loss (w/o reg) on all data: 0.20325857
Test loss (w/o reg) on all data: 0.14673056
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.8915995e-05
Norm of the params: 13.699192
Flipped loss: 0.14673. Accuracy: 0.950
### Flips: 208, rs: 0, checks: 52
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1431922
Train loss (w/o reg) on all data: 0.1315723
Test loss (w/o reg) on all data: 0.093113884
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7726681e-05
Norm of the params: 15.2446
     Influence (LOO): fixed  36 labels. Loss 0.09311. Accuracy 0.977.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09768614
Train loss (w/o reg) on all data: 0.08029327
Test loss (w/o reg) on all data: 0.098362565
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.383767e-06
Norm of the params: 18.65094
                Loss: fixed  49 labels. Loss 0.09836. Accuracy 0.966.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21176377
Train loss (w/o reg) on all data: 0.20235415
Test loss (w/o reg) on all data: 0.14432774
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.5418322e-05
Norm of the params: 13.718326
              Random: fixed   1 labels. Loss 0.14433. Accuracy 0.954.
### Flips: 208, rs: 0, checks: 104
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091498174
Train loss (w/o reg) on all data: 0.08134279
Test loss (w/o reg) on all data: 0.051644724
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.466878e-06
Norm of the params: 14.25159
     Influence (LOO): fixed  68 labels. Loss 0.05164. Accuracy 0.989.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033458017
Train loss (w/o reg) on all data: 0.022161536
Test loss (w/o reg) on all data: 0.011893702
Train acc on all data:  0.9933142311365807
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3837619e-06
Norm of the params: 15.030954
                Loss: fixed  89 labels. Loss 0.01189. Accuracy 1.000.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21123108
Train loss (w/o reg) on all data: 0.20187593
Test loss (w/o reg) on all data: 0.14225546
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.42212275e-05
Norm of the params: 13.678564
              Random: fixed   2 labels. Loss 0.14226. Accuracy 0.954.
### Flips: 208, rs: 0, checks: 156
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03937023
Train loss (w/o reg) on all data: 0.030735813
Test loss (w/o reg) on all data: 0.020442577
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6132955e-05
Norm of the params: 13.141096
     Influence (LOO): fixed  93 labels. Loss 0.02044. Accuracy 0.996.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016636666
Train loss (w/o reg) on all data: 0.0077781132
Test loss (w/o reg) on all data: 0.0121737365
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.277204e-06
Norm of the params: 13.310561
                Loss: fixed  99 labels. Loss 0.01217. Accuracy 0.996.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20479384
Train loss (w/o reg) on all data: 0.19587757
Test loss (w/o reg) on all data: 0.1288605
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.7280268e-05
Norm of the params: 13.353854
              Random: fixed   8 labels. Loss 0.12886. Accuracy 0.962.
### Flips: 208, rs: 0, checks: 208
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016058002
Train loss (w/o reg) on all data: 0.00982736
Test loss (w/o reg) on all data: 0.020568676
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8258942e-06
Norm of the params: 11.163012
     Influence (LOO): fixed 104 labels. Loss 0.02057. Accuracy 0.989.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008249325
Train loss (w/o reg) on all data: 0.0031787783
Test loss (w/o reg) on all data: 0.011616045
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.995727e-07
Norm of the params: 10.070299
                Loss: fixed 105 labels. Loss 0.01162. Accuracy 0.992.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20194481
Train loss (w/o reg) on all data: 0.19310157
Test loss (w/o reg) on all data: 0.12375089
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.1874402e-05
Norm of the params: 13.29905
              Random: fixed  14 labels. Loss 0.12375. Accuracy 0.969.
### Flips: 208, rs: 0, checks: 260
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002352
Train loss (w/o reg) on all data: 0.005447779
Test loss (w/o reg) on all data: 0.012244708
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.444756e-06
Norm of the params: 9.544187
     Influence (LOO): fixed 106 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075563444
Train loss (w/o reg) on all data: 0.0027567912
Test loss (w/o reg) on all data: 0.010964681
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6437882e-07
Norm of the params: 9.797503
                Loss: fixed 106 labels. Loss 0.01096. Accuracy 0.992.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19546819
Train loss (w/o reg) on all data: 0.18631142
Test loss (w/o reg) on all data: 0.11495035
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.4813536e-05
Norm of the params: 13.532754
              Random: fixed  18 labels. Loss 0.11495. Accuracy 0.973.
### Flips: 208, rs: 0, checks: 312
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002355
Train loss (w/o reg) on all data: 0.0054477593
Test loss (w/o reg) on all data: 0.012245602
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4858672e-06
Norm of the params: 9.5442095
     Influence (LOO): fixed 106 labels. Loss 0.01225. Accuracy 0.996.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729523
Test loss (w/o reg) on all data: 0.0120550785
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.9060706e-08
Norm of the params: 9.153213
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18903463
Train loss (w/o reg) on all data: 0.18039352
Test loss (w/o reg) on all data: 0.109716296
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.40986795e-05
Norm of the params: 13.146184
              Random: fixed  24 labels. Loss 0.10972. Accuracy 0.973.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20785025
Train loss (w/o reg) on all data: 0.19937964
Test loss (w/o reg) on all data: 0.14080699
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.289899e-05
Norm of the params: 13.015839
Flipped loss: 0.14081. Accuracy: 0.977
### Flips: 208, rs: 1, checks: 52
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14258869
Train loss (w/o reg) on all data: 0.13138169
Test loss (w/o reg) on all data: 0.08987306
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.379024e-05
Norm of the params: 14.971309
     Influence (LOO): fixed  36 labels. Loss 0.08987. Accuracy 0.981.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08795329
Train loss (w/o reg) on all data: 0.07124671
Test loss (w/o reg) on all data: 0.08822745
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.0416957e-06
Norm of the params: 18.279266
                Loss: fixed  52 labels. Loss 0.08823. Accuracy 0.977.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20365153
Train loss (w/o reg) on all data: 0.19507131
Test loss (w/o reg) on all data: 0.1420026
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3987595e-05
Norm of the params: 13.0997925
              Random: fixed   2 labels. Loss 0.14200. Accuracy 0.977.
### Flips: 208, rs: 1, checks: 104
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08200254
Train loss (w/o reg) on all data: 0.07099611
Test loss (w/o reg) on all data: 0.047960054
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.164539e-06
Norm of the params: 14.83673
     Influence (LOO): fixed  65 labels. Loss 0.04796. Accuracy 0.992.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017485207
Train loss (w/o reg) on all data: 0.008543309
Test loss (w/o reg) on all data: 0.027258577
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.006052e-06
Norm of the params: 13.373032
                Loss: fixed  87 labels. Loss 0.02726. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19764441
Train loss (w/o reg) on all data: 0.18875997
Test loss (w/o reg) on all data: 0.13977261
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0138384e-05
Norm of the params: 13.329997
              Random: fixed   5 labels. Loss 0.13977. Accuracy 0.969.
### Flips: 208, rs: 1, checks: 156
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019217432
Train loss (w/o reg) on all data: 0.012848004
Test loss (w/o reg) on all data: 0.018717432
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2297954e-07
Norm of the params: 11.286655
     Influence (LOO): fixed  92 labels. Loss 0.01872. Accuracy 0.992.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010791396
Train loss (w/o reg) on all data: 0.0045766085
Test loss (w/o reg) on all data: 0.0146125145
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.562592e-07
Norm of the params: 11.1488
                Loss: fixed  93 labels. Loss 0.01461. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18663327
Train loss (w/o reg) on all data: 0.17725663
Test loss (w/o reg) on all data: 0.12452446
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6274318e-05
Norm of the params: 13.694265
              Random: fixed  12 labels. Loss 0.12452. Accuracy 0.981.
### Flips: 208, rs: 1, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729912
Test loss (w/o reg) on all data: 0.012054893
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.464789e-07
Norm of the params: 9.1531725
     Influence (LOO): fixed  96 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729895
Test loss (w/o reg) on all data: 0.012054982
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4751764e-07
Norm of the params: 9.153173
                Loss: fixed  96 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18044946
Train loss (w/o reg) on all data: 0.17172818
Test loss (w/o reg) on all data: 0.11367857
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.36696935e-05
Norm of the params: 13.207028
              Random: fixed  19 labels. Loss 0.11368. Accuracy 0.981.
### Flips: 208, rs: 1, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172819
Test loss (w/o reg) on all data: 0.012055263
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4436118e-07
Norm of the params: 9.153358
     Influence (LOO): fixed  96 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [3] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729087
Test loss (w/o reg) on all data: 0.01205452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1566623e-07
Norm of the params: 9.153263
                Loss: fixed  96 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1722527
Train loss (w/o reg) on all data: 0.16303779
Test loss (w/o reg) on all data: 0.10914944
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.9000466e-06
Norm of the params: 13.575648
              Random: fixed  24 labels. Loss 0.10915. Accuracy 0.985.
### Flips: 208, rs: 1, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730259
Test loss (w/o reg) on all data: 0.012055037
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7171342e-07
Norm of the params: 9.153133
     Influence (LOO): fixed  96 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730252
Test loss (w/o reg) on all data: 0.0120550785
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4822842e-07
Norm of the params: 9.153134
                Loss: fixed  96 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1631544
Train loss (w/o reg) on all data: 0.15404901
Test loss (w/o reg) on all data: 0.1025773
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6597685e-05
Norm of the params: 13.494725
              Random: fixed  30 labels. Loss 0.10258. Accuracy 0.981.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21802008
Train loss (w/o reg) on all data: 0.21010917
Test loss (w/o reg) on all data: 0.13257472
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.3543284e-05
Norm of the params: 12.578485
Flipped loss: 0.13257. Accuracy: 0.985
### Flips: 208, rs: 2, checks: 52
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12704262
Train loss (w/o reg) on all data: 0.11659766
Test loss (w/o reg) on all data: 0.086104676
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.7249677e-06
Norm of the params: 14.453348
     Influence (LOO): fixed  41 labels. Loss 0.08610. Accuracy 0.977.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08142199
Train loss (w/o reg) on all data: 0.06591794
Test loss (w/o reg) on all data: 0.08249948
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.1112217e-06
Norm of the params: 17.609118
                Loss: fixed  52 labels. Loss 0.08250. Accuracy 0.966.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20327787
Train loss (w/o reg) on all data: 0.19530892
Test loss (w/o reg) on all data: 0.116566345
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.8221428e-05
Norm of the params: 12.62454
              Random: fixed   9 labels. Loss 0.11657. Accuracy 0.985.
### Flips: 208, rs: 2, checks: 104
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07923558
Train loss (w/o reg) on all data: 0.0697151
Test loss (w/o reg) on all data: 0.050500363
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4638369e-05
Norm of the params: 13.798905
     Influence (LOO): fixed  66 labels. Loss 0.05050. Accuracy 0.989.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018464357
Train loss (w/o reg) on all data: 0.009364047
Test loss (w/o reg) on all data: 0.028074648
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.287235e-07
Norm of the params: 13.490967
                Loss: fixed  90 labels. Loss 0.02807. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20005661
Train loss (w/o reg) on all data: 0.1919525
Test loss (w/o reg) on all data: 0.11385073
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.1195348e-05
Norm of the params: 12.731158
              Random: fixed  11 labels. Loss 0.11385. Accuracy 0.981.
### Flips: 208, rs: 2, checks: 156
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024758874
Train loss (w/o reg) on all data: 0.015386703
Test loss (w/o reg) on all data: 0.026225835
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8702525e-06
Norm of the params: 13.690997
     Influence (LOO): fixed  86 labels. Loss 0.02623. Accuracy 0.989.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011919516
Train loss (w/o reg) on all data: 0.0049141315
Test loss (w/o reg) on all data: 0.019332992
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.977653e-07
Norm of the params: 11.836709
                Loss: fixed  94 labels. Loss 0.01933. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19740255
Train loss (w/o reg) on all data: 0.18921404
Test loss (w/o reg) on all data: 0.11350224
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.3426942e-05
Norm of the params: 12.7972765
              Random: fixed  12 labels. Loss 0.11350. Accuracy 0.981.
### Flips: 208, rs: 2, checks: 208
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012758769
Train loss (w/o reg) on all data: 0.007435989
Test loss (w/o reg) on all data: 0.013298231
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7444647e-06
Norm of the params: 10.317732
     Influence (LOO): fixed  96 labels. Loss 0.01330. Accuracy 0.996.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010259982
Train loss (w/o reg) on all data: 0.0041823275
Test loss (w/o reg) on all data: 0.017418772
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.212575e-07
Norm of the params: 11.025112
                Loss: fixed  95 labels. Loss 0.01742. Accuracy 0.989.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1908236
Train loss (w/o reg) on all data: 0.18286745
Test loss (w/o reg) on all data: 0.10577055
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.318931e-05
Norm of the params: 12.614394
              Random: fixed  17 labels. Loss 0.10577. Accuracy 0.985.
### Flips: 208, rs: 2, checks: 260
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729993
Test loss (w/o reg) on all data: 0.012055418
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9427782e-07
Norm of the params: 9.153163
     Influence (LOO): fixed  97 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729984
Test loss (w/o reg) on all data: 0.0120554725
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4791833e-07
Norm of the params: 9.153164
                Loss: fixed  97 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1758232
Train loss (w/o reg) on all data: 0.16714081
Test loss (w/o reg) on all data: 0.09600316
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.2294887e-05
Norm of the params: 13.177549
              Random: fixed  24 labels. Loss 0.09600. Accuracy 0.989.
### Flips: 208, rs: 2, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172914
Test loss (w/o reg) on all data: 0.012055056
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2783875e-07
Norm of the params: 9.153257
     Influence (LOO): fixed  97 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729157
Test loss (w/o reg) on all data: 0.012055006
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0478301e-07
Norm of the params: 9.153255
                Loss: fixed  97 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16791913
Train loss (w/o reg) on all data: 0.15889809
Test loss (w/o reg) on all data: 0.09674187
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.9675702e-05
Norm of the params: 13.432089
              Random: fixed  28 labels. Loss 0.09674. Accuracy 0.989.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2310629
Train loss (w/o reg) on all data: 0.22488093
Test loss (w/o reg) on all data: 0.14754535
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.8447634e-05
Norm of the params: 11.119323
Flipped loss: 0.14755. Accuracy: 0.969
### Flips: 208, rs: 3, checks: 52
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15653943
Train loss (w/o reg) on all data: 0.14744475
Test loss (w/o reg) on all data: 0.11201484
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.810093e-06
Norm of the params: 13.486789
     Influence (LOO): fixed  39 labels. Loss 0.11201. Accuracy 0.973.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11220103
Train loss (w/o reg) on all data: 0.10019676
Test loss (w/o reg) on all data: 0.097924665
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.8989675e-06
Norm of the params: 15.494689
                Loss: fixed  52 labels. Loss 0.09792. Accuracy 0.962.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21926372
Train loss (w/o reg) on all data: 0.21246348
Test loss (w/o reg) on all data: 0.13832544
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.0703482e-05
Norm of the params: 11.6621
              Random: fixed   6 labels. Loss 0.13833. Accuracy 0.981.
### Flips: 208, rs: 3, checks: 104
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10590197
Train loss (w/o reg) on all data: 0.097075514
Test loss (w/o reg) on all data: 0.08377497
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.2923654e-06
Norm of the params: 13.286429
     Influence (LOO): fixed  65 labels. Loss 0.08377. Accuracy 0.985.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019273484
Train loss (w/o reg) on all data: 0.010064212
Test loss (w/o reg) on all data: 0.015376118
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.659933e-07
Norm of the params: 13.571494
                Loss: fixed  98 labels. Loss 0.01538. Accuracy 0.996.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21183458
Train loss (w/o reg) on all data: 0.20476304
Test loss (w/o reg) on all data: 0.12676916
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.9997412e-05
Norm of the params: 11.89247
              Random: fixed  13 labels. Loss 0.12677. Accuracy 0.981.
### Flips: 208, rs: 3, checks: 156
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055807073
Train loss (w/o reg) on all data: 0.0480234
Test loss (w/o reg) on all data: 0.049896933
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1564926e-05
Norm of the params: 12.476918
     Influence (LOO): fixed  89 labels. Loss 0.04990. Accuracy 0.989.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.012055275
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4041909e-07
Norm of the params: 9.153191
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20675813
Train loss (w/o reg) on all data: 0.19951297
Test loss (w/o reg) on all data: 0.12478116
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9494535e-05
Norm of the params: 12.037572
              Random: fixed  16 labels. Loss 0.12478. Accuracy 0.977.
### Flips: 208, rs: 3, checks: 208
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022821672
Train loss (w/o reg) on all data: 0.016790656
Test loss (w/o reg) on all data: 0.02517417
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0754586e-06
Norm of the params: 10.982729
     Influence (LOO): fixed 102 labels. Loss 0.02517. Accuracy 0.996.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172974
Test loss (w/o reg) on all data: 0.012055473
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2875412e-07
Norm of the params: 9.153191
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20211695
Train loss (w/o reg) on all data: 0.19475515
Test loss (w/o reg) on all data: 0.11712864
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.6845766e-05
Norm of the params: 12.134085
              Random: fixed  20 labels. Loss 0.11713. Accuracy 0.985.
### Flips: 208, rs: 3, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729397
Test loss (w/o reg) on all data: 0.012055071
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.315485e-08
Norm of the params: 9.153229
     Influence (LOO): fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729455
Test loss (w/o reg) on all data: 0.012055147
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9374425e-07
Norm of the params: 9.153222
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18956818
Train loss (w/o reg) on all data: 0.18226752
Test loss (w/o reg) on all data: 0.1152156
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.520493e-05
Norm of the params: 12.0835905
              Random: fixed  28 labels. Loss 0.11522. Accuracy 0.981.
### Flips: 208, rs: 3, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729502
Test loss (w/o reg) on all data: 0.012055595
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.8774027e-07
Norm of the params: 9.153217
     Influence (LOO): fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729495
Test loss (w/o reg) on all data: 0.012055435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5754802e-07
Norm of the params: 9.153217
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18176399
Train loss (w/o reg) on all data: 0.17428117
Test loss (w/o reg) on all data: 0.10898956
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.4123325e-06
Norm of the params: 12.2334175
              Random: fixed  33 labels. Loss 0.10899. Accuracy 0.985.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21750441
Train loss (w/o reg) on all data: 0.21018867
Test loss (w/o reg) on all data: 0.16008194
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.9153436e-05
Norm of the params: 12.096061
Flipped loss: 0.16008. Accuracy: 0.966
### Flips: 208, rs: 4, checks: 52
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14196733
Train loss (w/o reg) on all data: 0.13223384
Test loss (w/o reg) on all data: 0.11142003
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.2660516e-05
Norm of the params: 13.952406
     Influence (LOO): fixed  41 labels. Loss 0.11142. Accuracy 0.958.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09736991
Train loss (w/o reg) on all data: 0.081205264
Test loss (w/o reg) on all data: 0.11316757
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.9176753e-05
Norm of the params: 17.980349
                Loss: fixed  52 labels. Loss 0.11317. Accuracy 0.958.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2063641
Train loss (w/o reg) on all data: 0.19918555
Test loss (w/o reg) on all data: 0.14122236
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.0255636e-05
Norm of the params: 11.98211
              Random: fixed   9 labels. Loss 0.14122. Accuracy 0.973.
### Flips: 208, rs: 4, checks: 104
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082804136
Train loss (w/o reg) on all data: 0.071994215
Test loss (w/o reg) on all data: 0.07616969
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.1906523e-06
Norm of the params: 14.703687
     Influence (LOO): fixed  70 labels. Loss 0.07617. Accuracy 0.973.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018135918
Train loss (w/o reg) on all data: 0.009243159
Test loss (w/o reg) on all data: 0.02365667
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0066748e-06
Norm of the params: 13.336237
                Loss: fixed  95 labels. Loss 0.02366. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.192709
Train loss (w/o reg) on all data: 0.18521494
Test loss (w/o reg) on all data: 0.13089238
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1116904e-05
Norm of the params: 12.2426
              Random: fixed  17 labels. Loss 0.13089. Accuracy 0.981.
### Flips: 208, rs: 4, checks: 156
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049021166
Train loss (w/o reg) on all data: 0.038759895
Test loss (w/o reg) on all data: 0.057028692
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5293299e-05
Norm of the params: 14.325691
     Influence (LOO): fixed  86 labels. Loss 0.05703. Accuracy 0.981.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011473969
Train loss (w/o reg) on all data: 0.0048044776
Test loss (w/o reg) on all data: 0.019485854
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0578203e-06
Norm of the params: 11.549451
                Loss: fixed  99 labels. Loss 0.01949. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18642798
Train loss (w/o reg) on all data: 0.17871249
Test loss (w/o reg) on all data: 0.11678926
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5752874e-05
Norm of the params: 12.422151
              Random: fixed  23 labels. Loss 0.11679. Accuracy 0.985.
### Flips: 208, rs: 4, checks: 208
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02386277
Train loss (w/o reg) on all data: 0.01679155
Test loss (w/o reg) on all data: 0.023912495
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.940096e-06
Norm of the params: 11.892199
     Influence (LOO): fixed  96 labels. Loss 0.02391. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056525
Train loss (w/o reg) on all data: 0.0028170042
Test loss (w/o reg) on all data: 0.012755332
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.682282e-07
Norm of the params: 9.683644
                Loss: fixed 102 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18620503
Train loss (w/o reg) on all data: 0.17857943
Test loss (w/o reg) on all data: 0.11567253
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.392947e-05
Norm of the params: 12.349567
              Random: fixed  24 labels. Loss 0.11567. Accuracy 0.985.
### Flips: 208, rs: 4, checks: 260
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016447362
Train loss (w/o reg) on all data: 0.009915805
Test loss (w/o reg) on all data: 0.014801301
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.0086015e-06
Norm of the params: 11.429398
     Influence (LOO): fixed  99 labels. Loss 0.01480. Accuracy 0.996.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217307
Test loss (w/o reg) on all data: 0.01205483
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.203491e-07
Norm of the params: 9.153085
                Loss: fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17871231
Train loss (w/o reg) on all data: 0.171487
Test loss (w/o reg) on all data: 0.10874856
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5039586e-05
Norm of the params: 12.021073
              Random: fixed  30 labels. Loss 0.10875. Accuracy 0.985.
### Flips: 208, rs: 4, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730058
Test loss (w/o reg) on all data: 0.012054509
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8989347e-07
Norm of the params: 9.153153
     Influence (LOO): fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730082
Test loss (w/o reg) on all data: 0.012054591
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.497482e-07
Norm of the params: 9.153154
                Loss: fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1699796
Train loss (w/o reg) on all data: 0.162587
Test loss (w/o reg) on all data: 0.104569085
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4963156e-05
Norm of the params: 12.159443
              Random: fixed  35 labels. Loss 0.10457. Accuracy 0.989.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2015262
Train loss (w/o reg) on all data: 0.19486114
Test loss (w/o reg) on all data: 0.109110326
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6065109e-05
Norm of the params: 11.545607
Flipped loss: 0.10911. Accuracy: 0.985
### Flips: 208, rs: 5, checks: 52
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1304557
Train loss (w/o reg) on all data: 0.12172462
Test loss (w/o reg) on all data: 0.075458795
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.684532e-06
Norm of the params: 13.214454
     Influence (LOO): fixed  38 labels. Loss 0.07546. Accuracy 0.985.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075164616
Train loss (w/o reg) on all data: 0.059712704
Test loss (w/o reg) on all data: 0.049947884
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7141658e-06
Norm of the params: 17.579481
                Loss: fixed  51 labels. Loss 0.04995. Accuracy 0.985.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19517797
Train loss (w/o reg) on all data: 0.18835677
Test loss (w/o reg) on all data: 0.103335366
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.177183e-05
Norm of the params: 11.680069
              Random: fixed   7 labels. Loss 0.10334. Accuracy 0.992.
### Flips: 208, rs: 5, checks: 104
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06377404
Train loss (w/o reg) on all data: 0.052681938
Test loss (w/o reg) on all data: 0.045703363
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.817681e-06
Norm of the params: 14.894364
     Influence (LOO): fixed  68 labels. Loss 0.04570. Accuracy 0.981.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022224516
Train loss (w/o reg) on all data: 0.011920706
Test loss (w/o reg) on all data: 0.013926138
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9496645e-06
Norm of the params: 14.355355
                Loss: fixed  84 labels. Loss 0.01393. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18864192
Train loss (w/o reg) on all data: 0.18144761
Test loss (w/o reg) on all data: 0.09652183
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.077276e-05
Norm of the params: 11.995253
              Random: fixed  12 labels. Loss 0.09652. Accuracy 0.989.
### Flips: 208, rs: 5, checks: 156
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027780198
Train loss (w/o reg) on all data: 0.019402782
Test loss (w/o reg) on all data: 0.016658625
Train acc on all data:  0.994269340974212
Test acc on all data:   1.0
Norm of the mean of gradients: 7.047901e-06
Norm of the params: 12.944045
     Influence (LOO): fixed  85 labels. Loss 0.01666. Accuracy 1.000.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009835265
Train loss (w/o reg) on all data: 0.0038954061
Test loss (w/o reg) on all data: 0.0053872317
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8361478e-07
Norm of the params: 10.899411
                Loss: fixed  91 labels. Loss 0.00539. Accuracy 1.000.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17061178
Train loss (w/o reg) on all data: 0.1624066
Test loss (w/o reg) on all data: 0.08573453
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7298432e-05
Norm of the params: 12.810303
              Random: fixed  21 labels. Loss 0.08573. Accuracy 0.992.
### Flips: 208, rs: 5, checks: 208
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01611639
Train loss (w/o reg) on all data: 0.010058089
Test loss (w/o reg) on all data: 0.0074706296
Train acc on all data:  0.997134670487106
Test acc on all data:   1.0
Norm of the mean of gradients: 4.7768845e-07
Norm of the params: 11.0075445
     Influence (LOO): fixed  90 labels. Loss 0.00747. Accuracy 1.000.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006318258
Train loss (w/o reg) on all data: 0.0021985888
Test loss (w/o reg) on all data: 0.006434188
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4199187e-07
Norm of the params: 9.07708
                Loss: fixed  93 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16401662
Train loss (w/o reg) on all data: 0.15547293
Test loss (w/o reg) on all data: 0.081790335
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.909114e-05
Norm of the params: 13.071864
              Random: fixed  27 labels. Loss 0.08179. Accuracy 0.992.
### Flips: 208, rs: 5, checks: 260
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002355
Train loss (w/o reg) on all data: 0.0054477435
Test loss (w/o reg) on all data: 0.012244687
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4322679e-06
Norm of the params: 9.544225
     Influence (LOO): fixed  93 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00631826
Train loss (w/o reg) on all data: 0.0021986386
Test loss (w/o reg) on all data: 0.006433212
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2723055e-07
Norm of the params: 9.077027
                Loss: fixed  93 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15803751
Train loss (w/o reg) on all data: 0.14954312
Test loss (w/o reg) on all data: 0.07451755
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.4669592e-06
Norm of the params: 13.034103
              Random: fixed  30 labels. Loss 0.07452. Accuracy 0.989.
### Flips: 208, rs: 5, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729867
Test loss (w/o reg) on all data: 0.012055199
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3172215e-07
Norm of the params: 9.153176
     Influence (LOO): fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063182563
Train loss (w/o reg) on all data: 0.0021987294
Test loss (w/o reg) on all data: 0.0064326376
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2136896e-07
Norm of the params: 9.076923
                Loss: fixed  93 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1462365
Train loss (w/o reg) on all data: 0.13735686
Test loss (w/o reg) on all data: 0.06863999
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.361932e-06
Norm of the params: 13.3263855
              Random: fixed  36 labels. Loss 0.06864. Accuracy 0.996.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20796888
Train loss (w/o reg) on all data: 0.20053332
Test loss (w/o reg) on all data: 0.14621405
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.0656287e-05
Norm of the params: 12.19472
Flipped loss: 0.14621. Accuracy: 0.962
### Flips: 208, rs: 6, checks: 52
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13716614
Train loss (w/o reg) on all data: 0.12925489
Test loss (w/o reg) on all data: 0.10331552
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.193095e-05
Norm of the params: 12.578757
     Influence (LOO): fixed  36 labels. Loss 0.10332. Accuracy 0.966.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08016885
Train loss (w/o reg) on all data: 0.065389715
Test loss (w/o reg) on all data: 0.084409446
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.553696e-06
Norm of the params: 17.19252
                Loss: fixed  52 labels. Loss 0.08441. Accuracy 0.966.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20103404
Train loss (w/o reg) on all data: 0.1937253
Test loss (w/o reg) on all data: 0.1336883
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.585494e-05
Norm of the params: 12.090277
              Random: fixed   5 labels. Loss 0.13369. Accuracy 0.966.
### Flips: 208, rs: 6, checks: 104
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0805936
Train loss (w/o reg) on all data: 0.07184111
Test loss (w/o reg) on all data: 0.06818895
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.9986733e-06
Norm of the params: 13.230639
     Influence (LOO): fixed  63 labels. Loss 0.06819. Accuracy 0.977.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022595692
Train loss (w/o reg) on all data: 0.0135057
Test loss (w/o reg) on all data: 0.024851933
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.261419e-06
Norm of the params: 13.483317
                Loss: fixed  88 labels. Loss 0.02485. Accuracy 0.989.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19451526
Train loss (w/o reg) on all data: 0.187217
Test loss (w/o reg) on all data: 0.1213162
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.4544457e-05
Norm of the params: 12.081604
              Random: fixed  12 labels. Loss 0.12132. Accuracy 0.966.
### Flips: 208, rs: 6, checks: 156
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039281074
Train loss (w/o reg) on all data: 0.031074155
Test loss (w/o reg) on all data: 0.048038907
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.11998015e-05
Norm of the params: 12.81165
     Influence (LOO): fixed  85 labels. Loss 0.04804. Accuracy 0.985.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007505651
Train loss (w/o reg) on all data: 0.0028169393
Test loss (w/o reg) on all data: 0.012755226
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1543062e-07
Norm of the params: 9.683709
                Loss: fixed  97 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18566431
Train loss (w/o reg) on all data: 0.17823377
Test loss (w/o reg) on all data: 0.113682166
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2252027e-05
Norm of the params: 12.190609
              Random: fixed  20 labels. Loss 0.11368. Accuracy 0.962.
### Flips: 208, rs: 6, checks: 208
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009377585
Train loss (w/o reg) on all data: 0.003948222
Test loss (w/o reg) on all data: 0.016018325
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.4572343e-07
Norm of the params: 10.420521
     Influence (LOO): fixed  96 labels. Loss 0.01602. Accuracy 0.996.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056534
Train loss (w/o reg) on all data: 0.0028169828
Test loss (w/o reg) on all data: 0.012755777
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.989106e-06
Norm of the params: 9.683666
                Loss: fixed  97 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17947964
Train loss (w/o reg) on all data: 0.17222665
Test loss (w/o reg) on all data: 0.105866514
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.519069e-06
Norm of the params: 12.044073
              Random: fixed  25 labels. Loss 0.10587. Accuracy 0.973.
### Flips: 208, rs: 6, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728743
Test loss (w/o reg) on all data: 0.012055364
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9584529e-07
Norm of the params: 9.1533
     Influence (LOO): fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728808
Test loss (w/o reg) on all data: 0.012055379
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.055174e-07
Norm of the params: 9.15329
                Loss: fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16441481
Train loss (w/o reg) on all data: 0.1562684
Test loss (w/o reg) on all data: 0.094930604
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3970444e-05
Norm of the params: 12.764332
              Random: fixed  32 labels. Loss 0.09493. Accuracy 0.973.
### Flips: 208, rs: 6, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730103
Test loss (w/o reg) on all data: 0.012054408
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7579225e-07
Norm of the params: 9.153151
     Influence (LOO): fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730096
Test loss (w/o reg) on all data: 0.012054489
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2993021e-07
Norm of the params: 9.1531515
                Loss: fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16059542
Train loss (w/o reg) on all data: 0.15242581
Test loss (w/o reg) on all data: 0.0923275
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.393489e-05
Norm of the params: 12.782494
              Random: fixed  34 labels. Loss 0.09233. Accuracy 0.977.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20607732
Train loss (w/o reg) on all data: 0.19951133
Test loss (w/o reg) on all data: 0.1400276
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.2139887e-05
Norm of the params: 11.459479
Flipped loss: 0.14003. Accuracy: 0.969
### Flips: 208, rs: 7, checks: 52
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13593462
Train loss (w/o reg) on all data: 0.12624075
Test loss (w/o reg) on all data: 0.1174811
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.6725036e-05
Norm of the params: 13.923993
     Influence (LOO): fixed  35 labels. Loss 0.11748. Accuracy 0.969.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08896047
Train loss (w/o reg) on all data: 0.076807134
Test loss (w/o reg) on all data: 0.10242282
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.399537e-06
Norm of the params: 15.590599
                Loss: fixed  51 labels. Loss 0.10242. Accuracy 0.950.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20607731
Train loss (w/o reg) on all data: 0.19951236
Test loss (w/o reg) on all data: 0.14002079
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.2879305e-05
Norm of the params: 11.458577
              Random: fixed   0 labels. Loss 0.14002. Accuracy 0.969.
### Flips: 208, rs: 7, checks: 104
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07338814
Train loss (w/o reg) on all data: 0.063921385
Test loss (w/o reg) on all data: 0.07644863
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.439171e-06
Norm of the params: 13.759906
     Influence (LOO): fixed  66 labels. Loss 0.07645. Accuracy 0.973.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014932148
Train loss (w/o reg) on all data: 0.007682618
Test loss (w/o reg) on all data: 0.011180585
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.725421e-07
Norm of the params: 12.041204
                Loss: fixed  87 labels. Loss 0.01118. Accuracy 0.996.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20112361
Train loss (w/o reg) on all data: 0.19423902
Test loss (w/o reg) on all data: 0.1415875
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.217826e-05
Norm of the params: 11.734211
              Random: fixed   3 labels. Loss 0.14159. Accuracy 0.969.
### Flips: 208, rs: 7, checks: 156
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02948822
Train loss (w/o reg) on all data: 0.022752117
Test loss (w/o reg) on all data: 0.034443382
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.4678916e-07
Norm of the params: 11.606984
     Influence (LOO): fixed  86 labels. Loss 0.03444. Accuracy 0.985.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010381509
Train loss (w/o reg) on all data: 0.00441444
Test loss (w/o reg) on all data: 0.011719948
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.412008e-07
Norm of the params: 10.924348
                Loss: fixed  91 labels. Loss 0.01172. Accuracy 0.996.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18977849
Train loss (w/o reg) on all data: 0.18268014
Test loss (w/o reg) on all data: 0.13373868
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.6706664e-05
Norm of the params: 11.914993
              Random: fixed   9 labels. Loss 0.13374. Accuracy 0.969.
### Flips: 208, rs: 7, checks: 208
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014145981
Train loss (w/o reg) on all data: 0.009372476
Test loss (w/o reg) on all data: 0.014373272
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.793835e-07
Norm of the params: 9.770881
     Influence (LOO): fixed  92 labels. Loss 0.01437. Accuracy 0.996.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729982
Test loss (w/o reg) on all data: 0.012056396
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.726208e-07
Norm of the params: 9.153162
                Loss: fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18125074
Train loss (w/o reg) on all data: 0.17378405
Test loss (w/o reg) on all data: 0.12922686
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.2719235e-05
Norm of the params: 12.220218
              Random: fixed  14 labels. Loss 0.12923. Accuracy 0.981.
### Flips: 208, rs: 7, checks: 260
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002353
Train loss (w/o reg) on all data: 0.0054476014
Test loss (w/o reg) on all data: 0.012244681
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1762745e-06
Norm of the params: 9.544373
     Influence (LOO): fixed  93 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729332
Test loss (w/o reg) on all data: 0.01205476
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1350268e-07
Norm of the params: 9.153235
                Loss: fixed  94 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17181015
Train loss (w/o reg) on all data: 0.16437638
Test loss (w/o reg) on all data: 0.10975519
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.7840093e-05
Norm of the params: 12.193249
              Random: fixed  20 labels. Loss 0.10976. Accuracy 0.981.
### Flips: 208, rs: 7, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729537
Test loss (w/o reg) on all data: 0.012054841
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.979301e-07
Norm of the params: 9.1532135
     Influence (LOO): fixed  94 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729532
Test loss (w/o reg) on all data: 0.012054926
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.707754e-08
Norm of the params: 9.1532135
                Loss: fixed  94 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16438177
Train loss (w/o reg) on all data: 0.15685451
Test loss (w/o reg) on all data: 0.10612513
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.8193407e-05
Norm of the params: 12.26969
              Random: fixed  25 labels. Loss 0.10613. Accuracy 0.977.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23249058
Train loss (w/o reg) on all data: 0.22616825
Test loss (w/o reg) on all data: 0.14088854
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0386633e-05
Norm of the params: 11.244861
Flipped loss: 0.14089. Accuracy: 0.977
### Flips: 208, rs: 8, checks: 52
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15558453
Train loss (w/o reg) on all data: 0.1464593
Test loss (w/o reg) on all data: 0.109232716
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.3420871e-05
Norm of the params: 13.509428
     Influence (LOO): fixed  38 labels. Loss 0.10923. Accuracy 0.969.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10533904
Train loss (w/o reg) on all data: 0.09404445
Test loss (w/o reg) on all data: 0.08023331
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.5028443e-05
Norm of the params: 15.0297
                Loss: fixed  52 labels. Loss 0.08023. Accuracy 0.958.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2310524
Train loss (w/o reg) on all data: 0.22468376
Test loss (w/o reg) on all data: 0.14106052
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.419716e-05
Norm of the params: 11.28596
              Random: fixed   1 labels. Loss 0.14106. Accuracy 0.977.
### Flips: 208, rs: 8, checks: 104
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10313721
Train loss (w/o reg) on all data: 0.09403248
Test loss (w/o reg) on all data: 0.06638826
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.344908e-06
Norm of the params: 13.494242
     Influence (LOO): fixed  67 labels. Loss 0.06639. Accuracy 0.977.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02108237
Train loss (w/o reg) on all data: 0.011200697
Test loss (w/o reg) on all data: 0.017450524
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4911936e-06
Norm of the params: 14.058217
                Loss: fixed  97 labels. Loss 0.01745. Accuracy 0.996.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22168396
Train loss (w/o reg) on all data: 0.21533597
Test loss (w/o reg) on all data: 0.13076994
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7778691e-05
Norm of the params: 11.267647
              Random: fixed   8 labels. Loss 0.13077. Accuracy 0.985.
### Flips: 208, rs: 8, checks: 156
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050755285
Train loss (w/o reg) on all data: 0.0426717
Test loss (w/o reg) on all data: 0.032117445
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9115357e-06
Norm of the params: 12.715017
     Influence (LOO): fixed  91 labels. Loss 0.03212. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007944422
Train loss (w/o reg) on all data: 0.003027198
Test loss (w/o reg) on all data: 0.015965274
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9298607e-07
Norm of the params: 9.916879
                Loss: fixed 105 labels. Loss 0.01597. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21543972
Train loss (w/o reg) on all data: 0.20920855
Test loss (w/o reg) on all data: 0.12160475
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.425287e-05
Norm of the params: 11.163484
              Random: fixed  13 labels. Loss 0.12160. Accuracy 0.981.
### Flips: 208, rs: 8, checks: 208
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017574236
Train loss (w/o reg) on all data: 0.01158295
Test loss (w/o reg) on all data: 0.014864968
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.4881664e-07
Norm of the params: 10.946493
     Influence (LOO): fixed 104 labels. Loss 0.01486. Accuracy 0.989.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173071
Test loss (w/o reg) on all data: 0.012055057
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.756476e-07
Norm of the params: 9.153085
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21090059
Train loss (w/o reg) on all data: 0.2048256
Test loss (w/o reg) on all data: 0.115560606
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2915669e-05
Norm of the params: 11.022701
              Random: fixed  17 labels. Loss 0.11556. Accuracy 0.989.
### Flips: 208, rs: 8, checks: 260
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489633
Train loss (w/o reg) on all data: 0.0055395784
Test loss (w/o reg) on all data: 0.013846845
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.9584016e-07
Norm of the params: 9.94993
     Influence (LOO): fixed 106 labels. Loss 0.01385. Accuracy 0.989.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172994
Test loss (w/o reg) on all data: 0.012054624
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2596725e-07
Norm of the params: 9.15317
                Loss: fixed 107 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2035793
Train loss (w/o reg) on all data: 0.1975246
Test loss (w/o reg) on all data: 0.11161366
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8664475e-05
Norm of the params: 11.004276
              Random: fixed  22 labels. Loss 0.11161. Accuracy 0.989.
### Flips: 208, rs: 8, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729867
Test loss (w/o reg) on all data: 0.012055771
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5472255e-07
Norm of the params: 9.153175
     Influence (LOO): fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172988
Test loss (w/o reg) on all data: 0.0120558115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4551002e-07
Norm of the params: 9.153175
                Loss: fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19352409
Train loss (w/o reg) on all data: 0.18703516
Test loss (w/o reg) on all data: 0.09814127
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1460335e-05
Norm of the params: 11.392043
              Random: fixed  29 labels. Loss 0.09814. Accuracy 0.996.
Using normal model
LBFGS training took [322] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20729145
Train loss (w/o reg) on all data: 0.19963919
Test loss (w/o reg) on all data: 0.14251076
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.278053e-06
Norm of the params: 12.371154
Flipped loss: 0.14251. Accuracy: 0.966
### Flips: 208, rs: 9, checks: 52
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12900814
Train loss (w/o reg) on all data: 0.119277224
Test loss (w/o reg) on all data: 0.08865805
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.52523e-05
Norm of the params: 13.950567
     Influence (LOO): fixed  41 labels. Loss 0.08866. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091541916
Train loss (w/o reg) on all data: 0.07736844
Test loss (w/o reg) on all data: 0.083214045
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.309549e-06
Norm of the params: 16.836554
                Loss: fixed  52 labels. Loss 0.08321. Accuracy 0.962.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20081452
Train loss (w/o reg) on all data: 0.1933729
Test loss (w/o reg) on all data: 0.12749976
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5775942e-05
Norm of the params: 12.199683
              Random: fixed   7 labels. Loss 0.12750. Accuracy 0.981.
### Flips: 208, rs: 9, checks: 104
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07309362
Train loss (w/o reg) on all data: 0.06426091
Test loss (w/o reg) on all data: 0.059220836
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0892429e-05
Norm of the params: 13.291138
     Influence (LOO): fixed  69 labels. Loss 0.05922. Accuracy 0.981.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017409638
Train loss (w/o reg) on all data: 0.009331327
Test loss (w/o reg) on all data: 0.023373391
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.7875004e-07
Norm of the params: 12.710871
                Loss: fixed  91 labels. Loss 0.02337. Accuracy 0.996.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19534469
Train loss (w/o reg) on all data: 0.18788949
Test loss (w/o reg) on all data: 0.122090146
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.722519e-05
Norm of the params: 12.21081
              Random: fixed  10 labels. Loss 0.12209. Accuracy 0.985.
### Flips: 208, rs: 9, checks: 156
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039475903
Train loss (w/o reg) on all data: 0.031645767
Test loss (w/o reg) on all data: 0.029925887
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.4243615e-06
Norm of the params: 12.5141
     Influence (LOO): fixed  87 labels. Loss 0.02993. Accuracy 0.989.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015975099
Train loss (w/o reg) on all data: 0.008228997
Test loss (w/o reg) on all data: 0.023160113
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0403795e-06
Norm of the params: 12.446768
                Loss: fixed  92 labels. Loss 0.02316. Accuracy 0.996.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19100681
Train loss (w/o reg) on all data: 0.1835308
Test loss (w/o reg) on all data: 0.11586435
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4975067e-05
Norm of the params: 12.227848
              Random: fixed  15 labels. Loss 0.11586. Accuracy 0.985.
### Flips: 208, rs: 9, checks: 208
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020687608
Train loss (w/o reg) on all data: 0.014969822
Test loss (w/o reg) on all data: 0.01874435
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.8837534e-07
Norm of the params: 10.693723
     Influence (LOO): fixed  93 labels. Loss 0.01874. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008612608
Train loss (w/o reg) on all data: 0.0033304775
Test loss (w/o reg) on all data: 0.0122333495
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.984051e-07
Norm of the params: 10.278259
                Loss: fixed  95 labels. Loss 0.01223. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18544619
Train loss (w/o reg) on all data: 0.17750268
Test loss (w/o reg) on all data: 0.11082147
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.2674804e-05
Norm of the params: 12.604379
              Random: fixed  18 labels. Loss 0.11082. Accuracy 0.989.
### Flips: 208, rs: 9, checks: 260
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011973482
Train loss (w/o reg) on all data: 0.0064863954
Test loss (w/o reg) on all data: 0.013354957
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2752149e-06
Norm of the params: 10.475768
     Influence (LOO): fixed  96 labels. Loss 0.01335. Accuracy 0.996.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077838153
Train loss (w/o reg) on all data: 0.0029201808
Test loss (w/o reg) on all data: 0.0106340535
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.873021e-07
Norm of the params: 9.862692
                Loss: fixed  97 labels. Loss 0.01063. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16890116
Train loss (w/o reg) on all data: 0.16071042
Test loss (w/o reg) on all data: 0.0993007
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2012859e-05
Norm of the params: 12.799012
              Random: fixed  27 labels. Loss 0.09930. Accuracy 0.989.
### Flips: 208, rs: 9, checks: 312
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011973482
Train loss (w/o reg) on all data: 0.006486389
Test loss (w/o reg) on all data: 0.013355426
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3488476e-06
Norm of the params: 10.475776
     Influence (LOO): fixed  96 labels. Loss 0.01336. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077838153
Train loss (w/o reg) on all data: 0.0029201724
Test loss (w/o reg) on all data: 0.010634791
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.87758e-07
Norm of the params: 9.8627
                Loss: fixed  97 labels. Loss 0.01063. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16313736
Train loss (w/o reg) on all data: 0.1547788
Test loss (w/o reg) on all data: 0.09618919
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0629583e-05
Norm of the params: 12.929473
              Random: fixed  30 labels. Loss 0.09619. Accuracy 0.989.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19715829
Train loss (w/o reg) on all data: 0.18899615
Test loss (w/o reg) on all data: 0.12694378
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5965312e-05
Norm of the params: 12.776653
Flipped loss: 0.12694. Accuracy: 0.969
### Flips: 208, rs: 10, checks: 52
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11652092
Train loss (w/o reg) on all data: 0.106427565
Test loss (w/o reg) on all data: 0.07561719
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4645922e-05
Norm of the params: 14.207993
     Influence (LOO): fixed  38 labels. Loss 0.07562. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0723554
Train loss (w/o reg) on all data: 0.058206532
Test loss (w/o reg) on all data: 0.08445395
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.8260445e-05
Norm of the params: 16.82193
                Loss: fixed  52 labels. Loss 0.08445. Accuracy 0.973.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19239639
Train loss (w/o reg) on all data: 0.18406507
Test loss (w/o reg) on all data: 0.12497931
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.123218e-06
Norm of the params: 12.908384
              Random: fixed   2 labels. Loss 0.12498. Accuracy 0.969.
### Flips: 208, rs: 10, checks: 104
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05013281
Train loss (w/o reg) on all data: 0.03985101
Test loss (w/o reg) on all data: 0.027922291
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.234305e-06
Norm of the params: 14.340014
     Influence (LOO): fixed  68 labels. Loss 0.02792. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009126607
Train loss (w/o reg) on all data: 0.003852509
Test loss (w/o reg) on all data: 0.025170716
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.119261e-07
Norm of the params: 10.270442
                Loss: fixed  83 labels. Loss 0.02517. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19153723
Train loss (w/o reg) on all data: 0.1833755
Test loss (w/o reg) on all data: 0.12128556
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.5141446e-05
Norm of the params: 12.7763405
              Random: fixed   3 labels. Loss 0.12129. Accuracy 0.973.
### Flips: 208, rs: 10, checks: 156
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030747946
Train loss (w/o reg) on all data: 0.023889631
Test loss (w/o reg) on all data: 0.018688474
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9430574e-06
Norm of the params: 11.711802
     Influence (LOO): fixed  77 labels. Loss 0.01869. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007847015
Train loss (w/o reg) on all data: 0.0030392825
Test loss (w/o reg) on all data: 0.0153493155
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.0694896e-07
Norm of the params: 9.805848
                Loss: fixed  85 labels. Loss 0.01535. Accuracy 0.989.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18857476
Train loss (w/o reg) on all data: 0.18027864
Test loss (w/o reg) on all data: 0.117208764
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.6722015e-06
Norm of the params: 12.881081
              Random: fixed   5 labels. Loss 0.11721. Accuracy 0.981.
### Flips: 208, rs: 10, checks: 208
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014804027
Train loss (w/o reg) on all data: 0.009533063
Test loss (w/o reg) on all data: 0.013200329
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.314175e-07
Norm of the params: 10.26739
     Influence (LOO): fixed  84 labels. Loss 0.01320. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007847016
Train loss (w/o reg) on all data: 0.0030393952
Test loss (w/o reg) on all data: 0.015348511
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1220517e-07
Norm of the params: 9.805734
                Loss: fixed  85 labels. Loss 0.01535. Accuracy 0.989.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18654697
Train loss (w/o reg) on all data: 0.17820765
Test loss (w/o reg) on all data: 0.11516797
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4337007e-05
Norm of the params: 12.9145775
              Random: fixed   7 labels. Loss 0.11517. Accuracy 0.981.
### Flips: 208, rs: 10, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172936
Test loss (w/o reg) on all data: 0.012055382
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9398159e-07
Norm of the params: 9.153232
     Influence (LOO): fixed  86 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172937
Test loss (w/o reg) on all data: 0.012055317
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.431303e-07
Norm of the params: 9.153232
                Loss: fixed  86 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17998049
Train loss (w/o reg) on all data: 0.17209451
Test loss (w/o reg) on all data: 0.10086382
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.3719312e-05
Norm of the params: 12.558641
              Random: fixed  12 labels. Loss 0.10086. Accuracy 0.981.
### Flips: 208, rs: 10, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729788
Test loss (w/o reg) on all data: 0.012054785
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.803133e-08
Norm of the params: 9.153184
     Influence (LOO): fixed  86 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729795
Test loss (w/o reg) on all data: 0.012054816
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.477499e-07
Norm of the params: 9.153185
                Loss: fixed  86 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17124493
Train loss (w/o reg) on all data: 0.16397314
Test loss (w/o reg) on all data: 0.09124778
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.669606e-05
Norm of the params: 12.059683
              Random: fixed  19 labels. Loss 0.09125. Accuracy 0.989.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22331133
Train loss (w/o reg) on all data: 0.21603346
Test loss (w/o reg) on all data: 0.14630543
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4513597e-05
Norm of the params: 12.064723
Flipped loss: 0.14631. Accuracy: 0.966
### Flips: 208, rs: 11, checks: 52
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1544445
Train loss (w/o reg) on all data: 0.14524122
Test loss (w/o reg) on all data: 0.09978584
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.012628e-05
Norm of the params: 13.567084
     Influence (LOO): fixed  36 labels. Loss 0.09979. Accuracy 0.981.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103381075
Train loss (w/o reg) on all data: 0.089302115
Test loss (w/o reg) on all data: 0.10258338
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.377929e-06
Norm of the params: 16.780323
                Loss: fixed  51 labels. Loss 0.10258. Accuracy 0.958.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21822466
Train loss (w/o reg) on all data: 0.21091127
Test loss (w/o reg) on all data: 0.14000298
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5772093e-05
Norm of the params: 12.094124
              Random: fixed   6 labels. Loss 0.14000. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 104
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08828679
Train loss (w/o reg) on all data: 0.078017354
Test loss (w/o reg) on all data: 0.070142426
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.4288533e-06
Norm of the params: 14.331389
     Influence (LOO): fixed  67 labels. Loss 0.07014. Accuracy 0.981.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022283036
Train loss (w/o reg) on all data: 0.012661232
Test loss (w/o reg) on all data: 0.018220501
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0116298e-06
Norm of the params: 13.872132
                Loss: fixed  95 labels. Loss 0.01822. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21034129
Train loss (w/o reg) on all data: 0.20322981
Test loss (w/o reg) on all data: 0.13298784
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5079513e-05
Norm of the params: 11.926002
              Random: fixed  13 labels. Loss 0.13299. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 156
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048281085
Train loss (w/o reg) on all data: 0.039130483
Test loss (w/o reg) on all data: 0.04034114
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9608462e-06
Norm of the params: 13.528193
     Influence (LOO): fixed  87 labels. Loss 0.04034. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0097382665
Train loss (w/o reg) on all data: 0.0038560326
Test loss (w/o reg) on all data: 0.011847695
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.2942335e-07
Norm of the params: 10.846413
                Loss: fixed 103 labels. Loss 0.01185. Accuracy 0.996.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1939342
Train loss (w/o reg) on all data: 0.18659227
Test loss (w/o reg) on all data: 0.12737387
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.576393e-05
Norm of the params: 12.117702
              Random: fixed  22 labels. Loss 0.12737. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 208
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017536119
Train loss (w/o reg) on all data: 0.011365201
Test loss (w/o reg) on all data: 0.015291998
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.928813e-06
Norm of the params: 11.109381
     Influence (LOO): fixed 102 labels. Loss 0.01529. Accuracy 0.996.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00798821
Train loss (w/o reg) on all data: 0.0029990415
Test loss (w/o reg) on all data: 0.012528655
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.1173244e-07
Norm of the params: 9.989162
                Loss: fixed 104 labels. Loss 0.01253. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18723708
Train loss (w/o reg) on all data: 0.17986335
Test loss (w/o reg) on all data: 0.12292198
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.519137e-06
Norm of the params: 12.143917
              Random: fixed  26 labels. Loss 0.12292. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 260
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00985808
Train loss (w/o reg) on all data: 0.0049289926
Test loss (w/o reg) on all data: 0.013106188
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.2716997e-07
Norm of the params: 9.928835
     Influence (LOO): fixed 104 labels. Loss 0.01311. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00798821
Train loss (w/o reg) on all data: 0.0029991192
Test loss (w/o reg) on all data: 0.012528439
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.477422e-07
Norm of the params: 9.989085
                Loss: fixed 104 labels. Loss 0.01253. Accuracy 0.996.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17455137
Train loss (w/o reg) on all data: 0.16641958
Test loss (w/o reg) on all data: 0.121477306
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3071724e-05
Norm of the params: 12.752869
              Random: fixed  31 labels. Loss 0.12148. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729192
Test loss (w/o reg) on all data: 0.012055346
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4057672e-07
Norm of the params: 9.15325
     Influence (LOO): fixed 105 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [2] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172935
Test loss (w/o reg) on all data: 0.01205668
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8351175e-07
Norm of the params: 9.153234
                Loss: fixed 105 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16805795
Train loss (w/o reg) on all data: 0.15976538
Test loss (w/o reg) on all data: 0.11562512
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9736726e-05
Norm of the params: 12.878332
              Random: fixed  35 labels. Loss 0.11563. Accuracy 0.977.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20838557
Train loss (w/o reg) on all data: 0.20032918
Test loss (w/o reg) on all data: 0.12729399
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.9699516e-05
Norm of the params: 12.693611
Flipped loss: 0.12729. Accuracy: 0.981
### Flips: 208, rs: 12, checks: 52
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13345394
Train loss (w/o reg) on all data: 0.12296168
Test loss (w/o reg) on all data: 0.08892287
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9674315e-05
Norm of the params: 14.486032
     Influence (LOO): fixed  38 labels. Loss 0.08892. Accuracy 0.977.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07129825
Train loss (w/o reg) on all data: 0.055018395
Test loss (w/o reg) on all data: 0.072035275
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2664336e-05
Norm of the params: 18.044308
                Loss: fixed  52 labels. Loss 0.07204. Accuracy 0.966.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20317765
Train loss (w/o reg) on all data: 0.19506645
Test loss (w/o reg) on all data: 0.124839276
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9733403e-05
Norm of the params: 12.736714
              Random: fixed   3 labels. Loss 0.12484. Accuracy 0.977.
### Flips: 208, rs: 12, checks: 104
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0696954
Train loss (w/o reg) on all data: 0.059326883
Test loss (w/o reg) on all data: 0.05060284
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.598437e-06
Norm of the params: 14.400359
     Influence (LOO): fixed  68 labels. Loss 0.05060. Accuracy 0.989.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01609079
Train loss (w/o reg) on all data: 0.008579634
Test loss (w/o reg) on all data: 0.016797474
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0859158e-07
Norm of the params: 12.256555
                Loss: fixed  89 labels. Loss 0.01680. Accuracy 0.996.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19798452
Train loss (w/o reg) on all data: 0.18998668
Test loss (w/o reg) on all data: 0.12099842
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3586825e-05
Norm of the params: 12.647399
              Random: fixed   7 labels. Loss 0.12100. Accuracy 0.973.
### Flips: 208, rs: 12, checks: 156
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022049952
Train loss (w/o reg) on all data: 0.01317069
Test loss (w/o reg) on all data: 0.027399534
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5634445e-06
Norm of the params: 13.326111
     Influence (LOO): fixed  87 labels. Loss 0.02740. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056534
Train loss (w/o reg) on all data: 0.0028170673
Test loss (w/o reg) on all data: 0.01275575
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4652397e-07
Norm of the params: 9.68358
                Loss: fixed  93 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18802343
Train loss (w/o reg) on all data: 0.1798935
Test loss (w/o reg) on all data: 0.114869304
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.022641e-05
Norm of the params: 12.751425
              Random: fixed  13 labels. Loss 0.11487. Accuracy 0.969.
### Flips: 208, rs: 12, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729486
Test loss (w/o reg) on all data: 0.012055522
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.918631e-07
Norm of the params: 9.153216
     Influence (LOO): fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056506
Train loss (w/o reg) on all data: 0.0028169258
Test loss (w/o reg) on all data: 0.012756388
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5629743e-07
Norm of the params: 9.683723
                Loss: fixed  93 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1797919
Train loss (w/o reg) on all data: 0.17139414
Test loss (w/o reg) on all data: 0.10932696
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.1574586e-06
Norm of the params: 12.95975
              Random: fixed  17 labels. Loss 0.10933. Accuracy 0.977.
### Flips: 208, rs: 12, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729907
Test loss (w/o reg) on all data: 0.012055207
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0429413e-07
Norm of the params: 9.153172
     Influence (LOO): fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729895
Test loss (w/o reg) on all data: 0.01205516
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.78763e-07
Norm of the params: 9.153172
                Loss: fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17417552
Train loss (w/o reg) on all data: 0.16530001
Test loss (w/o reg) on all data: 0.10524897
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.28591e-06
Norm of the params: 13.323289
              Random: fixed  19 labels. Loss 0.10525. Accuracy 0.977.
### Flips: 208, rs: 12, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729232
Test loss (w/o reg) on all data: 0.012054552
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6950743e-07
Norm of the params: 9.153246
     Influence (LOO): fixed  94 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729292
Test loss (w/o reg) on all data: 0.012054705
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7619062e-07
Norm of the params: 9.153241
                Loss: fixed  94 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16802871
Train loss (w/o reg) on all data: 0.15934005
Test loss (w/o reg) on all data: 0.10561326
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5084919e-05
Norm of the params: 13.182304
              Random: fixed  23 labels. Loss 0.10561. Accuracy 0.977.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21599156
Train loss (w/o reg) on all data: 0.20829947
Test loss (w/o reg) on all data: 0.14618479
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.2800188e-05
Norm of the params: 12.403288
Flipped loss: 0.14618. Accuracy: 0.966
### Flips: 208, rs: 13, checks: 52
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1417344
Train loss (w/o reg) on all data: 0.13213375
Test loss (w/o reg) on all data: 0.08918838
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.98994e-06
Norm of the params: 13.856876
     Influence (LOO): fixed  39 labels. Loss 0.08919. Accuracy 0.981.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098542176
Train loss (w/o reg) on all data: 0.08512487
Test loss (w/o reg) on all data: 0.09516655
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2843041e-05
Norm of the params: 16.381273
                Loss: fixed  52 labels. Loss 0.09517. Accuracy 0.962.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21024925
Train loss (w/o reg) on all data: 0.20213099
Test loss (w/o reg) on all data: 0.1444641
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.2590844e-05
Norm of the params: 12.742264
              Random: fixed   4 labels. Loss 0.14446. Accuracy 0.966.
### Flips: 208, rs: 13, checks: 104
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08915345
Train loss (w/o reg) on all data: 0.07811901
Test loss (w/o reg) on all data: 0.07075729
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.4638983e-05
Norm of the params: 14.8556
     Influence (LOO): fixed  63 labels. Loss 0.07076. Accuracy 0.977.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027126383
Train loss (w/o reg) on all data: 0.016898824
Test loss (w/o reg) on all data: 0.048152845
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8857526e-06
Norm of the params: 14.302139
                Loss: fixed  91 labels. Loss 0.04815. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2054501
Train loss (w/o reg) on all data: 0.19723332
Test loss (w/o reg) on all data: 0.14195758
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9670044e-05
Norm of the params: 12.819343
              Random: fixed   8 labels. Loss 0.14196. Accuracy 0.973.
### Flips: 208, rs: 13, checks: 156
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04536164
Train loss (w/o reg) on all data: 0.03604322
Test loss (w/o reg) on all data: 0.031825274
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.996023e-06
Norm of the params: 13.651684
     Influence (LOO): fixed  87 labels. Loss 0.03183. Accuracy 0.989.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008787829
Train loss (w/o reg) on all data: 0.0035152491
Test loss (w/o reg) on all data: 0.011547858
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.153989e-07
Norm of the params: 10.268963
                Loss: fixed 101 labels. Loss 0.01155. Accuracy 0.996.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2020629
Train loss (w/o reg) on all data: 0.19369517
Test loss (w/o reg) on all data: 0.13987586
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.3669398e-05
Norm of the params: 12.936566
              Random: fixed  10 labels. Loss 0.13988. Accuracy 0.966.
### Flips: 208, rs: 13, checks: 208
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028128933
Train loss (w/o reg) on all data: 0.020738378
Test loss (w/o reg) on all data: 0.02655757
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4622494e-06
Norm of the params: 12.157758
     Influence (LOO): fixed  96 labels. Loss 0.02656. Accuracy 0.996.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056525
Train loss (w/o reg) on all data: 0.0028169153
Test loss (w/o reg) on all data: 0.012754597
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2689144e-07
Norm of the params: 9.683737
                Loss: fixed 102 labels. Loss 0.01275. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19774543
Train loss (w/o reg) on all data: 0.18949756
Test loss (w/o reg) on all data: 0.12950683
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.812848e-05
Norm of the params: 12.843575
              Random: fixed  14 labels. Loss 0.12951. Accuracy 0.977.
### Flips: 208, rs: 13, checks: 260
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016503323
Train loss (w/o reg) on all data: 0.010187984
Test loss (w/o reg) on all data: 0.018522238
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7086642e-06
Norm of the params: 11.238629
     Influence (LOO): fixed 100 labels. Loss 0.01852. Accuracy 0.996.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075056516
Train loss (w/o reg) on all data: 0.0028171267
Test loss (w/o reg) on all data: 0.012755053
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.678042e-07
Norm of the params: 9.6835165
                Loss: fixed 102 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19516043
Train loss (w/o reg) on all data: 0.1869513
Test loss (w/o reg) on all data: 0.122062504
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.617324e-06
Norm of the params: 12.813386
              Random: fixed  18 labels. Loss 0.12206. Accuracy 0.989.
### Flips: 208, rs: 13, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729579
Test loss (w/o reg) on all data: 0.01205508
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.612241e-07
Norm of the params: 9.153208
     Influence (LOO): fixed 103 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729583
Test loss (w/o reg) on all data: 0.0120551335
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.244047e-07
Norm of the params: 9.153208
                Loss: fixed 103 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19187176
Train loss (w/o reg) on all data: 0.18372175
Test loss (w/o reg) on all data: 0.11833468
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0137064e-05
Norm of the params: 12.767151
              Random: fixed  20 labels. Loss 0.11833. Accuracy 0.989.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22364998
Train loss (w/o reg) on all data: 0.21594958
Test loss (w/o reg) on all data: 0.14440438
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.985099e-05
Norm of the params: 12.409996
Flipped loss: 0.14440. Accuracy: 0.977
### Flips: 208, rs: 14, checks: 52
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15233485
Train loss (w/o reg) on all data: 0.14322013
Test loss (w/o reg) on all data: 0.10683945
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.519981e-05
Norm of the params: 13.501653
     Influence (LOO): fixed  41 labels. Loss 0.10684. Accuracy 0.981.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10253761
Train loss (w/o reg) on all data: 0.09191736
Test loss (w/o reg) on all data: 0.10375112
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.83772e-05
Norm of the params: 14.574121
                Loss: fixed  51 labels. Loss 0.10375. Accuracy 0.954.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21174219
Train loss (w/o reg) on all data: 0.20398314
Test loss (w/o reg) on all data: 0.1363942
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.1273085e-05
Norm of the params: 12.457162
              Random: fixed  10 labels. Loss 0.13639. Accuracy 0.973.
### Flips: 208, rs: 14, checks: 104
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090454236
Train loss (w/o reg) on all data: 0.08146755
Test loss (w/o reg) on all data: 0.06974421
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.6350467e-06
Norm of the params: 13.406484
     Influence (LOO): fixed  70 labels. Loss 0.06974. Accuracy 0.973.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021731678
Train loss (w/o reg) on all data: 0.011408643
Test loss (w/o reg) on all data: 0.03252469
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1376815e-06
Norm of the params: 14.368741
                Loss: fixed  96 labels. Loss 0.03252. Accuracy 0.977.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2040075
Train loss (w/o reg) on all data: 0.19597541
Test loss (w/o reg) on all data: 0.13491713
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1436473e-05
Norm of the params: 12.674462
              Random: fixed  14 labels. Loss 0.13492. Accuracy 0.966.
### Flips: 208, rs: 14, checks: 156
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04890973
Train loss (w/o reg) on all data: 0.03897468
Test loss (w/o reg) on all data: 0.042439144
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.5346027e-06
Norm of the params: 14.096134
     Influence (LOO): fixed  90 labels. Loss 0.04244. Accuracy 0.985.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012044381
Train loss (w/o reg) on all data: 0.0052153734
Test loss (w/o reg) on all data: 0.013662428
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8386033e-07
Norm of the params: 11.686751
                Loss: fixed 103 labels. Loss 0.01366. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1868958
Train loss (w/o reg) on all data: 0.17929478
Test loss (w/o reg) on all data: 0.12651195
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.3487965e-05
Norm of the params: 12.3296585
              Random: fixed  24 labels. Loss 0.12651. Accuracy 0.977.
### Flips: 208, rs: 14, checks: 208
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028951488
Train loss (w/o reg) on all data: 0.020471377
Test loss (w/o reg) on all data: 0.023937764
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2923601e-06
Norm of the params: 13.023143
     Influence (LOO): fixed  98 labels. Loss 0.02394. Accuracy 0.989.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009881265
Train loss (w/o reg) on all data: 0.003907871
Test loss (w/o reg) on all data: 0.011755759
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3022298e-07
Norm of the params: 10.930137
                Loss: fixed 104 labels. Loss 0.01176. Accuracy 0.996.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18150355
Train loss (w/o reg) on all data: 0.17355885
Test loss (w/o reg) on all data: 0.12110403
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.3363335e-05
Norm of the params: 12.605316
              Random: fixed  28 labels. Loss 0.12110. Accuracy 0.973.
### Flips: 208, rs: 14, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010055938
Train loss (w/o reg) on all data: 0.0051204204
Test loss (w/o reg) on all data: 0.014732652
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1459678e-07
Norm of the params: 9.935308
     Influence (LOO): fixed 106 labels. Loss 0.01473. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009881265
Train loss (w/o reg) on all data: 0.003908073
Test loss (w/o reg) on all data: 0.011755412
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0893442e-06
Norm of the params: 10.929952
                Loss: fixed 104 labels. Loss 0.01176. Accuracy 0.996.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16751726
Train loss (w/o reg) on all data: 0.15951128
Test loss (w/o reg) on all data: 0.107571945
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3680337e-05
Norm of the params: 12.6538315
              Random: fixed  37 labels. Loss 0.10757. Accuracy 0.973.
### Flips: 208, rs: 14, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730037
Test loss (w/o reg) on all data: 0.012055424
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1067832e-07
Norm of the params: 9.15316
     Influence (LOO): fixed 107 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007961925
Train loss (w/o reg) on all data: 0.0030190833
Test loss (w/o reg) on all data: 0.010875959
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6593565e-07
Norm of the params: 9.9426775
                Loss: fixed 106 labels. Loss 0.01088. Accuracy 0.996.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16483179
Train loss (w/o reg) on all data: 0.15747555
Test loss (w/o reg) on all data: 0.097726986
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.342909e-06
Norm of the params: 12.129502
              Random: fixed  42 labels. Loss 0.09773. Accuracy 0.992.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21967348
Train loss (w/o reg) on all data: 0.21376798
Test loss (w/o reg) on all data: 0.1378418
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.7352382e-05
Norm of the params: 10.86785
Flipped loss: 0.13784. Accuracy: 0.973
### Flips: 208, rs: 15, checks: 52
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14336443
Train loss (w/o reg) on all data: 0.13492303
Test loss (w/o reg) on all data: 0.09433892
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.6137946e-05
Norm of the params: 12.993386
     Influence (LOO): fixed  37 labels. Loss 0.09434. Accuracy 0.969.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.094599284
Train loss (w/o reg) on all data: 0.0823237
Test loss (w/o reg) on all data: 0.080928475
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.679785e-06
Norm of the params: 15.668813
                Loss: fixed  52 labels. Loss 0.08093. Accuracy 0.962.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21834275
Train loss (w/o reg) on all data: 0.21270058
Test loss (w/o reg) on all data: 0.1318718
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.96308e-05
Norm of the params: 10.622783
              Random: fixed   3 labels. Loss 0.13187. Accuracy 0.973.
### Flips: 208, rs: 15, checks: 104
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093587205
Train loss (w/o reg) on all data: 0.08451795
Test loss (w/o reg) on all data: 0.05454492
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5931144e-05
Norm of the params: 13.467928
     Influence (LOO): fixed  63 labels. Loss 0.05454. Accuracy 0.985.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027110994
Train loss (w/o reg) on all data: 0.016775537
Test loss (w/o reg) on all data: 0.012026761
Train acc on all data:  0.9952244508118434
Test acc on all data:   1.0
Norm of the mean of gradients: 3.3028655e-06
Norm of the params: 14.377381
                Loss: fixed  90 labels. Loss 0.01203. Accuracy 1.000.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21357602
Train loss (w/o reg) on all data: 0.20777708
Test loss (w/o reg) on all data: 0.12698388
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.9435392e-05
Norm of the params: 10.769346
              Random: fixed   6 labels. Loss 0.12698. Accuracy 0.973.
### Flips: 208, rs: 15, checks: 156
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051305767
Train loss (w/o reg) on all data: 0.04213507
Test loss (w/o reg) on all data: 0.039338455
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.525301e-06
Norm of the params: 13.54304
     Influence (LOO): fixed  86 labels. Loss 0.03934. Accuracy 0.985.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016282823
Train loss (w/o reg) on all data: 0.007956792
Test loss (w/o reg) on all data: 0.013266663
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.373279e-07
Norm of the params: 12.904286
                Loss: fixed 102 labels. Loss 0.01327. Accuracy 0.996.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20655389
Train loss (w/o reg) on all data: 0.20068464
Test loss (w/o reg) on all data: 0.11972733
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6138694e-05
Norm of the params: 10.8344345
              Random: fixed  12 labels. Loss 0.11973. Accuracy 0.981.
### Flips: 208, rs: 15, checks: 208
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026892541
Train loss (w/o reg) on all data: 0.019937865
Test loss (w/o reg) on all data: 0.016969088
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3079663e-06
Norm of the params: 11.793791
     Influence (LOO): fixed  99 labels. Loss 0.01697. Accuracy 0.996.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013479952
Train loss (w/o reg) on all data: 0.0061358814
Test loss (w/o reg) on all data: 0.011771074
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.041927e-06
Norm of the params: 12.119465
                Loss: fixed 104 labels. Loss 0.01177. Accuracy 0.996.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2038969
Train loss (w/o reg) on all data: 0.19805546
Test loss (w/o reg) on all data: 0.11808293
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6107959e-05
Norm of the params: 10.808732
              Random: fixed  15 labels. Loss 0.11808. Accuracy 0.977.
### Flips: 208, rs: 15, checks: 260
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016242001
Train loss (w/o reg) on all data: 0.009791302
Test loss (w/o reg) on all data: 0.013097727
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.30757e-07
Norm of the params: 11.358434
     Influence (LOO): fixed 104 labels. Loss 0.01310. Accuracy 0.996.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010329271
Train loss (w/o reg) on all data: 0.004169457
Test loss (w/o reg) on all data: 0.012970475
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6434623e-07
Norm of the params: 11.099382
                Loss: fixed 105 labels. Loss 0.01297. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19560027
Train loss (w/o reg) on all data: 0.18971048
Test loss (w/o reg) on all data: 0.10822275
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.1498741e-05
Norm of the params: 10.853378
              Random: fixed  21 labels. Loss 0.10822. Accuracy 0.981.
### Flips: 208, rs: 15, checks: 312
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012527458
Train loss (w/o reg) on all data: 0.0073336815
Test loss (w/o reg) on all data: 0.011877636
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1122731e-06
Norm of the params: 10.191936
     Influence (LOO): fixed 106 labels. Loss 0.01188. Accuracy 0.996.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074117463
Train loss (w/o reg) on all data: 0.0026373614
Test loss (w/o reg) on all data: 0.011813896
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.2946354e-07
Norm of the params: 9.771781
                Loss: fixed 106 labels. Loss 0.01181. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18214595
Train loss (w/o reg) on all data: 0.17576036
Test loss (w/o reg) on all data: 0.09951909
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.024395e-05
Norm of the params: 11.300973
              Random: fixed  29 labels. Loss 0.09952. Accuracy 0.985.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20408183
Train loss (w/o reg) on all data: 0.1969315
Test loss (w/o reg) on all data: 0.14483444
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.186433e-05
Norm of the params: 11.958543
Flipped loss: 0.14483. Accuracy: 0.962
### Flips: 208, rs: 16, checks: 52
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118988454
Train loss (w/o reg) on all data: 0.10713915
Test loss (w/o reg) on all data: 0.11219879
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.5756127e-06
Norm of the params: 15.394352
     Influence (LOO): fixed  39 labels. Loss 0.11220. Accuracy 0.954.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07801382
Train loss (w/o reg) on all data: 0.062774755
Test loss (w/o reg) on all data: 0.11116035
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.4253375e-06
Norm of the params: 17.457989
                Loss: fixed  51 labels. Loss 0.11116. Accuracy 0.954.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19602603
Train loss (w/o reg) on all data: 0.18918589
Test loss (w/o reg) on all data: 0.14317308
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.716626e-05
Norm of the params: 11.696267
              Random: fixed   5 labels. Loss 0.14317. Accuracy 0.958.
### Flips: 208, rs: 16, checks: 104
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07119743
Train loss (w/o reg) on all data: 0.05985249
Test loss (w/o reg) on all data: 0.059529077
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.8653135e-06
Norm of the params: 15.063161
     Influence (LOO): fixed  67 labels. Loss 0.05953. Accuracy 0.977.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018343773
Train loss (w/o reg) on all data: 0.009179599
Test loss (w/o reg) on all data: 0.028218146
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1367293e-06
Norm of the params: 13.538222
                Loss: fixed  85 labels. Loss 0.02822. Accuracy 0.989.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18876812
Train loss (w/o reg) on all data: 0.1813826
Test loss (w/o reg) on all data: 0.13157383
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.727525e-05
Norm of the params: 12.153622
              Random: fixed  13 labels. Loss 0.13157. Accuracy 0.958.
### Flips: 208, rs: 16, checks: 156
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03214526
Train loss (w/o reg) on all data: 0.022094553
Test loss (w/o reg) on all data: 0.0314287
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3098145e-06
Norm of the params: 14.177946
     Influence (LOO): fixed  83 labels. Loss 0.03143. Accuracy 0.992.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012358243
Train loss (w/o reg) on all data: 0.0055221533
Test loss (w/o reg) on all data: 0.023724379
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.149988e-07
Norm of the params: 11.69281
                Loss: fixed  93 labels. Loss 0.02372. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17755723
Train loss (w/o reg) on all data: 0.16996665
Test loss (w/o reg) on all data: 0.123423174
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.3772594e-05
Norm of the params: 12.321179
              Random: fixed  19 labels. Loss 0.12342. Accuracy 0.962.
### Flips: 208, rs: 16, checks: 208
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021393523
Train loss (w/o reg) on all data: 0.01398662
Test loss (w/o reg) on all data: 0.019343194
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.9763585e-06
Norm of the params: 12.171197
     Influence (LOO): fixed  92 labels. Loss 0.01934. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0083773285
Train loss (w/o reg) on all data: 0.0034014666
Test loss (w/o reg) on all data: 0.011191572
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3647809e-07
Norm of the params: 9.975833
                Loss: fixed  97 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16621043
Train loss (w/o reg) on all data: 0.15778476
Test loss (w/o reg) on all data: 0.1181879
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.848429e-06
Norm of the params: 12.981274
              Random: fixed  25 labels. Loss 0.11819. Accuracy 0.966.
### Flips: 208, rs: 16, checks: 260
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00837733
Train loss (w/o reg) on all data: 0.003401314
Test loss (w/o reg) on all data: 0.011191759
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3453936e-06
Norm of the params: 9.975988
     Influence (LOO): fixed  97 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008377329
Train loss (w/o reg) on all data: 0.0034013134
Test loss (w/o reg) on all data: 0.011191854
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6239364e-07
Norm of the params: 9.975987
                Loss: fixed  97 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15964837
Train loss (w/o reg) on all data: 0.151027
Test loss (w/o reg) on all data: 0.11643259
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.875963e-05
Norm of the params: 13.131172
              Random: fixed  28 labels. Loss 0.11643. Accuracy 0.973.
### Flips: 208, rs: 16, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172931
Test loss (w/o reg) on all data: 0.012054891
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7791946e-06
Norm of the params: 9.153237
     Influence (LOO): fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0083773285
Train loss (w/o reg) on all data: 0.0034013854
Test loss (w/o reg) on all data: 0.0111910775
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1949656e-07
Norm of the params: 9.975914
                Loss: fixed  97 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15481701
Train loss (w/o reg) on all data: 0.14627233
Test loss (w/o reg) on all data: 0.1131003
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.9430302e-05
Norm of the params: 13.072632
              Random: fixed  31 labels. Loss 0.11310. Accuracy 0.966.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21517642
Train loss (w/o reg) on all data: 0.20856844
Test loss (w/o reg) on all data: 0.1430882
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.408281e-05
Norm of the params: 11.496069
Flipped loss: 0.14309. Accuracy: 0.966
### Flips: 208, rs: 17, checks: 52
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13310933
Train loss (w/o reg) on all data: 0.122996554
Test loss (w/o reg) on all data: 0.11092481
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.1673299e-05
Norm of the params: 14.221656
     Influence (LOO): fixed  37 labels. Loss 0.11092. Accuracy 0.958.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089924216
Train loss (w/o reg) on all data: 0.078995205
Test loss (w/o reg) on all data: 0.09037166
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.2244639e-06
Norm of the params: 14.784458
                Loss: fixed  52 labels. Loss 0.09037. Accuracy 0.958.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20175885
Train loss (w/o reg) on all data: 0.19419551
Test loss (w/o reg) on all data: 0.13291033
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.7931128e-05
Norm of the params: 12.29905
              Random: fixed   8 labels. Loss 0.13291. Accuracy 0.962.
### Flips: 208, rs: 17, checks: 104
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083584815
Train loss (w/o reg) on all data: 0.07241812
Test loss (w/o reg) on all data: 0.08179301
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3048166e-05
Norm of the params: 14.944361
     Influence (LOO): fixed  64 labels. Loss 0.08179. Accuracy 0.973.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025446117
Train loss (w/o reg) on all data: 0.014998843
Test loss (w/o reg) on all data: 0.024200357
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6483582e-06
Norm of the params: 14.4549465
                Loss: fixed  87 labels. Loss 0.02420. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1901307
Train loss (w/o reg) on all data: 0.18234536
Test loss (w/o reg) on all data: 0.12328082
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2877774e-05
Norm of the params: 12.47825
              Random: fixed  14 labels. Loss 0.12328. Accuracy 0.973.
### Flips: 208, rs: 17, checks: 156
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050154053
Train loss (w/o reg) on all data: 0.040425044
Test loss (w/o reg) on all data: 0.04908143
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0004855e-06
Norm of the params: 13.949202
     Influence (LOO): fixed  83 labels. Loss 0.04908. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071227457
Train loss (w/o reg) on all data: 0.0025608507
Test loss (w/o reg) on all data: 0.013581991
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6835907e-07
Norm of the params: 9.551853
                Loss: fixed  99 labels. Loss 0.01358. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1846748
Train loss (w/o reg) on all data: 0.17688088
Test loss (w/o reg) on all data: 0.11291003
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.1225048e-05
Norm of the params: 12.485123
              Random: fixed  18 labels. Loss 0.11291. Accuracy 0.981.
### Flips: 208, rs: 17, checks: 208
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022976767
Train loss (w/o reg) on all data: 0.01603349
Test loss (w/o reg) on all data: 0.021343082
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9301442e-06
Norm of the params: 11.784123
     Influence (LOO): fixed  96 labels. Loss 0.02134. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071227457
Train loss (w/o reg) on all data: 0.0025608218
Test loss (w/o reg) on all data: 0.013581508
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5179835e-07
Norm of the params: 9.551883
                Loss: fixed  99 labels. Loss 0.01358. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18017723
Train loss (w/o reg) on all data: 0.17222291
Test loss (w/o reg) on all data: 0.10729838
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3516685e-05
Norm of the params: 12.612946
              Random: fixed  22 labels. Loss 0.10730. Accuracy 0.977.
### Flips: 208, rs: 17, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730778
Test loss (w/o reg) on all data: 0.012054223
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.515148e-06
Norm of the params: 9.153075
     Influence (LOO): fixed 102 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.005919027
Train loss (w/o reg) on all data: 0.0018800909
Test loss (w/o reg) on all data: 0.013727943
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8550095e-07
Norm of the params: 8.987699
                Loss: fixed 100 labels. Loss 0.01373. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17206898
Train loss (w/o reg) on all data: 0.16387972
Test loss (w/o reg) on all data: 0.103406705
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.14507775e-05
Norm of the params: 12.797856
              Random: fixed  26 labels. Loss 0.10341. Accuracy 0.981.
### Flips: 208, rs: 17, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729898
Test loss (w/o reg) on all data: 0.012055032
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.75167e-07
Norm of the params: 9.153173
     Influence (LOO): fixed 102 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0059190295
Train loss (w/o reg) on all data: 0.0018800899
Test loss (w/o reg) on all data: 0.0137280235
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9226936e-07
Norm of the params: 8.987702
                Loss: fixed 100 labels. Loss 0.01373. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1659944
Train loss (w/o reg) on all data: 0.15721114
Test loss (w/o reg) on all data: 0.0968482
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.1341867e-05
Norm of the params: 13.253878
              Random: fixed  31 labels. Loss 0.09685. Accuracy 0.981.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22294888
Train loss (w/o reg) on all data: 0.21576318
Test loss (w/o reg) on all data: 0.1330687
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.518674e-05
Norm of the params: 11.988077
Flipped loss: 0.13307. Accuracy: 0.973
### Flips: 208, rs: 18, checks: 52
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15309308
Train loss (w/o reg) on all data: 0.14280966
Test loss (w/o reg) on all data: 0.09638437
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.5801728e-05
Norm of the params: 14.341149
     Influence (LOO): fixed  35 labels. Loss 0.09638. Accuracy 0.977.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10519256
Train loss (w/o reg) on all data: 0.094206534
Test loss (w/o reg) on all data: 0.06053029
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9936153e-05
Norm of the params: 14.822972
                Loss: fixed  52 labels. Loss 0.06053. Accuracy 0.985.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21872263
Train loss (w/o reg) on all data: 0.21152876
Test loss (w/o reg) on all data: 0.12852375
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0223057e-05
Norm of the params: 11.994888
              Random: fixed   2 labels. Loss 0.12852. Accuracy 0.981.
### Flips: 208, rs: 18, checks: 104
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0939427
Train loss (w/o reg) on all data: 0.08454833
Test loss (w/o reg) on all data: 0.054618943
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.01760315e-05
Norm of the params: 13.7072
     Influence (LOO): fixed  63 labels. Loss 0.05462. Accuracy 0.985.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024012212
Train loss (w/o reg) on all data: 0.014179529
Test loss (w/o reg) on all data: 0.014293815
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.3150318e-06
Norm of the params: 14.023325
                Loss: fixed  96 labels. Loss 0.01429. Accuracy 0.996.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21325293
Train loss (w/o reg) on all data: 0.20603096
Test loss (w/o reg) on all data: 0.11977377
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.9091123e-05
Norm of the params: 12.018297
              Random: fixed   7 labels. Loss 0.11977. Accuracy 0.985.
### Flips: 208, rs: 18, checks: 156
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03811855
Train loss (w/o reg) on all data: 0.03020397
Test loss (w/o reg) on all data: 0.026317934
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2870896e-06
Norm of the params: 12.581398
     Influence (LOO): fixed  91 labels. Loss 0.02632. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00759721
Train loss (w/o reg) on all data: 0.0029205955
Test loss (w/o reg) on all data: 0.0133789135
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0928788e-07
Norm of the params: 9.671208
                Loss: fixed 104 labels. Loss 0.01338. Accuracy 0.992.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20020111
Train loss (w/o reg) on all data: 0.1927266
Test loss (w/o reg) on all data: 0.10951458
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.28684715e-05
Norm of the params: 12.226625
              Random: fixed  15 labels. Loss 0.10951. Accuracy 0.981.
### Flips: 208, rs: 18, checks: 208
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025649756
Train loss (w/o reg) on all data: 0.018467188
Test loss (w/o reg) on all data: 0.017848082
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.8353236e-07
Norm of the params: 11.985464
     Influence (LOO): fixed  97 labels. Loss 0.01785. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.012056747
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.1524885e-07
Norm of the params: 9.153191
                Loss: fixed 105 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19278623
Train loss (w/o reg) on all data: 0.18539442
Test loss (w/o reg) on all data: 0.10249766
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.6652036e-05
Norm of the params: 12.158793
              Random: fixed  20 labels. Loss 0.10250. Accuracy 0.981.
### Flips: 208, rs: 18, checks: 260
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016258473
Train loss (w/o reg) on all data: 0.009770672
Test loss (w/o reg) on all data: 0.016625462
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9156216e-06
Norm of the params: 11.39105
     Influence (LOO): fixed 101 labels. Loss 0.01663. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729758
Test loss (w/o reg) on all data: 0.012055802
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7755604e-07
Norm of the params: 9.153187
                Loss: fixed 105 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18455367
Train loss (w/o reg) on all data: 0.17665885
Test loss (w/o reg) on all data: 0.09730577
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.4794894e-05
Norm of the params: 12.56568
              Random: fixed  25 labels. Loss 0.09731. Accuracy 0.985.
### Flips: 208, rs: 18, checks: 312
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015065135
Train loss (w/o reg) on all data: 0.009018168
Test loss (w/o reg) on all data: 0.015713276
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.082179e-06
Norm of the params: 10.997243
     Influence (LOO): fixed 102 labels. Loss 0.01571. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729434
Test loss (w/o reg) on all data: 0.012055737
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.65848e-07
Norm of the params: 9.153223
                Loss: fixed 105 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17873147
Train loss (w/o reg) on all data: 0.17093363
Test loss (w/o reg) on all data: 0.08711692
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3303944e-05
Norm of the params: 12.48827
              Random: fixed  30 labels. Loss 0.08712. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21450889
Train loss (w/o reg) on all data: 0.2070188
Test loss (w/o reg) on all data: 0.15826716
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.89377e-05
Norm of the params: 12.239365
Flipped loss: 0.15827. Accuracy: 0.966
### Flips: 208, rs: 19, checks: 52
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14315186
Train loss (w/o reg) on all data: 0.13256732
Test loss (w/o reg) on all data: 0.120561294
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.904293e-06
Norm of the params: 14.549601
     Influence (LOO): fixed  38 labels. Loss 0.12056. Accuracy 0.969.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1044106
Train loss (w/o reg) on all data: 0.091509454
Test loss (w/o reg) on all data: 0.09536835
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5840715e-05
Norm of the params: 16.063095
                Loss: fixed  49 labels. Loss 0.09537. Accuracy 0.969.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20643286
Train loss (w/o reg) on all data: 0.198848
Test loss (w/o reg) on all data: 0.15553644
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 7.627906e-05
Norm of the params: 12.31655
              Random: fixed   3 labels. Loss 0.15554. Accuracy 0.943.
### Flips: 208, rs: 19, checks: 104
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10383696
Train loss (w/o reg) on all data: 0.09420714
Test loss (w/o reg) on all data: 0.080073945
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.2347176e-06
Norm of the params: 13.87791
     Influence (LOO): fixed  60 labels. Loss 0.08007. Accuracy 0.981.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04277052
Train loss (w/o reg) on all data: 0.030685833
Test loss (w/o reg) on all data: 0.0455979
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.485243e-06
Norm of the params: 15.546502
                Loss: fixed  87 labels. Loss 0.04560. Accuracy 0.981.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2011186
Train loss (w/o reg) on all data: 0.19371107
Test loss (w/o reg) on all data: 0.13728334
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2055185e-05
Norm of the params: 12.17171
              Random: fixed  10 labels. Loss 0.13728. Accuracy 0.966.
### Flips: 208, rs: 19, checks: 156
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06319124
Train loss (w/o reg) on all data: 0.0535071
Test loss (w/o reg) on all data: 0.050428633
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.939492e-06
Norm of the params: 13.916997
     Influence (LOO): fixed  82 labels. Loss 0.05043. Accuracy 0.989.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019748325
Train loss (w/o reg) on all data: 0.011231352
Test loss (w/o reg) on all data: 0.022212867
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2565791e-06
Norm of the params: 13.051416
                Loss: fixed 102 labels. Loss 0.02221. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19872767
Train loss (w/o reg) on all data: 0.19129653
Test loss (w/o reg) on all data: 0.13317709
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.687475e-05
Norm of the params: 12.191093
              Random: fixed  13 labels. Loss 0.13318. Accuracy 0.969.
### Flips: 208, rs: 19, checks: 208
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041819606
Train loss (w/o reg) on all data: 0.03354481
Test loss (w/o reg) on all data: 0.028862286
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.315841e-06
Norm of the params: 12.864523
     Influence (LOO): fixed  92 labels. Loss 0.02886. Accuracy 0.989.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0109330565
Train loss (w/o reg) on all data: 0.004688321
Test loss (w/o reg) on all data: 0.025571674
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.7768134e-07
Norm of the params: 11.175631
                Loss: fixed 106 labels. Loss 0.02557. Accuracy 0.989.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.190527
Train loss (w/o reg) on all data: 0.18278052
Test loss (w/o reg) on all data: 0.12447816
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.02030135e-05
Norm of the params: 12.447084
              Random: fixed  20 labels. Loss 0.12448. Accuracy 0.973.
### Flips: 208, rs: 19, checks: 260
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027552417
Train loss (w/o reg) on all data: 0.020208592
Test loss (w/o reg) on all data: 0.0399148
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6495313e-06
Norm of the params: 12.119262
     Influence (LOO): fixed 101 labels. Loss 0.03991. Accuracy 0.985.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008230703
Train loss (w/o reg) on all data: 0.0031267877
Test loss (w/o reg) on all data: 0.013310916
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6446417e-07
Norm of the params: 10.103381
                Loss: fixed 108 labels. Loss 0.01331. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18738429
Train loss (w/o reg) on all data: 0.17939205
Test loss (w/o reg) on all data: 0.12195287
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.6858324e-05
Norm of the params: 12.642968
              Random: fixed  22 labels. Loss 0.12195. Accuracy 0.973.
### Flips: 208, rs: 19, checks: 312
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0103666205
Train loss (w/o reg) on all data: 0.005355227
Test loss (w/o reg) on all data: 0.0154784275
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1248652e-07
Norm of the params: 10.011388
     Influence (LOO): fixed 108 labels. Loss 0.01548. Accuracy 0.992.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730596
Test loss (w/o reg) on all data: 0.01205505
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5116204e-07
Norm of the params: 9.153095
                Loss: fixed 109 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17463529
Train loss (w/o reg) on all data: 0.16648169
Test loss (w/o reg) on all data: 0.11689648
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4958554e-05
Norm of the params: 12.76997
              Random: fixed  31 labels. Loss 0.11690. Accuracy 0.966.
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21663605
Train loss (w/o reg) on all data: 0.20954594
Test loss (w/o reg) on all data: 0.121868454
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.8026947e-05
Norm of the params: 11.908067
Flipped loss: 0.12187. Accuracy: 0.977
### Flips: 208, rs: 20, checks: 52
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13671052
Train loss (w/o reg) on all data: 0.12510486
Test loss (w/o reg) on all data: 0.10904335
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2062307e-05
Norm of the params: 15.2352705
     Influence (LOO): fixed  37 labels. Loss 0.10904. Accuracy 0.962.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09203014
Train loss (w/o reg) on all data: 0.0781235
Test loss (w/o reg) on all data: 0.07014941
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.949993e-06
Norm of the params: 16.67731
                Loss: fixed  52 labels. Loss 0.07015. Accuracy 0.969.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21383965
Train loss (w/o reg) on all data: 0.20693913
Test loss (w/o reg) on all data: 0.11603882
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.828742e-05
Norm of the params: 11.747778
              Random: fixed   3 labels. Loss 0.11604. Accuracy 0.977.
### Flips: 208, rs: 20, checks: 104
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09095959
Train loss (w/o reg) on all data: 0.08057409
Test loss (w/o reg) on all data: 0.07921284
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.629134e-06
Norm of the params: 14.412153
     Influence (LOO): fixed  63 labels. Loss 0.07921. Accuracy 0.973.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014905756
Train loss (w/o reg) on all data: 0.0067718476
Test loss (w/o reg) on all data: 0.021173492
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3584153e-06
Norm of the params: 12.754536
                Loss: fixed  95 labels. Loss 0.02117. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20734131
Train loss (w/o reg) on all data: 0.20067613
Test loss (w/o reg) on all data: 0.10908127
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.189986e-05
Norm of the params: 11.545716
              Random: fixed   8 labels. Loss 0.10908. Accuracy 0.985.
### Flips: 208, rs: 20, checks: 156
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044508383
Train loss (w/o reg) on all data: 0.03587717
Test loss (w/o reg) on all data: 0.032236215
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7980595e-06
Norm of the params: 13.138656
     Influence (LOO): fixed  86 labels. Loss 0.03224. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010632633
Train loss (w/o reg) on all data: 0.0045494745
Test loss (w/o reg) on all data: 0.012493508
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5439592e-06
Norm of the params: 11.030104
                Loss: fixed  98 labels. Loss 0.01249. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20084956
Train loss (w/o reg) on all data: 0.194037
Test loss (w/o reg) on all data: 0.102753
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.3436764e-05
Norm of the params: 11.672666
              Random: fixed  13 labels. Loss 0.10275. Accuracy 0.989.
### Flips: 208, rs: 20, checks: 208
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009754283
Train loss (w/o reg) on all data: 0.004798918
Test loss (w/o reg) on all data: 0.0115428595
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5283778e-06
Norm of the params: 9.955266
     Influence (LOO): fixed  99 labels. Loss 0.01154. Accuracy 0.996.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0086649805
Train loss (w/o reg) on all data: 0.0033182174
Test loss (w/o reg) on all data: 0.013526649
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6272037e-07
Norm of the params: 10.340951
                Loss: fixed  99 labels. Loss 0.01353. Accuracy 0.992.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19219229
Train loss (w/o reg) on all data: 0.18517637
Test loss (w/o reg) on all data: 0.09686879
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.392879e-05
Norm of the params: 11.845599
              Random: fixed  18 labels. Loss 0.09687. Accuracy 0.989.
### Flips: 208, rs: 20, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009754284
Train loss (w/o reg) on all data: 0.0047987634
Test loss (w/o reg) on all data: 0.011543355
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.987294e-07
Norm of the params: 9.955421
     Influence (LOO): fixed  99 labels. Loss 0.01154. Accuracy 0.996.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008664982
Train loss (w/o reg) on all data: 0.0033182458
Test loss (w/o reg) on all data: 0.013528311
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7277534e-07
Norm of the params: 10.340925
                Loss: fixed  99 labels. Loss 0.01353. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19025236
Train loss (w/o reg) on all data: 0.18307768
Test loss (w/o reg) on all data: 0.096358515
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.0792285e-05
Norm of the params: 11.978887
              Random: fixed  19 labels. Loss 0.09636. Accuracy 0.981.
### Flips: 208, rs: 20, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172931
Test loss (w/o reg) on all data: 0.012055506
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0190676e-07
Norm of the params: 9.153235
     Influence (LOO): fixed 100 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008664982
Train loss (w/o reg) on all data: 0.003317933
Test loss (w/o reg) on all data: 0.013526023
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.729012e-07
Norm of the params: 10.341228
                Loss: fixed  99 labels. Loss 0.01353. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18456312
Train loss (w/o reg) on all data: 0.17737871
Test loss (w/o reg) on all data: 0.09288756
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.4621185e-05
Norm of the params: 11.986991
              Random: fixed  22 labels. Loss 0.09289. Accuracy 0.985.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20157637
Train loss (w/o reg) on all data: 0.19334586
Test loss (w/o reg) on all data: 0.14165226
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0090546e-05
Norm of the params: 12.830045
Flipped loss: 0.14165. Accuracy: 0.973
### Flips: 208, rs: 21, checks: 52
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13058132
Train loss (w/o reg) on all data: 0.118428186
Test loss (w/o reg) on all data: 0.0981502
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.648045e-06
Norm of the params: 15.590468
     Influence (LOO): fixed  34 labels. Loss 0.09815. Accuracy 0.985.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07910729
Train loss (w/o reg) on all data: 0.0624186
Test loss (w/o reg) on all data: 0.08189896
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.200231e-06
Norm of the params: 18.26948
                Loss: fixed  52 labels. Loss 0.08190. Accuracy 0.981.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19562995
Train loss (w/o reg) on all data: 0.18759866
Test loss (w/o reg) on all data: 0.13564247
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4998579e-05
Norm of the params: 12.673825
              Random: fixed   5 labels. Loss 0.13564. Accuracy 0.973.
### Flips: 208, rs: 21, checks: 104
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074831754
Train loss (w/o reg) on all data: 0.06249423
Test loss (w/o reg) on all data: 0.061575424
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3900964e-06
Norm of the params: 15.708292
     Influence (LOO): fixed  65 labels. Loss 0.06158. Accuracy 0.992.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01468881
Train loss (w/o reg) on all data: 0.0067325276
Test loss (w/o reg) on all data: 0.016988395
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3862917e-06
Norm of the params: 12.614501
                Loss: fixed  88 labels. Loss 0.01699. Accuracy 0.996.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19160081
Train loss (w/o reg) on all data: 0.18327369
Test loss (w/o reg) on all data: 0.13717662
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0964941e-05
Norm of the params: 12.905134
              Random: fixed   7 labels. Loss 0.13718. Accuracy 0.973.
### Flips: 208, rs: 21, checks: 156
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026812367
Train loss (w/o reg) on all data: 0.01999786
Test loss (w/o reg) on all data: 0.034993522
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.8354397e-06
Norm of the params: 11.674337
     Influence (LOO): fixed  84 labels. Loss 0.03499. Accuracy 0.985.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729
Test loss (w/o reg) on all data: 0.012055102
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0829926e-07
Norm of the params: 9.153274
                Loss: fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18294974
Train loss (w/o reg) on all data: 0.17467315
Test loss (w/o reg) on all data: 0.12775713
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1848509e-05
Norm of the params: 12.865909
              Random: fixed  14 labels. Loss 0.12776. Accuracy 0.973.
### Flips: 208, rs: 21, checks: 208
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009754284
Train loss (w/o reg) on all data: 0.00479889
Test loss (w/o reg) on all data: 0.011545261
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4220747e-07
Norm of the params: 9.955295
     Influence (LOO): fixed  91 labels. Loss 0.01155. Accuracy 0.996.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729828
Test loss (w/o reg) on all data: 0.012055062
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5088105e-07
Norm of the params: 9.153179
                Loss: fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17847028
Train loss (w/o reg) on all data: 0.17030343
Test loss (w/o reg) on all data: 0.11975796
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.508859e-06
Norm of the params: 12.780336
              Random: fixed  17 labels. Loss 0.11976. Accuracy 0.969.
### Flips: 208, rs: 21, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729348
Test loss (w/o reg) on all data: 0.012055316
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9371074e-07
Norm of the params: 9.153235
     Influence (LOO): fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729348
Test loss (w/o reg) on all data: 0.0120552685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.528127e-08
Norm of the params: 9.1532345
                Loss: fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17615664
Train loss (w/o reg) on all data: 0.16798466
Test loss (w/o reg) on all data: 0.11884687
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.1494605e-06
Norm of the params: 12.784342
              Random: fixed  19 labels. Loss 0.11885. Accuracy 0.973.
### Flips: 208, rs: 21, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729127
Test loss (w/o reg) on all data: 0.0120554855
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3931119e-07
Norm of the params: 9.153257
     Influence (LOO): fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729122
Test loss (w/o reg) on all data: 0.012055553
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6583593e-07
Norm of the params: 9.153257
                Loss: fixed  92 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1703625
Train loss (w/o reg) on all data: 0.16151062
Test loss (w/o reg) on all data: 0.11342182
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.253346e-06
Norm of the params: 13.305555
              Random: fixed  21 labels. Loss 0.11342. Accuracy 0.973.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2157309
Train loss (w/o reg) on all data: 0.20765902
Test loss (w/o reg) on all data: 0.14652283
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.6201504e-05
Norm of the params: 12.705812
Flipped loss: 0.14652. Accuracy: 0.958
### Flips: 208, rs: 22, checks: 52
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1472786
Train loss (w/o reg) on all data: 0.13594408
Test loss (w/o reg) on all data: 0.10277155
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.00686e-05
Norm of the params: 15.056246
     Influence (LOO): fixed  37 labels. Loss 0.10277. Accuracy 0.969.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09166695
Train loss (w/o reg) on all data: 0.07667735
Test loss (w/o reg) on all data: 0.12557736
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.640906e-06
Norm of the params: 17.3145
                Loss: fixed  51 labels. Loss 0.12558. Accuracy 0.950.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20578323
Train loss (w/o reg) on all data: 0.19792226
Test loss (w/o reg) on all data: 0.13417847
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.4786391e-05
Norm of the params: 12.538716
              Random: fixed   9 labels. Loss 0.13418. Accuracy 0.962.
### Flips: 208, rs: 22, checks: 104
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07777593
Train loss (w/o reg) on all data: 0.067292124
Test loss (w/o reg) on all data: 0.061258964
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.024266e-06
Norm of the params: 14.480201
     Influence (LOO): fixed  68 labels. Loss 0.06126. Accuracy 0.973.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0171842
Train loss (w/o reg) on all data: 0.009270051
Test loss (w/o reg) on all data: 0.020060739
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.571964e-07
Norm of the params: 12.581057
                Loss: fixed  92 labels. Loss 0.02006. Accuracy 0.992.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2013901
Train loss (w/o reg) on all data: 0.19371952
Test loss (w/o reg) on all data: 0.12353195
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.0505324e-05
Norm of the params: 12.385945
              Random: fixed  13 labels. Loss 0.12353. Accuracy 0.962.
### Flips: 208, rs: 22, checks: 156
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029426254
Train loss (w/o reg) on all data: 0.021301959
Test loss (w/o reg) on all data: 0.019275019
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0376303e-06
Norm of the params: 12.746997
     Influence (LOO): fixed  89 labels. Loss 0.01928. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729816
Test loss (w/o reg) on all data: 0.012054802
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1853285e-07
Norm of the params: 9.153184
                Loss: fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1979377
Train loss (w/o reg) on all data: 0.19012153
Test loss (w/o reg) on all data: 0.12260733
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.7529979e-05
Norm of the params: 12.502938
              Random: fixed  15 labels. Loss 0.12261. Accuracy 0.966.
### Flips: 208, rs: 22, checks: 208
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016183633
Train loss (w/o reg) on all data: 0.0107291695
Test loss (w/o reg) on all data: 0.015168329
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0590585e-06
Norm of the params: 10.444582
     Influence (LOO): fixed  95 labels. Loss 0.01517. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729502
Test loss (w/o reg) on all data: 0.01205475
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.18525e-08
Norm of the params: 9.153216
                Loss: fixed  98 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19248737
Train loss (w/o reg) on all data: 0.18510805
Test loss (w/o reg) on all data: 0.11392458
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2828408e-05
Norm of the params: 12.148512
              Random: fixed  20 labels. Loss 0.11392. Accuracy 0.977.
### Flips: 208, rs: 22, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730564
Test loss (w/o reg) on all data: 0.012056472
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1204247e-07
Norm of the params: 9.153101
     Influence (LOO): fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728817
Test loss (w/o reg) on all data: 0.012056094
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6001526e-07
Norm of the params: 9.15329
                Loss: fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18943648
Train loss (w/o reg) on all data: 0.1819438
Test loss (w/o reg) on all data: 0.112841
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3351556e-05
Norm of the params: 12.241469
              Random: fixed  22 labels. Loss 0.11284. Accuracy 0.977.
### Flips: 208, rs: 22, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729271
Test loss (w/o reg) on all data: 0.012055125
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.5979026e-08
Norm of the params: 9.153243
     Influence (LOO): fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729285
Test loss (w/o reg) on all data: 0.012055178
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1829902e-07
Norm of the params: 9.153241
                Loss: fixed  98 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17829436
Train loss (w/o reg) on all data: 0.17087471
Test loss (w/o reg) on all data: 0.1010688
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.712276e-05
Norm of the params: 12.181664
              Random: fixed  28 labels. Loss 0.10107. Accuracy 0.985.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20960133
Train loss (w/o reg) on all data: 0.20149596
Test loss (w/o reg) on all data: 0.1257768
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.7025157e-05
Norm of the params: 12.732134
Flipped loss: 0.12578. Accuracy: 0.973
### Flips: 208, rs: 23, checks: 52
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12856545
Train loss (w/o reg) on all data: 0.115953214
Test loss (w/o reg) on all data: 0.10053537
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.4546065e-05
Norm of the params: 15.882208
     Influence (LOO): fixed  38 labels. Loss 0.10054. Accuracy 0.962.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08561016
Train loss (w/o reg) on all data: 0.069613606
Test loss (w/o reg) on all data: 0.08299903
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.5455353e-06
Norm of the params: 17.886618
                Loss: fixed  52 labels. Loss 0.08300. Accuracy 0.973.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20453401
Train loss (w/o reg) on all data: 0.19666357
Test loss (w/o reg) on all data: 0.12084058
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.054053e-05
Norm of the params: 12.54626
              Random: fixed   6 labels. Loss 0.12084. Accuracy 0.973.
### Flips: 208, rs: 23, checks: 104
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08387983
Train loss (w/o reg) on all data: 0.07200387
Test loss (w/o reg) on all data: 0.068659596
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.9284469e-05
Norm of the params: 15.411656
     Influence (LOO): fixed  61 labels. Loss 0.06866. Accuracy 0.969.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020468201
Train loss (w/o reg) on all data: 0.011657036
Test loss (w/o reg) on all data: 0.035271943
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.7597025e-06
Norm of the params: 13.274912
                Loss: fixed  92 labels. Loss 0.03527. Accuracy 0.981.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2034084
Train loss (w/o reg) on all data: 0.19561383
Test loss (w/o reg) on all data: 0.117360294
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.227532e-05
Norm of the params: 12.485648
              Random: fixed   8 labels. Loss 0.11736. Accuracy 0.977.
### Flips: 208, rs: 23, checks: 156
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036445398
Train loss (w/o reg) on all data: 0.027173502
Test loss (w/o reg) on all data: 0.025745315
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1978361e-06
Norm of the params: 13.6175585
     Influence (LOO): fixed  84 labels. Loss 0.02575. Accuracy 0.992.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009996068
Train loss (w/o reg) on all data: 0.0040828725
Test loss (w/o reg) on all data: 0.021047108
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.109366e-07
Norm of the params: 10.874921
                Loss: fixed  97 labels. Loss 0.02105. Accuracy 0.989.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19292632
Train loss (w/o reg) on all data: 0.18518999
Test loss (w/o reg) on all data: 0.10971867
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.525184e-05
Norm of the params: 12.438913
              Random: fixed  14 labels. Loss 0.10972. Accuracy 0.977.
### Flips: 208, rs: 23, checks: 208
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018881522
Train loss (w/o reg) on all data: 0.011169878
Test loss (w/o reg) on all data: 0.024689235
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9484495e-06
Norm of the params: 12.419054
     Influence (LOO): fixed  94 labels. Loss 0.02469. Accuracy 0.989.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008817984
Train loss (w/o reg) on all data: 0.0034230705
Test loss (w/o reg) on all data: 0.0210083
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.29692e-07
Norm of the params: 10.387408
                Loss: fixed  98 labels. Loss 0.02101. Accuracy 0.992.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18978293
Train loss (w/o reg) on all data: 0.18211268
Test loss (w/o reg) on all data: 0.104281545
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.829375e-05
Norm of the params: 12.385681
              Random: fixed  17 labels. Loss 0.10428. Accuracy 0.977.
### Flips: 208, rs: 23, checks: 260
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016881382
Train loss (w/o reg) on all data: 0.0101460805
Test loss (w/o reg) on all data: 0.024221474
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.853143e-07
Norm of the params: 11.606293
     Influence (LOO): fixed  96 labels. Loss 0.02422. Accuracy 0.989.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730834
Test loss (w/o reg) on all data: 0.012056013
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.53426e-07
Norm of the params: 9.15307
                Loss: fixed 100 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18052934
Train loss (w/o reg) on all data: 0.17249267
Test loss (w/o reg) on all data: 0.109480076
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5891688e-05
Norm of the params: 12.678069
              Random: fixed  23 labels. Loss 0.10948. Accuracy 0.985.
### Flips: 208, rs: 23, checks: 312
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011468942
Train loss (w/o reg) on all data: 0.005932528
Test loss (w/o reg) on all data: 0.016597018
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.2136744e-07
Norm of the params: 10.522751
     Influence (LOO): fixed  99 labels. Loss 0.01660. Accuracy 0.989.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012055526
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5008823e-07
Norm of the params: 9.1532
                Loss: fixed 100 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1753945
Train loss (w/o reg) on all data: 0.16721545
Test loss (w/o reg) on all data: 0.10590806
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1458492e-05
Norm of the params: 12.7898855
              Random: fixed  26 labels. Loss 0.10591. Accuracy 0.985.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21538822
Train loss (w/o reg) on all data: 0.20744006
Test loss (w/o reg) on all data: 0.13769059
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 0.0001290448
Norm of the params: 12.608065
Flipped loss: 0.13769. Accuracy: 0.977
### Flips: 208, rs: 24, checks: 52
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13298702
Train loss (w/o reg) on all data: 0.12017423
Test loss (w/o reg) on all data: 0.08589146
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.9223945e-05
Norm of the params: 16.00799
     Influence (LOO): fixed  42 labels. Loss 0.08589. Accuracy 0.977.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08742032
Train loss (w/o reg) on all data: 0.07214845
Test loss (w/o reg) on all data: 0.083033144
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.1681694e-06
Norm of the params: 17.476768
                Loss: fixed  51 labels. Loss 0.08303. Accuracy 0.977.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20978428
Train loss (w/o reg) on all data: 0.2021952
Test loss (w/o reg) on all data: 0.13136049
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.1393966e-05
Norm of the params: 12.319978
              Random: fixed   6 labels. Loss 0.13136. Accuracy 0.981.
### Flips: 208, rs: 24, checks: 104
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09072054
Train loss (w/o reg) on all data: 0.078149684
Test loss (w/o reg) on all data: 0.06139035
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.240861e-06
Norm of the params: 15.856141
     Influence (LOO): fixed  62 labels. Loss 0.06139. Accuracy 0.977.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017745037
Train loss (w/o reg) on all data: 0.008688785
Test loss (w/o reg) on all data: 0.02242176
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.951838e-07
Norm of the params: 13.458269
                Loss: fixed  93 labels. Loss 0.02242. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2064467
Train loss (w/o reg) on all data: 0.19895178
Test loss (w/o reg) on all data: 0.12646359
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.9289327e-05
Norm of the params: 12.243307
              Random: fixed  11 labels. Loss 0.12646. Accuracy 0.981.
### Flips: 208, rs: 24, checks: 156
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041392263
Train loss (w/o reg) on all data: 0.031655036
Test loss (w/o reg) on all data: 0.027403047
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2698534e-06
Norm of the params: 13.955091
     Influence (LOO): fixed  86 labels. Loss 0.02740. Accuracy 0.992.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008961579
Train loss (w/o reg) on all data: 0.0034165788
Test loss (w/o reg) on all data: 0.017991835
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.81868e-07
Norm of the params: 10.530908
                Loss: fixed  99 labels. Loss 0.01799. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20139869
Train loss (w/o reg) on all data: 0.193267
Test loss (w/o reg) on all data: 0.12624583
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.5017333e-05
Norm of the params: 12.752789
              Random: fixed  15 labels. Loss 0.12625. Accuracy 0.981.
### Flips: 208, rs: 24, checks: 208
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014688464
Train loss (w/o reg) on all data: 0.008089258
Test loss (w/o reg) on all data: 0.014034609
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4591363e-07
Norm of the params: 11.488433
     Influence (LOO): fixed  98 labels. Loss 0.01403. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008288393
Train loss (w/o reg) on all data: 0.0030799857
Test loss (w/o reg) on all data: 0.011127007
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.4124884e-07
Norm of the params: 10.20628
                Loss: fixed 100 labels. Loss 0.01113. Accuracy 0.996.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19637322
Train loss (w/o reg) on all data: 0.1883241
Test loss (w/o reg) on all data: 0.11783883
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.1567294e-05
Norm of the params: 12.687888
              Random: fixed  20 labels. Loss 0.11784. Accuracy 0.985.
### Flips: 208, rs: 24, checks: 260
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011224471
Train loss (w/o reg) on all data: 0.0062452788
Test loss (w/o reg) on all data: 0.012817268
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7262789e-07
Norm of the params: 9.979171
     Influence (LOO): fixed 101 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008288394
Train loss (w/o reg) on all data: 0.0030800335
Test loss (w/o reg) on all data: 0.011126762
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.524316e-07
Norm of the params: 10.206234
                Loss: fixed 100 labels. Loss 0.01113. Accuracy 0.996.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18977213
Train loss (w/o reg) on all data: 0.18153872
Test loss (w/o reg) on all data: 0.113200895
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.0530147e-05
Norm of the params: 12.8323145
              Random: fixed  24 labels. Loss 0.11320. Accuracy 0.985.
### Flips: 208, rs: 24, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729392
Test loss (w/o reg) on all data: 0.0120554175
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8517592e-07
Norm of the params: 9.153231
     Influence (LOO): fixed 102 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076660495
Train loss (w/o reg) on all data: 0.0028513419
Test loss (w/o reg) on all data: 0.01224759
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.742577e-07
Norm of the params: 9.812959
                Loss: fixed 101 labels. Loss 0.01225. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17789814
Train loss (w/o reg) on all data: 0.16916645
Test loss (w/o reg) on all data: 0.110041626
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.372816e-06
Norm of the params: 13.214911
              Random: fixed  30 labels. Loss 0.11004. Accuracy 0.977.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23140971
Train loss (w/o reg) on all data: 0.22247618
Test loss (w/o reg) on all data: 0.16572978
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.958767e-05
Norm of the params: 13.366773
Flipped loss: 0.16573. Accuracy: 0.966
### Flips: 208, rs: 25, checks: 52
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17721075
Train loss (w/o reg) on all data: 0.16739894
Test loss (w/o reg) on all data: 0.12757865
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.3804572e-05
Norm of the params: 14.008424
     Influence (LOO): fixed  35 labels. Loss 0.12758. Accuracy 0.985.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11959751
Train loss (w/o reg) on all data: 0.10373719
Test loss (w/o reg) on all data: 0.10022319
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3976707e-05
Norm of the params: 17.81029
                Loss: fixed  52 labels. Loss 0.10022. Accuracy 0.973.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22486228
Train loss (w/o reg) on all data: 0.215854
Test loss (w/o reg) on all data: 0.15742059
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.3022294e-05
Norm of the params: 13.422579
              Random: fixed   6 labels. Loss 0.15742. Accuracy 0.973.
### Flips: 208, rs: 25, checks: 104
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11484224
Train loss (w/o reg) on all data: 0.10377585
Test loss (w/o reg) on all data: 0.09239995
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.631551e-05
Norm of the params: 14.8770895
     Influence (LOO): fixed  67 labels. Loss 0.09240. Accuracy 0.981.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042176936
Train loss (w/o reg) on all data: 0.028172199
Test loss (w/o reg) on all data: 0.043794036
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.4703876e-06
Norm of the params: 16.73603
                Loss: fixed  93 labels. Loss 0.04379. Accuracy 0.989.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22319932
Train loss (w/o reg) on all data: 0.21430273
Test loss (w/o reg) on all data: 0.15313584
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.622216e-05
Norm of the params: 13.339103
              Random: fixed   9 labels. Loss 0.15314. Accuracy 0.969.
### Flips: 208, rs: 25, checks: 156
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060663164
Train loss (w/o reg) on all data: 0.051217575
Test loss (w/o reg) on all data: 0.034997627
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.906318e-06
Norm of the params: 13.744519
     Influence (LOO): fixed  93 labels. Loss 0.03500. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012962259
Train loss (w/o reg) on all data: 0.005771796
Test loss (w/o reg) on all data: 0.026052544
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2910274e-06
Norm of the params: 11.99205
                Loss: fixed 109 labels. Loss 0.02605. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2185309
Train loss (w/o reg) on all data: 0.20950393
Test loss (w/o reg) on all data: 0.14788876
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.75188e-05
Norm of the params: 13.43649
              Random: fixed  14 labels. Loss 0.14789. Accuracy 0.969.
### Flips: 208, rs: 25, checks: 208
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043019235
Train loss (w/o reg) on all data: 0.0357645
Test loss (w/o reg) on all data: 0.024642132
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.282047e-06
Norm of the params: 12.045525
     Influence (LOO): fixed 103 labels. Loss 0.02464. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008440812
Train loss (w/o reg) on all data: 0.0031227567
Test loss (w/o reg) on all data: 0.023274308
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.1191647e-07
Norm of the params: 10.313152
                Loss: fixed 113 labels. Loss 0.02327. Accuracy 0.992.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21444826
Train loss (w/o reg) on all data: 0.20505333
Test loss (w/o reg) on all data: 0.1441584
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.5678164e-05
Norm of the params: 13.707609
              Random: fixed  16 labels. Loss 0.14416. Accuracy 0.966.
### Flips: 208, rs: 25, checks: 260
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015858604
Train loss (w/o reg) on all data: 0.010131729
Test loss (w/o reg) on all data: 0.014019067
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9257702e-06
Norm of the params: 10.702219
     Influence (LOO): fixed 113 labels. Loss 0.01402. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729846
Test loss (w/o reg) on all data: 0.012054475
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.17041225e-07
Norm of the params: 9.15318
                Loss: fixed 115 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20659183
Train loss (w/o reg) on all data: 0.19790903
Test loss (w/o reg) on all data: 0.13341641
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.929795e-05
Norm of the params: 13.177861
              Random: fixed  24 labels. Loss 0.13342. Accuracy 0.977.
### Flips: 208, rs: 25, checks: 312
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013471333
Train loss (w/o reg) on all data: 0.008455272
Test loss (w/o reg) on all data: 0.013946436
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9627525e-06
Norm of the params: 10.016049
     Influence (LOO): fixed 114 labels. Loss 0.01395. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729688
Test loss (w/o reg) on all data: 0.012055764
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6811249e-07
Norm of the params: 9.153196
                Loss: fixed 115 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19961625
Train loss (w/o reg) on all data: 0.19137281
Test loss (w/o reg) on all data: 0.12805863
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.5623285e-05
Norm of the params: 12.840125
              Random: fixed  29 labels. Loss 0.12806. Accuracy 0.981.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2084544
Train loss (w/o reg) on all data: 0.19970147
Test loss (w/o reg) on all data: 0.13613626
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.511487e-05
Norm of the params: 13.230968
Flipped loss: 0.13614. Accuracy: 0.977
### Flips: 208, rs: 26, checks: 52
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1317846
Train loss (w/o reg) on all data: 0.11998853
Test loss (w/o reg) on all data: 0.09751732
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.119008e-05
Norm of the params: 15.359735
     Influence (LOO): fixed  39 labels. Loss 0.09752. Accuracy 0.969.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08000112
Train loss (w/o reg) on all data: 0.063501224
Test loss (w/o reg) on all data: 0.08635136
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.4128365e-06
Norm of the params: 18.165846
                Loss: fixed  52 labels. Loss 0.08635. Accuracy 0.981.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20007066
Train loss (w/o reg) on all data: 0.19163716
Test loss (w/o reg) on all data: 0.13556857
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2877787e-05
Norm of the params: 12.987307
              Random: fixed   6 labels. Loss 0.13557. Accuracy 0.966.
### Flips: 208, rs: 26, checks: 104
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0789656
Train loss (w/o reg) on all data: 0.06924168
Test loss (w/o reg) on all data: 0.06417194
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.964459e-06
Norm of the params: 13.945551
     Influence (LOO): fixed  70 labels. Loss 0.06417. Accuracy 0.977.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029421404
Train loss (w/o reg) on all data: 0.017178647
Test loss (w/o reg) on all data: 0.030680027
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5146735e-06
Norm of the params: 15.647847
                Loss: fixed  84 labels. Loss 0.03068. Accuracy 0.989.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19040538
Train loss (w/o reg) on all data: 0.18198644
Test loss (w/o reg) on all data: 0.12418033
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.3403663e-05
Norm of the params: 12.976095
              Random: fixed  16 labels. Loss 0.12418. Accuracy 0.981.
### Flips: 208, rs: 26, checks: 156
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021225352
Train loss (w/o reg) on all data: 0.0125812525
Test loss (w/o reg) on all data: 0.024344737
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.3054224e-07
Norm of the params: 13.148459
     Influence (LOO): fixed  92 labels. Loss 0.02434. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01322545
Train loss (w/o reg) on all data: 0.0059512667
Test loss (w/o reg) on all data: 0.017455708
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4791566e-06
Norm of the params: 12.061662
                Loss: fixed  96 labels. Loss 0.01746. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18830608
Train loss (w/o reg) on all data: 0.1799987
Test loss (w/o reg) on all data: 0.12359921
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.2976823e-05
Norm of the params: 12.889826
              Random: fixed  17 labels. Loss 0.12360. Accuracy 0.981.
### Flips: 208, rs: 26, checks: 208
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010055935
Train loss (w/o reg) on all data: 0.005120527
Test loss (w/o reg) on all data: 0.014731407
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.824966e-07
Norm of the params: 9.935199
     Influence (LOO): fixed 102 labels. Loss 0.01473. Accuracy 0.992.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010354659
Train loss (w/o reg) on all data: 0.004282211
Test loss (w/o reg) on all data: 0.0140510695
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9440986e-07
Norm of the params: 11.020389
                Loss: fixed  99 labels. Loss 0.01405. Accuracy 0.996.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18558028
Train loss (w/o reg) on all data: 0.1772395
Test loss (w/o reg) on all data: 0.12553862
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.5145158e-05
Norm of the params: 12.915708
              Random: fixed  19 labels. Loss 0.12554. Accuracy 0.981.
### Flips: 208, rs: 26, checks: 260
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.012055028
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.880444e-08
Norm of the params: 9.153198
     Influence (LOO): fixed 103 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007961925
Train loss (w/o reg) on all data: 0.0030191187
Test loss (w/o reg) on all data: 0.010874631
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.721699e-07
Norm of the params: 9.942642
                Loss: fixed 102 labels. Loss 0.01087. Accuracy 0.996.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17617469
Train loss (w/o reg) on all data: 0.1680431
Test loss (w/o reg) on all data: 0.11474086
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.849821e-06
Norm of the params: 12.752707
              Random: fixed  26 labels. Loss 0.11474. Accuracy 0.981.
### Flips: 208, rs: 26, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730713
Test loss (w/o reg) on all data: 0.012055693
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9032137e-07
Norm of the params: 9.153083
     Influence (LOO): fixed 103 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007961924
Train loss (w/o reg) on all data: 0.0030190183
Test loss (w/o reg) on all data: 0.010874509
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.8032903e-07
Norm of the params: 9.942742
                Loss: fixed 102 labels. Loss 0.01087. Accuracy 0.996.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16419406
Train loss (w/o reg) on all data: 0.15593368
Test loss (w/o reg) on all data: 0.1108774
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.7060612e-05
Norm of the params: 12.853319
              Random: fixed  32 labels. Loss 0.11088. Accuracy 0.981.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2235809
Train loss (w/o reg) on all data: 0.21602759
Test loss (w/o reg) on all data: 0.12722674
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3033588e-05
Norm of the params: 12.2909
Flipped loss: 0.12723. Accuracy: 0.989
### Flips: 208, rs: 27, checks: 52
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13765712
Train loss (w/o reg) on all data: 0.12690237
Test loss (w/o reg) on all data: 0.0785392
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6055881e-05
Norm of the params: 14.666117
     Influence (LOO): fixed  42 labels. Loss 0.07854. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1021098
Train loss (w/o reg) on all data: 0.08852195
Test loss (w/o reg) on all data: 0.06027892
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1991971e-05
Norm of the params: 16.485054
                Loss: fixed  52 labels. Loss 0.06028. Accuracy 0.989.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21919495
Train loss (w/o reg) on all data: 0.21110113
Test loss (w/o reg) on all data: 0.12661164
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4193656e-05
Norm of the params: 12.723064
              Random: fixed   3 labels. Loss 0.12661. Accuracy 0.985.
### Flips: 208, rs: 27, checks: 104
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09582839
Train loss (w/o reg) on all data: 0.084523134
Test loss (w/o reg) on all data: 0.06269957
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.647879e-06
Norm of the params: 15.036796
     Influence (LOO): fixed  62 labels. Loss 0.06270. Accuracy 0.992.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015696727
Train loss (w/o reg) on all data: 0.007251429
Test loss (w/o reg) on all data: 0.017215213
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1240495e-06
Norm of the params: 12.996383
                Loss: fixed  96 labels. Loss 0.01722. Accuracy 0.989.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21123604
Train loss (w/o reg) on all data: 0.2028585
Test loss (w/o reg) on all data: 0.12544948
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.743754e-06
Norm of the params: 12.944146
              Random: fixed   8 labels. Loss 0.12545. Accuracy 0.989.
### Flips: 208, rs: 27, checks: 156
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047685027
Train loss (w/o reg) on all data: 0.040549196
Test loss (w/o reg) on all data: 0.031848002
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8575913e-06
Norm of the params: 11.946407
     Influence (LOO): fixed  87 labels. Loss 0.03185. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012229864
Train loss (w/o reg) on all data: 0.005322286
Test loss (w/o reg) on all data: 0.015296181
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2950793e-07
Norm of the params: 11.753789
                Loss: fixed  98 labels. Loss 0.01530. Accuracy 0.992.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20322424
Train loss (w/o reg) on all data: 0.1945783
Test loss (w/o reg) on all data: 0.11806648
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3645366e-05
Norm of the params: 13.149855
              Random: fixed  14 labels. Loss 0.11807. Accuracy 0.985.
### Flips: 208, rs: 27, checks: 208
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020609912
Train loss (w/o reg) on all data: 0.015112541
Test loss (w/o reg) on all data: 0.023310943
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.678795e-07
Norm of the params: 10.485582
     Influence (LOO): fixed  96 labels. Loss 0.02331. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00938946
Train loss (w/o reg) on all data: 0.0036656067
Test loss (w/o reg) on all data: 0.017823514
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3329361e-06
Norm of the params: 10.699396
                Loss: fixed  99 labels. Loss 0.01782. Accuracy 0.992.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19750728
Train loss (w/o reg) on all data: 0.18908444
Test loss (w/o reg) on all data: 0.10995461
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0963538e-05
Norm of the params: 12.979087
              Random: fixed  19 labels. Loss 0.10995. Accuracy 0.989.
### Flips: 208, rs: 27, checks: 260
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016548716
Train loss (w/o reg) on all data: 0.011264139
Test loss (w/o reg) on all data: 0.016452419
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9774355e-06
Norm of the params: 10.280639
     Influence (LOO): fixed  97 labels. Loss 0.01645. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009389462
Train loss (w/o reg) on all data: 0.0036658056
Test loss (w/o reg) on all data: 0.017823651
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8332363e-07
Norm of the params: 10.699212
                Loss: fixed  99 labels. Loss 0.01782. Accuracy 0.992.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1917074
Train loss (w/o reg) on all data: 0.18345587
Test loss (w/o reg) on all data: 0.10842821
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.2430948e-05
Norm of the params: 12.846432
              Random: fixed  23 labels. Loss 0.10843. Accuracy 0.985.
### Flips: 208, rs: 27, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729644
Test loss (w/o reg) on all data: 0.012054925
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0264116e-07
Norm of the params: 9.153201
     Influence (LOO): fixed 100 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009389462
Train loss (w/o reg) on all data: 0.0036659148
Test loss (w/o reg) on all data: 0.017822765
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9761603e-07
Norm of the params: 10.69911
                Loss: fixed  99 labels. Loss 0.01782. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18498796
Train loss (w/o reg) on all data: 0.17634758
Test loss (w/o reg) on all data: 0.10095004
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.46790735e-05
Norm of the params: 13.145628
              Random: fixed  27 labels. Loss 0.10095. Accuracy 0.989.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20572191
Train loss (w/o reg) on all data: 0.1978064
Test loss (w/o reg) on all data: 0.12224531
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.4556647e-06
Norm of the params: 12.58214
Flipped loss: 0.12225. Accuracy: 0.981
### Flips: 208, rs: 28, checks: 52
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122041225
Train loss (w/o reg) on all data: 0.11188817
Test loss (w/o reg) on all data: 0.08581672
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5956044e-05
Norm of the params: 14.24995
     Influence (LOO): fixed  41 labels. Loss 0.08582. Accuracy 0.973.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06669383
Train loss (w/o reg) on all data: 0.052849952
Test loss (w/o reg) on all data: 0.057848066
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1027318e-05
Norm of the params: 16.639637
                Loss: fixed  52 labels. Loss 0.05785. Accuracy 0.985.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1980505
Train loss (w/o reg) on all data: 0.18997225
Test loss (w/o reg) on all data: 0.11624675
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6251786e-05
Norm of the params: 12.710826
              Random: fixed   4 labels. Loss 0.11625. Accuracy 0.985.
### Flips: 208, rs: 28, checks: 104
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05905831
Train loss (w/o reg) on all data: 0.049584884
Test loss (w/o reg) on all data: 0.037070673
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2460217e-05
Norm of the params: 13.764756
     Influence (LOO): fixed  68 labels. Loss 0.03707. Accuracy 0.989.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014950378
Train loss (w/o reg) on all data: 0.0070591928
Test loss (w/o reg) on all data: 0.0139867775
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.169633e-06
Norm of the params: 12.56279
                Loss: fixed  82 labels. Loss 0.01399. Accuracy 0.992.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19390734
Train loss (w/o reg) on all data: 0.18601294
Test loss (w/o reg) on all data: 0.10728683
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.03302e-05
Norm of the params: 12.565346
              Random: fixed   9 labels. Loss 0.10729. Accuracy 0.981.
### Flips: 208, rs: 28, checks: 156
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031195233
Train loss (w/o reg) on all data: 0.023216957
Test loss (w/o reg) on all data: 0.01742423
Train acc on all data:  0.994269340974212
Test acc on all data:   1.0
Norm of the mean of gradients: 9.19003e-07
Norm of the params: 12.631925
     Influence (LOO): fixed  84 labels. Loss 0.01742. Accuracy 1.000.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013425242
Train loss (w/o reg) on all data: 0.005813743
Test loss (w/o reg) on all data: 0.010314323
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3638381e-06
Norm of the params: 12.338153
                Loss: fixed  84 labels. Loss 0.01031. Accuracy 0.996.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18588029
Train loss (w/o reg) on all data: 0.17780724
Test loss (w/o reg) on all data: 0.10324832
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.020345e-05
Norm of the params: 12.706724
              Random: fixed  13 labels. Loss 0.10325. Accuracy 0.977.
### Flips: 208, rs: 28, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007247484
Train loss (w/o reg) on all data: 0.002613077
Test loss (w/o reg) on all data: 0.011079491
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.670169e-08
Norm of the params: 9.627468
     Influence (LOO): fixed  93 labels. Loss 0.01108. Accuracy 0.996.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010025831
Train loss (w/o reg) on all data: 0.004013357
Test loss (w/o reg) on all data: 0.010993759
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7201085e-07
Norm of the params: 10.965833
                Loss: fixed  87 labels. Loss 0.01099. Accuracy 0.996.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17295453
Train loss (w/o reg) on all data: 0.16511627
Test loss (w/o reg) on all data: 0.093028896
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.4084114e-05
Norm of the params: 12.520592
              Random: fixed  20 labels. Loss 0.09303. Accuracy 0.985.
### Flips: 208, rs: 28, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730142
Test loss (w/o reg) on all data: 0.012055122
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0680891e-07
Norm of the params: 9.153145
     Influence (LOO): fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008997532
Train loss (w/o reg) on all data: 0.0035537835
Test loss (w/o reg) on all data: 0.012286974
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4660745e-07
Norm of the params: 10.434317
                Loss: fixed  89 labels. Loss 0.01229. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1693024
Train loss (w/o reg) on all data: 0.16137056
Test loss (w/o reg) on all data: 0.08871074
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0717896e-05
Norm of the params: 12.595118
              Random: fixed  22 labels. Loss 0.08871. Accuracy 0.989.
### Flips: 208, rs: 28, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729684
Test loss (w/o reg) on all data: 0.012054617
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9367542e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  94 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00859732
Train loss (w/o reg) on all data: 0.0032919683
Test loss (w/o reg) on all data: 0.013554999
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0466941e-06
Norm of the params: 10.300827
                Loss: fixed  90 labels. Loss 0.01355. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1598897
Train loss (w/o reg) on all data: 0.15187712
Test loss (w/o reg) on all data: 0.08355255
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.496614e-05
Norm of the params: 12.659053
              Random: fixed  28 labels. Loss 0.08355. Accuracy 0.989.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22933923
Train loss (w/o reg) on all data: 0.22068554
Test loss (w/o reg) on all data: 0.156905
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7719958e-05
Norm of the params: 13.15575
Flipped loss: 0.15690. Accuracy: 0.981
### Flips: 208, rs: 29, checks: 52
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1681826
Train loss (w/o reg) on all data: 0.15854399
Test loss (w/o reg) on all data: 0.12784573
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.4238576e-05
Norm of the params: 13.884242
     Influence (LOO): fixed  36 labels. Loss 0.12785. Accuracy 0.977.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110881336
Train loss (w/o reg) on all data: 0.09645599
Test loss (w/o reg) on all data: 0.10089147
Train acc on all data:  0.958930276981853
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.6613717e-06
Norm of the params: 16.98549
                Loss: fixed  51 labels. Loss 0.10089. Accuracy 0.950.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21914966
Train loss (w/o reg) on all data: 0.21047927
Test loss (w/o reg) on all data: 0.14393371
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6409334e-05
Norm of the params: 13.168439
              Random: fixed   9 labels. Loss 0.14393. Accuracy 0.977.
### Flips: 208, rs: 29, checks: 104
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10443565
Train loss (w/o reg) on all data: 0.095721684
Test loss (w/o reg) on all data: 0.080882326
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0539273e-05
Norm of the params: 13.201489
     Influence (LOO): fixed  70 labels. Loss 0.08088. Accuracy 0.985.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030623958
Train loss (w/o reg) on all data: 0.018504117
Test loss (w/o reg) on all data: 0.026685197
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2150567e-06
Norm of the params: 15.5690975
                Loss: fixed  95 labels. Loss 0.02669. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2133639
Train loss (w/o reg) on all data: 0.20486708
Test loss (w/o reg) on all data: 0.13716157
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1647995e-05
Norm of the params: 13.035965
              Random: fixed  14 labels. Loss 0.13716. Accuracy 0.977.
### Flips: 208, rs: 29, checks: 156
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045006074
Train loss (w/o reg) on all data: 0.03693953
Test loss (w/o reg) on all data: 0.03362672
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.042492e-06
Norm of the params: 12.701608
     Influence (LOO): fixed  97 labels. Loss 0.03363. Accuracy 0.996.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011110352
Train loss (w/o reg) on all data: 0.0049207513
Test loss (w/o reg) on all data: 0.015925441
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6143562e-06
Norm of the params: 11.126186
                Loss: fixed 107 labels. Loss 0.01593. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21185023
Train loss (w/o reg) on all data: 0.20379771
Test loss (w/o reg) on all data: 0.13646387
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.1553736e-05
Norm of the params: 12.690557
              Random: fixed  16 labels. Loss 0.13646. Accuracy 0.977.
### Flips: 208, rs: 29, checks: 208
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020014396
Train loss (w/o reg) on all data: 0.013990406
Test loss (w/o reg) on all data: 0.017195435
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3508196e-06
Norm of the params: 10.976329
     Influence (LOO): fixed 107 labels. Loss 0.01720. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01102487
Train loss (w/o reg) on all data: 0.0048076278
Test loss (w/o reg) on all data: 0.014995461
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.101076e-07
Norm of the params: 11.151002
                Loss: fixed 108 labels. Loss 0.01500. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20725472
Train loss (w/o reg) on all data: 0.19943482
Test loss (w/o reg) on all data: 0.13171498
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.021781e-05
Norm of the params: 12.5059185
              Random: fixed  21 labels. Loss 0.13171. Accuracy 0.977.
### Flips: 208, rs: 29, checks: 260
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011766944
Train loss (w/o reg) on all data: 0.0056686653
Test loss (w/o reg) on all data: 0.0092123
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.9259727e-07
Norm of the params: 11.043803
     Influence (LOO): fixed 109 labels. Loss 0.00921. Accuracy 0.996.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729278
Test loss (w/o reg) on all data: 0.012054985
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5850796e-07
Norm of the params: 9.15324
                Loss: fixed 111 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.200034
Train loss (w/o reg) on all data: 0.19236277
Test loss (w/o reg) on all data: 0.124670215
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.351112e-05
Norm of the params: 12.386469
              Random: fixed  26 labels. Loss 0.12467. Accuracy 0.977.
### Flips: 208, rs: 29, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730636
Test loss (w/o reg) on all data: 0.0120546
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2819043e-07
Norm of the params: 9.153091
     Influence (LOO): fixed 111 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002173061
Test loss (w/o reg) on all data: 0.012054699
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1823393e-07
Norm of the params: 9.153094
                Loss: fixed 111 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19560714
Train loss (w/o reg) on all data: 0.18786763
Test loss (w/o reg) on all data: 0.12004373
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.6598975e-05
Norm of the params: 12.44147
              Random: fixed  29 labels. Loss 0.12004. Accuracy 0.981.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21515666
Train loss (w/o reg) on all data: 0.2078655
Test loss (w/o reg) on all data: 0.13624473
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0334254e-05
Norm of the params: 12.075726
Flipped loss: 0.13624. Accuracy: 0.977
### Flips: 208, rs: 30, checks: 52
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13216
Train loss (w/o reg) on all data: 0.12247003
Test loss (w/o reg) on all data: 0.094497845
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.3474636e-05
Norm of the params: 13.92118
     Influence (LOO): fixed  40 labels. Loss 0.09450. Accuracy 0.985.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08590748
Train loss (w/o reg) on all data: 0.0726042
Test loss (w/o reg) on all data: 0.09124671
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.32238065e-05
Norm of the params: 16.31152
                Loss: fixed  52 labels. Loss 0.09125. Accuracy 0.962.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20342897
Train loss (w/o reg) on all data: 0.19581771
Test loss (w/o reg) on all data: 0.1292254
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.287852e-05
Norm of the params: 12.337958
              Random: fixed   7 labels. Loss 0.12923. Accuracy 0.981.
### Flips: 208, rs: 30, checks: 104
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07810957
Train loss (w/o reg) on all data: 0.06704301
Test loss (w/o reg) on all data: 0.07256879
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.7703425e-06
Norm of the params: 14.877206
     Influence (LOO): fixed  62 labels. Loss 0.07257. Accuracy 0.981.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01115608
Train loss (w/o reg) on all data: 0.0047606328
Test loss (w/o reg) on all data: 0.011579606
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.224291e-07
Norm of the params: 11.309685
                Loss: fixed  91 labels. Loss 0.01158. Accuracy 0.996.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19342183
Train loss (w/o reg) on all data: 0.18590799
Test loss (w/o reg) on all data: 0.12496643
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0572504e-05
Norm of the params: 12.258736
              Random: fixed  12 labels. Loss 0.12497. Accuracy 0.977.
### Flips: 208, rs: 30, checks: 156
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03112477
Train loss (w/o reg) on all data: 0.022718893
Test loss (w/o reg) on all data: 0.022951849
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5512883e-06
Norm of the params: 12.966017
     Influence (LOO): fixed  84 labels. Loss 0.02295. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077833827
Train loss (w/o reg) on all data: 0.0028982572
Test loss (w/o reg) on all data: 0.01200112
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4392027e-07
Norm of the params: 9.884458
                Loss: fixed  94 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18464635
Train loss (w/o reg) on all data: 0.17688711
Test loss (w/o reg) on all data: 0.12257866
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0613588e-05
Norm of the params: 12.457318
              Random: fixed  18 labels. Loss 0.12258. Accuracy 0.973.
### Flips: 208, rs: 30, checks: 208
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020229727
Train loss (w/o reg) on all data: 0.014105832
Test loss (w/o reg) on all data: 0.017197443
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.862211e-07
Norm of the params: 11.066974
     Influence (LOO): fixed  90 labels. Loss 0.01720. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007783384
Train loss (w/o reg) on all data: 0.0028983715
Test loss (w/o reg) on all data: 0.012002357
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.226145e-07
Norm of the params: 9.884344
                Loss: fixed  94 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18251401
Train loss (w/o reg) on all data: 0.1745681
Test loss (w/o reg) on all data: 0.123631895
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1035397e-05
Norm of the params: 12.606281
              Random: fixed  20 labels. Loss 0.12363. Accuracy 0.977.
### Flips: 208, rs: 30, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002352
Train loss (w/o reg) on all data: 0.0054477034
Test loss (w/o reg) on all data: 0.012244485
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.2458884e-07
Norm of the params: 9.544265
     Influence (LOO): fixed  94 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172946
Test loss (w/o reg) on all data: 0.01205499
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.421665e-07
Norm of the params: 9.153219
                Loss: fixed  95 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17512737
Train loss (w/o reg) on all data: 0.16752878
Test loss (w/o reg) on all data: 0.11406956
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.869139e-05
Norm of the params: 12.327692
              Random: fixed  25 labels. Loss 0.11407. Accuracy 0.981.
### Flips: 208, rs: 30, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729576
Test loss (w/o reg) on all data: 0.012054832
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.030157e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  95 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729583
Test loss (w/o reg) on all data: 0.01205476
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5356084e-07
Norm of the params: 9.153206
                Loss: fixed  95 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16514717
Train loss (w/o reg) on all data: 0.15763538
Test loss (w/o reg) on all data: 0.10792829
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.22755e-06
Norm of the params: 12.257072
              Random: fixed  30 labels. Loss 0.10793. Accuracy 0.977.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21678115
Train loss (w/o reg) on all data: 0.20876306
Test loss (w/o reg) on all data: 0.14424257
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.069144e-05
Norm of the params: 12.663403
Flipped loss: 0.14424. Accuracy: 0.973
### Flips: 208, rs: 31, checks: 52
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13678929
Train loss (w/o reg) on all data: 0.1270776
Test loss (w/o reg) on all data: 0.10411302
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2510461e-05
Norm of the params: 13.936786
     Influence (LOO): fixed  39 labels. Loss 0.10411. Accuracy 0.966.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09264777
Train loss (w/o reg) on all data: 0.077131435
Test loss (w/o reg) on all data: 0.10418932
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2022398e-05
Norm of the params: 17.616093
                Loss: fixed  51 labels. Loss 0.10419. Accuracy 0.962.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20917518
Train loss (w/o reg) on all data: 0.2008177
Test loss (w/o reg) on all data: 0.14018764
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.5038618e-05
Norm of the params: 12.92863
              Random: fixed   4 labels. Loss 0.14019. Accuracy 0.962.
### Flips: 208, rs: 31, checks: 104
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09000331
Train loss (w/o reg) on all data: 0.07978438
Test loss (w/o reg) on all data: 0.07394736
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.524393e-06
Norm of the params: 14.296106
     Influence (LOO): fixed  66 labels. Loss 0.07395. Accuracy 0.973.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023812678
Train loss (w/o reg) on all data: 0.013650422
Test loss (w/o reg) on all data: 0.027277127
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8281331e-06
Norm of the params: 14.256406
                Loss: fixed  92 labels. Loss 0.02728. Accuracy 0.989.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20006035
Train loss (w/o reg) on all data: 0.19171335
Test loss (w/o reg) on all data: 0.13116486
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.011258e-05
Norm of the params: 12.920528
              Random: fixed  10 labels. Loss 0.13116. Accuracy 0.966.
### Flips: 208, rs: 31, checks: 156
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0438791
Train loss (w/o reg) on all data: 0.034289047
Test loss (w/o reg) on all data: 0.033413682
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.994902e-06
Norm of the params: 13.849228
     Influence (LOO): fixed  91 labels. Loss 0.03341. Accuracy 0.996.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014471242
Train loss (w/o reg) on all data: 0.006671209
Test loss (w/o reg) on all data: 0.014809471
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.7898375e-07
Norm of the params: 12.490023
                Loss: fixed 101 labels. Loss 0.01481. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19382453
Train loss (w/o reg) on all data: 0.18553257
Test loss (w/o reg) on all data: 0.11750657
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.6611007e-05
Norm of the params: 12.877856
              Random: fixed  16 labels. Loss 0.11751. Accuracy 0.973.
### Flips: 208, rs: 31, checks: 208
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014138761
Train loss (w/o reg) on all data: 0.007958392
Test loss (w/o reg) on all data: 0.013346332
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7114238e-06
Norm of the params: 11.117887
     Influence (LOO): fixed 105 labels. Loss 0.01335. Accuracy 0.996.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011093647
Train loss (w/o reg) on all data: 0.004518873
Test loss (w/o reg) on all data: 0.010212055
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7139262e-07
Norm of the params: 11.467148
                Loss: fixed 104 labels. Loss 0.01021. Accuracy 0.996.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1919108
Train loss (w/o reg) on all data: 0.18378039
Test loss (w/o reg) on all data: 0.113320194
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.3179032e-05
Norm of the params: 12.751793
              Random: fixed  20 labels. Loss 0.11332. Accuracy 0.977.
### Flips: 208, rs: 31, checks: 260
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009260062
Train loss (w/o reg) on all data: 0.004402036
Test loss (w/o reg) on all data: 0.012441017
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5055707e-07
Norm of the params: 9.857004
     Influence (LOO): fixed 107 labels. Loss 0.01244. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00923354
Train loss (w/o reg) on all data: 0.0035736267
Test loss (w/o reg) on all data: 0.011999169
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.2634883e-07
Norm of the params: 10.639468
                Loss: fixed 105 labels. Loss 0.01200. Accuracy 0.992.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18474375
Train loss (w/o reg) on all data: 0.17723277
Test loss (w/o reg) on all data: 0.09588685
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2318734e-05
Norm of the params: 12.256411
              Random: fixed  29 labels. Loss 0.09589. Accuracy 0.981.
### Flips: 208, rs: 31, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729616
Test loss (w/o reg) on all data: 0.0120553775
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4390567e-07
Norm of the params: 9.153203
     Influence (LOO): fixed 108 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007829886
Train loss (w/o reg) on all data: 0.0028234299
Test loss (w/o reg) on all data: 0.01240827
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2840115e-07
Norm of the params: 10.0064535
                Loss: fixed 106 labels. Loss 0.01241. Accuracy 0.992.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17520118
Train loss (w/o reg) on all data: 0.16770414
Test loss (w/o reg) on all data: 0.09292128
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.369081e-05
Norm of the params: 12.245032
              Random: fixed  35 labels. Loss 0.09292. Accuracy 0.985.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2153861
Train loss (w/o reg) on all data: 0.20761906
Test loss (w/o reg) on all data: 0.13760003
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.153708e-05
Norm of the params: 12.463579
Flipped loss: 0.13760. Accuracy: 0.966
### Flips: 208, rs: 32, checks: 52
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13773714
Train loss (w/o reg) on all data: 0.12733306
Test loss (w/o reg) on all data: 0.10819311
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.6233846e-05
Norm of the params: 14.42503
     Influence (LOO): fixed  39 labels. Loss 0.10819. Accuracy 0.962.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09327924
Train loss (w/o reg) on all data: 0.07891373
Test loss (w/o reg) on all data: 0.122806124
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 7.615774e-06
Norm of the params: 16.950226
                Loss: fixed  52 labels. Loss 0.12281. Accuracy 0.939.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20710891
Train loss (w/o reg) on all data: 0.19901241
Test loss (w/o reg) on all data: 0.12964542
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.1102434e-05
Norm of the params: 12.72517
              Random: fixed   5 labels. Loss 0.12965. Accuracy 0.966.
### Flips: 208, rs: 32, checks: 104
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07834821
Train loss (w/o reg) on all data: 0.06761377
Test loss (w/o reg) on all data: 0.0642442
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.5655823e-06
Norm of the params: 14.652262
     Influence (LOO): fixed  68 labels. Loss 0.06424. Accuracy 0.985.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02144089
Train loss (w/o reg) on all data: 0.01147156
Test loss (w/o reg) on all data: 0.044231195
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3912864e-06
Norm of the params: 14.120432
                Loss: fixed  92 labels. Loss 0.04423. Accuracy 0.989.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19687817
Train loss (w/o reg) on all data: 0.188142
Test loss (w/o reg) on all data: 0.1194559
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.7396806e-05
Norm of the params: 13.218291
              Random: fixed  13 labels. Loss 0.11946. Accuracy 0.969.
### Flips: 208, rs: 32, checks: 156
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036158495
Train loss (w/o reg) on all data: 0.02612429
Test loss (w/o reg) on all data: 0.036253452
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2696762e-06
Norm of the params: 14.166302
     Influence (LOO): fixed  91 labels. Loss 0.03625. Accuracy 0.992.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014449154
Train loss (w/o reg) on all data: 0.006462548
Test loss (w/o reg) on all data: 0.019278953
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0187265e-06
Norm of the params: 12.638517
                Loss: fixed  97 labels. Loss 0.01928. Accuracy 0.989.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1876749
Train loss (w/o reg) on all data: 0.1788675
Test loss (w/o reg) on all data: 0.11438469
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.018342e-05
Norm of the params: 13.272074
              Random: fixed  18 labels. Loss 0.11438. Accuracy 0.981.
### Flips: 208, rs: 32, checks: 208
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02629283
Train loss (w/o reg) on all data: 0.018260334
Test loss (w/o reg) on all data: 0.024307191
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.901385e-06
Norm of the params: 12.674777
     Influence (LOO): fixed  97 labels. Loss 0.02431. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012141865
Train loss (w/o reg) on all data: 0.005142549
Test loss (w/o reg) on all data: 0.012830979
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3953863e-07
Norm of the params: 11.831581
                Loss: fixed 100 labels. Loss 0.01283. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18365668
Train loss (w/o reg) on all data: 0.17596473
Test loss (w/o reg) on all data: 0.10657264
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.92599e-06
Norm of the params: 12.403182
              Random: fixed  23 labels. Loss 0.10657. Accuracy 0.989.
### Flips: 208, rs: 32, checks: 260
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017210424
Train loss (w/o reg) on all data: 0.011103486
Test loss (w/o reg) on all data: 0.0191965
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1692483e-06
Norm of the params: 11.051641
     Influence (LOO): fixed 100 labels. Loss 0.01920. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007961923
Train loss (w/o reg) on all data: 0.0030192102
Test loss (w/o reg) on all data: 0.010875608
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.647694e-07
Norm of the params: 9.942548
                Loss: fixed 102 labels. Loss 0.01088. Accuracy 0.996.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17640018
Train loss (w/o reg) on all data: 0.16856185
Test loss (w/o reg) on all data: 0.106523134
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.352231e-06
Norm of the params: 12.52065
              Random: fixed  27 labels. Loss 0.10652. Accuracy 0.973.
### Flips: 208, rs: 32, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729786
Test loss (w/o reg) on all data: 0.012055502
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5058134e-07
Norm of the params: 9.153185
     Influence (LOO): fixed 103 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007961923
Train loss (w/o reg) on all data: 0.0030190947
Test loss (w/o reg) on all data: 0.010874359
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.540266e-07
Norm of the params: 9.942664
                Loss: fixed 102 labels. Loss 0.01087. Accuracy 0.996.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17640015
Train loss (w/o reg) on all data: 0.16856028
Test loss (w/o reg) on all data: 0.10653752
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4817584e-05
Norm of the params: 12.521883
              Random: fixed  27 labels. Loss 0.10654. Accuracy 0.973.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19357626
Train loss (w/o reg) on all data: 0.1851501
Test loss (w/o reg) on all data: 0.11625701
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.063428e-05
Norm of the params: 12.981649
Flipped loss: 0.11626. Accuracy: 0.966
### Flips: 208, rs: 33, checks: 52
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10742146
Train loss (w/o reg) on all data: 0.09778712
Test loss (w/o reg) on all data: 0.0885934
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.9473102e-06
Norm of the params: 13.881165
     Influence (LOO): fixed  42 labels. Loss 0.08859. Accuracy 0.962.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06663034
Train loss (w/o reg) on all data: 0.05142026
Test loss (w/o reg) on all data: 0.09911288
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.6947274e-06
Norm of the params: 17.441376
                Loss: fixed  51 labels. Loss 0.09911. Accuracy 0.958.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19175306
Train loss (w/o reg) on all data: 0.18330236
Test loss (w/o reg) on all data: 0.11390676
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.5944986e-05
Norm of the params: 13.0005455
              Random: fixed   1 labels. Loss 0.11391. Accuracy 0.962.
### Flips: 208, rs: 33, checks: 104
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05065398
Train loss (w/o reg) on all data: 0.041622814
Test loss (w/o reg) on all data: 0.04480732
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7266345e-06
Norm of the params: 13.439616
     Influence (LOO): fixed  71 labels. Loss 0.04481. Accuracy 0.977.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018914517
Train loss (w/o reg) on all data: 0.0098438375
Test loss (w/o reg) on all data: 0.023816548
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.695036e-06
Norm of the params: 13.4689865
                Loss: fixed  80 labels. Loss 0.02382. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18712927
Train loss (w/o reg) on all data: 0.17854057
Test loss (w/o reg) on all data: 0.103755526
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9651128e-05
Norm of the params: 13.10626
              Random: fixed   7 labels. Loss 0.10376. Accuracy 0.981.
### Flips: 208, rs: 33, checks: 156
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01913977
Train loss (w/o reg) on all data: 0.012251179
Test loss (w/o reg) on all data: 0.014371697
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.547323e-07
Norm of the params: 11.737624
     Influence (LOO): fixed  84 labels. Loss 0.01437. Accuracy 0.996.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010432985
Train loss (w/o reg) on all data: 0.004482261
Test loss (w/o reg) on all data: 0.006142414
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.5692857e-07
Norm of the params: 10.909375
                Loss: fixed  86 labels. Loss 0.00614. Accuracy 0.996.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18340299
Train loss (w/o reg) on all data: 0.1747439
Test loss (w/o reg) on all data: 0.10364975
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.545761e-06
Norm of the params: 13.159843
              Random: fixed   9 labels. Loss 0.10365. Accuracy 0.973.
### Flips: 208, rs: 33, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730321
Test loss (w/o reg) on all data: 0.012056768
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1346625e-06
Norm of the params: 9.153128
     Influence (LOO): fixed  90 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006318257
Train loss (w/o reg) on all data: 0.0021987061
Test loss (w/o reg) on all data: 0.006432838
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.281446e-07
Norm of the params: 9.07695
                Loss: fixed  89 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18169029
Train loss (w/o reg) on all data: 0.17300163
Test loss (w/o reg) on all data: 0.101407774
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.4438337e-05
Norm of the params: 13.182303
              Random: fixed  11 labels. Loss 0.10141. Accuracy 0.977.
### Flips: 208, rs: 33, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730175
Test loss (w/o reg) on all data: 0.012055258
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5563536e-07
Norm of the params: 9.153143
     Influence (LOO): fixed  90 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063182577
Train loss (w/o reg) on all data: 0.0021986826
Test loss (w/o reg) on all data: 0.0064332676
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.12018434e-07
Norm of the params: 9.076977
                Loss: fixed  89 labels. Loss 0.00643. Accuracy 0.996.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17081426
Train loss (w/o reg) on all data: 0.16209747
Test loss (w/o reg) on all data: 0.09142921
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0058827e-05
Norm of the params: 13.203629
              Random: fixed  20 labels. Loss 0.09143. Accuracy 0.977.
### Flips: 208, rs: 33, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728566
Test loss (w/o reg) on all data: 0.012054896
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.4707286e-07
Norm of the params: 9.1533165
     Influence (LOO): fixed  90 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.00217286
Test loss (w/o reg) on all data: 0.012055075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2739563e-07
Norm of the params: 9.1533165
                Loss: fixed  90 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1658148
Train loss (w/o reg) on all data: 0.15747628
Test loss (w/o reg) on all data: 0.09349204
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.730455e-05
Norm of the params: 12.913972
              Random: fixed  23 labels. Loss 0.09349. Accuracy 0.973.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21376723
Train loss (w/o reg) on all data: 0.2052884
Test loss (w/o reg) on all data: 0.16361542
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3277106e-05
Norm of the params: 13.022167
Flipped loss: 0.16362. Accuracy: 0.969
### Flips: 208, rs: 34, checks: 52
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13790204
Train loss (w/o reg) on all data: 0.12747882
Test loss (w/o reg) on all data: 0.12096094
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.1270871e-05
Norm of the params: 14.43829
     Influence (LOO): fixed  36 labels. Loss 0.12096. Accuracy 0.969.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090263866
Train loss (w/o reg) on all data: 0.07379804
Test loss (w/o reg) on all data: 0.1054794
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.611775e-05
Norm of the params: 18.147081
                Loss: fixed  52 labels. Loss 0.10548. Accuracy 0.966.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1966931
Train loss (w/o reg) on all data: 0.1875669
Test loss (w/o reg) on all data: 0.15666464
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.485984e-05
Norm of the params: 13.510133
              Random: fixed  12 labels. Loss 0.15666. Accuracy 0.966.
### Flips: 208, rs: 34, checks: 104
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07727298
Train loss (w/o reg) on all data: 0.065870665
Test loss (w/o reg) on all data: 0.07985281
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.6254493e-06
Norm of the params: 15.101201
     Influence (LOO): fixed  66 labels. Loss 0.07985. Accuracy 0.977.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03005675
Train loss (w/o reg) on all data: 0.018632052
Test loss (w/o reg) on all data: 0.04032918
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.114769e-06
Norm of the params: 15.116017
                Loss: fixed  88 labels. Loss 0.04033. Accuracy 0.992.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1935663
Train loss (w/o reg) on all data: 0.18473342
Test loss (w/o reg) on all data: 0.15190803
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.0297902e-05
Norm of the params: 13.29127
              Random: fixed  16 labels. Loss 0.15191. Accuracy 0.966.
### Flips: 208, rs: 34, checks: 156
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050620798
Train loss (w/o reg) on all data: 0.039510313
Test loss (w/o reg) on all data: 0.06387079
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1323135e-06
Norm of the params: 14.906699
     Influence (LOO): fixed  80 labels. Loss 0.06387. Accuracy 0.977.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0118416175
Train loss (w/o reg) on all data: 0.004935762
Test loss (w/o reg) on all data: 0.021599693
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.3989096e-07
Norm of the params: 11.752324
                Loss: fixed  99 labels. Loss 0.02160. Accuracy 0.989.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18758687
Train loss (w/o reg) on all data: 0.1783739
Test loss (w/o reg) on all data: 0.14530465
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.9968691e-05
Norm of the params: 13.57422
              Random: fixed  20 labels. Loss 0.14530. Accuracy 0.958.
### Flips: 208, rs: 34, checks: 208
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032839023
Train loss (w/o reg) on all data: 0.023544237
Test loss (w/o reg) on all data: 0.041226897
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.2395252e-06
Norm of the params: 13.634357
     Influence (LOO): fixed  93 labels. Loss 0.04123. Accuracy 0.981.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011841617
Train loss (w/o reg) on all data: 0.0049354536
Test loss (w/o reg) on all data: 0.021600248
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.292244e-06
Norm of the params: 11.752585
                Loss: fixed  99 labels. Loss 0.02160. Accuracy 0.989.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18023147
Train loss (w/o reg) on all data: 0.17097287
Test loss (w/o reg) on all data: 0.13227555
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.5501686e-05
Norm of the params: 13.607791
              Random: fixed  26 labels. Loss 0.13228. Accuracy 0.962.
### Flips: 208, rs: 34, checks: 260
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0117885005
Train loss (w/o reg) on all data: 0.0064338287
Test loss (w/o reg) on all data: 0.0151586635
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.026244e-07
Norm of the params: 10.348596
     Influence (LOO): fixed 102 labels. Loss 0.01516. Accuracy 0.992.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011146607
Train loss (w/o reg) on all data: 0.0045808624
Test loss (w/o reg) on all data: 0.02033033
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5193408e-06
Norm of the params: 11.459271
                Loss: fixed 100 labels. Loss 0.02033. Accuracy 0.989.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17175002
Train loss (w/o reg) on all data: 0.1626169
Test loss (w/o reg) on all data: 0.12933362
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.1019463e-05
Norm of the params: 13.515279
              Random: fixed  33 labels. Loss 0.12933. Accuracy 0.966.
### Flips: 208, rs: 34, checks: 312
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002354
Train loss (w/o reg) on all data: 0.0054477863
Test loss (w/o reg) on all data: 0.012245991
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.9208507e-07
Norm of the params: 9.54418
     Influence (LOO): fixed 103 labels. Loss 0.01225. Accuracy 0.996.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071804468
Train loss (w/o reg) on all data: 0.0026112373
Test loss (w/o reg) on all data: 0.011525752
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4699465e-07
Norm of the params: 9.559508
                Loss: fixed 103 labels. Loss 0.01153. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16595417
Train loss (w/o reg) on all data: 0.15683998
Test loss (w/o reg) on all data: 0.12369772
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.4017873e-05
Norm of the params: 13.501255
              Random: fixed  37 labels. Loss 0.12370. Accuracy 0.962.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22840971
Train loss (w/o reg) on all data: 0.22106925
Test loss (w/o reg) on all data: 0.14358157
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.3179662e-05
Norm of the params: 12.116485
Flipped loss: 0.14358. Accuracy: 0.973
### Flips: 208, rs: 35, checks: 52
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15272103
Train loss (w/o reg) on all data: 0.14356096
Test loss (w/o reg) on all data: 0.08877265
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5167436e-05
Norm of the params: 13.535193
     Influence (LOO): fixed  43 labels. Loss 0.08877. Accuracy 0.985.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09862734
Train loss (w/o reg) on all data: 0.08407399
Test loss (w/o reg) on all data: 0.12211176
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 7.27451e-06
Norm of the params: 17.060684
                Loss: fixed  52 labels. Loss 0.12211. Accuracy 0.943.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22386351
Train loss (w/o reg) on all data: 0.21690956
Test loss (w/o reg) on all data: 0.1428194
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.4547038e-05
Norm of the params: 11.793184
              Random: fixed   6 labels. Loss 0.14282. Accuracy 0.973.
### Flips: 208, rs: 35, checks: 104
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09772858
Train loss (w/o reg) on all data: 0.089241385
Test loss (w/o reg) on all data: 0.046566132
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7425423e-06
Norm of the params: 13.02858
     Influence (LOO): fixed  72 labels. Loss 0.04657. Accuracy 0.992.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02478997
Train loss (w/o reg) on all data: 0.013995289
Test loss (w/o reg) on all data: 0.01757132
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.937844e-07
Norm of the params: 14.693319
                Loss: fixed  95 labels. Loss 0.01757. Accuracy 0.992.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21584941
Train loss (w/o reg) on all data: 0.20891492
Test loss (w/o reg) on all data: 0.12662728
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0586138e-05
Norm of the params: 11.776664
              Random: fixed  13 labels. Loss 0.12663. Accuracy 0.977.
### Flips: 208, rs: 35, checks: 156
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03911893
Train loss (w/o reg) on all data: 0.029549833
Test loss (w/o reg) on all data: 0.023471884
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.808485e-06
Norm of the params: 13.834087
     Influence (LOO): fixed  93 labels. Loss 0.02347. Accuracy 0.989.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010307571
Train loss (w/o reg) on all data: 0.004106897
Test loss (w/o reg) on all data: 0.014144687
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1589416e-07
Norm of the params: 11.136134
                Loss: fixed 103 labels. Loss 0.01414. Accuracy 0.992.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20598723
Train loss (w/o reg) on all data: 0.19926143
Test loss (w/o reg) on all data: 0.10770083
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.208337e-05
Norm of the params: 11.598104
              Random: fixed  19 labels. Loss 0.10770. Accuracy 0.992.
### Flips: 208, rs: 35, checks: 208
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0214835
Train loss (w/o reg) on all data: 0.014982042
Test loss (w/o reg) on all data: 0.01915778
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0207945e-06
Norm of the params: 11.403033
     Influence (LOO): fixed 101 labels. Loss 0.01916. Accuracy 0.989.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00872796
Train loss (w/o reg) on all data: 0.0033852016
Test loss (w/o reg) on all data: 0.011834851
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8057288e-07
Norm of the params: 10.337077
                Loss: fixed 104 labels. Loss 0.01183. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2023673
Train loss (w/o reg) on all data: 0.19533162
Test loss (w/o reg) on all data: 0.102949254
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.258016e-06
Norm of the params: 11.862284
              Random: fixed  21 labels. Loss 0.10295. Accuracy 0.992.
### Flips: 208, rs: 35, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007292861
Train loss (w/o reg) on all data: 0.002575783
Test loss (w/o reg) on all data: 0.011689991
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7770549e-07
Norm of the params: 9.712958
     Influence (LOO): fixed 105 labels. Loss 0.01169. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007292861
Train loss (w/o reg) on all data: 0.002575812
Test loss (w/o reg) on all data: 0.011689516
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6304844e-07
Norm of the params: 9.712929
                Loss: fixed 105 labels. Loss 0.01169. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19325767
Train loss (w/o reg) on all data: 0.18643518
Test loss (w/o reg) on all data: 0.091712065
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0843457e-05
Norm of the params: 11.681183
              Random: fixed  28 labels. Loss 0.09171. Accuracy 0.989.
### Flips: 208, rs: 35, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007292861
Train loss (w/o reg) on all data: 0.0025757637
Test loss (w/o reg) on all data: 0.011688778
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6214087e-07
Norm of the params: 9.712978
     Influence (LOO): fixed 105 labels. Loss 0.01169. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007292864
Train loss (w/o reg) on all data: 0.0025757668
Test loss (w/o reg) on all data: 0.011688748
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1463595e-07
Norm of the params: 9.712978
                Loss: fixed 105 labels. Loss 0.01169. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17847934
Train loss (w/o reg) on all data: 0.17131773
Test loss (w/o reg) on all data: 0.08490931
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4814931e-05
Norm of the params: 11.9679785
              Random: fixed  35 labels. Loss 0.08491. Accuracy 0.989.
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22079991
Train loss (w/o reg) on all data: 0.21136908
Test loss (w/o reg) on all data: 0.14679119
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.5593705e-06
Norm of the params: 13.733773
Flipped loss: 0.14679. Accuracy: 0.981
### Flips: 208, rs: 36, checks: 52
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14594066
Train loss (w/o reg) on all data: 0.13363157
Test loss (w/o reg) on all data: 0.104294315
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4112875e-05
Norm of the params: 15.690177
     Influence (LOO): fixed  39 labels. Loss 0.10429. Accuracy 0.973.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10258829
Train loss (w/o reg) on all data: 0.08587877
Test loss (w/o reg) on all data: 0.10578086
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.042163e-06
Norm of the params: 18.280878
                Loss: fixed  52 labels. Loss 0.10578. Accuracy 0.958.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21308087
Train loss (w/o reg) on all data: 0.20346715
Test loss (w/o reg) on all data: 0.13138075
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.4574106e-05
Norm of the params: 13.866308
              Random: fixed   6 labels. Loss 0.13138. Accuracy 0.985.
### Flips: 208, rs: 36, checks: 104
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09816861
Train loss (w/o reg) on all data: 0.08657136
Test loss (w/o reg) on all data: 0.0782736
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.793199e-06
Norm of the params: 15.229744
     Influence (LOO): fixed  64 labels. Loss 0.07827. Accuracy 0.977.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029747605
Train loss (w/o reg) on all data: 0.017371586
Test loss (w/o reg) on all data: 0.030622132
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7321025e-06
Norm of the params: 15.732781
                Loss: fixed  94 labels. Loss 0.03062. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.203917
Train loss (w/o reg) on all data: 0.19438308
Test loss (w/o reg) on all data: 0.12102069
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.2494885e-05
Norm of the params: 13.808635
              Random: fixed  13 labels. Loss 0.12102. Accuracy 0.981.
### Flips: 208, rs: 36, checks: 156
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058737352
Train loss (w/o reg) on all data: 0.048710342
Test loss (w/o reg) on all data: 0.06436967
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.3489886e-06
Norm of the params: 14.161221
     Influence (LOO): fixed  83 labels. Loss 0.06437. Accuracy 0.973.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015464731
Train loss (w/o reg) on all data: 0.0070043006
Test loss (w/o reg) on all data: 0.01157372
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5495591e-06
Norm of the params: 13.00802
                Loss: fixed 101 labels. Loss 0.01157. Accuracy 0.996.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19598722
Train loss (w/o reg) on all data: 0.1864399
Test loss (w/o reg) on all data: 0.11467143
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3260723e-05
Norm of the params: 13.818341
              Random: fixed  18 labels. Loss 0.11467. Accuracy 0.977.
### Flips: 208, rs: 36, checks: 208
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02585363
Train loss (w/o reg) on all data: 0.016904235
Test loss (w/o reg) on all data: 0.035380766
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.956415e-06
Norm of the params: 13.378636
     Influence (LOO): fixed  98 labels. Loss 0.03538. Accuracy 0.992.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012556648
Train loss (w/o reg) on all data: 0.005587277
Test loss (w/o reg) on all data: 0.012384451
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.025074e-07
Norm of the params: 11.806245
                Loss: fixed 104 labels. Loss 0.01238. Accuracy 0.996.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19454536
Train loss (w/o reg) on all data: 0.18477407
Test loss (w/o reg) on all data: 0.11532666
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4107813e-05
Norm of the params: 13.979478
              Random: fixed  19 labels. Loss 0.11533. Accuracy 0.985.
### Flips: 208, rs: 36, checks: 260
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489634
Train loss (w/o reg) on all data: 0.005539469
Test loss (w/o reg) on all data: 0.01384534
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.563605e-07
Norm of the params: 9.950041
     Influence (LOO): fixed 105 labels. Loss 0.01385. Accuracy 0.989.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009010771
Train loss (w/o reg) on all data: 0.0034767503
Test loss (w/o reg) on all data: 0.012949121
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6745735e-07
Norm of the params: 10.520476
                Loss: fixed 105 labels. Loss 0.01295. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19250642
Train loss (w/o reg) on all data: 0.18298885
Test loss (w/o reg) on all data: 0.11041519
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0083639e-05
Norm of the params: 13.796788
              Random: fixed  21 labels. Loss 0.11042. Accuracy 0.985.
### Flips: 208, rs: 36, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729479
Test loss (w/o reg) on all data: 0.012056493
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.925559e-07
Norm of the params: 9.153219
     Influence (LOO): fixed 106 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729523
Test loss (w/o reg) on all data: 0.012056025
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1897639e-06
Norm of the params: 9.1532135
                Loss: fixed 106 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18427189
Train loss (w/o reg) on all data: 0.1749678
Test loss (w/o reg) on all data: 0.10172612
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5726406e-05
Norm of the params: 13.641187
              Random: fixed  25 labels. Loss 0.10173. Accuracy 0.989.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20879802
Train loss (w/o reg) on all data: 0.20037833
Test loss (w/o reg) on all data: 0.1284005
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.5019488e-05
Norm of the params: 12.976666
Flipped loss: 0.12840. Accuracy: 0.977
### Flips: 208, rs: 37, checks: 52
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1312302
Train loss (w/o reg) on all data: 0.11871346
Test loss (w/o reg) on all data: 0.10094567
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.969658e-06
Norm of the params: 15.821977
     Influence (LOO): fixed  40 labels. Loss 0.10095. Accuracy 0.969.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08221001
Train loss (w/o reg) on all data: 0.06578014
Test loss (w/o reg) on all data: 0.08966521
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.431865e-06
Norm of the params: 18.127256
                Loss: fixed  52 labels. Loss 0.08967. Accuracy 0.969.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20753364
Train loss (w/o reg) on all data: 0.19907461
Test loss (w/o reg) on all data: 0.12435489
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.8447673e-05
Norm of the params: 13.006947
              Random: fixed   2 labels. Loss 0.12435. Accuracy 0.981.
### Flips: 208, rs: 37, checks: 104
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077426925
Train loss (w/o reg) on all data: 0.06479136
Test loss (w/o reg) on all data: 0.06641665
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.697257e-06
Norm of the params: 15.896896
     Influence (LOO): fixed  68 labels. Loss 0.06642. Accuracy 0.985.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017482698
Train loss (w/o reg) on all data: 0.008137951
Test loss (w/o reg) on all data: 0.024279892
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.2286065e-07
Norm of the params: 13.670953
                Loss: fixed  88 labels. Loss 0.02428. Accuracy 0.989.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19922327
Train loss (w/o reg) on all data: 0.19111668
Test loss (w/o reg) on all data: 0.11406962
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.147341e-05
Norm of the params: 12.733093
              Random: fixed  10 labels. Loss 0.11407. Accuracy 0.992.
### Flips: 208, rs: 37, checks: 156
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039911292
Train loss (w/o reg) on all data: 0.031542856
Test loss (w/o reg) on all data: 0.02234269
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2367683e-06
Norm of the params: 12.937105
     Influence (LOO): fixed  88 labels. Loss 0.02234. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013836842
Train loss (w/o reg) on all data: 0.0062595843
Test loss (w/o reg) on all data: 0.02562182
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5206284e-06
Norm of the params: 12.310369
                Loss: fixed  92 labels. Loss 0.02562. Accuracy 0.992.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19689645
Train loss (w/o reg) on all data: 0.18910389
Test loss (w/o reg) on all data: 0.10266603
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3350653e-05
Norm of the params: 12.484042
              Random: fixed  13 labels. Loss 0.10267. Accuracy 0.996.
### Flips: 208, rs: 37, checks: 208
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018346256
Train loss (w/o reg) on all data: 0.011920012
Test loss (w/o reg) on all data: 0.014264639
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7415173e-06
Norm of the params: 11.336882
     Influence (LOO): fixed  95 labels. Loss 0.01426. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010595834
Train loss (w/o reg) on all data: 0.0044023665
Test loss (w/o reg) on all data: 0.0230179
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7137034e-07
Norm of the params: 11.129661
                Loss: fixed  96 labels. Loss 0.02302. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18842511
Train loss (w/o reg) on all data: 0.17973521
Test loss (w/o reg) on all data: 0.10148053
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2591782e-05
Norm of the params: 13.183246
              Random: fixed  18 labels. Loss 0.10148. Accuracy 0.989.
### Flips: 208, rs: 37, checks: 260
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011224471
Train loss (w/o reg) on all data: 0.006245447
Test loss (w/o reg) on all data: 0.012816662
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.221377e-07
Norm of the params: 9.979002
     Influence (LOO): fixed  98 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728987
Test loss (w/o reg) on all data: 0.012055359
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8376633e-06
Norm of the params: 9.153274
                Loss: fixed  99 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17633706
Train loss (w/o reg) on all data: 0.16667977
Test loss (w/o reg) on all data: 0.10199254
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4214331e-05
Norm of the params: 13.897693
              Random: fixed  24 labels. Loss 0.10199. Accuracy 0.985.
### Flips: 208, rs: 37, checks: 312
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011224473
Train loss (w/o reg) on all data: 0.0062454618
Test loss (w/o reg) on all data: 0.012817585
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.986817e-07
Norm of the params: 9.978989
     Influence (LOO): fixed  98 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729239
Test loss (w/o reg) on all data: 0.012055105
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.809004e-07
Norm of the params: 9.153245
                Loss: fixed  99 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15194373
Train loss (w/o reg) on all data: 0.14135031
Test loss (w/o reg) on all data: 0.092093214
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.057475e-06
Norm of the params: 14.555696
              Random: fixed  35 labels. Loss 0.09209. Accuracy 0.992.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20351772
Train loss (w/o reg) on all data: 0.19496985
Test loss (w/o reg) on all data: 0.13748565
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1481722e-05
Norm of the params: 13.075065
Flipped loss: 0.13749. Accuracy: 0.962
### Flips: 208, rs: 38, checks: 52
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12817442
Train loss (w/o reg) on all data: 0.11739442
Test loss (w/o reg) on all data: 0.120486856
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.954835e-06
Norm of the params: 14.6833315
     Influence (LOO): fixed  36 labels. Loss 0.12049. Accuracy 0.973.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07510505
Train loss (w/o reg) on all data: 0.06073414
Test loss (w/o reg) on all data: 0.08897528
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.4755926e-06
Norm of the params: 16.953413
                Loss: fixed  52 labels. Loss 0.08898. Accuracy 0.966.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19807506
Train loss (w/o reg) on all data: 0.18956663
Test loss (w/o reg) on all data: 0.1349713
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.854579e-06
Norm of the params: 13.044869
              Random: fixed   5 labels. Loss 0.13497. Accuracy 0.962.
### Flips: 208, rs: 38, checks: 104
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067877986
Train loss (w/o reg) on all data: 0.057501018
Test loss (w/o reg) on all data: 0.0949787
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.0781764e-06
Norm of the params: 14.406225
     Influence (LOO): fixed  68 labels. Loss 0.09498. Accuracy 0.977.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022819106
Train loss (w/o reg) on all data: 0.011622525
Test loss (w/o reg) on all data: 0.03982866
Train acc on all data:  1.0
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.0085855e-06
Norm of the params: 14.964344
                Loss: fixed  85 labels. Loss 0.03983. Accuracy 0.981.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19091727
Train loss (w/o reg) on all data: 0.18241085
Test loss (w/o reg) on all data: 0.13011716
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.5633992e-05
Norm of the params: 13.043326
              Random: fixed  10 labels. Loss 0.13012. Accuracy 0.966.
### Flips: 208, rs: 38, checks: 156
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0327548
Train loss (w/o reg) on all data: 0.0238521
Test loss (w/o reg) on all data: 0.04816254
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8322964e-06
Norm of the params: 13.343689
     Influence (LOO): fixed  86 labels. Loss 0.04816. Accuracy 0.989.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017181221
Train loss (w/o reg) on all data: 0.00804619
Test loss (w/o reg) on all data: 0.033112723
Train acc on all data:  1.0
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.622719e-07
Norm of the params: 13.51668
                Loss: fixed  91 labels. Loss 0.03311. Accuracy 0.981.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17880987
Train loss (w/o reg) on all data: 0.17012422
Test loss (w/o reg) on all data: 0.12134718
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.507069e-05
Norm of the params: 13.180027
              Random: fixed  17 labels. Loss 0.12135. Accuracy 0.969.
### Flips: 208, rs: 38, checks: 208
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019096224
Train loss (w/o reg) on all data: 0.011490933
Test loss (w/o reg) on all data: 0.036646128
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3983461e-06
Norm of the params: 12.333119
     Influence (LOO): fixed  93 labels. Loss 0.03665. Accuracy 0.985.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012694931
Train loss (w/o reg) on all data: 0.0055483654
Test loss (w/o reg) on all data: 0.021088589
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3979999e-06
Norm of the params: 11.955388
                Loss: fixed  95 labels. Loss 0.02109. Accuracy 0.989.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17183486
Train loss (w/o reg) on all data: 0.16282792
Test loss (w/o reg) on all data: 0.114068285
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.3699562e-05
Norm of the params: 13.4215765
              Random: fixed  20 labels. Loss 0.11407. Accuracy 0.977.
### Flips: 208, rs: 38, checks: 260
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0133455545
Train loss (w/o reg) on all data: 0.0069356905
Test loss (w/o reg) on all data: 0.019616172
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0278171e-06
Norm of the params: 11.322425
     Influence (LOO): fixed  97 labels. Loss 0.01962. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008839531
Train loss (w/o reg) on all data: 0.0033499086
Test loss (w/o reg) on all data: 0.02364473
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.994555e-07
Norm of the params: 10.4781885
                Loss: fixed  99 labels. Loss 0.02364. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17028572
Train loss (w/o reg) on all data: 0.1609087
Test loss (w/o reg) on all data: 0.10759183
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.269297e-06
Norm of the params: 13.694537
              Random: fixed  23 labels. Loss 0.10759. Accuracy 0.981.
### Flips: 208, rs: 38, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012054886
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7029322e-07
Norm of the params: 9.153169
     Influence (LOO): fixed 100 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008839531
Train loss (w/o reg) on all data: 0.0033499536
Test loss (w/o reg) on all data: 0.023644852
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.340665e-07
Norm of the params: 10.478147
                Loss: fixed  99 labels. Loss 0.02364. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16856289
Train loss (w/o reg) on all data: 0.15928948
Test loss (w/o reg) on all data: 0.103296615
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.003951e-06
Norm of the params: 13.618676
              Random: fixed  24 labels. Loss 0.10330. Accuracy 0.989.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21850389
Train loss (w/o reg) on all data: 0.20953448
Test loss (w/o reg) on all data: 0.14892648
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.1737592e-05
Norm of the params: 13.393591
Flipped loss: 0.14893. Accuracy: 0.966
### Flips: 208, rs: 39, checks: 52
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13485855
Train loss (w/o reg) on all data: 0.122853495
Test loss (w/o reg) on all data: 0.11553734
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.250924e-06
Norm of the params: 15.495191
     Influence (LOO): fixed  38 labels. Loss 0.11554. Accuracy 0.966.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089495495
Train loss (w/o reg) on all data: 0.073020175
Test loss (w/o reg) on all data: 0.089785665
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.847705e-06
Norm of the params: 18.152311
                Loss: fixed  52 labels. Loss 0.08979. Accuracy 0.966.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21666433
Train loss (w/o reg) on all data: 0.20805766
Test loss (w/o reg) on all data: 0.1353046
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 9.729692e-06
Norm of the params: 13.1199665
              Random: fixed   3 labels. Loss 0.13530. Accuracy 0.962.
### Flips: 208, rs: 39, checks: 104
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07852103
Train loss (w/o reg) on all data: 0.067018345
Test loss (w/o reg) on all data: 0.07494072
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.3065852e-05
Norm of the params: 15.167522
     Influence (LOO): fixed  70 labels. Loss 0.07494. Accuracy 0.973.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024021901
Train loss (w/o reg) on all data: 0.012686626
Test loss (w/o reg) on all data: 0.06761747
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.074021e-06
Norm of the params: 15.056743
                Loss: fixed  89 labels. Loss 0.06762. Accuracy 0.966.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20599258
Train loss (w/o reg) on all data: 0.19757669
Test loss (w/o reg) on all data: 0.118995726
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0111545e-05
Norm of the params: 12.973742
              Random: fixed  12 labels. Loss 0.11900. Accuracy 0.969.
### Flips: 208, rs: 39, checks: 156
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041301824
Train loss (w/o reg) on all data: 0.032426056
Test loss (w/o reg) on all data: 0.032241743
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.505998e-06
Norm of the params: 13.323488
     Influence (LOO): fixed  88 labels. Loss 0.03224. Accuracy 0.992.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014724091
Train loss (w/o reg) on all data: 0.00604128
Test loss (w/o reg) on all data: 0.04268236
Train acc on all data:  1.0
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9618e-06
Norm of the params: 13.177868
                Loss: fixed  95 labels. Loss 0.04268. Accuracy 0.977.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20445734
Train loss (w/o reg) on all data: 0.196559
Test loss (w/o reg) on all data: 0.118443154
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.7905647e-05
Norm of the params: 12.568493
              Random: fixed  15 labels. Loss 0.11844. Accuracy 0.966.
### Flips: 208, rs: 39, checks: 208
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025040872
Train loss (w/o reg) on all data: 0.018343283
Test loss (w/o reg) on all data: 0.022293232
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.257431e-06
Norm of the params: 11.573755
     Influence (LOO): fixed  95 labels. Loss 0.02229. Accuracy 0.996.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011871375
Train loss (w/o reg) on all data: 0.0048815017
Test loss (w/o reg) on all data: 0.02037136
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.698856e-06
Norm of the params: 11.823599
                Loss: fixed  97 labels. Loss 0.02037. Accuracy 0.989.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19910966
Train loss (w/o reg) on all data: 0.1910444
Test loss (w/o reg) on all data: 0.11035858
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.5915767e-05
Norm of the params: 12.700591
              Random: fixed  18 labels. Loss 0.11036. Accuracy 0.969.
### Flips: 208, rs: 39, checks: 260
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013405752
Train loss (w/o reg) on all data: 0.007833074
Test loss (w/o reg) on all data: 0.015720349
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.813425e-07
Norm of the params: 10.557158
     Influence (LOO): fixed  98 labels. Loss 0.01572. Accuracy 0.996.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01047097
Train loss (w/o reg) on all data: 0.0042007905
Test loss (w/o reg) on all data: 0.02191624
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3908513e-06
Norm of the params: 11.198374
                Loss: fixed  98 labels. Loss 0.02192. Accuracy 0.985.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19113171
Train loss (w/o reg) on all data: 0.18244027
Test loss (w/o reg) on all data: 0.1015858
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.2102624e-05
Norm of the params: 13.1844225
              Random: fixed  22 labels. Loss 0.10159. Accuracy 0.969.
### Flips: 208, rs: 39, checks: 312
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011165382
Train loss (w/o reg) on all data: 0.006348282
Test loss (w/o reg) on all data: 0.015150815
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.788106e-07
Norm of the params: 9.815396
     Influence (LOO): fixed  99 labels. Loss 0.01515. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009827385
Train loss (w/o reg) on all data: 0.0039160233
Test loss (w/o reg) on all data: 0.014778403
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.955541e-07
Norm of the params: 10.873235
                Loss: fixed  99 labels. Loss 0.01478. Accuracy 0.996.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1893052
Train loss (w/o reg) on all data: 0.1805875
Test loss (w/o reg) on all data: 0.10054017
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4182485e-05
Norm of the params: 13.204321
              Random: fixed  24 labels. Loss 0.10054. Accuracy 0.969.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22430567
Train loss (w/o reg) on all data: 0.2152988
Test loss (w/o reg) on all data: 0.18687734
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.404582e-05
Norm of the params: 13.421535
Flipped loss: 0.18688. Accuracy: 0.950
### Flips: 260, rs: 0, checks: 52
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17229286
Train loss (w/o reg) on all data: 0.16133884
Test loss (w/o reg) on all data: 0.12869333
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.02508175e-05
Norm of the params: 14.801362
     Influence (LOO): fixed  34 labels. Loss 0.12869. Accuracy 0.947.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11515253
Train loss (w/o reg) on all data: 0.099499695
Test loss (w/o reg) on all data: 0.15266393
Train acc on all data:  0.956064947468959
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 6.4375495e-06
Norm of the params: 17.693409
                Loss: fixed  50 labels. Loss 0.15266. Accuracy 0.950.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21578784
Train loss (w/o reg) on all data: 0.20695029
Test loss (w/o reg) on all data: 0.17736149
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.2150698e-05
Norm of the params: 13.294775
              Random: fixed   9 labels. Loss 0.17736. Accuracy 0.943.
### Flips: 260, rs: 0, checks: 104
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12515762
Train loss (w/o reg) on all data: 0.11393076
Test loss (w/o reg) on all data: 0.07327439
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4952379e-05
Norm of the params: 14.984563
     Influence (LOO): fixed  63 labels. Loss 0.07327. Accuracy 0.977.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053520072
Train loss (w/o reg) on all data: 0.03753608
Test loss (w/o reg) on all data: 0.085219145
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.6559937e-06
Norm of the params: 17.879593
                Loss: fixed  91 labels. Loss 0.08522. Accuracy 0.969.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20686838
Train loss (w/o reg) on all data: 0.19781528
Test loss (w/o reg) on all data: 0.17392063
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.3457783e-05
Norm of the params: 13.455926
              Random: fixed  15 labels. Loss 0.17392. Accuracy 0.947.
### Flips: 260, rs: 0, checks: 156
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08039542
Train loss (w/o reg) on all data: 0.0705545
Test loss (w/o reg) on all data: 0.049395006
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6075392e-05
Norm of the params: 14.029197
     Influence (LOO): fixed  89 labels. Loss 0.04940. Accuracy 0.985.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021634962
Train loss (w/o reg) on all data: 0.011224074
Test loss (w/o reg) on all data: 0.030632138
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9509727e-06
Norm of the params: 14.429752
                Loss: fixed 106 labels. Loss 0.03063. Accuracy 0.989.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19818981
Train loss (w/o reg) on all data: 0.18897146
Test loss (w/o reg) on all data: 0.1678421
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.1527065e-05
Norm of the params: 13.578183
              Random: fixed  24 labels. Loss 0.16784. Accuracy 0.950.
### Flips: 260, rs: 0, checks: 208
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027727844
Train loss (w/o reg) on all data: 0.018968606
Test loss (w/o reg) on all data: 0.02965388
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1958577e-06
Norm of the params: 13.235738
     Influence (LOO): fixed 112 labels. Loss 0.02965. Accuracy 0.985.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018421948
Train loss (w/o reg) on all data: 0.0089544
Test loss (w/o reg) on all data: 0.017601596
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.7830847e-07
Norm of the params: 13.760485
                Loss: fixed 112 labels. Loss 0.01760. Accuracy 0.989.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19139141
Train loss (w/o reg) on all data: 0.18189998
Test loss (w/o reg) on all data: 0.16127497
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.631357e-05
Norm of the params: 13.777824
              Random: fixed  30 labels. Loss 0.16127. Accuracy 0.958.
### Flips: 260, rs: 0, checks: 260
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01250721
Train loss (w/o reg) on all data: 0.006109429
Test loss (w/o reg) on all data: 0.016257102
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0181014e-06
Norm of the params: 11.311746
     Influence (LOO): fixed 120 labels. Loss 0.01626. Accuracy 0.996.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017637089
Train loss (w/o reg) on all data: 0.008535764
Test loss (w/o reg) on all data: 0.017723331
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2604207e-06
Norm of the params: 13.491718
                Loss: fixed 113 labels. Loss 0.01772. Accuracy 0.989.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18833365
Train loss (w/o reg) on all data: 0.17863244
Test loss (w/o reg) on all data: 0.15744455
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.8786448e-05
Norm of the params: 13.9292555
              Random: fixed  33 labels. Loss 0.15744. Accuracy 0.962.
### Flips: 260, rs: 0, checks: 312
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009357563
Train loss (w/o reg) on all data: 0.0039549484
Test loss (w/o reg) on all data: 0.011970226
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1272176e-07
Norm of the params: 10.39482
     Influence (LOO): fixed 122 labels. Loss 0.01197. Accuracy 0.992.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01436249
Train loss (w/o reg) on all data: 0.0066398527
Test loss (w/o reg) on all data: 0.018569816
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.214349e-07
Norm of the params: 12.427902
                Loss: fixed 116 labels. Loss 0.01857. Accuracy 0.989.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18392862
Train loss (w/o reg) on all data: 0.17424849
Test loss (w/o reg) on all data: 0.14637594
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.2706214e-05
Norm of the params: 13.91412
              Random: fixed  37 labels. Loss 0.14638. Accuracy 0.958.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23262009
Train loss (w/o reg) on all data: 0.22387885
Test loss (w/o reg) on all data: 0.152011
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.302611e-05
Norm of the params: 13.222138
Flipped loss: 0.15201. Accuracy: 0.962
### Flips: 260, rs: 1, checks: 52
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17021161
Train loss (w/o reg) on all data: 0.15965444
Test loss (w/o reg) on all data: 0.11088467
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.667006e-05
Norm of the params: 14.530776
     Influence (LOO): fixed  35 labels. Loss 0.11088. Accuracy 0.973.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1226592
Train loss (w/o reg) on all data: 0.10798993
Test loss (w/o reg) on all data: 0.108599395
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.787972e-06
Norm of the params: 17.128496
                Loss: fixed  52 labels. Loss 0.10860. Accuracy 0.966.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22963172
Train loss (w/o reg) on all data: 0.22066033
Test loss (w/o reg) on all data: 0.14743435
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.6773363e-05
Norm of the params: 13.395068
              Random: fixed   3 labels. Loss 0.14743. Accuracy 0.962.
### Flips: 260, rs: 1, checks: 104
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12526003
Train loss (w/o reg) on all data: 0.113098264
Test loss (w/o reg) on all data: 0.08560945
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4205034e-05
Norm of the params: 15.595999
     Influence (LOO): fixed  61 labels. Loss 0.08561. Accuracy 0.989.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036219567
Train loss (w/o reg) on all data: 0.022903467
Test loss (w/o reg) on all data: 0.029568465
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4455043e-06
Norm of the params: 16.319374
                Loss: fixed  98 labels. Loss 0.02957. Accuracy 0.992.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22336277
Train loss (w/o reg) on all data: 0.21459855
Test loss (w/o reg) on all data: 0.14094266
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.813857e-06
Norm of the params: 13.239498
              Random: fixed   8 labels. Loss 0.14094. Accuracy 0.966.
### Flips: 260, rs: 1, checks: 156
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069904566
Train loss (w/o reg) on all data: 0.057524938
Test loss (w/o reg) on all data: 0.058006175
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.350489e-06
Norm of the params: 15.735077
     Influence (LOO): fixed  88 labels. Loss 0.05801. Accuracy 0.985.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012305335
Train loss (w/o reg) on all data: 0.00553564
Test loss (w/o reg) on all data: 0.01567294
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.876143e-07
Norm of the params: 11.635889
                Loss: fixed 113 labels. Loss 0.01567. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21782732
Train loss (w/o reg) on all data: 0.20911986
Test loss (w/o reg) on all data: 0.12680373
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.2118823e-05
Norm of the params: 13.196562
              Random: fixed  14 labels. Loss 0.12680. Accuracy 0.973.
### Flips: 260, rs: 1, checks: 208
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03352479
Train loss (w/o reg) on all data: 0.023633664
Test loss (w/o reg) on all data: 0.031489816
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3812115e-06
Norm of the params: 14.0649395
     Influence (LOO): fixed 105 labels. Loss 0.03149. Accuracy 0.992.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008018332
Train loss (w/o reg) on all data: 0.0030734036
Test loss (w/o reg) on all data: 0.0125020025
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3434002e-07
Norm of the params: 9.944776
                Loss: fixed 115 labels. Loss 0.01250. Accuracy 0.992.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21259739
Train loss (w/o reg) on all data: 0.20340717
Test loss (w/o reg) on all data: 0.12218544
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.3995678e-05
Norm of the params: 13.557446
              Random: fixed  17 labels. Loss 0.12219. Accuracy 0.973.
### Flips: 260, rs: 1, checks: 260
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020542018
Train loss (w/o reg) on all data: 0.012886366
Test loss (w/o reg) on all data: 0.024219753
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6276307e-06
Norm of the params: 12.373885
     Influence (LOO): fixed 111 labels. Loss 0.02422. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008018332
Train loss (w/o reg) on all data: 0.0030733694
Test loss (w/o reg) on all data: 0.012502599
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.414705e-06
Norm of the params: 9.94481
                Loss: fixed 115 labels. Loss 0.01250. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20130932
Train loss (w/o reg) on all data: 0.1923891
Test loss (w/o reg) on all data: 0.113070235
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.4364172e-05
Norm of the params: 13.356815
              Random: fixed  24 labels. Loss 0.11307. Accuracy 0.977.
### Flips: 260, rs: 1, checks: 312
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014790939
Train loss (w/o reg) on all data: 0.009551074
Test loss (w/o reg) on all data: 0.0144604575
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0446703e-06
Norm of the params: 10.237056
     Influence (LOO): fixed 114 labels. Loss 0.01446. Accuracy 0.996.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008018331
Train loss (w/o reg) on all data: 0.0030734611
Test loss (w/o reg) on all data: 0.012502382
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.376485e-07
Norm of the params: 9.944717
                Loss: fixed 115 labels. Loss 0.01250. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19014594
Train loss (w/o reg) on all data: 0.18166418
Test loss (w/o reg) on all data: 0.108208865
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.132657e-06
Norm of the params: 13.024407
              Random: fixed  30 labels. Loss 0.10821. Accuracy 0.969.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24793644
Train loss (w/o reg) on all data: 0.24141775
Test loss (w/o reg) on all data: 0.1659854
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.0033236e-05
Norm of the params: 11.418134
Flipped loss: 0.16599. Accuracy: 0.966
### Flips: 260, rs: 2, checks: 52
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19661571
Train loss (w/o reg) on all data: 0.18682796
Test loss (w/o reg) on all data: 0.15918508
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.014305e-05
Norm of the params: 13.991244
     Influence (LOO): fixed  32 labels. Loss 0.15919. Accuracy 0.958.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1301905
Train loss (w/o reg) on all data: 0.11659927
Test loss (w/o reg) on all data: 0.12442622
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.5566135e-06
Norm of the params: 16.487112
                Loss: fixed  51 labels. Loss 0.12443. Accuracy 0.950.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24238676
Train loss (w/o reg) on all data: 0.23589346
Test loss (w/o reg) on all data: 0.15720056
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1196331e-05
Norm of the params: 11.395881
              Random: fixed   6 labels. Loss 0.15720. Accuracy 0.966.
### Flips: 260, rs: 2, checks: 104
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13850072
Train loss (w/o reg) on all data: 0.12887266
Test loss (w/o reg) on all data: 0.10973474
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.8776256e-05
Norm of the params: 13.87664
     Influence (LOO): fixed  67 labels. Loss 0.10973. Accuracy 0.969.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04879197
Train loss (w/o reg) on all data: 0.034279272
Test loss (w/o reg) on all data: 0.047639146
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.4980235e-06
Norm of the params: 17.03684
                Loss: fixed  96 labels. Loss 0.04764. Accuracy 0.973.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23812439
Train loss (w/o reg) on all data: 0.231564
Test loss (w/o reg) on all data: 0.15362215
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5228201e-05
Norm of the params: 11.454591
              Random: fixed   8 labels. Loss 0.15362. Accuracy 0.969.
### Flips: 260, rs: 2, checks: 156
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07925158
Train loss (w/o reg) on all data: 0.07004115
Test loss (w/o reg) on all data: 0.052131124
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.700337e-06
Norm of the params: 13.572346
     Influence (LOO): fixed  97 labels. Loss 0.05213. Accuracy 0.985.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024467675
Train loss (w/o reg) on all data: 0.014643665
Test loss (w/o reg) on all data: 0.026150841
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1494286e-06
Norm of the params: 14.017139
                Loss: fixed 113 labels. Loss 0.02615. Accuracy 0.985.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22851016
Train loss (w/o reg) on all data: 0.22112817
Test loss (w/o reg) on all data: 0.13379942
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.675257e-05
Norm of the params: 12.150711
              Random: fixed  15 labels. Loss 0.13380. Accuracy 0.977.
### Flips: 260, rs: 2, checks: 208
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02713056
Train loss (w/o reg) on all data: 0.020391207
Test loss (w/o reg) on all data: 0.023636581
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.8742235e-07
Norm of the params: 11.609782
     Influence (LOO): fixed 122 labels. Loss 0.02364. Accuracy 0.992.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017553745
Train loss (w/o reg) on all data: 0.008854413
Test loss (w/o reg) on all data: 0.015684577
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.0827226e-07
Norm of the params: 13.1904
                Loss: fixed 121 labels. Loss 0.01568. Accuracy 0.996.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2217126
Train loss (w/o reg) on all data: 0.21399643
Test loss (w/o reg) on all data: 0.12686542
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.8481998e-05
Norm of the params: 12.4227
              Random: fixed  22 labels. Loss 0.12687. Accuracy 0.981.
### Flips: 260, rs: 2, checks: 260
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008916484
Train loss (w/o reg) on all data: 0.0038530505
Test loss (w/o reg) on all data: 0.0120256115
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.8874146e-07
Norm of the params: 10.063234
     Influence (LOO): fixed 128 labels. Loss 0.01203. Accuracy 0.996.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015962403
Train loss (w/o reg) on all data: 0.007637069
Test loss (w/o reg) on all data: 0.014399054
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0382925e-06
Norm of the params: 12.903747
                Loss: fixed 123 labels. Loss 0.01440. Accuracy 0.996.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20874627
Train loss (w/o reg) on all data: 0.200712
Test loss (w/o reg) on all data: 0.12189213
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.3194872e-05
Norm of the params: 12.676174
              Random: fixed  29 labels. Loss 0.12189. Accuracy 0.981.
### Flips: 260, rs: 2, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172975
Test loss (w/o reg) on all data: 0.012055328
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6044953e-07
Norm of the params: 9.153188
     Influence (LOO): fixed 129 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008787829
Train loss (w/o reg) on all data: 0.0035152216
Test loss (w/o reg) on all data: 0.011547345
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2357132e-07
Norm of the params: 10.2689905
                Loss: fixed 127 labels. Loss 0.01155. Accuracy 0.996.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19860198
Train loss (w/o reg) on all data: 0.19071065
Test loss (w/o reg) on all data: 0.11231045
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.669521e-05
Norm of the params: 12.562905
              Random: fixed  37 labels. Loss 0.11231. Accuracy 0.977.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23822324
Train loss (w/o reg) on all data: 0.229884
Test loss (w/o reg) on all data: 0.16597696
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.098633e-05
Norm of the params: 12.914523
Flipped loss: 0.16598. Accuracy: 0.962
### Flips: 260, rs: 3, checks: 52
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18153195
Train loss (w/o reg) on all data: 0.17199624
Test loss (w/o reg) on all data: 0.110237196
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.2499938e-05
Norm of the params: 13.809933
     Influence (LOO): fixed  35 labels. Loss 0.11024. Accuracy 0.973.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11931972
Train loss (w/o reg) on all data: 0.10305624
Test loss (w/o reg) on all data: 0.14968777
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.3229257e-05
Norm of the params: 18.035234
                Loss: fixed  50 labels. Loss 0.14969. Accuracy 0.943.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23039243
Train loss (w/o reg) on all data: 0.22201073
Test loss (w/o reg) on all data: 0.16665307
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.1756997e-05
Norm of the params: 12.947354
              Random: fixed   4 labels. Loss 0.16665. Accuracy 0.958.
### Flips: 260, rs: 3, checks: 104
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11270464
Train loss (w/o reg) on all data: 0.101584144
Test loss (w/o reg) on all data: 0.07021053
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.571311e-06
Norm of the params: 14.913417
     Influence (LOO): fixed  74 labels. Loss 0.07021. Accuracy 0.989.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047992513
Train loss (w/o reg) on all data: 0.03187444
Test loss (w/o reg) on all data: 0.084923215
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.329831e-06
Norm of the params: 17.954426
                Loss: fixed  92 labels. Loss 0.08492. Accuracy 0.973.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22356912
Train loss (w/o reg) on all data: 0.21549627
Test loss (w/o reg) on all data: 0.14653644
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.1351814e-05
Norm of the params: 12.706573
              Random: fixed  13 labels. Loss 0.14654. Accuracy 0.973.
### Flips: 260, rs: 3, checks: 156
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06812277
Train loss (w/o reg) on all data: 0.058982886
Test loss (w/o reg) on all data: 0.039572634
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.383911e-06
Norm of the params: 13.520266
     Influence (LOO): fixed 101 labels. Loss 0.03957. Accuracy 0.989.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022101525
Train loss (w/o reg) on all data: 0.011608455
Test loss (w/o reg) on all data: 0.02897803
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.653982e-06
Norm of the params: 14.486593
                Loss: fixed 113 labels. Loss 0.02898. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21966109
Train loss (w/o reg) on all data: 0.21153937
Test loss (w/o reg) on all data: 0.13873054
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.914553e-05
Norm of the params: 12.74497
              Random: fixed  19 labels. Loss 0.13873. Accuracy 0.969.
### Flips: 260, rs: 3, checks: 208
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033210475
Train loss (w/o reg) on all data: 0.02593623
Test loss (w/o reg) on all data: 0.019672668
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7075378e-06
Norm of the params: 12.061712
     Influence (LOO): fixed 118 labels. Loss 0.01967. Accuracy 0.992.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017750934
Train loss (w/o reg) on all data: 0.0085654855
Test loss (w/o reg) on all data: 0.0254053
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2539523e-06
Norm of the params: 13.553927
                Loss: fixed 116 labels. Loss 0.02541. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21588722
Train loss (w/o reg) on all data: 0.20794797
Test loss (w/o reg) on all data: 0.13387012
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8880628e-05
Norm of the params: 12.6009865
              Random: fixed  24 labels. Loss 0.13387. Accuracy 0.981.
### Flips: 260, rs: 3, checks: 260
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017016696
Train loss (w/o reg) on all data: 0.011284086
Test loss (w/o reg) on all data: 0.013734822
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3919497e-06
Norm of the params: 10.707577
     Influence (LOO): fixed 124 labels. Loss 0.01373. Accuracy 0.992.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013069527
Train loss (w/o reg) on all data: 0.005696375
Test loss (w/o reg) on all data: 0.024078142
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.875142e-07
Norm of the params: 12.143436
                Loss: fixed 120 labels. Loss 0.02408. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20849718
Train loss (w/o reg) on all data: 0.20058455
Test loss (w/o reg) on all data: 0.12538776
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.467808e-05
Norm of the params: 12.579852
              Random: fixed  31 labels. Loss 0.12539. Accuracy 0.985.
### Flips: 260, rs: 3, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172963
Test loss (w/o reg) on all data: 0.012054554
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6027698e-07
Norm of the params: 9.153202
     Influence (LOO): fixed 127 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009105502
Train loss (w/o reg) on all data: 0.003587309
Test loss (w/o reg) on all data: 0.014599666
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.1517044e-07
Norm of the params: 10.505421
                Loss: fixed 123 labels. Loss 0.01460. Accuracy 0.992.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19073933
Train loss (w/o reg) on all data: 0.18338272
Test loss (w/o reg) on all data: 0.10531099
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.015977e-05
Norm of the params: 12.129814
              Random: fixed  43 labels. Loss 0.10531. Accuracy 0.985.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25298935
Train loss (w/o reg) on all data: 0.24796224
Test loss (w/o reg) on all data: 0.19246013
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.5783484e-05
Norm of the params: 10.027082
Flipped loss: 0.19246. Accuracy: 0.943
### Flips: 260, rs: 4, checks: 52
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19841594
Train loss (w/o reg) on all data: 0.19081616
Test loss (w/o reg) on all data: 0.13772452
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.870703e-05
Norm of the params: 12.328637
     Influence (LOO): fixed  36 labels. Loss 0.13772. Accuracy 0.962.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13398844
Train loss (w/o reg) on all data: 0.122538134
Test loss (w/o reg) on all data: 0.14379713
Train acc on all data:  0.936007640878701
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.5778273e-05
Norm of the params: 15.132953
                Loss: fixed  52 labels. Loss 0.14380. Accuracy 0.950.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24205513
Train loss (w/o reg) on all data: 0.23696521
Test loss (w/o reg) on all data: 0.18391337
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.7357072e-05
Norm of the params: 10.08952
              Random: fixed  10 labels. Loss 0.18391. Accuracy 0.935.
### Flips: 260, rs: 4, checks: 104
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14001758
Train loss (w/o reg) on all data: 0.13069804
Test loss (w/o reg) on all data: 0.09140439
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9187466e-05
Norm of the params: 13.652499
     Influence (LOO): fixed  69 labels. Loss 0.09140. Accuracy 0.985.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059445925
Train loss (w/o reg) on all data: 0.044889133
Test loss (w/o reg) on all data: 0.100216515
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.179094e-05
Norm of the params: 17.062704
                Loss: fixed  96 labels. Loss 0.10022. Accuracy 0.950.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23538108
Train loss (w/o reg) on all data: 0.23039038
Test loss (w/o reg) on all data: 0.17234811
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.2560991e-05
Norm of the params: 9.990693
              Random: fixed  18 labels. Loss 0.17235. Accuracy 0.950.
### Flips: 260, rs: 4, checks: 156
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09211905
Train loss (w/o reg) on all data: 0.08334942
Test loss (w/o reg) on all data: 0.06412544
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6019349e-05
Norm of the params: 13.243588
     Influence (LOO): fixed  92 labels. Loss 0.06413. Accuracy 0.981.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018441502
Train loss (w/o reg) on all data: 0.009854283
Test loss (w/o reg) on all data: 0.025040535
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1297678e-06
Norm of the params: 13.105128
                Loss: fixed 123 labels. Loss 0.02504. Accuracy 0.992.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22813165
Train loss (w/o reg) on all data: 0.22275983
Test loss (w/o reg) on all data: 0.16368203
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7228478e-05
Norm of the params: 10.365151
              Random: fixed  24 labels. Loss 0.16368. Accuracy 0.966.
### Flips: 260, rs: 4, checks: 208
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045697488
Train loss (w/o reg) on all data: 0.03666739
Test loss (w/o reg) on all data: 0.027543724
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.3267105e-06
Norm of the params: 13.438822
     Influence (LOO): fixed 114 labels. Loss 0.02754. Accuracy 0.989.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010996518
Train loss (w/o reg) on all data: 0.0047205873
Test loss (w/o reg) on all data: 0.015895063
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5826722e-07
Norm of the params: 11.203508
                Loss: fixed 128 labels. Loss 0.01590. Accuracy 0.996.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21576504
Train loss (w/o reg) on all data: 0.21024181
Test loss (w/o reg) on all data: 0.14409854
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.633568e-05
Norm of the params: 10.510215
              Random: fixed  33 labels. Loss 0.14410. Accuracy 0.966.
### Flips: 260, rs: 4, checks: 260
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030952742
Train loss (w/o reg) on all data: 0.023455275
Test loss (w/o reg) on all data: 0.018768862
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.261412e-06
Norm of the params: 12.24538
     Influence (LOO): fixed 123 labels. Loss 0.01877. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010996517
Train loss (w/o reg) on all data: 0.0047203596
Test loss (w/o reg) on all data: 0.01589527
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.024258e-07
Norm of the params: 11.203711
                Loss: fixed 128 labels. Loss 0.01590. Accuracy 0.996.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20608433
Train loss (w/o reg) on all data: 0.20045565
Test loss (w/o reg) on all data: 0.13956389
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.486772e-06
Norm of the params: 10.610068
              Random: fixed  42 labels. Loss 0.13956. Accuracy 0.973.
### Flips: 260, rs: 4, checks: 312
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026015121
Train loss (w/o reg) on all data: 0.019194396
Test loss (w/o reg) on all data: 0.014787107
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4999892e-06
Norm of the params: 11.679662
     Influence (LOO): fixed 125 labels. Loss 0.01479. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008888101
Train loss (w/o reg) on all data: 0.0035020323
Test loss (w/o reg) on all data: 0.012814807
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9866602e-07
Norm of the params: 10.378892
                Loss: fixed 130 labels. Loss 0.01281. Accuracy 0.996.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20176019
Train loss (w/o reg) on all data: 0.19604963
Test loss (w/o reg) on all data: 0.13317774
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.528782e-05
Norm of the params: 10.686967
              Random: fixed  45 labels. Loss 0.13318. Accuracy 0.977.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2524982
Train loss (w/o reg) on all data: 0.2446285
Test loss (w/o reg) on all data: 0.1724552
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.8501013e-05
Norm of the params: 12.545679
Flipped loss: 0.17246. Accuracy: 0.966
### Flips: 260, rs: 5, checks: 52
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19428828
Train loss (w/o reg) on all data: 0.18346454
Test loss (w/o reg) on all data: 0.1341072
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.654691e-05
Norm of the params: 14.713086
     Influence (LOO): fixed  37 labels. Loss 0.13411. Accuracy 0.950.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14803913
Train loss (w/o reg) on all data: 0.13311557
Test loss (w/o reg) on all data: 0.12406204
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.4595916e-05
Norm of the params: 17.276318
                Loss: fixed  51 labels. Loss 0.12406. Accuracy 0.954.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24596909
Train loss (w/o reg) on all data: 0.23787977
Test loss (w/o reg) on all data: 0.16549706
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.970053e-05
Norm of the params: 12.719523
              Random: fixed   6 labels. Loss 0.16550. Accuracy 0.966.
### Flips: 260, rs: 5, checks: 104
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14741187
Train loss (w/o reg) on all data: 0.136308
Test loss (w/o reg) on all data: 0.103697695
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.1879066e-05
Norm of the params: 14.90226
     Influence (LOO): fixed  64 labels. Loss 0.10370. Accuracy 0.981.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06689894
Train loss (w/o reg) on all data: 0.050545674
Test loss (w/o reg) on all data: 0.047360677
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.562983e-06
Norm of the params: 18.08495
                Loss: fixed  98 labels. Loss 0.04736. Accuracy 0.985.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23789412
Train loss (w/o reg) on all data: 0.23056553
Test loss (w/o reg) on all data: 0.15195574
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5122786e-05
Norm of the params: 12.106682
              Random: fixed  16 labels. Loss 0.15196. Accuracy 0.969.
### Flips: 260, rs: 5, checks: 156
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.099762045
Train loss (w/o reg) on all data: 0.09011024
Test loss (w/o reg) on all data: 0.07052366
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.2536254e-06
Norm of the params: 13.89374
     Influence (LOO): fixed  88 labels. Loss 0.07052. Accuracy 0.992.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027878152
Train loss (w/o reg) on all data: 0.015645111
Test loss (w/o reg) on all data: 0.014718782
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.671415e-06
Norm of the params: 15.641637
                Loss: fixed 121 labels. Loss 0.01472. Accuracy 0.996.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22788362
Train loss (w/o reg) on all data: 0.22017352
Test loss (w/o reg) on all data: 0.1421413
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.83912e-05
Norm of the params: 12.417808
              Random: fixed  25 labels. Loss 0.14214. Accuracy 0.973.
### Flips: 260, rs: 5, checks: 208
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0730886
Train loss (w/o reg) on all data: 0.063575506
Test loss (w/o reg) on all data: 0.050174393
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.9718034e-06
Norm of the params: 13.793545
     Influence (LOO): fixed 103 labels. Loss 0.05017. Accuracy 0.985.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017905006
Train loss (w/o reg) on all data: 0.008251033
Test loss (w/o reg) on all data: 0.013623558
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.2839896e-07
Norm of the params: 13.895305
                Loss: fixed 127 labels. Loss 0.01362. Accuracy 0.996.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22530586
Train loss (w/o reg) on all data: 0.21763429
Test loss (w/o reg) on all data: 0.14111374
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.9151739e-05
Norm of the params: 12.386742
              Random: fixed  28 labels. Loss 0.14111. Accuracy 0.981.
### Flips: 260, rs: 5, checks: 260
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03551959
Train loss (w/o reg) on all data: 0.027714316
Test loss (w/o reg) on all data: 0.023836289
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0187283e-06
Norm of the params: 12.494217
     Influence (LOO): fixed 122 labels. Loss 0.02384. Accuracy 0.992.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016147695
Train loss (w/o reg) on all data: 0.007088987
Test loss (w/o reg) on all data: 0.0139997285
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.211397e-07
Norm of the params: 13.460095
                Loss: fixed 128 labels. Loss 0.01400. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2116629
Train loss (w/o reg) on all data: 0.20381023
Test loss (w/o reg) on all data: 0.1266041
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.0726918e-05
Norm of the params: 12.532102
              Random: fixed  39 labels. Loss 0.12660. Accuracy 0.985.
### Flips: 260, rs: 5, checks: 312
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023152227
Train loss (w/o reg) on all data: 0.016870031
Test loss (w/o reg) on all data: 0.015023362
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.55086e-06
Norm of the params: 11.209099
     Influence (LOO): fixed 127 labels. Loss 0.01502. Accuracy 0.996.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012294388
Train loss (w/o reg) on all data: 0.005322203
Test loss (w/o reg) on all data: 0.013574117
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1183112e-06
Norm of the params: 11.808628
                Loss: fixed 130 labels. Loss 0.01357. Accuracy 0.996.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20297414
Train loss (w/o reg) on all data: 0.19531888
Test loss (w/o reg) on all data: 0.12043727
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.869705e-05
Norm of the params: 12.373576
              Random: fixed  45 labels. Loss 0.12044. Accuracy 0.981.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24962607
Train loss (w/o reg) on all data: 0.24311739
Test loss (w/o reg) on all data: 0.16291751
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4891804e-05
Norm of the params: 11.40936
Flipped loss: 0.16292. Accuracy: 0.966
### Flips: 260, rs: 6, checks: 52
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18988822
Train loss (w/o reg) on all data: 0.18054391
Test loss (w/o reg) on all data: 0.12607288
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.346037e-05
Norm of the params: 13.670635
     Influence (LOO): fixed  34 labels. Loss 0.12607. Accuracy 0.977.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12563232
Train loss (w/o reg) on all data: 0.11296196
Test loss (w/o reg) on all data: 0.11428591
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.3000615e-05
Norm of the params: 15.918762
                Loss: fixed  52 labels. Loss 0.11429. Accuracy 0.939.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23906818
Train loss (w/o reg) on all data: 0.23231047
Test loss (w/o reg) on all data: 0.15667112
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.5405533e-05
Norm of the params: 11.625582
              Random: fixed   8 labels. Loss 0.15667. Accuracy 0.958.
### Flips: 260, rs: 6, checks: 104
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13503265
Train loss (w/o reg) on all data: 0.123859905
Test loss (w/o reg) on all data: 0.07997478
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.135571e-06
Norm of the params: 14.948411
     Influence (LOO): fixed  65 labels. Loss 0.07997. Accuracy 0.989.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04079166
Train loss (w/o reg) on all data: 0.028699346
Test loss (w/o reg) on all data: 0.035274133
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2052573e-06
Norm of the params: 15.551408
                Loss: fixed 100 labels. Loss 0.03527. Accuracy 0.985.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23463547
Train loss (w/o reg) on all data: 0.22792009
Test loss (w/o reg) on all data: 0.1505987
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.2778756e-05
Norm of the params: 11.589117
              Random: fixed  13 labels. Loss 0.15060. Accuracy 0.969.
### Flips: 260, rs: 6, checks: 156
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078611694
Train loss (w/o reg) on all data: 0.06719212
Test loss (w/o reg) on all data: 0.04897028
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.39204285e-05
Norm of the params: 15.112624
     Influence (LOO): fixed  90 labels. Loss 0.04897. Accuracy 0.992.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014456782
Train loss (w/o reg) on all data: 0.0069201826
Test loss (w/o reg) on all data: 0.011835721
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6446633e-06
Norm of the params: 12.277296
                Loss: fixed 118 labels. Loss 0.01184. Accuracy 0.996.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22326557
Train loss (w/o reg) on all data: 0.21654916
Test loss (w/o reg) on all data: 0.1490997
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.7216407e-05
Norm of the params: 11.590014
              Random: fixed  22 labels. Loss 0.14910. Accuracy 0.973.
### Flips: 260, rs: 6, checks: 208
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046614796
Train loss (w/o reg) on all data: 0.03574207
Test loss (w/o reg) on all data: 0.035318613
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3642402e-05
Norm of the params: 14.746338
     Influence (LOO): fixed 105 labels. Loss 0.03532. Accuracy 0.992.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008380539
Train loss (w/o reg) on all data: 0.0030429729
Test loss (w/o reg) on all data: 0.010245388
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3602117e-07
Norm of the params: 10.332053
                Loss: fixed 122 labels. Loss 0.01025. Accuracy 0.992.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20860842
Train loss (w/o reg) on all data: 0.20118263
Test loss (w/o reg) on all data: 0.13856491
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.9055074e-05
Norm of the params: 12.186708
              Random: fixed  32 labels. Loss 0.13856. Accuracy 0.973.
### Flips: 260, rs: 6, checks: 260
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03527814
Train loss (w/o reg) on all data: 0.026523205
Test loss (w/o reg) on all data: 0.02519816
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0376608e-06
Norm of the params: 13.23249
     Influence (LOO): fixed 113 labels. Loss 0.02520. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008380538
Train loss (w/o reg) on all data: 0.003042987
Test loss (w/o reg) on all data: 0.010245367
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8023346e-07
Norm of the params: 10.332038
                Loss: fixed 122 labels. Loss 0.01025. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20402557
Train loss (w/o reg) on all data: 0.19652507
Test loss (w/o reg) on all data: 0.13315225
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4847574e-05
Norm of the params: 12.247859
              Random: fixed  36 labels. Loss 0.13315. Accuracy 0.969.
### Flips: 260, rs: 6, checks: 312
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018434923
Train loss (w/o reg) on all data: 0.013030082
Test loss (w/o reg) on all data: 0.014397051
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.9568623e-07
Norm of the params: 10.396964
     Influence (LOO): fixed 121 labels. Loss 0.01440. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007623193
Train loss (w/o reg) on all data: 0.0027306192
Test loss (w/o reg) on all data: 0.009577543
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0268924e-07
Norm of the params: 9.891991
                Loss: fixed 123 labels. Loss 0.00958. Accuracy 0.992.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20239174
Train loss (w/o reg) on all data: 0.19484249
Test loss (w/o reg) on all data: 0.13003212
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1705755e-05
Norm of the params: 12.287605
              Random: fixed  38 labels. Loss 0.13003. Accuracy 0.977.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24346158
Train loss (w/o reg) on all data: 0.23604558
Test loss (w/o reg) on all data: 0.14061353
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2564993e-05
Norm of the params: 12.178664
Flipped loss: 0.14061. Accuracy: 0.992
### Flips: 260, rs: 7, checks: 52
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18704793
Train loss (w/o reg) on all data: 0.17660291
Test loss (w/o reg) on all data: 0.105696075
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.0609717e-05
Norm of the params: 14.453387
     Influence (LOO): fixed  31 labels. Loss 0.10570. Accuracy 0.985.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12423003
Train loss (w/o reg) on all data: 0.11227144
Test loss (w/o reg) on all data: 0.084105864
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5204601e-05
Norm of the params: 15.465179
                Loss: fixed  52 labels. Loss 0.08411. Accuracy 0.973.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23706274
Train loss (w/o reg) on all data: 0.23000155
Test loss (w/o reg) on all data: 0.13347815
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.7694285e-06
Norm of the params: 11.883759
              Random: fixed   6 labels. Loss 0.13348. Accuracy 0.985.
### Flips: 260, rs: 7, checks: 104
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1279378
Train loss (w/o reg) on all data: 0.11680879
Test loss (w/o reg) on all data: 0.080511846
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.3741783e-06
Norm of the params: 14.919121
     Influence (LOO): fixed  66 labels. Loss 0.08051. Accuracy 0.989.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035333194
Train loss (w/o reg) on all data: 0.023334181
Test loss (w/o reg) on all data: 0.037832707
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.532641e-05
Norm of the params: 15.491297
                Loss: fixed  98 labels. Loss 0.03783. Accuracy 0.985.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23096126
Train loss (w/o reg) on all data: 0.22384006
Test loss (w/o reg) on all data: 0.12642209
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0139559e-05
Norm of the params: 11.9341545
              Random: fixed  13 labels. Loss 0.12642. Accuracy 0.985.
### Flips: 260, rs: 7, checks: 156
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079084404
Train loss (w/o reg) on all data: 0.0684558
Test loss (w/o reg) on all data: 0.051861398
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.4461035e-06
Norm of the params: 14.57985
     Influence (LOO): fixed  92 labels. Loss 0.05186. Accuracy 0.989.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017530609
Train loss (w/o reg) on all data: 0.009299006
Test loss (w/o reg) on all data: 0.019323004
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1887122e-06
Norm of the params: 12.830903
                Loss: fixed 112 labels. Loss 0.01932. Accuracy 0.989.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21822928
Train loss (w/o reg) on all data: 0.21092874
Test loss (w/o reg) on all data: 0.11863723
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.4643708e-05
Norm of the params: 12.083498
              Random: fixed  21 labels. Loss 0.11864. Accuracy 0.985.
### Flips: 260, rs: 7, checks: 208
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043823145
Train loss (w/o reg) on all data: 0.034902193
Test loss (w/o reg) on all data: 0.020695562
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.098051e-06
Norm of the params: 13.357361
     Influence (LOO): fixed 109 labels. Loss 0.02070. Accuracy 0.992.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011826774
Train loss (w/o reg) on all data: 0.004824889
Test loss (w/o reg) on all data: 0.0152839925
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.1167155e-07
Norm of the params: 11.833754
                Loss: fixed 117 labels. Loss 0.01528. Accuracy 0.992.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21220371
Train loss (w/o reg) on all data: 0.20475368
Test loss (w/o reg) on all data: 0.11593438
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.4096385e-05
Norm of the params: 12.206584
              Random: fixed  27 labels. Loss 0.11593. Accuracy 0.985.
### Flips: 260, rs: 7, checks: 260
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031681642
Train loss (w/o reg) on all data: 0.023702366
Test loss (w/o reg) on all data: 0.014496648
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.15566e-06
Norm of the params: 12.632717
     Influence (LOO): fixed 115 labels. Loss 0.01450. Accuracy 0.996.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009019531
Train loss (w/o reg) on all data: 0.0033456297
Test loss (w/o reg) on all data: 0.012374944
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.172485e-07
Norm of the params: 10.652607
                Loss: fixed 119 labels. Loss 0.01237. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20770404
Train loss (w/o reg) on all data: 0.20026049
Test loss (w/o reg) on all data: 0.114414066
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0291329e-05
Norm of the params: 12.201262
              Random: fixed  31 labels. Loss 0.11441. Accuracy 0.989.
### Flips: 260, rs: 7, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729714
Test loss (w/o reg) on all data: 0.012055916
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0115624e-07
Norm of the params: 9.153193
     Influence (LOO): fixed 123 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009019535
Train loss (w/o reg) on all data: 0.0033454301
Test loss (w/o reg) on all data: 0.012372861
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.972212e-07
Norm of the params: 10.652798
                Loss: fixed 119 labels. Loss 0.01237. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19922414
Train loss (w/o reg) on all data: 0.19158353
Test loss (w/o reg) on all data: 0.10746694
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.493675e-05
Norm of the params: 12.361729
              Random: fixed  35 labels. Loss 0.10747. Accuracy 0.989.
Using normal model
LBFGS training took [317] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24783608
Train loss (w/o reg) on all data: 0.24004534
Test loss (w/o reg) on all data: 0.19040753
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.4734666e-05
Norm of the params: 12.482585
Flipped loss: 0.19041. Accuracy: 0.954
### Flips: 260, rs: 8, checks: 52
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18362123
Train loss (w/o reg) on all data: 0.17336015
Test loss (w/o reg) on all data: 0.14509767
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.8033642e-05
Norm of the params: 14.325551
     Influence (LOO): fixed  36 labels. Loss 0.14510. Accuracy 0.954.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13984542
Train loss (w/o reg) on all data: 0.12626567
Test loss (w/o reg) on all data: 0.1534631
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.133417e-06
Norm of the params: 16.480137
                Loss: fixed  51 labels. Loss 0.15346. Accuracy 0.950.
Using normal model
LBFGS training took [318] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24194844
Train loss (w/o reg) on all data: 0.23427975
Test loss (w/o reg) on all data: 0.17566793
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.9113768e-05
Norm of the params: 12.384419
              Random: fixed   7 labels. Loss 0.17567. Accuracy 0.962.
### Flips: 260, rs: 8, checks: 104
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13085435
Train loss (w/o reg) on all data: 0.1201815
Test loss (w/o reg) on all data: 0.11850745
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.1669792e-05
Norm of the params: 14.610173
     Influence (LOO): fixed  65 labels. Loss 0.11851. Accuracy 0.950.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061058663
Train loss (w/o reg) on all data: 0.04427345
Test loss (w/o reg) on all data: 0.059028413
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.1498525e-06
Norm of the params: 18.322235
                Loss: fixed  95 labels. Loss 0.05903. Accuracy 0.989.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23382373
Train loss (w/o reg) on all data: 0.22595488
Test loss (w/o reg) on all data: 0.16615322
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.6884953e-05
Norm of the params: 12.545006
              Random: fixed  13 labels. Loss 0.16615. Accuracy 0.969.
### Flips: 260, rs: 8, checks: 156
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080559894
Train loss (w/o reg) on all data: 0.06856858
Test loss (w/o reg) on all data: 0.08710024
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.411975e-06
Norm of the params: 15.486326
     Influence (LOO): fixed  91 labels. Loss 0.08710. Accuracy 0.966.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025876671
Train loss (w/o reg) on all data: 0.01418069
Test loss (w/o reg) on all data: 0.027448796
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2201958e-05
Norm of the params: 15.29443
                Loss: fixed 116 labels. Loss 0.02745. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2269028
Train loss (w/o reg) on all data: 0.2191828
Test loss (w/o reg) on all data: 0.15603037
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.4018477e-05
Norm of the params: 12.425778
              Random: fixed  19 labels. Loss 0.15603. Accuracy 0.969.
### Flips: 260, rs: 8, checks: 208
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049963467
Train loss (w/o reg) on all data: 0.03985323
Test loss (w/o reg) on all data: 0.05460103
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.5078012e-06
Norm of the params: 14.2198715
     Influence (LOO): fixed 110 labels. Loss 0.05460. Accuracy 0.977.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019525206
Train loss (w/o reg) on all data: 0.010041751
Test loss (w/o reg) on all data: 0.018837499
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5015404e-06
Norm of the params: 13.772039
                Loss: fixed 121 labels. Loss 0.01884. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21768405
Train loss (w/o reg) on all data: 0.21002018
Test loss (w/o reg) on all data: 0.1464345
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.627336e-05
Norm of the params: 12.380519
              Random: fixed  26 labels. Loss 0.14643. Accuracy 0.969.
### Flips: 260, rs: 8, checks: 260
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028950091
Train loss (w/o reg) on all data: 0.021789387
Test loss (w/o reg) on all data: 0.03259641
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.2340797e-06
Norm of the params: 11.967209
     Influence (LOO): fixed 121 labels. Loss 0.03260. Accuracy 0.989.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015453152
Train loss (w/o reg) on all data: 0.0070824935
Test loss (w/o reg) on all data: 0.0131052695
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9640495e-06
Norm of the params: 12.938825
                Loss: fixed 124 labels. Loss 0.01311. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20855047
Train loss (w/o reg) on all data: 0.2006761
Test loss (w/o reg) on all data: 0.13849443
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.2108192e-05
Norm of the params: 12.549401
              Random: fixed  32 labels. Loss 0.13849. Accuracy 0.966.
### Flips: 260, rs: 8, checks: 312
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022954889
Train loss (w/o reg) on all data: 0.016281094
Test loss (w/o reg) on all data: 0.021770809
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.152381e-06
Norm of the params: 11.553176
     Influence (LOO): fixed 124 labels. Loss 0.02177. Accuracy 0.992.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014158806
Train loss (w/o reg) on all data: 0.0064216196
Test loss (w/o reg) on all data: 0.01375333
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.342909e-07
Norm of the params: 12.439603
                Loss: fixed 126 labels. Loss 0.01375. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19978537
Train loss (w/o reg) on all data: 0.19196235
Test loss (w/o reg) on all data: 0.12447084
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.6744015e-05
Norm of the params: 12.5084095
              Random: fixed  38 labels. Loss 0.12447. Accuracy 0.973.
Using normal model
LBFGS training took [323] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25298738
Train loss (w/o reg) on all data: 0.24596597
Test loss (w/o reg) on all data: 0.20742643
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 5.4086733e-05
Norm of the params: 11.850254
Flipped loss: 0.20743. Accuracy: 0.916
### Flips: 260, rs: 9, checks: 52
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2039531
Train loss (w/o reg) on all data: 0.19441459
Test loss (w/o reg) on all data: 0.18329975
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.3226422e-05
Norm of the params: 13.81196
     Influence (LOO): fixed  29 labels. Loss 0.18330. Accuracy 0.916.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14749077
Train loss (w/o reg) on all data: 0.13476698
Test loss (w/o reg) on all data: 0.17721874
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 5.151028e-06
Norm of the params: 15.9523
                Loss: fixed  50 labels. Loss 0.17722. Accuracy 0.931.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24600825
Train loss (w/o reg) on all data: 0.23886943
Test loss (w/o reg) on all data: 0.19945937
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 4.5742378e-05
Norm of the params: 11.948908
              Random: fixed   7 labels. Loss 0.19946. Accuracy 0.931.
### Flips: 260, rs: 9, checks: 104
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16318186
Train loss (w/o reg) on all data: 0.15300876
Test loss (w/o reg) on all data: 0.14720568
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 7.381183e-06
Norm of the params: 14.264008
     Influence (LOO): fixed  57 labels. Loss 0.14721. Accuracy 0.954.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075382024
Train loss (w/o reg) on all data: 0.059199963
Test loss (w/o reg) on all data: 0.1135098
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.2698902e-06
Norm of the params: 17.990032
                Loss: fixed  94 labels. Loss 0.11351. Accuracy 0.962.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24283838
Train loss (w/o reg) on all data: 0.2360154
Test loss (w/o reg) on all data: 0.18465795
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.660868e-05
Norm of the params: 11.6816025
              Random: fixed  12 labels. Loss 0.18466. Accuracy 0.947.
### Flips: 260, rs: 9, checks: 156
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11465763
Train loss (w/o reg) on all data: 0.1037631
Test loss (w/o reg) on all data: 0.096368365
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0309937e-05
Norm of the params: 14.76112
     Influence (LOO): fixed  88 labels. Loss 0.09637. Accuracy 0.969.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027906436
Train loss (w/o reg) on all data: 0.015540564
Test loss (w/o reg) on all data: 0.05446283
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0531478e-06
Norm of the params: 15.726329
                Loss: fixed 120 labels. Loss 0.05446. Accuracy 0.973.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2378458
Train loss (w/o reg) on all data: 0.23086157
Test loss (w/o reg) on all data: 0.1764545
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.3191459e-05
Norm of the params: 11.818817
              Random: fixed  18 labels. Loss 0.17645. Accuracy 0.958.
### Flips: 260, rs: 9, checks: 208
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06313375
Train loss (w/o reg) on all data: 0.053795174
Test loss (w/o reg) on all data: 0.048706118
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7909388e-06
Norm of the params: 13.666438
     Influence (LOO): fixed 117 labels. Loss 0.04871. Accuracy 0.996.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020962693
Train loss (w/o reg) on all data: 0.011316293
Test loss (w/o reg) on all data: 0.03561849
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9732263e-06
Norm of the params: 13.889853
                Loss: fixed 126 labels. Loss 0.03562. Accuracy 0.985.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22215366
Train loss (w/o reg) on all data: 0.21466216
Test loss (w/o reg) on all data: 0.15698184
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.749824e-05
Norm of the params: 12.240512
              Random: fixed  32 labels. Loss 0.15698. Accuracy 0.958.
### Flips: 260, rs: 9, checks: 260
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035146423
Train loss (w/o reg) on all data: 0.026640618
Test loss (w/o reg) on all data: 0.025011739
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8272477e-06
Norm of the params: 13.042855
     Influence (LOO): fixed 129 labels. Loss 0.02501. Accuracy 0.992.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016446061
Train loss (w/o reg) on all data: 0.008108604
Test loss (w/o reg) on all data: 0.019334648
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.077607e-07
Norm of the params: 12.913139
                Loss: fixed 131 labels. Loss 0.01933. Accuracy 0.996.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21075949
Train loss (w/o reg) on all data: 0.20310728
Test loss (w/o reg) on all data: 0.13834566
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.3434484e-05
Norm of the params: 12.371103
              Random: fixed  41 labels. Loss 0.13835. Accuracy 0.973.
### Flips: 260, rs: 9, checks: 312
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011224471
Train loss (w/o reg) on all data: 0.006245113
Test loss (w/o reg) on all data: 0.012817133
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7822947e-06
Norm of the params: 9.979337
     Influence (LOO): fixed 137 labels. Loss 0.01282. Accuracy 0.996.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008026177
Train loss (w/o reg) on all data: 0.002998623
Test loss (w/o reg) on all data: 0.015570822
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1252168e-07
Norm of the params: 10.027516
                Loss: fixed 136 labels. Loss 0.01557. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20228413
Train loss (w/o reg) on all data: 0.19480394
Test loss (w/o reg) on all data: 0.12812202
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.1094374e-05
Norm of the params: 12.231258
              Random: fixed  48 labels. Loss 0.12812. Accuracy 0.966.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2400386
Train loss (w/o reg) on all data: 0.23344955
Test loss (w/o reg) on all data: 0.15467167
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9371038e-05
Norm of the params: 11.479602
Flipped loss: 0.15467. Accuracy: 0.973
### Flips: 260, rs: 10, checks: 52
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16810694
Train loss (w/o reg) on all data: 0.15880476
Test loss (w/o reg) on all data: 0.11822627
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.676517e-05
Norm of the params: 13.639789
     Influence (LOO): fixed  40 labels. Loss 0.11823. Accuracy 0.969.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1257907
Train loss (w/o reg) on all data: 0.11357507
Test loss (w/o reg) on all data: 0.117113516
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 6.881744e-06
Norm of the params: 15.630503
                Loss: fixed  51 labels. Loss 0.11711. Accuracy 0.950.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23137116
Train loss (w/o reg) on all data: 0.22457099
Test loss (w/o reg) on all data: 0.14492399
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.71984e-05
Norm of the params: 11.662049
              Random: fixed   6 labels. Loss 0.14492. Accuracy 0.969.
### Flips: 260, rs: 10, checks: 104
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116773196
Train loss (w/o reg) on all data: 0.10604535
Test loss (w/o reg) on all data: 0.09720596
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.6341962e-05
Norm of the params: 14.647762
     Influence (LOO): fixed  68 labels. Loss 0.09721. Accuracy 0.969.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034448866
Train loss (w/o reg) on all data: 0.022047885
Test loss (w/o reg) on all data: 0.060436808
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.911483e-06
Norm of the params: 15.748639
                Loss: fixed  98 labels. Loss 0.06044. Accuracy 0.969.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22550556
Train loss (w/o reg) on all data: 0.21830231
Test loss (w/o reg) on all data: 0.13731503
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3605754e-05
Norm of the params: 12.002712
              Random: fixed  11 labels. Loss 0.13732. Accuracy 0.973.
### Flips: 260, rs: 10, checks: 156
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06965953
Train loss (w/o reg) on all data: 0.059624016
Test loss (w/o reg) on all data: 0.061500266
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.781114e-06
Norm of the params: 14.167226
     Influence (LOO): fixed  93 labels. Loss 0.06150. Accuracy 0.985.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017898545
Train loss (w/o reg) on all data: 0.009042014
Test loss (w/o reg) on all data: 0.025870094
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.287584e-07
Norm of the params: 13.309043
                Loss: fixed 111 labels. Loss 0.02587. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21599013
Train loss (w/o reg) on all data: 0.20880766
Test loss (w/o reg) on all data: 0.13050042
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.3331958e-05
Norm of the params: 11.985384
              Random: fixed  19 labels. Loss 0.13050. Accuracy 0.973.
### Flips: 260, rs: 10, checks: 208
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038351744
Train loss (w/o reg) on all data: 0.02943221
Test loss (w/o reg) on all data: 0.03078886
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.5316374e-06
Norm of the params: 13.356299
     Influence (LOO): fixed 109 labels. Loss 0.03079. Accuracy 0.989.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011400558
Train loss (w/o reg) on all data: 0.004863958
Test loss (w/o reg) on all data: 0.017277995
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.496391e-07
Norm of the params: 11.433809
                Loss: fixed 116 labels. Loss 0.01728. Accuracy 0.992.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20698284
Train loss (w/o reg) on all data: 0.19927044
Test loss (w/o reg) on all data: 0.12700647
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.8875317e-05
Norm of the params: 12.419654
              Random: fixed  24 labels. Loss 0.12701. Accuracy 0.977.
### Flips: 260, rs: 10, checks: 260
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01766676
Train loss (w/o reg) on all data: 0.011349191
Test loss (w/o reg) on all data: 0.018977854
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.552324e-07
Norm of the params: 11.240613
     Influence (LOO): fixed 118 labels. Loss 0.01898. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010281924
Train loss (w/o reg) on all data: 0.0041533895
Test loss (w/o reg) on all data: 0.015278095
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.526064e-07
Norm of the params: 11.071165
                Loss: fixed 117 labels. Loss 0.01528. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1996254
Train loss (w/o reg) on all data: 0.19173203
Test loss (w/o reg) on all data: 0.109500095
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.642987e-05
Norm of the params: 12.564527
              Random: fixed  32 labels. Loss 0.10950. Accuracy 0.985.
### Flips: 260, rs: 10, checks: 312
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011366565
Train loss (w/o reg) on all data: 0.0062761013
Test loss (w/o reg) on all data: 0.010717285
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6740863e-07
Norm of the params: 10.090057
     Influence (LOO): fixed 121 labels. Loss 0.01072. Accuracy 0.996.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008177741
Train loss (w/o reg) on all data: 0.002981334
Test loss (w/o reg) on all data: 0.013065126
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.0160525e-07
Norm of the params: 10.194515
                Loss: fixed 119 labels. Loss 0.01307. Accuracy 0.992.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18981756
Train loss (w/o reg) on all data: 0.18194577
Test loss (w/o reg) on all data: 0.09745689
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.22108e-05
Norm of the params: 12.5473385
              Random: fixed  39 labels. Loss 0.09746. Accuracy 0.989.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24632964
Train loss (w/o reg) on all data: 0.23879148
Test loss (w/o reg) on all data: 0.17550912
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.2032378e-05
Norm of the params: 12.278565
Flipped loss: 0.17551. Accuracy: 0.954
### Flips: 260, rs: 11, checks: 52
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19001319
Train loss (w/o reg) on all data: 0.1786775
Test loss (w/o reg) on all data: 0.14926307
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.349768e-05
Norm of the params: 15.057013
     Influence (LOO): fixed  32 labels. Loss 0.14926. Accuracy 0.969.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13536146
Train loss (w/o reg) on all data: 0.1205826
Test loss (w/o reg) on all data: 0.11545832
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.0168436e-06
Norm of the params: 17.192358
                Loss: fixed  52 labels. Loss 0.11546. Accuracy 0.954.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24214917
Train loss (w/o reg) on all data: 0.23487599
Test loss (w/o reg) on all data: 0.16971135
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.7847182e-05
Norm of the params: 12.060825
              Random: fixed   6 labels. Loss 0.16971. Accuracy 0.958.
### Flips: 260, rs: 11, checks: 104
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13760327
Train loss (w/o reg) on all data: 0.12585062
Test loss (w/o reg) on all data: 0.10743361
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.9615727e-05
Norm of the params: 15.331442
     Influence (LOO): fixed  61 labels. Loss 0.10743. Accuracy 0.969.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052487023
Train loss (w/o reg) on all data: 0.037283733
Test loss (w/o reg) on all data: 0.058628432
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9641043e-06
Norm of the params: 17.437485
                Loss: fixed 100 labels. Loss 0.05863. Accuracy 0.977.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23401229
Train loss (w/o reg) on all data: 0.22624457
Test loss (w/o reg) on all data: 0.16836889
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.7775273e-05
Norm of the params: 12.464121
              Random: fixed  13 labels. Loss 0.16837. Accuracy 0.958.
### Flips: 260, rs: 11, checks: 156
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08333584
Train loss (w/o reg) on all data: 0.072705455
Test loss (w/o reg) on all data: 0.059586603
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.9457307e-06
Norm of the params: 14.581075
     Influence (LOO): fixed  94 labels. Loss 0.05959. Accuracy 0.981.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028662203
Train loss (w/o reg) on all data: 0.01664832
Test loss (w/o reg) on all data: 0.039796438
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4042166e-06
Norm of the params: 15.500892
                Loss: fixed 117 labels. Loss 0.03980. Accuracy 0.985.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22184598
Train loss (w/o reg) on all data: 0.21398409
Test loss (w/o reg) on all data: 0.15484625
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4005154e-05
Norm of the params: 12.5394535
              Random: fixed  22 labels. Loss 0.15485. Accuracy 0.958.
### Flips: 260, rs: 11, checks: 208
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054741994
Train loss (w/o reg) on all data: 0.045679495
Test loss (w/o reg) on all data: 0.04493808
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.9999496e-06
Norm of the params: 13.462911
     Influence (LOO): fixed 108 labels. Loss 0.04494. Accuracy 0.985.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019738898
Train loss (w/o reg) on all data: 0.010056035
Test loss (w/o reg) on all data: 0.02271868
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5322482e-06
Norm of the params: 13.9160795
                Loss: fixed 123 labels. Loss 0.02272. Accuracy 0.996.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20543978
Train loss (w/o reg) on all data: 0.19663426
Test loss (w/o reg) on all data: 0.13350199
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0912425e-05
Norm of the params: 13.270653
              Random: fixed  35 labels. Loss 0.13350. Accuracy 0.966.
### Flips: 260, rs: 11, checks: 260
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034725867
Train loss (w/o reg) on all data: 0.026760874
Test loss (w/o reg) on all data: 0.027221004
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.869336e-06
Norm of the params: 12.621407
     Influence (LOO): fixed 122 labels. Loss 0.02722. Accuracy 0.989.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017834859
Train loss (w/o reg) on all data: 0.008656926
Test loss (w/o reg) on all data: 0.024080275
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.9694704e-07
Norm of the params: 13.548381
                Loss: fixed 124 labels. Loss 0.02408. Accuracy 0.996.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20276164
Train loss (w/o reg) on all data: 0.19410457
Test loss (w/o reg) on all data: 0.12856035
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.6023863e-05
Norm of the params: 13.158321
              Random: fixed  38 labels. Loss 0.12856. Accuracy 0.969.
### Flips: 260, rs: 11, checks: 312
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022689201
Train loss (w/o reg) on all data: 0.016163189
Test loss (w/o reg) on all data: 0.019443883
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9256886e-06
Norm of the params: 11.424545
     Influence (LOO): fixed 127 labels. Loss 0.01944. Accuracy 0.996.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013559151
Train loss (w/o reg) on all data: 0.006217308
Test loss (w/o reg) on all data: 0.015107812
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.0790594e-07
Norm of the params: 12.117627
                Loss: fixed 127 labels. Loss 0.01511. Accuracy 0.996.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19413301
Train loss (w/o reg) on all data: 0.18548319
Test loss (w/o reg) on all data: 0.12159236
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.6584388e-05
Norm of the params: 13.152812
              Random: fixed  45 labels. Loss 0.12159. Accuracy 0.966.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23474853
Train loss (w/o reg) on all data: 0.22610824
Test loss (w/o reg) on all data: 0.20620556
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.5860574e-05
Norm of the params: 13.145559
Flipped loss: 0.20621. Accuracy: 0.927
### Flips: 260, rs: 12, checks: 52
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17079483
Train loss (w/o reg) on all data: 0.16127023
Test loss (w/o reg) on all data: 0.15089118
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.4367479e-05
Norm of the params: 13.801881
     Influence (LOO): fixed  37 labels. Loss 0.15089. Accuracy 0.943.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11804343
Train loss (w/o reg) on all data: 0.10254728
Test loss (w/o reg) on all data: 0.16818076
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 4.1688654e-06
Norm of the params: 17.604631
                Loss: fixed  49 labels. Loss 0.16818. Accuracy 0.927.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22989033
Train loss (w/o reg) on all data: 0.2212964
Test loss (w/o reg) on all data: 0.20134021
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.8284605e-05
Norm of the params: 13.110253
              Random: fixed   4 labels. Loss 0.20134. Accuracy 0.931.
### Flips: 260, rs: 12, checks: 104
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115720846
Train loss (w/o reg) on all data: 0.10594483
Test loss (w/o reg) on all data: 0.09300989
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.8504932e-05
Norm of the params: 13.982859
     Influence (LOO): fixed  70 labels. Loss 0.09301. Accuracy 0.969.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04371372
Train loss (w/o reg) on all data: 0.028909927
Test loss (w/o reg) on all data: 0.06706428
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2777624e-06
Norm of the params: 17.206854
                Loss: fixed  91 labels. Loss 0.06706. Accuracy 0.966.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21913531
Train loss (w/o reg) on all data: 0.21023177
Test loss (w/o reg) on all data: 0.19520345
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.819578e-05
Norm of the params: 13.344326
              Random: fixed  12 labels. Loss 0.19520. Accuracy 0.939.
### Flips: 260, rs: 12, checks: 156
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0654465
Train loss (w/o reg) on all data: 0.055901535
Test loss (w/o reg) on all data: 0.054381516
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.535027e-06
Norm of the params: 13.816635
     Influence (LOO): fixed  95 labels. Loss 0.05438. Accuracy 0.985.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026143845
Train loss (w/o reg) on all data: 0.014875227
Test loss (w/o reg) on all data: 0.027485225
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.719235e-06
Norm of the params: 15.012407
                Loss: fixed 107 labels. Loss 0.02749. Accuracy 0.989.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21253486
Train loss (w/o reg) on all data: 0.20356333
Test loss (w/o reg) on all data: 0.19486368
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.7362085e-05
Norm of the params: 13.395165
              Random: fixed  17 labels. Loss 0.19486. Accuracy 0.939.
### Flips: 260, rs: 12, checks: 208
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02885611
Train loss (w/o reg) on all data: 0.020507796
Test loss (w/o reg) on all data: 0.028041864
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9409924e-06
Norm of the params: 12.921544
     Influence (LOO): fixed 113 labels. Loss 0.02804. Accuracy 0.992.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022530137
Train loss (w/o reg) on all data: 0.012479172
Test loss (w/o reg) on all data: 0.017260471
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4107144e-06
Norm of the params: 14.178127
                Loss: fixed 113 labels. Loss 0.01726. Accuracy 0.996.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20775214
Train loss (w/o reg) on all data: 0.19888848
Test loss (w/o reg) on all data: 0.18046753
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.17424015e-05
Norm of the params: 13.314396
              Random: fixed  23 labels. Loss 0.18047. Accuracy 0.943.
### Flips: 260, rs: 12, checks: 260
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012206844
Train loss (w/o reg) on all data: 0.0065271184
Test loss (w/o reg) on all data: 0.013512913
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.070468e-07
Norm of the params: 10.658073
     Influence (LOO): fixed 122 labels. Loss 0.01351. Accuracy 0.996.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013092184
Train loss (w/o reg) on all data: 0.0060735773
Test loss (w/o reg) on all data: 0.020320283
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6656239e-07
Norm of the params: 11.847876
                Loss: fixed 118 labels. Loss 0.02032. Accuracy 0.996.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2024179
Train loss (w/o reg) on all data: 0.19413668
Test loss (w/o reg) on all data: 0.15650252
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.9156123e-05
Norm of the params: 12.869511
              Random: fixed  29 labels. Loss 0.15650. Accuracy 0.954.
### Flips: 260, rs: 12, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008062667
Train loss (w/o reg) on all data: 0.0030147887
Test loss (w/o reg) on all data: 0.010265189
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.126796e-07
Norm of the params: 10.047766
     Influence (LOO): fixed 123 labels. Loss 0.01027. Accuracy 0.996.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728433
Test loss (w/o reg) on all data: 0.012056284
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5299397e-07
Norm of the params: 9.153335
                Loss: fixed 124 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18921696
Train loss (w/o reg) on all data: 0.18044087
Test loss (w/o reg) on all data: 0.13816646
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.976324e-05
Norm of the params: 13.24846
              Random: fixed  40 labels. Loss 0.13817. Accuracy 0.958.
Using normal model
LBFGS training took [313] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2369321
Train loss (w/o reg) on all data: 0.22840081
Test loss (w/o reg) on all data: 0.17871387
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.6967886e-05
Norm of the params: 13.062379
Flipped loss: 0.17871. Accuracy: 0.947
### Flips: 260, rs: 13, checks: 52
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17376448
Train loss (w/o reg) on all data: 0.16342238
Test loss (w/o reg) on all data: 0.122588225
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.9810133e-05
Norm of the params: 14.382004
     Influence (LOO): fixed  37 labels. Loss 0.12259. Accuracy 0.958.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12381701
Train loss (w/o reg) on all data: 0.110171355
Test loss (w/o reg) on all data: 0.15621117
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.5415653e-05
Norm of the params: 16.520082
                Loss: fixed  51 labels. Loss 0.15621. Accuracy 0.947.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22964782
Train loss (w/o reg) on all data: 0.22106159
Test loss (w/o reg) on all data: 0.17122391
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.8963502e-05
Norm of the params: 13.104368
              Random: fixed   5 labels. Loss 0.17122. Accuracy 0.950.
### Flips: 260, rs: 13, checks: 104
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117324576
Train loss (w/o reg) on all data: 0.106787756
Test loss (w/o reg) on all data: 0.07873239
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0838077e-05
Norm of the params: 14.516761
     Influence (LOO): fixed  66 labels. Loss 0.07873. Accuracy 0.981.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048349135
Train loss (w/o reg) on all data: 0.03471554
Test loss (w/o reg) on all data: 0.059406087
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.0780427e-06
Norm of the params: 16.512777
                Loss: fixed  96 labels. Loss 0.05941. Accuracy 0.973.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22632377
Train loss (w/o reg) on all data: 0.21735261
Test loss (w/o reg) on all data: 0.17242949
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.8408187e-05
Norm of the params: 13.394893
              Random: fixed   9 labels. Loss 0.17243. Accuracy 0.958.
### Flips: 260, rs: 13, checks: 156
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064844176
Train loss (w/o reg) on all data: 0.05450879
Test loss (w/o reg) on all data: 0.044907987
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6552194e-06
Norm of the params: 14.377332
     Influence (LOO): fixed  93 labels. Loss 0.04491. Accuracy 0.989.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017743548
Train loss (w/o reg) on all data: 0.00880902
Test loss (w/o reg) on all data: 0.014447143
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.1425818e-06
Norm of the params: 13.367518
                Loss: fixed 112 labels. Loss 0.01445. Accuracy 0.996.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22258444
Train loss (w/o reg) on all data: 0.21351838
Test loss (w/o reg) on all data: 0.16358118
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.6753954e-05
Norm of the params: 13.465558
              Random: fixed  12 labels. Loss 0.16358. Accuracy 0.966.
### Flips: 260, rs: 13, checks: 208
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037311107
Train loss (w/o reg) on all data: 0.0288371
Test loss (w/o reg) on all data: 0.034171537
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.4746647e-06
Norm of the params: 13.0184555
     Influence (LOO): fixed 108 labels. Loss 0.03417. Accuracy 0.989.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00939632
Train loss (w/o reg) on all data: 0.0036863852
Test loss (w/o reg) on all data: 0.010868269
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.514164e-07
Norm of the params: 10.686379
                Loss: fixed 117 labels. Loss 0.01087. Accuracy 0.996.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21762687
Train loss (w/o reg) on all data: 0.20882593
Test loss (w/o reg) on all data: 0.14743754
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.0295396e-05
Norm of the params: 13.267201
              Random: fixed  17 labels. Loss 0.14744. Accuracy 0.969.
### Flips: 260, rs: 13, checks: 260
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021283869
Train loss (w/o reg) on all data: 0.014824714
Test loss (w/o reg) on all data: 0.020234207
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0950407e-07
Norm of the params: 11.365875
     Influence (LOO): fixed 115 labels. Loss 0.02023. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008345536
Train loss (w/o reg) on all data: 0.0031105557
Test loss (w/o reg) on all data: 0.011751856
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.7981716e-07
Norm of the params: 10.232283
                Loss: fixed 118 labels. Loss 0.01175. Accuracy 0.996.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21081918
Train loss (w/o reg) on all data: 0.20190558
Test loss (w/o reg) on all data: 0.14509118
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8104436e-05
Norm of the params: 13.351855
              Random: fixed  23 labels. Loss 0.14509. Accuracy 0.966.
### Flips: 260, rs: 13, checks: 312
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0128750065
Train loss (w/o reg) on all data: 0.0075582685
Test loss (w/o reg) on all data: 0.014518571
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2200513e-06
Norm of the params: 10.311874
     Influence (LOO): fixed 118 labels. Loss 0.01452. Accuracy 0.989.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006971534
Train loss (w/o reg) on all data: 0.0025172313
Test loss (w/o reg) on all data: 0.01118771
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0015395e-07
Norm of the params: 9.43854
                Loss: fixed 119 labels. Loss 0.01119. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20603469
Train loss (w/o reg) on all data: 0.19776128
Test loss (w/o reg) on all data: 0.12997416
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1277926e-05
Norm of the params: 12.8634405
              Random: fixed  29 labels. Loss 0.12997. Accuracy 0.977.
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24570182
Train loss (w/o reg) on all data: 0.23854102
Test loss (w/o reg) on all data: 0.18195534
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.0695707e-05
Norm of the params: 11.967287
Flipped loss: 0.18196. Accuracy: 0.954
### Flips: 260, rs: 14, checks: 52
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18920974
Train loss (w/o reg) on all data: 0.18060459
Test loss (w/o reg) on all data: 0.13735321
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1452302e-05
Norm of the params: 13.118805
     Influence (LOO): fixed  35 labels. Loss 0.13735. Accuracy 0.977.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13356614
Train loss (w/o reg) on all data: 0.122107275
Test loss (w/o reg) on all data: 0.12278812
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.4649014e-06
Norm of the params: 15.138599
                Loss: fixed  51 labels. Loss 0.12279. Accuracy 0.966.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23053701
Train loss (w/o reg) on all data: 0.22300221
Test loss (w/o reg) on all data: 0.16446123
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.28388165e-05
Norm of the params: 12.275833
              Random: fixed  10 labels. Loss 0.16446. Accuracy 0.958.
### Flips: 260, rs: 14, checks: 104
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13752986
Train loss (w/o reg) on all data: 0.1279136
Test loss (w/o reg) on all data: 0.09336472
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7787519e-05
Norm of the params: 13.868144
     Influence (LOO): fixed  66 labels. Loss 0.09336. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046818905
Train loss (w/o reg) on all data: 0.03365703
Test loss (w/o reg) on all data: 0.05167238
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.325142e-06
Norm of the params: 16.224596
                Loss: fixed  98 labels. Loss 0.05167. Accuracy 0.981.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22173047
Train loss (w/o reg) on all data: 0.21454926
Test loss (w/o reg) on all data: 0.1508145
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.8679878e-05
Norm of the params: 11.984339
              Random: fixed  18 labels. Loss 0.15081. Accuracy 0.969.
### Flips: 260, rs: 14, checks: 156
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08272735
Train loss (w/o reg) on all data: 0.072442144
Test loss (w/o reg) on all data: 0.055831566
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5083502e-05
Norm of the params: 14.342388
     Influence (LOO): fixed  94 labels. Loss 0.05583. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019431284
Train loss (w/o reg) on all data: 0.010418824
Test loss (w/o reg) on all data: 0.01972608
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.675908e-06
Norm of the params: 13.425692
                Loss: fixed 116 labels. Loss 0.01973. Accuracy 0.992.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21413085
Train loss (w/o reg) on all data: 0.207245
Test loss (w/o reg) on all data: 0.14279538
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0350539e-05
Norm of the params: 11.735283
              Random: fixed  24 labels. Loss 0.14280. Accuracy 0.966.
### Flips: 260, rs: 14, checks: 208
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04752702
Train loss (w/o reg) on all data: 0.03987886
Test loss (w/o reg) on all data: 0.035786416
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.098377e-05
Norm of the params: 12.367829
     Influence (LOO): fixed 113 labels. Loss 0.03579. Accuracy 0.989.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01277332
Train loss (w/o reg) on all data: 0.0056373156
Test loss (w/o reg) on all data: 0.01355274
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1776647e-07
Norm of the params: 11.946551
                Loss: fixed 121 labels. Loss 0.01355. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20790946
Train loss (w/o reg) on all data: 0.20133315
Test loss (w/o reg) on all data: 0.12816322
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.649219e-05
Norm of the params: 11.468485
              Random: fixed  31 labels. Loss 0.12816. Accuracy 0.977.
### Flips: 260, rs: 14, checks: 260
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015799087
Train loss (w/o reg) on all data: 0.009905443
Test loss (w/o reg) on all data: 0.024311915
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.7371994e-07
Norm of the params: 10.856927
     Influence (LOO): fixed 122 labels. Loss 0.02431. Accuracy 0.989.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011847427
Train loss (w/o reg) on all data: 0.005039942
Test loss (w/o reg) on all data: 0.012507952
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0336184e-06
Norm of the params: 11.668322
                Loss: fixed 122 labels. Loss 0.01251. Accuracy 0.992.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20434988
Train loss (w/o reg) on all data: 0.19781546
Test loss (w/o reg) on all data: 0.12653904
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.935759e-05
Norm of the params: 11.4319
              Random: fixed  35 labels. Loss 0.12654. Accuracy 0.973.
### Flips: 260, rs: 14, checks: 312
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011851836
Train loss (w/o reg) on all data: 0.0066671288
Test loss (w/o reg) on all data: 0.012345612
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5746876e-07
Norm of the params: 10.183033
     Influence (LOO): fixed 124 labels. Loss 0.01235. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009739218
Train loss (w/o reg) on all data: 0.0038063184
Test loss (w/o reg) on all data: 0.01678557
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.171416e-07
Norm of the params: 10.893025
                Loss: fixed 123 labels. Loss 0.01679. Accuracy 0.992.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19844607
Train loss (w/o reg) on all data: 0.19192319
Test loss (w/o reg) on all data: 0.116735175
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.15678395e-05
Norm of the params: 11.421807
              Random: fixed  42 labels. Loss 0.11674. Accuracy 0.989.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24088313
Train loss (w/o reg) on all data: 0.234037
Test loss (w/o reg) on all data: 0.15550765
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.5868429e-05
Norm of the params: 11.701389
Flipped loss: 0.15551. Accuracy: 0.977
### Flips: 260, rs: 15, checks: 52
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1802215
Train loss (w/o reg) on all data: 0.17104372
Test loss (w/o reg) on all data: 0.11559798
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.9555056e-05
Norm of the params: 13.548262
     Influence (LOO): fixed  37 labels. Loss 0.11560. Accuracy 0.973.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124136165
Train loss (w/o reg) on all data: 0.11326332
Test loss (w/o reg) on all data: 0.103979744
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6515196e-05
Norm of the params: 14.746424
                Loss: fixed  50 labels. Loss 0.10398. Accuracy 0.969.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23571739
Train loss (w/o reg) on all data: 0.22873284
Test loss (w/o reg) on all data: 0.14508627
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.394976e-05
Norm of the params: 11.819088
              Random: fixed   6 labels. Loss 0.14509. Accuracy 0.973.
### Flips: 260, rs: 15, checks: 104
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1323555
Train loss (w/o reg) on all data: 0.12339682
Test loss (w/o reg) on all data: 0.097222775
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.31926e-06
Norm of the params: 13.385572
     Influence (LOO): fixed  62 labels. Loss 0.09722. Accuracy 0.969.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04662082
Train loss (w/o reg) on all data: 0.032770112
Test loss (w/o reg) on all data: 0.041916635
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0993017e-05
Norm of the params: 16.643742
                Loss: fixed  94 labels. Loss 0.04192. Accuracy 0.981.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23259652
Train loss (w/o reg) on all data: 0.22570826
Test loss (w/o reg) on all data: 0.14259624
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 0.00013089649
Norm of the params: 11.737343
              Random: fixed  10 labels. Loss 0.14260. Accuracy 0.973.
### Flips: 260, rs: 15, checks: 156
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08128932
Train loss (w/o reg) on all data: 0.07267801
Test loss (w/o reg) on all data: 0.072625265
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.802276e-06
Norm of the params: 13.1235
     Influence (LOO): fixed  86 labels. Loss 0.07263. Accuracy 0.969.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018214196
Train loss (w/o reg) on all data: 0.009943233
Test loss (w/o reg) on all data: 0.01633646
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.3349615e-07
Norm of the params: 12.861543
                Loss: fixed 114 labels. Loss 0.01634. Accuracy 0.992.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22333111
Train loss (w/o reg) on all data: 0.21651796
Test loss (w/o reg) on all data: 0.12970006
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.2086857e-05
Norm of the params: 11.6731825
              Random: fixed  17 labels. Loss 0.12970. Accuracy 0.973.
### Flips: 260, rs: 15, checks: 208
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03085778
Train loss (w/o reg) on all data: 0.023673171
Test loss (w/o reg) on all data: 0.024298944
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.541171e-06
Norm of the params: 11.9871645
     Influence (LOO): fixed 108 labels. Loss 0.02430. Accuracy 0.992.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011271952
Train loss (w/o reg) on all data: 0.0050860187
Test loss (w/o reg) on all data: 0.012288212
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0372785e-06
Norm of the params: 11.122889
                Loss: fixed 119 labels. Loss 0.01229. Accuracy 0.996.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20862235
Train loss (w/o reg) on all data: 0.20170505
Test loss (w/o reg) on all data: 0.11697408
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7021644e-05
Norm of the params: 11.76206
              Random: fixed  28 labels. Loss 0.11697. Accuracy 0.985.
### Flips: 260, rs: 15, checks: 260
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016013233
Train loss (w/o reg) on all data: 0.008954834
Test loss (w/o reg) on all data: 0.01058323
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5136492e-06
Norm of the params: 11.8814125
     Influence (LOO): fixed 115 labels. Loss 0.01058. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172979
Test loss (w/o reg) on all data: 0.012055248
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2096063e-07
Norm of the params: 9.153184
                Loss: fixed 122 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1918229
Train loss (w/o reg) on all data: 0.18410785
Test loss (w/o reg) on all data: 0.10466306
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5602891e-05
Norm of the params: 12.421788
              Random: fixed  36 labels. Loss 0.10466. Accuracy 0.981.
### Flips: 260, rs: 15, checks: 312
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011884764
Train loss (w/o reg) on all data: 0.0056841667
Test loss (w/o reg) on all data: 0.012078661
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1557182e-06
Norm of the params: 11.1360655
     Influence (LOO): fixed 119 labels. Loss 0.01208. Accuracy 0.996.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172998
Test loss (w/o reg) on all data: 0.012054651
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4097494e-07
Norm of the params: 9.153164
                Loss: fixed 122 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17932832
Train loss (w/o reg) on all data: 0.17144127
Test loss (w/o reg) on all data: 0.09626539
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.503154e-05
Norm of the params: 12.5595045
              Random: fixed  43 labels. Loss 0.09627. Accuracy 0.985.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24631318
Train loss (w/o reg) on all data: 0.23868255
Test loss (w/o reg) on all data: 0.20466135
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 8.5279986e-05
Norm of the params: 12.353653
Flipped loss: 0.20466. Accuracy: 0.924
### Flips: 260, rs: 16, checks: 52
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18969023
Train loss (w/o reg) on all data: 0.17927046
Test loss (w/o reg) on all data: 0.17482306
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.5287772e-05
Norm of the params: 14.435911
     Influence (LOO): fixed  37 labels. Loss 0.17482. Accuracy 0.935.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13871035
Train loss (w/o reg) on all data: 0.122753635
Test loss (w/o reg) on all data: 0.17655954
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.5354788e-05
Norm of the params: 17.864325
                Loss: fixed  52 labels. Loss 0.17656. Accuracy 0.920.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24293646
Train loss (w/o reg) on all data: 0.23513915
Test loss (w/o reg) on all data: 0.20167099
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.3260298e-05
Norm of the params: 12.487842
              Random: fixed   4 labels. Loss 0.20167. Accuracy 0.924.
### Flips: 260, rs: 16, checks: 104
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13470162
Train loss (w/o reg) on all data: 0.122987546
Test loss (w/o reg) on all data: 0.12663144
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.849748e-05
Norm of the params: 15.306255
     Influence (LOO): fixed  67 labels. Loss 0.12663. Accuracy 0.962.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06393824
Train loss (w/o reg) on all data: 0.048367135
Test loss (w/o reg) on all data: 0.111807615
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.1594007e-05
Norm of the params: 17.647152
                Loss: fixed  92 labels. Loss 0.11181. Accuracy 0.954.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23505917
Train loss (w/o reg) on all data: 0.22659686
Test loss (w/o reg) on all data: 0.20107374
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 2.5515214e-05
Norm of the params: 13.009472
              Random: fixed  10 labels. Loss 0.20107. Accuracy 0.916.
### Flips: 260, rs: 16, checks: 156
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09077034
Train loss (w/o reg) on all data: 0.07903141
Test loss (w/o reg) on all data: 0.079022706
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5587982e-05
Norm of the params: 15.322489
     Influence (LOO): fixed  92 labels. Loss 0.07902. Accuracy 0.985.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031717636
Train loss (w/o reg) on all data: 0.018726448
Test loss (w/o reg) on all data: 0.059773047
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.705131e-06
Norm of the params: 16.11905
                Loss: fixed 113 labels. Loss 0.05977. Accuracy 0.973.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22872353
Train loss (w/o reg) on all data: 0.21977396
Test loss (w/o reg) on all data: 0.20033462
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 5.536042e-05
Norm of the params: 13.378759
              Random: fixed  15 labels. Loss 0.20033. Accuracy 0.924.
### Flips: 260, rs: 16, checks: 208
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06462543
Train loss (w/o reg) on all data: 0.05540122
Test loss (w/o reg) on all data: 0.049473055
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.0849443e-06
Norm of the params: 13.582494
     Influence (LOO): fixed 110 labels. Loss 0.04947. Accuracy 0.989.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020123016
Train loss (w/o reg) on all data: 0.01095666
Test loss (w/o reg) on all data: 0.023884777
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.1027137e-06
Norm of the params: 13.539834
                Loss: fixed 124 labels. Loss 0.02388. Accuracy 0.996.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2195583
Train loss (w/o reg) on all data: 0.21039733
Test loss (w/o reg) on all data: 0.18589874
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.7687902e-05
Norm of the params: 13.535851
              Random: fixed  21 labels. Loss 0.18590. Accuracy 0.935.
### Flips: 260, rs: 16, checks: 260
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025208157
Train loss (w/o reg) on all data: 0.018216345
Test loss (w/o reg) on all data: 0.02276779
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9119586e-06
Norm of the params: 11.825236
     Influence (LOO): fixed 126 labels. Loss 0.02277. Accuracy 0.989.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016105955
Train loss (w/o reg) on all data: 0.007854587
Test loss (w/o reg) on all data: 0.017589632
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3462704e-06
Norm of the params: 12.846298
                Loss: fixed 127 labels. Loss 0.01759. Accuracy 0.996.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20884112
Train loss (w/o reg) on all data: 0.19834673
Test loss (w/o reg) on all data: 0.19073814
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 7.65642e-05
Norm of the params: 14.4875
              Random: fixed  29 labels. Loss 0.19074. Accuracy 0.935.
### Flips: 260, rs: 16, checks: 312
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012126273
Train loss (w/o reg) on all data: 0.006587199
Test loss (w/o reg) on all data: 0.017489059
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.0440293e-07
Norm of the params: 10.525278
     Influence (LOO): fixed 131 labels. Loss 0.01749. Accuracy 0.992.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008001954
Train loss (w/o reg) on all data: 0.003120574
Test loss (w/o reg) on all data: 0.0134952245
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.8332647e-07
Norm of the params: 9.880669
                Loss: fixed 132 labels. Loss 0.01350. Accuracy 0.992.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20176616
Train loss (w/o reg) on all data: 0.19128531
Test loss (w/o reg) on all data: 0.16559188
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.2819578e-05
Norm of the params: 14.478159
              Random: fixed  37 labels. Loss 0.16559. Accuracy 0.947.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24758431
Train loss (w/o reg) on all data: 0.23997356
Test loss (w/o reg) on all data: 0.15559246
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4288931e-05
Norm of the params: 12.337547
Flipped loss: 0.15559. Accuracy: 0.977
### Flips: 260, rs: 17, checks: 52
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18595287
Train loss (w/o reg) on all data: 0.17507805
Test loss (w/o reg) on all data: 0.13244055
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.7065404e-05
Norm of the params: 14.747766
     Influence (LOO): fixed  37 labels. Loss 0.13244. Accuracy 0.954.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13175403
Train loss (w/o reg) on all data: 0.11442329
Test loss (w/o reg) on all data: 0.099698916
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.942332e-06
Norm of the params: 18.617594
                Loss: fixed  51 labels. Loss 0.09970. Accuracy 0.966.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23446377
Train loss (w/o reg) on all data: 0.22670434
Test loss (w/o reg) on all data: 0.14569478
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.3532259e-05
Norm of the params: 12.45747
              Random: fixed  13 labels. Loss 0.14569. Accuracy 0.969.
### Flips: 260, rs: 17, checks: 104
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13114229
Train loss (w/o reg) on all data: 0.11829407
Test loss (w/o reg) on all data: 0.114126995
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.4845965e-06
Norm of the params: 16.030107
     Influence (LOO): fixed  65 labels. Loss 0.11413. Accuracy 0.973.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0565404
Train loss (w/o reg) on all data: 0.03796403
Test loss (w/o reg) on all data: 0.05056053
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1368582e-05
Norm of the params: 19.275047
                Loss: fixed  98 labels. Loss 0.05056. Accuracy 0.989.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2265446
Train loss (w/o reg) on all data: 0.21876521
Test loss (w/o reg) on all data: 0.13135868
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.6063924e-05
Norm of the params: 12.473487
              Random: fixed  20 labels. Loss 0.13136. Accuracy 0.981.
### Flips: 260, rs: 17, checks: 156
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086715735
Train loss (w/o reg) on all data: 0.07633317
Test loss (w/o reg) on all data: 0.087441936
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.3617627e-05
Norm of the params: 14.4101095
     Influence (LOO): fixed  93 labels. Loss 0.08744. Accuracy 0.973.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030906059
Train loss (w/o reg) on all data: 0.017110314
Test loss (w/o reg) on all data: 0.031582627
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9996303e-06
Norm of the params: 16.610685
                Loss: fixed 116 labels. Loss 0.03158. Accuracy 0.989.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22041135
Train loss (w/o reg) on all data: 0.21295847
Test loss (w/o reg) on all data: 0.122194014
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.38822625e-05
Norm of the params: 12.208912
              Random: fixed  24 labels. Loss 0.12219. Accuracy 0.981.
### Flips: 260, rs: 17, checks: 208
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052476414
Train loss (w/o reg) on all data: 0.04380013
Test loss (w/o reg) on all data: 0.034845706
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.5741298e-06
Norm of the params: 13.1729145
     Influence (LOO): fixed 113 labels. Loss 0.03485. Accuracy 0.989.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0191413
Train loss (w/o reg) on all data: 0.009505106
Test loss (w/o reg) on all data: 0.011840136
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 2.1929627e-06
Norm of the params: 13.882503
                Loss: fixed 123 labels. Loss 0.01184. Accuracy 1.000.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2119109
Train loss (w/o reg) on all data: 0.20354769
Test loss (w/o reg) on all data: 0.11710603
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3959322e-05
Norm of the params: 12.93307
              Random: fixed  30 labels. Loss 0.11711. Accuracy 0.985.
### Flips: 260, rs: 17, checks: 260
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030076165
Train loss (w/o reg) on all data: 0.023663377
Test loss (w/o reg) on all data: 0.024079591
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.652731e-06
Norm of the params: 11.325005
     Influence (LOO): fixed 124 labels. Loss 0.02408. Accuracy 0.992.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016610058
Train loss (w/o reg) on all data: 0.0077847834
Test loss (w/o reg) on all data: 0.015146641
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6446182e-06
Norm of the params: 13.285537
                Loss: fixed 125 labels. Loss 0.01515. Accuracy 0.992.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20544948
Train loss (w/o reg) on all data: 0.19701314
Test loss (w/o reg) on all data: 0.10332257
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8265586e-05
Norm of the params: 12.989485
              Random: fixed  36 labels. Loss 0.10332. Accuracy 0.996.
### Flips: 260, rs: 17, checks: 312
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023814414
Train loss (w/o reg) on all data: 0.017794348
Test loss (w/o reg) on all data: 0.022936298
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9962741e-06
Norm of the params: 10.972753
     Influence (LOO): fixed 127 labels. Loss 0.02294. Accuracy 0.992.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014214227
Train loss (w/o reg) on all data: 0.006422531
Test loss (w/o reg) on all data: 0.010460522
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3968236e-06
Norm of the params: 12.483346
                Loss: fixed 128 labels. Loss 0.01046. Accuracy 0.996.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20145749
Train loss (w/o reg) on all data: 0.19378208
Test loss (w/o reg) on all data: 0.101789325
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5617697e-05
Norm of the params: 12.389839
              Random: fixed  41 labels. Loss 0.10179. Accuracy 0.996.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24249408
Train loss (w/o reg) on all data: 0.23582521
Test loss (w/o reg) on all data: 0.14373037
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.028726e-05
Norm of the params: 11.548907
Flipped loss: 0.14373. Accuracy: 0.985
### Flips: 260, rs: 18, checks: 52
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1788698
Train loss (w/o reg) on all data: 0.16817594
Test loss (w/o reg) on all data: 0.115162626
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.361115e-05
Norm of the params: 14.624541
     Influence (LOO): fixed  35 labels. Loss 0.11516. Accuracy 0.977.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12397054
Train loss (w/o reg) on all data: 0.11124485
Test loss (w/o reg) on all data: 0.09677914
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.099881e-06
Norm of the params: 15.953488
                Loss: fixed  52 labels. Loss 0.09678. Accuracy 0.977.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23365171
Train loss (w/o reg) on all data: 0.22705384
Test loss (w/o reg) on all data: 0.13197705
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9625338e-05
Norm of the params: 11.487282
              Random: fixed   8 labels. Loss 0.13198. Accuracy 0.985.
### Flips: 260, rs: 18, checks: 104
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11533075
Train loss (w/o reg) on all data: 0.10320559
Test loss (w/o reg) on all data: 0.07885071
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4723314e-05
Norm of the params: 15.572513
     Influence (LOO): fixed  67 labels. Loss 0.07885. Accuracy 0.977.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039344713
Train loss (w/o reg) on all data: 0.024618272
Test loss (w/o reg) on all data: 0.03866281
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7197136e-06
Norm of the params: 17.161842
                Loss: fixed  95 labels. Loss 0.03866. Accuracy 0.985.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22141427
Train loss (w/o reg) on all data: 0.21480104
Test loss (w/o reg) on all data: 0.12186003
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.565027e-05
Norm of the params: 11.500625
              Random: fixed  16 labels. Loss 0.12186. Accuracy 0.989.
### Flips: 260, rs: 18, checks: 156
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060006708
Train loss (w/o reg) on all data: 0.04932955
Test loss (w/o reg) on all data: 0.038518142
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.364871e-06
Norm of the params: 14.613118
     Influence (LOO): fixed  98 labels. Loss 0.03852. Accuracy 0.989.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014693778
Train loss (w/o reg) on all data: 0.0066676037
Test loss (w/o reg) on all data: 0.015788434
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.620423e-07
Norm of the params: 12.6697855
                Loss: fixed 115 labels. Loss 0.01579. Accuracy 0.992.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21199553
Train loss (w/o reg) on all data: 0.20512427
Test loss (w/o reg) on all data: 0.1141245
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.669457e-05
Norm of the params: 11.722847
              Random: fixed  23 labels. Loss 0.11412. Accuracy 0.989.
### Flips: 260, rs: 18, checks: 208
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022103548
Train loss (w/o reg) on all data: 0.013506337
Test loss (w/o reg) on all data: 0.016484922
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3899169e-06
Norm of the params: 13.112751
     Influence (LOO): fixed 112 labels. Loss 0.01648. Accuracy 0.992.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011027014
Train loss (w/o reg) on all data: 0.004542818
Test loss (w/o reg) on all data: 0.012273617
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.444852e-07
Norm of the params: 11.387884
                Loss: fixed 117 labels. Loss 0.01227. Accuracy 0.996.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20693852
Train loss (w/o reg) on all data: 0.20027623
Test loss (w/o reg) on all data: 0.114182495
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.983537e-05
Norm of the params: 11.543224
              Random: fixed  28 labels. Loss 0.11418. Accuracy 0.989.
### Flips: 260, rs: 18, checks: 260
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012861235
Train loss (w/o reg) on all data: 0.007255161
Test loss (w/o reg) on all data: 0.012153672
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.73448e-07
Norm of the params: 10.588743
     Influence (LOO): fixed 118 labels. Loss 0.01215. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011027012
Train loss (w/o reg) on all data: 0.004542604
Test loss (w/o reg) on all data: 0.01227335
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7572166e-06
Norm of the params: 11.388071
                Loss: fixed 117 labels. Loss 0.01227. Accuracy 0.996.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19993478
Train loss (w/o reg) on all data: 0.1936328
Test loss (w/o reg) on all data: 0.11175792
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.1337637e-05
Norm of the params: 11.226737
              Random: fixed  33 labels. Loss 0.11176. Accuracy 0.989.
### Flips: 260, rs: 18, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172996
Test loss (w/o reg) on all data: 0.01205493
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4068362e-07
Norm of the params: 9.153166
     Influence (LOO): fixed 120 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008934775
Train loss (w/o reg) on all data: 0.0035452722
Test loss (w/o reg) on all data: 0.012451518
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.364639e-07
Norm of the params: 10.382199
                Loss: fixed 118 labels. Loss 0.01245. Accuracy 0.996.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19467996
Train loss (w/o reg) on all data: 0.1881161
Test loss (w/o reg) on all data: 0.108547665
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.001694e-05
Norm of the params: 11.457618
              Random: fixed  37 labels. Loss 0.10855. Accuracy 0.985.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23707533
Train loss (w/o reg) on all data: 0.23104005
Test loss (w/o reg) on all data: 0.20231207
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.552951e-05
Norm of the params: 10.986614
Flipped loss: 0.20231. Accuracy: 0.916
### Flips: 260, rs: 19, checks: 52
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17679974
Train loss (w/o reg) on all data: 0.16829167
Test loss (w/o reg) on all data: 0.16312331
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 7.4362756e-06
Norm of the params: 13.044588
     Influence (LOO): fixed  37 labels. Loss 0.16312. Accuracy 0.947.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123481594
Train loss (w/o reg) on all data: 0.11009102
Test loss (w/o reg) on all data: 0.14962602
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.7588876e-05
Norm of the params: 16.364943
                Loss: fixed  50 labels. Loss 0.14963. Accuracy 0.931.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23633374
Train loss (w/o reg) on all data: 0.23028588
Test loss (w/o reg) on all data: 0.20085691
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 2.6180278e-05
Norm of the params: 10.998058
              Random: fixed   2 labels. Loss 0.20086. Accuracy 0.916.
### Flips: 260, rs: 19, checks: 104
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12486624
Train loss (w/o reg) on all data: 0.11508777
Test loss (w/o reg) on all data: 0.11364878
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.453956e-06
Norm of the params: 13.984615
     Influence (LOO): fixed  68 labels. Loss 0.11365. Accuracy 0.962.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053591058
Train loss (w/o reg) on all data: 0.037278507
Test loss (w/o reg) on all data: 0.110052235
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.622044e-06
Norm of the params: 18.062422
                Loss: fixed  90 labels. Loss 0.11005. Accuracy 0.954.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23167023
Train loss (w/o reg) on all data: 0.22529308
Test loss (w/o reg) on all data: 0.19212733
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.852763e-05
Norm of the params: 11.293496
              Random: fixed   7 labels. Loss 0.19213. Accuracy 0.920.
### Flips: 260, rs: 19, checks: 156
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071046576
Train loss (w/o reg) on all data: 0.060405318
Test loss (w/o reg) on all data: 0.09013834
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.7423827e-06
Norm of the params: 14.588528
     Influence (LOO): fixed  94 labels. Loss 0.09014. Accuracy 0.954.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030091684
Train loss (w/o reg) on all data: 0.017430551
Test loss (w/o reg) on all data: 0.045788825
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.069031e-06
Norm of the params: 15.912971
                Loss: fixed 109 labels. Loss 0.04579. Accuracy 0.977.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22842567
Train loss (w/o reg) on all data: 0.2219586
Test loss (w/o reg) on all data: 0.17663078
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 4.5231e-05
Norm of the params: 11.372825
              Random: fixed  13 labels. Loss 0.17663. Accuracy 0.935.
### Flips: 260, rs: 19, checks: 208
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041932225
Train loss (w/o reg) on all data: 0.032127734
Test loss (w/o reg) on all data: 0.07962355
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.8373668e-06
Norm of the params: 14.003208
     Influence (LOO): fixed 108 labels. Loss 0.07962. Accuracy 0.966.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023181573
Train loss (w/o reg) on all data: 0.0122604985
Test loss (w/o reg) on all data: 0.037829056
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8998091e-06
Norm of the params: 14.77909
                Loss: fixed 117 labels. Loss 0.03783. Accuracy 0.985.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2166203
Train loss (w/o reg) on all data: 0.20985977
Test loss (w/o reg) on all data: 0.1690433
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.397035e-05
Norm of the params: 11.628006
              Random: fixed  21 labels. Loss 0.16904. Accuracy 0.943.
### Flips: 260, rs: 19, checks: 260
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02497743
Train loss (w/o reg) on all data: 0.015853122
Test loss (w/o reg) on all data: 0.06573298
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.809699e-06
Norm of the params: 13.508745
     Influence (LOO): fixed 115 labels. Loss 0.06573. Accuracy 0.969.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01419461
Train loss (w/o reg) on all data: 0.006414579
Test loss (w/o reg) on all data: 0.02401585
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.813942e-07
Norm of the params: 12.473997
                Loss: fixed 123 labels. Loss 0.02402. Accuracy 0.989.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21071838
Train loss (w/o reg) on all data: 0.20369203
Test loss (w/o reg) on all data: 0.15687476
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 8.0725724e-05
Norm of the params: 11.854404
              Random: fixed  27 labels. Loss 0.15687. Accuracy 0.943.
### Flips: 260, rs: 19, checks: 312
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011371432
Train loss (w/o reg) on all data: 0.00589168
Test loss (w/o reg) on all data: 0.0242818
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.1239093e-07
Norm of the params: 10.468764
     Influence (LOO): fixed 126 labels. Loss 0.02428. Accuracy 0.989.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011434047
Train loss (w/o reg) on all data: 0.004718407
Test loss (w/o reg) on all data: 0.017106853
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0016793e-06
Norm of the params: 11.589339
                Loss: fixed 125 labels. Loss 0.01711. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2043997
Train loss (w/o reg) on all data: 0.19778195
Test loss (w/o reg) on all data: 0.1471326
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.834944e-05
Norm of the params: 11.504569
              Random: fixed  34 labels. Loss 0.14713. Accuracy 0.954.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24502786
Train loss (w/o reg) on all data: 0.23731622
Test loss (w/o reg) on all data: 0.17498153
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.192558e-06
Norm of the params: 12.419044
Flipped loss: 0.17498. Accuracy: 0.958
### Flips: 260, rs: 20, checks: 52
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18398544
Train loss (w/o reg) on all data: 0.17358026
Test loss (w/o reg) on all data: 0.13758303
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.2838984e-05
Norm of the params: 14.425803
     Influence (LOO): fixed  37 labels. Loss 0.13758. Accuracy 0.981.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13701677
Train loss (w/o reg) on all data: 0.12386559
Test loss (w/o reg) on all data: 0.12482873
Train acc on all data:  0.941738299904489
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.8397777e-06
Norm of the params: 16.218002
                Loss: fixed  51 labels. Loss 0.12483. Accuracy 0.950.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23826732
Train loss (w/o reg) on all data: 0.23090255
Test loss (w/o reg) on all data: 0.16397956
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3246986e-05
Norm of the params: 12.136532
              Random: fixed   8 labels. Loss 0.16398. Accuracy 0.973.
### Flips: 260, rs: 20, checks: 104
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13263328
Train loss (w/o reg) on all data: 0.12192708
Test loss (w/o reg) on all data: 0.10821078
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.5016646e-06
Norm of the params: 14.632976
     Influence (LOO): fixed  65 labels. Loss 0.10821. Accuracy 0.962.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052793246
Train loss (w/o reg) on all data: 0.038169406
Test loss (w/o reg) on all data: 0.0727944
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.7742983e-06
Norm of the params: 17.101954
                Loss: fixed 100 labels. Loss 0.07279. Accuracy 0.985.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23223275
Train loss (w/o reg) on all data: 0.22495772
Test loss (w/o reg) on all data: 0.15846936
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.071749e-05
Norm of the params: 12.062367
              Random: fixed  14 labels. Loss 0.15847. Accuracy 0.966.
### Flips: 260, rs: 20, checks: 156
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073840186
Train loss (w/o reg) on all data: 0.063746005
Test loss (w/o reg) on all data: 0.056608997
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.5037763e-06
Norm of the params: 14.208577
     Influence (LOO): fixed  94 labels. Loss 0.05661. Accuracy 0.977.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02350492
Train loss (w/o reg) on all data: 0.012283045
Test loss (w/o reg) on all data: 0.040891565
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.7160133e-06
Norm of the params: 14.981238
                Loss: fixed 115 labels. Loss 0.04089. Accuracy 0.989.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22328444
Train loss (w/o reg) on all data: 0.21595262
Test loss (w/o reg) on all data: 0.15452486
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.0730906e-05
Norm of the params: 12.109348
              Random: fixed  20 labels. Loss 0.15452. Accuracy 0.966.
### Flips: 260, rs: 20, checks: 208
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053810004
Train loss (w/o reg) on all data: 0.04348976
Test loss (w/o reg) on all data: 0.049622014
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.899787e-06
Norm of the params: 14.366798
     Influence (LOO): fixed 104 labels. Loss 0.04962. Accuracy 0.977.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01727261
Train loss (w/o reg) on all data: 0.0076582753
Test loss (w/o reg) on all data: 0.043796208
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9717245e-06
Norm of the params: 13.866749
                Loss: fixed 118 labels. Loss 0.04380. Accuracy 0.989.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21413216
Train loss (w/o reg) on all data: 0.20615172
Test loss (w/o reg) on all data: 0.14401464
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5731373e-05
Norm of the params: 12.633633
              Random: fixed  26 labels. Loss 0.14401. Accuracy 0.973.
### Flips: 260, rs: 20, checks: 260
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027270151
Train loss (w/o reg) on all data: 0.018452313
Test loss (w/o reg) on all data: 0.028854134
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3666331e-06
Norm of the params: 13.279939
     Influence (LOO): fixed 117 labels. Loss 0.02885. Accuracy 0.985.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014752887
Train loss (w/o reg) on all data: 0.00630534
Test loss (w/o reg) on all data: 0.041109618
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4743174e-06
Norm of the params: 12.998113
                Loss: fixed 120 labels. Loss 0.04111. Accuracy 0.985.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20823035
Train loss (w/o reg) on all data: 0.20045744
Test loss (w/o reg) on all data: 0.13625737
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.493584e-05
Norm of the params: 12.468282
              Random: fixed  32 labels. Loss 0.13626. Accuracy 0.981.
### Flips: 260, rs: 20, checks: 312
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017818794
Train loss (w/o reg) on all data: 0.010719148
Test loss (w/o reg) on all data: 0.016776778
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.917838e-07
Norm of the params: 11.916077
     Influence (LOO): fixed 122 labels. Loss 0.01678. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011228532
Train loss (w/o reg) on all data: 0.00427959
Test loss (w/o reg) on all data: 0.03052554
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1936538e-07
Norm of the params: 11.788929
                Loss: fixed 121 labels. Loss 0.03053. Accuracy 0.985.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19967817
Train loss (w/o reg) on all data: 0.19189914
Test loss (w/o reg) on all data: 0.12951253
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.616043e-05
Norm of the params: 12.473192
              Random: fixed  38 labels. Loss 0.12951. Accuracy 0.977.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23637651
Train loss (w/o reg) on all data: 0.22725974
Test loss (w/o reg) on all data: 0.16081183
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.269225e-05
Norm of the params: 13.5031595
Flipped loss: 0.16081. Accuracy: 0.969
### Flips: 260, rs: 21, checks: 52
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18015876
Train loss (w/o reg) on all data: 0.16798642
Test loss (w/o reg) on all data: 0.10991995
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.612396e-05
Norm of the params: 15.602785
     Influence (LOO): fixed  34 labels. Loss 0.10992. Accuracy 0.985.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12495912
Train loss (w/o reg) on all data: 0.10953352
Test loss (w/o reg) on all data: 0.12240209
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 9.510537e-06
Norm of the params: 17.56451
                Loss: fixed  51 labels. Loss 0.12240. Accuracy 0.943.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23204422
Train loss (w/o reg) on all data: 0.22289541
Test loss (w/o reg) on all data: 0.14742616
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.833505e-05
Norm of the params: 13.526867
              Random: fixed   7 labels. Loss 0.14743. Accuracy 0.977.
### Flips: 260, rs: 21, checks: 104
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11860544
Train loss (w/o reg) on all data: 0.10720789
Test loss (w/o reg) on all data: 0.07252324
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.332263e-06
Norm of the params: 15.098051
     Influence (LOO): fixed  68 labels. Loss 0.07252. Accuracy 0.981.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048489034
Train loss (w/o reg) on all data: 0.032780092
Test loss (w/o reg) on all data: 0.0421664
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.032966e-06
Norm of the params: 17.72509
                Loss: fixed  95 labels. Loss 0.04217. Accuracy 0.985.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2266678
Train loss (w/o reg) on all data: 0.21802264
Test loss (w/o reg) on all data: 0.14349014
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.2144835e-05
Norm of the params: 13.149262
              Random: fixed  14 labels. Loss 0.14349. Accuracy 0.977.
### Flips: 260, rs: 21, checks: 156
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06840945
Train loss (w/o reg) on all data: 0.05975811
Test loss (w/o reg) on all data: 0.042503256
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.846701e-06
Norm of the params: 13.153963
     Influence (LOO): fixed  96 labels. Loss 0.04250. Accuracy 0.985.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025908217
Train loss (w/o reg) on all data: 0.0142182065
Test loss (w/o reg) on all data: 0.02993783
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.0032315e-06
Norm of the params: 15.290527
                Loss: fixed 108 labels. Loss 0.02994. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21707177
Train loss (w/o reg) on all data: 0.2078625
Test loss (w/o reg) on all data: 0.13858545
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6689022e-05
Norm of the params: 13.571495
              Random: fixed  20 labels. Loss 0.13859. Accuracy 0.977.
### Flips: 260, rs: 21, checks: 208
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029771712
Train loss (w/o reg) on all data: 0.022237279
Test loss (w/o reg) on all data: 0.01544157
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.9925063e-06
Norm of the params: 12.275532
     Influence (LOO): fixed 112 labels. Loss 0.01544. Accuracy 0.996.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018582797
Train loss (w/o reg) on all data: 0.008980525
Test loss (w/o reg) on all data: 0.02318778
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.47038e-07
Norm of the params: 13.858047
                Loss: fixed 112 labels. Loss 0.02319. Accuracy 0.989.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2014949
Train loss (w/o reg) on all data: 0.19133683
Test loss (w/o reg) on all data: 0.1280239
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.9837367e-05
Norm of the params: 14.25348
              Random: fixed  28 labels. Loss 0.12802. Accuracy 0.973.
### Flips: 260, rs: 21, checks: 260
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016455805
Train loss (w/o reg) on all data: 0.010376028
Test loss (w/o reg) on all data: 0.01636417
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9418574e-07
Norm of the params: 11.027038
     Influence (LOO): fixed 119 labels. Loss 0.01636. Accuracy 0.992.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016684912
Train loss (w/o reg) on all data: 0.0077888058
Test loss (w/o reg) on all data: 0.015619141
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9970235e-07
Norm of the params: 13.338746
                Loss: fixed 114 labels. Loss 0.01562. Accuracy 0.992.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18899037
Train loss (w/o reg) on all data: 0.17875071
Test loss (w/o reg) on all data: 0.12156353
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8855997e-05
Norm of the params: 14.310597
              Random: fixed  38 labels. Loss 0.12156. Accuracy 0.973.
### Flips: 260, rs: 21, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729276
Test loss (w/o reg) on all data: 0.012054223
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6357907e-07
Norm of the params: 9.153241
     Influence (LOO): fixed 122 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016684912
Train loss (w/o reg) on all data: 0.007788906
Test loss (w/o reg) on all data: 0.015619433
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.961535e-07
Norm of the params: 13.338671
                Loss: fixed 114 labels. Loss 0.01562. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1811102
Train loss (w/o reg) on all data: 0.17025474
Test loss (w/o reg) on all data: 0.119434215
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.0764953e-05
Norm of the params: 14.734626
              Random: fixed  42 labels. Loss 0.11943. Accuracy 0.977.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2533691
Train loss (w/o reg) on all data: 0.24652578
Test loss (w/o reg) on all data: 0.17095576
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.9106162e-05
Norm of the params: 11.698998
Flipped loss: 0.17096. Accuracy: 0.973
### Flips: 260, rs: 22, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18887055
Train loss (w/o reg) on all data: 0.17816612
Test loss (w/o reg) on all data: 0.12919138
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.8405504e-05
Norm of the params: 14.631765
     Influence (LOO): fixed  39 labels. Loss 0.12919. Accuracy 0.977.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13975506
Train loss (w/o reg) on all data: 0.12792741
Test loss (w/o reg) on all data: 0.10130582
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.531725e-05
Norm of the params: 15.380274
                Loss: fixed  52 labels. Loss 0.10131. Accuracy 0.958.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24759232
Train loss (w/o reg) on all data: 0.24068536
Test loss (w/o reg) on all data: 0.1664844
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.602061e-05
Norm of the params: 11.75326
              Random: fixed   4 labels. Loss 0.16648. Accuracy 0.973.
### Flips: 260, rs: 22, checks: 104
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14276949
Train loss (w/o reg) on all data: 0.13026752
Test loss (w/o reg) on all data: 0.10251026
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.401548e-05
Norm of the params: 15.812637
     Influence (LOO): fixed  65 labels. Loss 0.10251. Accuracy 0.981.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05095611
Train loss (w/o reg) on all data: 0.037448376
Test loss (w/o reg) on all data: 0.03591373
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1943237e-05
Norm of the params: 16.436382
                Loss: fixed 100 labels. Loss 0.03591. Accuracy 0.989.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24578108
Train loss (w/o reg) on all data: 0.23864459
Test loss (w/o reg) on all data: 0.16479157
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.834031e-05
Norm of the params: 11.946958
              Random: fixed   7 labels. Loss 0.16479. Accuracy 0.977.
### Flips: 260, rs: 22, checks: 156
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09968338
Train loss (w/o reg) on all data: 0.08846681
Test loss (w/o reg) on all data: 0.0652885
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.258691e-05
Norm of the params: 14.9777
     Influence (LOO): fixed  91 labels. Loss 0.06529. Accuracy 0.989.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018308138
Train loss (w/o reg) on all data: 0.009276224
Test loss (w/o reg) on all data: 0.018082181
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9411627e-06
Norm of the params: 13.440175
                Loss: fixed 122 labels. Loss 0.01808. Accuracy 0.992.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23796436
Train loss (w/o reg) on all data: 0.23075962
Test loss (w/o reg) on all data: 0.16270247
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.0672137e-05
Norm of the params: 12.003956
              Random: fixed  13 labels. Loss 0.16270. Accuracy 0.973.
### Flips: 260, rs: 22, checks: 208
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05992101
Train loss (w/o reg) on all data: 0.050647642
Test loss (w/o reg) on all data: 0.032486062
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.3571166e-06
Norm of the params: 13.618641
     Influence (LOO): fixed 109 labels. Loss 0.03249. Accuracy 0.996.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015574966
Train loss (w/o reg) on all data: 0.007276618
Test loss (w/o reg) on all data: 0.018626902
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4657517e-07
Norm of the params: 12.882816
                Loss: fixed 124 labels. Loss 0.01863. Accuracy 0.992.
Using normal model
LBFGS training took [323] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22736008
Train loss (w/o reg) on all data: 0.21931358
Test loss (w/o reg) on all data: 0.1501724
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0418769e-05
Norm of the params: 12.6858225
              Random: fixed  20 labels. Loss 0.15017. Accuracy 0.973.
### Flips: 260, rs: 22, checks: 260
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02173854
Train loss (w/o reg) on all data: 0.015049501
Test loss (w/o reg) on all data: 0.013740826
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9847266e-06
Norm of the params: 11.566363
     Influence (LOO): fixed 125 labels. Loss 0.01374. Accuracy 0.996.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012244193
Train loss (w/o reg) on all data: 0.005581116
Test loss (w/o reg) on all data: 0.01745112
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.272796e-07
Norm of the params: 11.543896
                Loss: fixed 125 labels. Loss 0.01745. Accuracy 0.992.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22108234
Train loss (w/o reg) on all data: 0.21389258
Test loss (w/o reg) on all data: 0.13464698
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 0.0001184632
Norm of the params: 11.991468
              Random: fixed  29 labels. Loss 0.13465. Accuracy 0.985.
### Flips: 260, rs: 22, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013299495
Train loss (w/o reg) on all data: 0.008382315
Test loss (w/o reg) on all data: 0.014037035
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6268975e-06
Norm of the params: 9.916835
     Influence (LOO): fixed 128 labels. Loss 0.01404. Accuracy 0.996.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009419197
Train loss (w/o reg) on all data: 0.003744018
Test loss (w/o reg) on all data: 0.012033536
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.738389e-07
Norm of the params: 10.653807
                Loss: fixed 127 labels. Loss 0.01203. Accuracy 0.996.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21315289
Train loss (w/o reg) on all data: 0.20596893
Test loss (w/o reg) on all data: 0.12045052
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4244539e-05
Norm of the params: 11.986615
              Random: fixed  35 labels. Loss 0.12045. Accuracy 0.989.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23858161
Train loss (w/o reg) on all data: 0.23014344
Test loss (w/o reg) on all data: 0.16866848
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.2354786e-05
Norm of the params: 12.990897
Flipped loss: 0.16867. Accuracy: 0.966
### Flips: 260, rs: 23, checks: 52
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16516405
Train loss (w/o reg) on all data: 0.155253
Test loss (w/o reg) on all data: 0.14725094
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.9560644e-05
Norm of the params: 14.079106
     Influence (LOO): fixed  39 labels. Loss 0.14725. Accuracy 0.947.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12291561
Train loss (w/o reg) on all data: 0.10852207
Test loss (w/o reg) on all data: 0.12152552
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.594984e-06
Norm of the params: 16.966753
                Loss: fixed  51 labels. Loss 0.12153. Accuracy 0.950.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22720718
Train loss (w/o reg) on all data: 0.21857394
Test loss (w/o reg) on all data: 0.15256138
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.5820674e-05
Norm of the params: 13.140195
              Random: fixed   9 labels. Loss 0.15256. Accuracy 0.962.
### Flips: 260, rs: 23, checks: 104
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11227176
Train loss (w/o reg) on all data: 0.09979108
Test loss (w/o reg) on all data: 0.13136859
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.0727615e-05
Norm of the params: 15.799166
     Influence (LOO): fixed  65 labels. Loss 0.13137. Accuracy 0.943.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041702434
Train loss (w/o reg) on all data: 0.026191657
Test loss (w/o reg) on all data: 0.059818722
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.378542e-05
Norm of the params: 17.612936
                Loss: fixed  97 labels. Loss 0.05982. Accuracy 0.981.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22286291
Train loss (w/o reg) on all data: 0.21443981
Test loss (w/o reg) on all data: 0.14674307
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2466488e-05
Norm of the params: 12.979293
              Random: fixed  13 labels. Loss 0.14674. Accuracy 0.973.
### Flips: 260, rs: 23, checks: 156
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07343808
Train loss (w/o reg) on all data: 0.0623136
Test loss (w/o reg) on all data: 0.10692048
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.0752794e-05
Norm of the params: 14.916085
     Influence (LOO): fixed  92 labels. Loss 0.10692. Accuracy 0.962.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020769652
Train loss (w/o reg) on all data: 0.010110018
Test loss (w/o reg) on all data: 0.018809918
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0649288e-06
Norm of the params: 14.601119
                Loss: fixed 112 labels. Loss 0.01881. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21301481
Train loss (w/o reg) on all data: 0.20410793
Test loss (w/o reg) on all data: 0.14186285
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.6719862e-05
Norm of the params: 13.346825
              Random: fixed  21 labels. Loss 0.14186. Accuracy 0.973.
### Flips: 260, rs: 23, checks: 208
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036232922
Train loss (w/o reg) on all data: 0.02835937
Test loss (w/o reg) on all data: 0.047050793
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.206114e-06
Norm of the params: 12.548746
     Influence (LOO): fixed 111 labels. Loss 0.04705. Accuracy 0.985.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013789039
Train loss (w/o reg) on all data: 0.0062282705
Test loss (w/o reg) on all data: 0.013391732
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.325276e-06
Norm of the params: 12.296967
                Loss: fixed 117 labels. Loss 0.01339. Accuracy 0.996.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20261635
Train loss (w/o reg) on all data: 0.19401124
Test loss (w/o reg) on all data: 0.12787978
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.504327e-05
Norm of the params: 13.118767
              Random: fixed  29 labels. Loss 0.12788. Accuracy 0.985.
### Flips: 260, rs: 23, checks: 260
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023872297
Train loss (w/o reg) on all data: 0.01741524
Test loss (w/o reg) on all data: 0.020210117
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3137874e-06
Norm of the params: 11.364029
     Influence (LOO): fixed 118 labels. Loss 0.02021. Accuracy 0.996.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009296382
Train loss (w/o reg) on all data: 0.0037495592
Test loss (w/o reg) on all data: 0.012859901
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.612872e-07
Norm of the params: 10.532639
                Loss: fixed 119 labels. Loss 0.01286. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19476038
Train loss (w/o reg) on all data: 0.18640195
Test loss (w/o reg) on all data: 0.12427218
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8354945e-05
Norm of the params: 12.929367
              Random: fixed  33 labels. Loss 0.12427. Accuracy 0.985.
### Flips: 260, rs: 23, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728955
Test loss (w/o reg) on all data: 0.012055052
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.18224e-07
Norm of the params: 9.1532755
     Influence (LOO): fixed 124 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076053618
Train loss (w/o reg) on all data: 0.002793455
Test loss (w/o reg) on all data: 0.011898592
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4031525e-06
Norm of the params: 9.810103
                Loss: fixed 121 labels. Loss 0.01190. Accuracy 0.996.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1865706
Train loss (w/o reg) on all data: 0.17843167
Test loss (w/o reg) on all data: 0.115998015
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.7919515e-05
Norm of the params: 12.758468
              Random: fixed  40 labels. Loss 0.11600. Accuracy 0.981.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24663405
Train loss (w/o reg) on all data: 0.23975958
Test loss (w/o reg) on all data: 0.15187448
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5560794e-05
Norm of the params: 11.725585
Flipped loss: 0.15187. Accuracy: 0.985
### Flips: 260, rs: 24, checks: 52
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18661419
Train loss (w/o reg) on all data: 0.17696927
Test loss (w/o reg) on all data: 0.11813963
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3235581e-05
Norm of the params: 13.888786
     Influence (LOO): fixed  34 labels. Loss 0.11814. Accuracy 0.989.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13207585
Train loss (w/o reg) on all data: 0.11986765
Test loss (w/o reg) on all data: 0.07927232
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.8855865e-06
Norm of the params: 15.62574
                Loss: fixed  52 labels. Loss 0.07927. Accuracy 0.981.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23833176
Train loss (w/o reg) on all data: 0.23157236
Test loss (w/o reg) on all data: 0.1474057
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.6686997e-05
Norm of the params: 11.627043
              Random: fixed   6 labels. Loss 0.14741. Accuracy 0.981.
### Flips: 260, rs: 24, checks: 104
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13440363
Train loss (w/o reg) on all data: 0.12420115
Test loss (w/o reg) on all data: 0.08827894
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4463612e-05
Norm of the params: 14.28459
     Influence (LOO): fixed  63 labels. Loss 0.08828. Accuracy 0.989.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04153587
Train loss (w/o reg) on all data: 0.027256222
Test loss (w/o reg) on all data: 0.033652883
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0651168e-05
Norm of the params: 16.899496
                Loss: fixed  96 labels. Loss 0.03365. Accuracy 0.989.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22761743
Train loss (w/o reg) on all data: 0.22089523
Test loss (w/o reg) on all data: 0.13545182
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8371923e-05
Norm of the params: 11.59499
              Random: fixed  14 labels. Loss 0.13545. Accuracy 0.989.
### Flips: 260, rs: 24, checks: 156
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08410188
Train loss (w/o reg) on all data: 0.07259327
Test loss (w/o reg) on all data: 0.055206936
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.16254605e-05
Norm of the params: 15.171424
     Influence (LOO): fixed  89 labels. Loss 0.05521. Accuracy 0.989.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014426155
Train loss (w/o reg) on all data: 0.0066048615
Test loss (w/o reg) on all data: 0.015919665
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3147656e-07
Norm of the params: 12.507033
                Loss: fixed 115 labels. Loss 0.01592. Accuracy 0.996.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22481658
Train loss (w/o reg) on all data: 0.21810511
Test loss (w/o reg) on all data: 0.13367638
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.542245e-05
Norm of the params: 11.585738
              Random: fixed  18 labels. Loss 0.13368. Accuracy 0.981.
### Flips: 260, rs: 24, checks: 208
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049858138
Train loss (w/o reg) on all data: 0.040951923
Test loss (w/o reg) on all data: 0.03428965
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.437454e-06
Norm of the params: 13.346323
     Influence (LOO): fixed 106 labels. Loss 0.03429. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007345075
Train loss (w/o reg) on all data: 0.002587031
Test loss (w/o reg) on all data: 0.013703188
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0134024e-07
Norm of the params: 9.755043
                Loss: fixed 120 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21024963
Train loss (w/o reg) on all data: 0.20295106
Test loss (w/o reg) on all data: 0.11534816
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0936098e-05
Norm of the params: 12.081869
              Random: fixed  27 labels. Loss 0.11535. Accuracy 0.981.
### Flips: 260, rs: 24, checks: 260
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489635
Train loss (w/o reg) on all data: 0.005539658
Test loss (w/o reg) on all data: 0.013844459
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.6029266e-07
Norm of the params: 9.949851
     Influence (LOO): fixed 120 labels. Loss 0.01384. Accuracy 0.989.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073450767
Train loss (w/o reg) on all data: 0.0025869834
Test loss (w/o reg) on all data: 0.013702378
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.643036e-07
Norm of the params: 9.755095
                Loss: fixed 120 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20207025
Train loss (w/o reg) on all data: 0.19468652
Test loss (w/o reg) on all data: 0.11122145
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.5680734e-05
Norm of the params: 12.152146
              Random: fixed  33 labels. Loss 0.11122. Accuracy 0.981.
### Flips: 260, rs: 24, checks: 312
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489633
Train loss (w/o reg) on all data: 0.005539244
Test loss (w/o reg) on all data: 0.013844387
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.5347164e-07
Norm of the params: 9.950266
     Influence (LOO): fixed 120 labels. Loss 0.01384. Accuracy 0.989.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073450776
Train loss (w/o reg) on all data: 0.0025869922
Test loss (w/o reg) on all data: 0.013702921
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3292084e-07
Norm of the params: 9.755086
                Loss: fixed 120 labels. Loss 0.01370. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.187806
Train loss (w/o reg) on all data: 0.18095261
Test loss (w/o reg) on all data: 0.10395035
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.3467083e-05
Norm of the params: 11.707599
              Random: fixed  44 labels. Loss 0.10395. Accuracy 0.989.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25013202
Train loss (w/o reg) on all data: 0.243766
Test loss (w/o reg) on all data: 0.16733658
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.864505e-05
Norm of the params: 11.283631
Flipped loss: 0.16734. Accuracy: 0.969
### Flips: 260, rs: 25, checks: 52
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17930964
Train loss (w/o reg) on all data: 0.17089666
Test loss (w/o reg) on all data: 0.10444689
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.507803e-05
Norm of the params: 12.97148
     Influence (LOO): fixed  42 labels. Loss 0.10445. Accuracy 0.985.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13677737
Train loss (w/o reg) on all data: 0.123395026
Test loss (w/o reg) on all data: 0.10872628
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1193188e-05
Norm of the params: 16.359919
                Loss: fixed  52 labels. Loss 0.10873. Accuracy 0.962.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24077675
Train loss (w/o reg) on all data: 0.23404394
Test loss (w/o reg) on all data: 0.1560883
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.280725e-05
Norm of the params: 11.604137
              Random: fixed   6 labels. Loss 0.15609. Accuracy 0.966.
### Flips: 260, rs: 25, checks: 104
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12312625
Train loss (w/o reg) on all data: 0.11362167
Test loss (w/o reg) on all data: 0.06923919
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.7698742e-06
Norm of the params: 13.787374
     Influence (LOO): fixed  73 labels. Loss 0.06924. Accuracy 0.977.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050727457
Train loss (w/o reg) on all data: 0.037169933
Test loss (w/o reg) on all data: 0.036596622
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8406512e-06
Norm of the params: 16.466648
                Loss: fixed  99 labels. Loss 0.03660. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2304001
Train loss (w/o reg) on all data: 0.22379012
Test loss (w/o reg) on all data: 0.1360702
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4498483e-05
Norm of the params: 11.497805
              Random: fixed  15 labels. Loss 0.13607. Accuracy 0.981.
### Flips: 260, rs: 25, checks: 156
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0838182
Train loss (w/o reg) on all data: 0.07460456
Test loss (w/o reg) on all data: 0.0486382
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.0709652e-06
Norm of the params: 13.574709
     Influence (LOO): fixed  93 labels. Loss 0.04864. Accuracy 0.981.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016812578
Train loss (w/o reg) on all data: 0.007971716
Test loss (w/o reg) on all data: 0.020544233
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.026932e-07
Norm of the params: 13.297263
                Loss: fixed 116 labels. Loss 0.02054. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21875387
Train loss (w/o reg) on all data: 0.21190675
Test loss (w/o reg) on all data: 0.13004492
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.4594602e-05
Norm of the params: 11.702246
              Random: fixed  24 labels. Loss 0.13004. Accuracy 0.973.
### Flips: 260, rs: 25, checks: 208
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04263689
Train loss (w/o reg) on all data: 0.035432424
Test loss (w/o reg) on all data: 0.018433575
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.175418e-06
Norm of the params: 12.00372
     Influence (LOO): fixed 115 labels. Loss 0.01843. Accuracy 0.992.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016415082
Train loss (w/o reg) on all data: 0.007738789
Test loss (w/o reg) on all data: 0.02313567
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9456226e-06
Norm of the params: 13.172923
                Loss: fixed 118 labels. Loss 0.02314. Accuracy 0.989.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2108412
Train loss (w/o reg) on all data: 0.20405418
Test loss (w/o reg) on all data: 0.12232256
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.345604e-05
Norm of the params: 11.650766
              Random: fixed  31 labels. Loss 0.12232. Accuracy 0.973.
### Flips: 260, rs: 25, checks: 260
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0152252205
Train loss (w/o reg) on all data: 0.008902317
Test loss (w/o reg) on all data: 0.01989745
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.9799004e-07
Norm of the params: 11.2453575
     Influence (LOO): fixed 123 labels. Loss 0.01990. Accuracy 0.996.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015287604
Train loss (w/o reg) on all data: 0.006951154
Test loss (w/o reg) on all data: 0.024896106
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2845171e-06
Norm of the params: 12.912358
                Loss: fixed 119 labels. Loss 0.02490. Accuracy 0.989.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20552674
Train loss (w/o reg) on all data: 0.19856483
Test loss (w/o reg) on all data: 0.11342725
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4203275e-05
Norm of the params: 11.799923
              Random: fixed  36 labels. Loss 0.11343. Accuracy 0.981.
### Flips: 260, rs: 25, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729555
Test loss (w/o reg) on all data: 0.0120546175
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9977238e-07
Norm of the params: 9.153209
     Influence (LOO): fixed 126 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008785678
Train loss (w/o reg) on all data: 0.0033048776
Test loss (w/o reg) on all data: 0.022251613
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7466315e-07
Norm of the params: 10.469767
                Loss: fixed 123 labels. Loss 0.02225. Accuracy 0.992.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19766639
Train loss (w/o reg) on all data: 0.19053587
Test loss (w/o reg) on all data: 0.105128445
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.8513484e-05
Norm of the params: 11.941964
              Random: fixed  43 labels. Loss 0.10513. Accuracy 0.985.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2459453
Train loss (w/o reg) on all data: 0.23962906
Test loss (w/o reg) on all data: 0.17910852
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.32913065e-05
Norm of the params: 11.239434
Flipped loss: 0.17911. Accuracy: 0.958
### Flips: 260, rs: 26, checks: 52
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19423357
Train loss (w/o reg) on all data: 0.18512335
Test loss (w/o reg) on all data: 0.15049411
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4899474e-05
Norm of the params: 13.498309
     Influence (LOO): fixed  34 labels. Loss 0.15049. Accuracy 0.966.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13143477
Train loss (w/o reg) on all data: 0.1161549
Test loss (w/o reg) on all data: 0.13109872
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 8.143838e-06
Norm of the params: 17.481342
                Loss: fixed  51 labels. Loss 0.13110. Accuracy 0.954.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24117638
Train loss (w/o reg) on all data: 0.23481892
Test loss (w/o reg) on all data: 0.17225061
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.5682024e-05
Norm of the params: 11.27605
              Random: fixed   6 labels. Loss 0.17225. Accuracy 0.958.
### Flips: 260, rs: 26, checks: 104
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1315502
Train loss (w/o reg) on all data: 0.1189648
Test loss (w/o reg) on all data: 0.11519026
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9263069e-05
Norm of the params: 15.8653
     Influence (LOO): fixed  66 labels. Loss 0.11519. Accuracy 0.977.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058567293
Train loss (w/o reg) on all data: 0.042638417
Test loss (w/o reg) on all data: 0.07864292
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.5947732e-06
Norm of the params: 17.84874
                Loss: fixed  91 labels. Loss 0.07864. Accuracy 0.958.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23595923
Train loss (w/o reg) on all data: 0.22953947
Test loss (w/o reg) on all data: 0.15766102
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.5319764e-05
Norm of the params: 11.331158
              Random: fixed  14 labels. Loss 0.15766. Accuracy 0.969.
### Flips: 260, rs: 26, checks: 156
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09527124
Train loss (w/o reg) on all data: 0.083541475
Test loss (w/o reg) on all data: 0.07517131
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.8757336e-06
Norm of the params: 15.316503
     Influence (LOO): fixed  88 labels. Loss 0.07517. Accuracy 0.981.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029305365
Train loss (w/o reg) on all data: 0.01725259
Test loss (w/o reg) on all data: 0.023700569
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.407348e-06
Norm of the params: 15.525962
                Loss: fixed 115 labels. Loss 0.02370. Accuracy 0.992.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23276708
Train loss (w/o reg) on all data: 0.22624737
Test loss (w/o reg) on all data: 0.15453146
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2091043e-05
Norm of the params: 11.419018
              Random: fixed  17 labels. Loss 0.15453. Accuracy 0.973.
### Flips: 260, rs: 26, checks: 208
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07184114
Train loss (w/o reg) on all data: 0.06188397
Test loss (w/o reg) on all data: 0.052093115
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.152146e-06
Norm of the params: 14.111819
     Influence (LOO): fixed 104 labels. Loss 0.05209. Accuracy 0.992.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017817594
Train loss (w/o reg) on all data: 0.009110481
Test loss (w/o reg) on all data: 0.01594496
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.697139e-07
Norm of the params: 13.196297
                Loss: fixed 123 labels. Loss 0.01594. Accuracy 0.989.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22927676
Train loss (w/o reg) on all data: 0.22298585
Test loss (w/o reg) on all data: 0.14694494
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.0534985e-05
Norm of the params: 11.216874
              Random: fixed  22 labels. Loss 0.14694. Accuracy 0.973.
### Flips: 260, rs: 26, checks: 260
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040041618
Train loss (w/o reg) on all data: 0.031563926
Test loss (w/o reg) on all data: 0.027594414
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3752716e-06
Norm of the params: 13.021284
     Influence (LOO): fixed 120 labels. Loss 0.02759. Accuracy 0.992.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010863328
Train loss (w/o reg) on all data: 0.0046855058
Test loss (w/o reg) on all data: 0.010361113
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.602541e-07
Norm of the params: 11.115595
                Loss: fixed 128 labels. Loss 0.01036. Accuracy 0.996.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22221723
Train loss (w/o reg) on all data: 0.21535903
Test loss (w/o reg) on all data: 0.14415279
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3167113e-05
Norm of the params: 11.711703
              Random: fixed  29 labels. Loss 0.14415. Accuracy 0.973.
### Flips: 260, rs: 26, checks: 312
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018421967
Train loss (w/o reg) on all data: 0.012005953
Test loss (w/o reg) on all data: 0.012391529
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3117515e-06
Norm of the params: 11.327855
     Influence (LOO): fixed 128 labels. Loss 0.01239. Accuracy 0.996.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010043119
Train loss (w/o reg) on all data: 0.0045273826
Test loss (w/o reg) on all data: 0.008518823
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7989693e-07
Norm of the params: 10.503082
                Loss: fixed 130 labels. Loss 0.00852. Accuracy 0.996.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21613485
Train loss (w/o reg) on all data: 0.20927066
Test loss (w/o reg) on all data: 0.12641697
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.48324125e-05
Norm of the params: 11.716818
              Random: fixed  36 labels. Loss 0.12642. Accuracy 0.981.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2522779
Train loss (w/o reg) on all data: 0.24467957
Test loss (w/o reg) on all data: 0.19886553
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 7.799746e-06
Norm of the params: 12.327472
Flipped loss: 0.19887. Accuracy: 0.943
### Flips: 260, rs: 27, checks: 52
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19569767
Train loss (w/o reg) on all data: 0.18576732
Test loss (w/o reg) on all data: 0.15336804
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.9422523e-05
Norm of the params: 14.092794
     Influence (LOO): fixed  35 labels. Loss 0.15337. Accuracy 0.954.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14166619
Train loss (w/o reg) on all data: 0.12825489
Test loss (w/o reg) on all data: 0.14337955
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 7.6187216e-06
Norm of the params: 16.377604
                Loss: fixed  51 labels. Loss 0.14338. Accuracy 0.931.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24317786
Train loss (w/o reg) on all data: 0.23564231
Test loss (w/o reg) on all data: 0.18616438
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.5353024e-05
Norm of the params: 12.276437
              Random: fixed   8 labels. Loss 0.18616. Accuracy 0.943.
### Flips: 260, rs: 27, checks: 104
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15163302
Train loss (w/o reg) on all data: 0.14111671
Test loss (w/o reg) on all data: 0.11535587
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.371288e-05
Norm of the params: 14.502636
     Influence (LOO): fixed  64 labels. Loss 0.11536. Accuracy 0.966.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058852635
Train loss (w/o reg) on all data: 0.043325715
Test loss (w/o reg) on all data: 0.068498336
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0902841e-05
Norm of the params: 17.622099
                Loss: fixed  98 labels. Loss 0.06850. Accuracy 0.981.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24074642
Train loss (w/o reg) on all data: 0.23333144
Test loss (w/o reg) on all data: 0.17183273
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2995831e-05
Norm of the params: 12.177837
              Random: fixed  14 labels. Loss 0.17183. Accuracy 0.962.
### Flips: 260, rs: 27, checks: 156
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10603022
Train loss (w/o reg) on all data: 0.09586975
Test loss (w/o reg) on all data: 0.07544493
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5814374e-05
Norm of the params: 14.255151
     Influence (LOO): fixed  92 labels. Loss 0.07544. Accuracy 0.985.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02520881
Train loss (w/o reg) on all data: 0.0143398205
Test loss (w/o reg) on all data: 0.031116558
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.262324e-06
Norm of the params: 14.743805
                Loss: fixed 122 labels. Loss 0.03112. Accuracy 0.981.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23254071
Train loss (w/o reg) on all data: 0.22513007
Test loss (w/o reg) on all data: 0.15667213
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.6559472e-05
Norm of the params: 12.174277
              Random: fixed  22 labels. Loss 0.15667. Accuracy 0.962.
### Flips: 260, rs: 27, checks: 208
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06469352
Train loss (w/o reg) on all data: 0.05565947
Test loss (w/o reg) on all data: 0.043975998
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.360763e-06
Norm of the params: 13.441763
     Influence (LOO): fixed 113 labels. Loss 0.04398. Accuracy 0.996.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016087662
Train loss (w/o reg) on all data: 0.0072535994
Test loss (w/o reg) on all data: 0.01956357
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5789184e-06
Norm of the params: 13.292151
                Loss: fixed 128 labels. Loss 0.01956. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2256861
Train loss (w/o reg) on all data: 0.21859148
Test loss (w/o reg) on all data: 0.14522584
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.8807119e-05
Norm of the params: 11.911861
              Random: fixed  29 labels. Loss 0.14523. Accuracy 0.962.
### Flips: 260, rs: 27, checks: 260
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02423336
Train loss (w/o reg) on all data: 0.017488664
Test loss (w/o reg) on all data: 0.01776567
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4340743e-06
Norm of the params: 11.614385
     Influence (LOO): fixed 130 labels. Loss 0.01777. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011459667
Train loss (w/o reg) on all data: 0.0047174827
Test loss (w/o reg) on all data: 0.01905945
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.212283e-07
Norm of the params: 11.612222
                Loss: fixed 131 labels. Loss 0.01906. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21817689
Train loss (w/o reg) on all data: 0.21044563
Test loss (w/o reg) on all data: 0.13686566
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.427037e-05
Norm of the params: 12.434835
              Random: fixed  34 labels. Loss 0.13687. Accuracy 0.966.
### Flips: 260, rs: 27, checks: 312
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013004341
Train loss (w/o reg) on all data: 0.0077468613
Test loss (w/o reg) on all data: 0.013980506
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2515765e-07
Norm of the params: 10.254248
     Influence (LOO): fixed 134 labels. Loss 0.01398. Accuracy 0.992.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010201583
Train loss (w/o reg) on all data: 0.004012326
Test loss (w/o reg) on all data: 0.017737124
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3243259e-07
Norm of the params: 11.125876
                Loss: fixed 132 labels. Loss 0.01774. Accuracy 0.992.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21310331
Train loss (w/o reg) on all data: 0.20502108
Test loss (w/o reg) on all data: 0.13638484
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.9924731e-05
Norm of the params: 12.713946
              Random: fixed  38 labels. Loss 0.13638. Accuracy 0.973.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26557222
Train loss (w/o reg) on all data: 0.2606899
Test loss (w/o reg) on all data: 0.1720031
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.0094673e-05
Norm of the params: 9.8816185
Flipped loss: 0.17200. Accuracy: 0.977
### Flips: 260, rs: 28, checks: 52
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20873809
Train loss (w/o reg) on all data: 0.20037457
Test loss (w/o reg) on all data: 0.13624617
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.2838815e-05
Norm of the params: 12.933297
     Influence (LOO): fixed  35 labels. Loss 0.13625. Accuracy 0.954.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15955302
Train loss (w/o reg) on all data: 0.15019386
Test loss (w/o reg) on all data: 0.10998333
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.8312756e-06
Norm of the params: 13.681495
                Loss: fixed  51 labels. Loss 0.10998. Accuracy 0.966.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2579811
Train loss (w/o reg) on all data: 0.2526573
Test loss (w/o reg) on all data: 0.1647739
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.265781e-05
Norm of the params: 10.3187275
              Random: fixed   5 labels. Loss 0.16477. Accuracy 0.977.
### Flips: 260, rs: 28, checks: 104
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14674547
Train loss (w/o reg) on all data: 0.13538688
Test loss (w/o reg) on all data: 0.0921311
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.9912719e-05
Norm of the params: 15.072219
     Influence (LOO): fixed  69 labels. Loss 0.09213. Accuracy 0.973.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060210098
Train loss (w/o reg) on all data: 0.047004294
Test loss (w/o reg) on all data: 0.055632394
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.911267e-06
Norm of the params: 16.251648
                Loss: fixed 102 labels. Loss 0.05563. Accuracy 0.973.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24951813
Train loss (w/o reg) on all data: 0.24400185
Test loss (w/o reg) on all data: 0.15220158
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.6946632e-05
Norm of the params: 10.503591
              Random: fixed  14 labels. Loss 0.15220. Accuracy 0.981.
### Flips: 260, rs: 28, checks: 156
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.092435196
Train loss (w/o reg) on all data: 0.080642216
Test loss (w/o reg) on all data: 0.071898386
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.6184095e-06
Norm of the params: 15.357723
     Influence (LOO): fixed  99 labels. Loss 0.07190. Accuracy 0.977.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01672143
Train loss (w/o reg) on all data: 0.00838221
Test loss (w/o reg) on all data: 0.028920842
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6116353e-06
Norm of the params: 12.914503
                Loss: fixed 129 labels. Loss 0.02892. Accuracy 0.992.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24205637
Train loss (w/o reg) on all data: 0.2367293
Test loss (w/o reg) on all data: 0.14571138
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.661665e-05
Norm of the params: 10.3218975
              Random: fixed  22 labels. Loss 0.14571. Accuracy 0.977.
### Flips: 260, rs: 28, checks: 208
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057918493
Train loss (w/o reg) on all data: 0.04847602
Test loss (w/o reg) on all data: 0.04021994
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.846957e-06
Norm of the params: 13.74225
     Influence (LOO): fixed 118 labels. Loss 0.04022. Accuracy 0.996.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01276195
Train loss (w/o reg) on all data: 0.00599233
Test loss (w/o reg) on all data: 0.024279205
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0308198e-07
Norm of the params: 11.635823
                Loss: fixed 132 labels. Loss 0.02428. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22720209
Train loss (w/o reg) on all data: 0.22112769
Test loss (w/o reg) on all data: 0.1319512
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.6248813e-05
Norm of the params: 11.022161
              Random: fixed  30 labels. Loss 0.13195. Accuracy 0.985.
### Flips: 260, rs: 28, checks: 260
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029932503
Train loss (w/o reg) on all data: 0.022162668
Test loss (w/o reg) on all data: 0.023226798
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7975356e-06
Norm of the params: 12.46582
     Influence (LOO): fixed 129 labels. Loss 0.02323. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008919613
Train loss (w/o reg) on all data: 0.0034988374
Test loss (w/o reg) on all data: 0.021754997
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3341294e-07
Norm of the params: 10.412278
                Loss: fixed 135 labels. Loss 0.02175. Accuracy 0.992.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22251187
Train loss (w/o reg) on all data: 0.21634312
Test loss (w/o reg) on all data: 0.12818514
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6489521e-05
Norm of the params: 11.107431
              Random: fixed  34 labels. Loss 0.12819. Accuracy 0.989.
### Flips: 260, rs: 28, checks: 312
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015367979
Train loss (w/o reg) on all data: 0.008858412
Test loss (w/o reg) on all data: 0.013294496
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7862716e-06
Norm of the params: 11.410142
     Influence (LOO): fixed 134 labels. Loss 0.01329. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008919612
Train loss (w/o reg) on all data: 0.003498983
Test loss (w/o reg) on all data: 0.021755604
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1841976e-07
Norm of the params: 10.412136
                Loss: fixed 135 labels. Loss 0.02176. Accuracy 0.992.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21384689
Train loss (w/o reg) on all data: 0.20770848
Test loss (w/o reg) on all data: 0.12258923
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.091024e-05
Norm of the params: 11.08008
              Random: fixed  40 labels. Loss 0.12259. Accuracy 0.981.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.250157
Train loss (w/o reg) on all data: 0.24168092
Test loss (w/o reg) on all data: 0.17775239
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.4120726e-05
Norm of the params: 13.020057
Flipped loss: 0.17775. Accuracy: 0.939
### Flips: 260, rs: 29, checks: 52
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18818942
Train loss (w/o reg) on all data: 0.17566822
Test loss (w/o reg) on all data: 0.124476485
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.6842016e-05
Norm of the params: 15.824791
     Influence (LOO): fixed  35 labels. Loss 0.12448. Accuracy 0.962.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13391183
Train loss (w/o reg) on all data: 0.11730117
Test loss (w/o reg) on all data: 0.10888679
Train acc on all data:  0.944603629417383
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.0514175e-06
Norm of the params: 18.226717
                Loss: fixed  52 labels. Loss 0.10889. Accuracy 0.950.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24599668
Train loss (w/o reg) on all data: 0.23683405
Test loss (w/o reg) on all data: 0.16570346
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5469546e-05
Norm of the params: 13.537085
              Random: fixed   7 labels. Loss 0.16570. Accuracy 0.969.
### Flips: 260, rs: 29, checks: 104
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13869128
Train loss (w/o reg) on all data: 0.12471118
Test loss (w/o reg) on all data: 0.09720182
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.2487916e-05
Norm of the params: 16.721298
     Influence (LOO): fixed  58 labels. Loss 0.09720. Accuracy 0.969.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056144636
Train loss (w/o reg) on all data: 0.040624812
Test loss (w/o reg) on all data: 0.07347619
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.3608626e-05
Norm of the params: 17.618073
                Loss: fixed 100 labels. Loss 0.07348. Accuracy 0.966.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24137017
Train loss (w/o reg) on all data: 0.23244089
Test loss (w/o reg) on all data: 0.16215046
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2047725e-05
Norm of the params: 13.363588
              Random: fixed  11 labels. Loss 0.16215. Accuracy 0.973.
### Flips: 260, rs: 29, checks: 156
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09771696
Train loss (w/o reg) on all data: 0.08493792
Test loss (w/o reg) on all data: 0.064970024
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.4303223e-06
Norm of the params: 15.986892
     Influence (LOO): fixed  86 labels. Loss 0.06497. Accuracy 0.977.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021682162
Train loss (w/o reg) on all data: 0.011651743
Test loss (w/o reg) on all data: 0.03275149
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.293937e-07
Norm of the params: 14.163629
                Loss: fixed 121 labels. Loss 0.03275. Accuracy 0.977.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23706535
Train loss (w/o reg) on all data: 0.22842723
Test loss (w/o reg) on all data: 0.15804788
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.7317404e-05
Norm of the params: 13.14391
              Random: fixed  16 labels. Loss 0.15805. Accuracy 0.962.
### Flips: 260, rs: 29, checks: 208
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06082175
Train loss (w/o reg) on all data: 0.049041387
Test loss (w/o reg) on all data: 0.03652248
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.993482e-06
Norm of the params: 15.349504
     Influence (LOO): fixed 107 labels. Loss 0.03652. Accuracy 0.992.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019958649
Train loss (w/o reg) on all data: 0.010563479
Test loss (w/o reg) on all data: 0.035077475
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3838402e-06
Norm of the params: 13.707786
                Loss: fixed 123 labels. Loss 0.03508. Accuracy 0.985.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23661545
Train loss (w/o reg) on all data: 0.2283929
Test loss (w/o reg) on all data: 0.15413024
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2455332e-05
Norm of the params: 12.823849
              Random: fixed  18 labels. Loss 0.15413. Accuracy 0.973.
### Flips: 260, rs: 29, checks: 260
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047812413
Train loss (w/o reg) on all data: 0.037648156
Test loss (w/o reg) on all data: 0.025266388
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.365278e-06
Norm of the params: 14.25781
     Influence (LOO): fixed 116 labels. Loss 0.02527. Accuracy 0.996.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014376717
Train loss (w/o reg) on all data: 0.006910922
Test loss (w/o reg) on all data: 0.026016824
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8743044e-06
Norm of the params: 12.219489
                Loss: fixed 128 labels. Loss 0.02602. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22602695
Train loss (w/o reg) on all data: 0.21774279
Test loss (w/o reg) on all data: 0.14603862
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0278449e-05
Norm of the params: 12.871807
              Random: fixed  26 labels. Loss 0.14604. Accuracy 0.977.
### Flips: 260, rs: 29, checks: 312
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030113533
Train loss (w/o reg) on all data: 0.021271495
Test loss (w/o reg) on all data: 0.02326861
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5623605e-06
Norm of the params: 13.29815
     Influence (LOO): fixed 124 labels. Loss 0.02327. Accuracy 0.996.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009614047
Train loss (w/o reg) on all data: 0.0037693328
Test loss (w/o reg) on all data: 0.021741915
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5291643e-07
Norm of the params: 10.811766
                Loss: fixed 130 labels. Loss 0.02174. Accuracy 0.996.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22392058
Train loss (w/o reg) on all data: 0.21563742
Test loss (w/o reg) on all data: 0.14359799
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1558917e-05
Norm of the params: 12.871027
              Random: fixed  27 labels. Loss 0.14360. Accuracy 0.977.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24560371
Train loss (w/o reg) on all data: 0.23901287
Test loss (w/o reg) on all data: 0.20535608
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.2428338e-05
Norm of the params: 11.481151
Flipped loss: 0.20536. Accuracy: 0.935
### Flips: 260, rs: 30, checks: 52
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18170273
Train loss (w/o reg) on all data: 0.17280658
Test loss (w/o reg) on all data: 0.16965269
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.8818744e-05
Norm of the params: 13.338782
     Influence (LOO): fixed  36 labels. Loss 0.16965. Accuracy 0.935.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122079074
Train loss (w/o reg) on all data: 0.1086602
Test loss (w/o reg) on all data: 0.1762027
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.7345308e-05
Norm of the params: 16.382235
                Loss: fixed  51 labels. Loss 0.17620. Accuracy 0.939.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2394374
Train loss (w/o reg) on all data: 0.23266776
Test loss (w/o reg) on all data: 0.19944184
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 6.41976e-05
Norm of the params: 11.635848
              Random: fixed   4 labels. Loss 0.19944. Accuracy 0.935.
### Flips: 260, rs: 30, checks: 104
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13799274
Train loss (w/o reg) on all data: 0.12729475
Test loss (w/o reg) on all data: 0.12918827
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.406004e-06
Norm of the params: 14.627367
     Influence (LOO): fixed  63 labels. Loss 0.12919. Accuracy 0.954.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05608245
Train loss (w/o reg) on all data: 0.040192727
Test loss (w/o reg) on all data: 0.09505347
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.8125995e-06
Norm of the params: 17.826792
                Loss: fixed  93 labels. Loss 0.09505. Accuracy 0.962.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23077068
Train loss (w/o reg) on all data: 0.22368114
Test loss (w/o reg) on all data: 0.19318473
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.6323773e-05
Norm of the params: 11.9075985
              Random: fixed  10 labels. Loss 0.19318. Accuracy 0.939.
### Flips: 260, rs: 30, checks: 156
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0877394
Train loss (w/o reg) on all data: 0.07717864
Test loss (w/o reg) on all data: 0.072693415
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.313873e-06
Norm of the params: 14.533242
     Influence (LOO): fixed  92 labels. Loss 0.07269. Accuracy 0.973.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024666335
Train loss (w/o reg) on all data: 0.013241874
Test loss (w/o reg) on all data: 0.052313406
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.2397918e-06
Norm of the params: 15.11586
                Loss: fixed 114 labels. Loss 0.05231. Accuracy 0.985.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22376935
Train loss (w/o reg) on all data: 0.21647856
Test loss (w/o reg) on all data: 0.18450367
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.9790077e-05
Norm of the params: 12.075421
              Random: fixed  16 labels. Loss 0.18450. Accuracy 0.935.
### Flips: 260, rs: 30, checks: 208
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043491416
Train loss (w/o reg) on all data: 0.032928202
Test loss (w/o reg) on all data: 0.064458944
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.011628e-06
Norm of the params: 14.534933
     Influence (LOO): fixed 112 labels. Loss 0.06446. Accuracy 0.973.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01604971
Train loss (w/o reg) on all data: 0.007519561
Test loss (w/o reg) on all data: 0.03809953
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.3724287e-07
Norm of the params: 13.061507
                Loss: fixed 120 labels. Loss 0.03810. Accuracy 0.985.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21848059
Train loss (w/o reg) on all data: 0.21106616
Test loss (w/o reg) on all data: 0.17185558
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.083117e-05
Norm of the params: 12.177386
              Random: fixed  23 labels. Loss 0.17186. Accuracy 0.947.
### Flips: 260, rs: 30, checks: 260
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016343154
Train loss (w/o reg) on all data: 0.0094874455
Test loss (w/o reg) on all data: 0.030151177
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.706909e-07
Norm of the params: 11.709578
     Influence (LOO): fixed 126 labels. Loss 0.03015. Accuracy 0.985.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010782183
Train loss (w/o reg) on all data: 0.004757413
Test loss (w/o reg) on all data: 0.016656986
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.044991e-07
Norm of the params: 10.97704
                Loss: fixed 126 labels. Loss 0.01666. Accuracy 0.996.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20854697
Train loss (w/o reg) on all data: 0.20115556
Test loss (w/o reg) on all data: 0.14626685
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.97728e-05
Norm of the params: 12.158458
              Random: fixed  31 labels. Loss 0.14627. Accuracy 0.954.
### Flips: 260, rs: 30, checks: 312
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008166055
Train loss (w/o reg) on all data: 0.0033436127
Test loss (w/o reg) on all data: 0.01801023
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0511802e-07
Norm of the params: 9.820838
     Influence (LOO): fixed 129 labels. Loss 0.01801. Accuracy 0.996.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0094912555
Train loss (w/o reg) on all data: 0.0040450064
Test loss (w/o reg) on all data: 0.018073803
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9043053e-06
Norm of the params: 10.436712
                Loss: fixed 127 labels. Loss 0.01807. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20527333
Train loss (w/o reg) on all data: 0.19814327
Test loss (w/o reg) on all data: 0.14203171
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.6747426e-05
Norm of the params: 11.941567
              Random: fixed  36 labels. Loss 0.14203. Accuracy 0.958.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25690964
Train loss (w/o reg) on all data: 0.25080663
Test loss (w/o reg) on all data: 0.16324323
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.502791e-05
Norm of the params: 11.048078
Flipped loss: 0.16324. Accuracy: 0.973
### Flips: 260, rs: 31, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19918391
Train loss (w/o reg) on all data: 0.18958245
Test loss (w/o reg) on all data: 0.14077428
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7626333e-05
Norm of the params: 13.857457
     Influence (LOO): fixed  31 labels. Loss 0.14077. Accuracy 0.966.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13837907
Train loss (w/o reg) on all data: 0.1252839
Test loss (w/o reg) on all data: 0.10835934
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2466667e-05
Norm of the params: 16.183434
                Loss: fixed  52 labels. Loss 0.10836. Accuracy 0.962.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25570717
Train loss (w/o reg) on all data: 0.24995238
Test loss (w/o reg) on all data: 0.15687227
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9563977e-05
Norm of the params: 10.7282915
              Random: fixed   4 labels. Loss 0.15687. Accuracy 0.977.
### Flips: 260, rs: 31, checks: 104
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13975525
Train loss (w/o reg) on all data: 0.12865774
Test loss (w/o reg) on all data: 0.12323755
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.112243e-06
Norm of the params: 14.897993
     Influence (LOO): fixed  62 labels. Loss 0.12324. Accuracy 0.958.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06462694
Train loss (w/o reg) on all data: 0.049127724
Test loss (w/o reg) on all data: 0.0565823
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.591737e-06
Norm of the params: 17.60637
                Loss: fixed  97 labels. Loss 0.05658. Accuracy 0.977.
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25005257
Train loss (w/o reg) on all data: 0.24431545
Test loss (w/o reg) on all data: 0.15091524
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7598177e-05
Norm of the params: 10.7118
              Random: fixed  10 labels. Loss 0.15092. Accuracy 0.981.
### Flips: 260, rs: 31, checks: 156
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0913861
Train loss (w/o reg) on all data: 0.08010601
Test loss (w/o reg) on all data: 0.0823221
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.782194e-06
Norm of the params: 15.020047
     Influence (LOO): fixed  90 labels. Loss 0.08232. Accuracy 0.966.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016625438
Train loss (w/o reg) on all data: 0.008238207
Test loss (w/o reg) on all data: 0.022053253
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1566199e-06
Norm of the params: 12.951627
                Loss: fixed 126 labels. Loss 0.02205. Accuracy 0.992.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24244712
Train loss (w/o reg) on all data: 0.2367625
Test loss (w/o reg) on all data: 0.14542262
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2099512e-05
Norm of the params: 10.662679
              Random: fixed  16 labels. Loss 0.14542. Accuracy 0.989.
### Flips: 260, rs: 31, checks: 208
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051174354
Train loss (w/o reg) on all data: 0.041033864
Test loss (w/o reg) on all data: 0.043563545
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5564374e-06
Norm of the params: 14.24113
     Influence (LOO): fixed 112 labels. Loss 0.04356. Accuracy 0.981.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730317
Test loss (w/o reg) on all data: 0.012054287
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.942343e-07
Norm of the params: 9.153126
                Loss: fixed 131 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23661385
Train loss (w/o reg) on all data: 0.23112501
Test loss (w/o reg) on all data: 0.13632907
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1491307e-05
Norm of the params: 10.477441
              Random: fixed  22 labels. Loss 0.13633. Accuracy 0.985.
### Flips: 260, rs: 31, checks: 260
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02462891
Train loss (w/o reg) on all data: 0.015736938
Test loss (w/o reg) on all data: 0.017705258
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.815725e-07
Norm of the params: 13.335646
     Influence (LOO): fixed 125 labels. Loss 0.01771. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729253
Test loss (w/o reg) on all data: 0.012054687
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4400926e-07
Norm of the params: 9.153245
                Loss: fixed 131 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22563615
Train loss (w/o reg) on all data: 0.219964
Test loss (w/o reg) on all data: 0.12096616
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9483337e-05
Norm of the params: 10.650965
              Random: fixed  30 labels. Loss 0.12097. Accuracy 0.992.
### Flips: 260, rs: 31, checks: 312
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01516013
Train loss (w/o reg) on all data: 0.0084363315
Test loss (w/o reg) on all data: 0.015779851
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3363823e-06
Norm of the params: 11.596376
     Influence (LOO): fixed 129 labels. Loss 0.01578. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730051
Test loss (w/o reg) on all data: 0.01205553
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1176586e-07
Norm of the params: 9.153155
                Loss: fixed 131 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21505226
Train loss (w/o reg) on all data: 0.20927718
Test loss (w/o reg) on all data: 0.11144101
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.299427e-05
Norm of the params: 10.74716
              Random: fixed  37 labels. Loss 0.11144. Accuracy 0.996.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23497784
Train loss (w/o reg) on all data: 0.22907063
Test loss (w/o reg) on all data: 0.17107321
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.3002412e-05
Norm of the params: 10.869419
Flipped loss: 0.17107. Accuracy: 0.954
### Flips: 260, rs: 32, checks: 52
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17156029
Train loss (w/o reg) on all data: 0.16226378
Test loss (w/o reg) on all data: 0.13728301
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.4399578e-05
Norm of the params: 13.635616
     Influence (LOO): fixed  35 labels. Loss 0.13728. Accuracy 0.954.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1132505
Train loss (w/o reg) on all data: 0.10003938
Test loss (w/o reg) on all data: 0.13169552
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 9.294121e-06
Norm of the params: 16.254921
                Loss: fixed  52 labels. Loss 0.13170. Accuracy 0.939.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22387828
Train loss (w/o reg) on all data: 0.21780056
Test loss (w/o reg) on all data: 0.16520639
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.8150478e-05
Norm of the params: 11.025169
              Random: fixed   8 labels. Loss 0.16521. Accuracy 0.954.
### Flips: 260, rs: 32, checks: 104
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12249972
Train loss (w/o reg) on all data: 0.113252535
Test loss (w/o reg) on all data: 0.109639145
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3314107e-05
Norm of the params: 13.5994
     Influence (LOO): fixed  64 labels. Loss 0.10964. Accuracy 0.962.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035043813
Train loss (w/o reg) on all data: 0.02112247
Test loss (w/o reg) on all data: 0.057916783
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.593749e-06
Norm of the params: 16.686129
                Loss: fixed  98 labels. Loss 0.05792. Accuracy 0.981.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21493846
Train loss (w/o reg) on all data: 0.2088707
Test loss (w/o reg) on all data: 0.15120114
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2760491e-05
Norm of the params: 11.016138
              Random: fixed  16 labels. Loss 0.15120. Accuracy 0.962.
### Flips: 260, rs: 32, checks: 156
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066540934
Train loss (w/o reg) on all data: 0.056889825
Test loss (w/o reg) on all data: 0.067593046
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.9708306e-06
Norm of the params: 13.893244
     Influence (LOO): fixed  93 labels. Loss 0.06759. Accuracy 0.977.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017709762
Train loss (w/o reg) on all data: 0.008712268
Test loss (w/o reg) on all data: 0.02039568
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.208571e-07
Norm of the params: 13.414539
                Loss: fixed 112 labels. Loss 0.02040. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20492224
Train loss (w/o reg) on all data: 0.19867618
Test loss (w/o reg) on all data: 0.14341235
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.361811e-05
Norm of the params: 11.176816
              Random: fixed  24 labels. Loss 0.14341. Accuracy 0.962.
### Flips: 260, rs: 32, checks: 208
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028317902
Train loss (w/o reg) on all data: 0.019951027
Test loss (w/o reg) on all data: 0.04111563
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7323626e-06
Norm of the params: 12.9359
     Influence (LOO): fixed 109 labels. Loss 0.04112. Accuracy 0.985.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011212504
Train loss (w/o reg) on all data: 0.0046746917
Test loss (w/o reg) on all data: 0.015688784
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.804819e-07
Norm of the params: 11.43487
                Loss: fixed 116 labels. Loss 0.01569. Accuracy 0.992.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19211929
Train loss (w/o reg) on all data: 0.18553743
Test loss (w/o reg) on all data: 0.13063209
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7016997e-05
Norm of the params: 11.473323
              Random: fixed  34 labels. Loss 0.13063. Accuracy 0.966.
### Flips: 260, rs: 32, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172991
Test loss (w/o reg) on all data: 0.012053826
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.932761e-07
Norm of the params: 9.153173
     Influence (LOO): fixed 120 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008695586
Train loss (w/o reg) on all data: 0.0032692223
Test loss (w/o reg) on all data: 0.01668232
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5121362e-07
Norm of the params: 10.417643
                Loss: fixed 117 labels. Loss 0.01668. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1892033
Train loss (w/o reg) on all data: 0.18270779
Test loss (w/o reg) on all data: 0.12829179
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0148938e-05
Norm of the params: 11.397825
              Random: fixed  36 labels. Loss 0.12829. Accuracy 0.969.
### Flips: 260, rs: 32, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729958
Test loss (w/o reg) on all data: 0.012054897
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.95521e-08
Norm of the params: 9.153168
     Influence (LOO): fixed 120 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008695589
Train loss (w/o reg) on all data: 0.0032692596
Test loss (w/o reg) on all data: 0.016683603
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.282347e-07
Norm of the params: 10.41761
                Loss: fixed 117 labels. Loss 0.01668. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18338263
Train loss (w/o reg) on all data: 0.177259
Test loss (w/o reg) on all data: 0.1229162
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.919128e-06
Norm of the params: 11.066742
              Random: fixed  41 labels. Loss 0.12292. Accuracy 0.969.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24326728
Train loss (w/o reg) on all data: 0.23446834
Test loss (w/o reg) on all data: 0.16736268
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.44578025e-05
Norm of the params: 13.265702
Flipped loss: 0.16736. Accuracy: 0.958
### Flips: 260, rs: 33, checks: 52
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18664807
Train loss (w/o reg) on all data: 0.17690048
Test loss (w/o reg) on all data: 0.10701794
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.1235828e-05
Norm of the params: 13.962514
     Influence (LOO): fixed  33 labels. Loss 0.10702. Accuracy 0.973.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12904313
Train loss (w/o reg) on all data: 0.11333576
Test loss (w/o reg) on all data: 0.11181058
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.0208123e-05
Norm of the params: 17.724207
                Loss: fixed  51 labels. Loss 0.11181. Accuracy 0.962.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24074924
Train loss (w/o reg) on all data: 0.23175229
Test loss (w/o reg) on all data: 0.16220589
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.3285234e-05
Norm of the params: 13.41414
              Random: fixed   4 labels. Loss 0.16221. Accuracy 0.962.
### Flips: 260, rs: 33, checks: 104
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12867907
Train loss (w/o reg) on all data: 0.1172443
Test loss (w/o reg) on all data: 0.08688857
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.734317e-06
Norm of the params: 15.122676
     Influence (LOO): fixed  66 labels. Loss 0.08689. Accuracy 0.969.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057202425
Train loss (w/o reg) on all data: 0.039979186
Test loss (w/o reg) on all data: 0.053268492
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9506152e-06
Norm of the params: 18.559761
                Loss: fixed  93 labels. Loss 0.05327. Accuracy 0.985.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23468551
Train loss (w/o reg) on all data: 0.225959
Test loss (w/o reg) on all data: 0.15168846
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.9172461e-05
Norm of the params: 13.210992
              Random: fixed  12 labels. Loss 0.15169. Accuracy 0.966.
### Flips: 260, rs: 33, checks: 156
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08843988
Train loss (w/o reg) on all data: 0.07607545
Test loss (w/o reg) on all data: 0.07339702
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.3048004e-06
Norm of the params: 15.725415
     Influence (LOO): fixed  88 labels. Loss 0.07340. Accuracy 0.973.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038652103
Train loss (w/o reg) on all data: 0.023980483
Test loss (w/o reg) on all data: 0.036761228
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.637479e-06
Norm of the params: 17.12987
                Loss: fixed 109 labels. Loss 0.03676. Accuracy 0.981.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22722416
Train loss (w/o reg) on all data: 0.21825339
Test loss (w/o reg) on all data: 0.14651236
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.7666249e-05
Norm of the params: 13.394604
              Random: fixed  19 labels. Loss 0.14651. Accuracy 0.969.
### Flips: 260, rs: 33, checks: 208
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061920296
Train loss (w/o reg) on all data: 0.04965634
Test loss (w/o reg) on all data: 0.054155514
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8602221e-06
Norm of the params: 15.66139
     Influence (LOO): fixed 104 labels. Loss 0.05416. Accuracy 0.981.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02207293
Train loss (w/o reg) on all data: 0.011371099
Test loss (w/o reg) on all data: 0.019197796
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4804415e-06
Norm of the params: 14.629991
                Loss: fixed 122 labels. Loss 0.01920. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2248064
Train loss (w/o reg) on all data: 0.21580663
Test loss (w/o reg) on all data: 0.14657566
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.8666906e-05
Norm of the params: 13.416234
              Random: fixed  21 labels. Loss 0.14658. Accuracy 0.973.
### Flips: 260, rs: 33, checks: 260
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02227194
Train loss (w/o reg) on all data: 0.014755187
Test loss (w/o reg) on all data: 0.025075825
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2228795e-06
Norm of the params: 12.261121
     Influence (LOO): fixed 124 labels. Loss 0.02508. Accuracy 0.996.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014034329
Train loss (w/o reg) on all data: 0.006448093
Test loss (w/o reg) on all data: 0.017632931
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9959687e-07
Norm of the params: 12.317659
                Loss: fixed 128 labels. Loss 0.01763. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21813644
Train loss (w/o reg) on all data: 0.20870666
Test loss (w/o reg) on all data: 0.14230643
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.4582969e-05
Norm of the params: 13.733018
              Random: fixed  28 labels. Loss 0.14231. Accuracy 0.969.
### Flips: 260, rs: 33, checks: 312
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015800057
Train loss (w/o reg) on all data: 0.010517952
Test loss (w/o reg) on all data: 0.014428303
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4728797e-07
Norm of the params: 10.2782345
     Influence (LOO): fixed 129 labels. Loss 0.01443. Accuracy 0.996.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010151774
Train loss (w/o reg) on all data: 0.0039736787
Test loss (w/o reg) on all data: 0.016602555
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4490924e-07
Norm of the params: 11.11584
                Loss: fixed 130 labels. Loss 0.01660. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20996603
Train loss (w/o reg) on all data: 0.20103742
Test loss (w/o reg) on all data: 0.12644003
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6374288e-05
Norm of the params: 13.363094
              Random: fixed  35 labels. Loss 0.12644. Accuracy 0.977.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24228494
Train loss (w/o reg) on all data: 0.234683
Test loss (w/o reg) on all data: 0.17035924
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 0.00010211934
Norm of the params: 12.3303995
Flipped loss: 0.17036. Accuracy: 0.958
### Flips: 260, rs: 34, checks: 52
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18499666
Train loss (w/o reg) on all data: 0.17508118
Test loss (w/o reg) on all data: 0.12701322
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9155439e-05
Norm of the params: 14.08225
     Influence (LOO): fixed  37 labels. Loss 0.12701. Accuracy 0.981.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13161847
Train loss (w/o reg) on all data: 0.11790228
Test loss (w/o reg) on all data: 0.103070155
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.4051722e-05
Norm of the params: 16.562721
                Loss: fixed  52 labels. Loss 0.10307. Accuracy 0.962.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2373686
Train loss (w/o reg) on all data: 0.22967362
Test loss (w/o reg) on all data: 0.16513875
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.1869793e-05
Norm of the params: 12.405621
              Random: fixed   3 labels. Loss 0.16514. Accuracy 0.954.
### Flips: 260, rs: 34, checks: 104
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12555155
Train loss (w/o reg) on all data: 0.113904186
Test loss (w/o reg) on all data: 0.08532888
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.7841415e-05
Norm of the params: 15.26261
     Influence (LOO): fixed  69 labels. Loss 0.08533. Accuracy 0.977.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048342817
Train loss (w/o reg) on all data: 0.033479568
Test loss (w/o reg) on all data: 0.046668172
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9044843e-06
Norm of the params: 17.241375
                Loss: fixed  99 labels. Loss 0.04667. Accuracy 0.989.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23076056
Train loss (w/o reg) on all data: 0.22294128
Test loss (w/o reg) on all data: 0.15017444
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1763463e-05
Norm of the params: 12.505426
              Random: fixed  13 labels. Loss 0.15017. Accuracy 0.973.
### Flips: 260, rs: 34, checks: 156
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07528601
Train loss (w/o reg) on all data: 0.06392614
Test loss (w/o reg) on all data: 0.047226876
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.165444e-06
Norm of the params: 15.0730715
     Influence (LOO): fixed  94 labels. Loss 0.04723. Accuracy 0.989.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019088976
Train loss (w/o reg) on all data: 0.009215135
Test loss (w/o reg) on all data: 0.020032309
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6761636e-06
Norm of the params: 14.052644
                Loss: fixed 116 labels. Loss 0.02003. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22469401
Train loss (w/o reg) on all data: 0.21659197
Test loss (w/o reg) on all data: 0.13589224
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.1461706e-05
Norm of the params: 12.729525
              Random: fixed  18 labels. Loss 0.13589. Accuracy 0.981.
### Flips: 260, rs: 34, checks: 208
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034950763
Train loss (w/o reg) on all data: 0.0262849
Test loss (w/o reg) on all data: 0.024717929
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.423434e-06
Norm of the params: 13.165001
     Influence (LOO): fixed 114 labels. Loss 0.02472. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013360029
Train loss (w/o reg) on all data: 0.0058001825
Test loss (w/o reg) on all data: 0.01415677
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4510246e-06
Norm of the params: 12.296216
                Loss: fixed 119 labels. Loss 0.01416. Accuracy 0.996.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21869613
Train loss (w/o reg) on all data: 0.21045439
Test loss (w/o reg) on all data: 0.13167512
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7269187e-05
Norm of the params: 12.838807
              Random: fixed  22 labels. Loss 0.13168. Accuracy 0.985.
### Flips: 260, rs: 34, checks: 260
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018020894
Train loss (w/o reg) on all data: 0.011585568
Test loss (w/o reg) on all data: 0.013757898
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.228363e-06
Norm of the params: 11.34489
     Influence (LOO): fixed 120 labels. Loss 0.01376. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012127226
Train loss (w/o reg) on all data: 0.0051825424
Test loss (w/o reg) on all data: 0.012372404
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0999003e-06
Norm of the params: 11.785316
                Loss: fixed 120 labels. Loss 0.01237. Accuracy 0.996.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21388695
Train loss (w/o reg) on all data: 0.20585103
Test loss (w/o reg) on all data: 0.12558886
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1628966e-05
Norm of the params: 12.677465
              Random: fixed  27 labels. Loss 0.12559. Accuracy 0.985.
### Flips: 260, rs: 34, checks: 312
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010002354
Train loss (w/o reg) on all data: 0.005447888
Test loss (w/o reg) on all data: 0.012244856
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.8714916e-07
Norm of the params: 9.544073
     Influence (LOO): fixed 123 labels. Loss 0.01224. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009416001
Train loss (w/o reg) on all data: 0.0036925988
Test loss (w/o reg) on all data: 0.010480372
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.702524e-07
Norm of the params: 10.698974
                Loss: fixed 122 labels. Loss 0.01048. Accuracy 0.996.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20512421
Train loss (w/o reg) on all data: 0.19697724
Test loss (w/o reg) on all data: 0.12262315
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.0573625e-05
Norm of the params: 12.764775
              Random: fixed  32 labels. Loss 0.12262. Accuracy 0.981.
Using normal model
LBFGS training took [318] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24607635
Train loss (w/o reg) on all data: 0.23906992
Test loss (w/o reg) on all data: 0.15091124
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4769798e-05
Norm of the params: 11.837589
Flipped loss: 0.15091. Accuracy: 0.969
### Flips: 260, rs: 35, checks: 52
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17569973
Train loss (w/o reg) on all data: 0.16367145
Test loss (w/o reg) on all data: 0.09955414
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.425139e-05
Norm of the params: 15.510178
     Influence (LOO): fixed  39 labels. Loss 0.09955. Accuracy 0.973.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12843661
Train loss (w/o reg) on all data: 0.11319197
Test loss (w/o reg) on all data: 0.09371795
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.259924e-05
Norm of the params: 17.46118
                Loss: fixed  52 labels. Loss 0.09372. Accuracy 0.966.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23902097
Train loss (w/o reg) on all data: 0.23272635
Test loss (w/o reg) on all data: 0.13038892
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6100934e-05
Norm of the params: 11.220181
              Random: fixed  11 labels. Loss 0.13039. Accuracy 0.981.
### Flips: 260, rs: 35, checks: 104
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12735608
Train loss (w/o reg) on all data: 0.11358605
Test loss (w/o reg) on all data: 0.067604646
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0317159e-05
Norm of the params: 16.595196
     Influence (LOO): fixed  66 labels. Loss 0.06760. Accuracy 0.989.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041981958
Train loss (w/o reg) on all data: 0.028403584
Test loss (w/o reg) on all data: 0.03355905
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7959452e-06
Norm of the params: 16.479305
                Loss: fixed  98 labels. Loss 0.03356. Accuracy 0.989.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2323171
Train loss (w/o reg) on all data: 0.22652653
Test loss (w/o reg) on all data: 0.122037634
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.2338703e-05
Norm of the params: 10.761577
              Random: fixed  17 labels. Loss 0.12204. Accuracy 0.989.
### Flips: 260, rs: 35, checks: 156
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08658737
Train loss (w/o reg) on all data: 0.07485657
Test loss (w/o reg) on all data: 0.045366157
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1282534e-05
Norm of the params: 15.317181
     Influence (LOO): fixed  88 labels. Loss 0.04537. Accuracy 0.996.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017528951
Train loss (w/o reg) on all data: 0.009007353
Test loss (w/o reg) on all data: 0.012938405
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.835841e-07
Norm of the params: 13.05496
                Loss: fixed 115 labels. Loss 0.01294. Accuracy 0.996.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22313723
Train loss (w/o reg) on all data: 0.21755818
Test loss (w/o reg) on all data: 0.114171825
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7218499e-05
Norm of the params: 10.563194
              Random: fixed  23 labels. Loss 0.11417. Accuracy 0.992.
### Flips: 260, rs: 35, checks: 208
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046639353
Train loss (w/o reg) on all data: 0.03731399
Test loss (w/o reg) on all data: 0.025579542
Train acc on all data:  0.9866284622731614
Test acc on all data:   1.0
Norm of the mean of gradients: 3.6721724e-06
Norm of the params: 13.656766
     Influence (LOO): fixed 106 labels. Loss 0.02558. Accuracy 1.000.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009135032
Train loss (w/o reg) on all data: 0.0035335673
Test loss (w/o reg) on all data: 0.005869263
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.7893054e-07
Norm of the params: 10.58439
                Loss: fixed 120 labels. Loss 0.00587. Accuracy 1.000.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21342427
Train loss (w/o reg) on all data: 0.20785674
Test loss (w/o reg) on all data: 0.1131884
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4121237e-05
Norm of the params: 10.552275
              Random: fixed  29 labels. Loss 0.11319. Accuracy 0.992.
### Flips: 260, rs: 35, checks: 260
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01160491
Train loss (w/o reg) on all data: 0.0065737506
Test loss (w/o reg) on all data: 0.01292127
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8903266e-06
Norm of the params: 10.031111
     Influence (LOO): fixed 120 labels. Loss 0.01292. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009135033
Train loss (w/o reg) on all data: 0.0035337491
Test loss (w/o reg) on all data: 0.0058694524
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.5697033e-07
Norm of the params: 10.584218
                Loss: fixed 120 labels. Loss 0.00587. Accuracy 1.000.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20046286
Train loss (w/o reg) on all data: 0.19440304
Test loss (w/o reg) on all data: 0.10201784
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.008291e-06
Norm of the params: 11.008929
              Random: fixed  37 labels. Loss 0.10202. Accuracy 0.989.
### Flips: 260, rs: 35, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009954043
Train loss (w/o reg) on all data: 0.0054101576
Test loss (w/o reg) on all data: 0.006021349
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 6.9789934e-07
Norm of the params: 9.532979
     Influence (LOO): fixed 121 labels. Loss 0.00602. Accuracy 1.000.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009135031
Train loss (w/o reg) on all data: 0.003533782
Test loss (w/o reg) on all data: 0.0058692084
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 3.8070644e-07
Norm of the params: 10.584186
                Loss: fixed 120 labels. Loss 0.00587. Accuracy 1.000.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1963502
Train loss (w/o reg) on all data: 0.19011608
Test loss (w/o reg) on all data: 0.09885933
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.09082375e-05
Norm of the params: 11.166132
              Random: fixed  40 labels. Loss 0.09886. Accuracy 0.989.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2379034
Train loss (w/o reg) on all data: 0.23002154
Test loss (w/o reg) on all data: 0.15981302
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.771606e-05
Norm of the params: 12.555373
Flipped loss: 0.15981. Accuracy: 0.966
### Flips: 260, rs: 36, checks: 52
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17053191
Train loss (w/o reg) on all data: 0.15801704
Test loss (w/o reg) on all data: 0.13047686
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0432301e-05
Norm of the params: 15.82079
     Influence (LOO): fixed  39 labels. Loss 0.13048. Accuracy 0.969.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12895043
Train loss (w/o reg) on all data: 0.11281313
Test loss (w/o reg) on all data: 0.116391644
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.6377326e-06
Norm of the params: 17.965134
                Loss: fixed  51 labels. Loss 0.11639. Accuracy 0.958.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23467839
Train loss (w/o reg) on all data: 0.22693352
Test loss (w/o reg) on all data: 0.15785417
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.245575e-05
Norm of the params: 12.445774
              Random: fixed   3 labels. Loss 0.15785. Accuracy 0.966.
### Flips: 260, rs: 36, checks: 104
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13042866
Train loss (w/o reg) on all data: 0.11790736
Test loss (w/o reg) on all data: 0.098530285
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6912636e-05
Norm of the params: 15.82485
     Influence (LOO): fixed  67 labels. Loss 0.09853. Accuracy 0.981.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043077543
Train loss (w/o reg) on all data: 0.027094468
Test loss (w/o reg) on all data: 0.06820017
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4337566e-05
Norm of the params: 17.87908
                Loss: fixed  99 labels. Loss 0.06820. Accuracy 0.966.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23051548
Train loss (w/o reg) on all data: 0.22249316
Test loss (w/o reg) on all data: 0.15691634
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.320911e-05
Norm of the params: 12.666742
              Random: fixed   5 labels. Loss 0.15692. Accuracy 0.966.
### Flips: 260, rs: 36, checks: 156
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08240365
Train loss (w/o reg) on all data: 0.07029739
Test loss (w/o reg) on all data: 0.060025483
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.8147326e-06
Norm of the params: 15.560374
     Influence (LOO): fixed  94 labels. Loss 0.06003. Accuracy 0.977.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02639855
Train loss (w/o reg) on all data: 0.014053786
Test loss (w/o reg) on all data: 0.023620227
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.735016e-06
Norm of the params: 15.712903
                Loss: fixed 111 labels. Loss 0.02362. Accuracy 0.992.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2230655
Train loss (w/o reg) on all data: 0.21477094
Test loss (w/o reg) on all data: 0.15108652
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.9426304e-05
Norm of the params: 12.879871
              Random: fixed  11 labels. Loss 0.15109. Accuracy 0.966.
### Flips: 260, rs: 36, checks: 208
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039874587
Train loss (w/o reg) on all data: 0.030223897
Test loss (w/o reg) on all data: 0.028859809
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.2424536e-06
Norm of the params: 13.892942
     Influence (LOO): fixed 112 labels. Loss 0.02886. Accuracy 0.996.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019540109
Train loss (w/o reg) on all data: 0.009170419
Test loss (w/o reg) on all data: 0.014528542
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2289178e-06
Norm of the params: 14.401174
                Loss: fixed 116 labels. Loss 0.01453. Accuracy 0.996.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21922942
Train loss (w/o reg) on all data: 0.2107929
Test loss (w/o reg) on all data: 0.13891888
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.969486e-05
Norm of the params: 12.989626
              Random: fixed  17 labels. Loss 0.13892. Accuracy 0.973.
### Flips: 260, rs: 36, checks: 260
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021098364
Train loss (w/o reg) on all data: 0.014752341
Test loss (w/o reg) on all data: 0.01799074
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.001512e-06
Norm of the params: 11.265899
     Influence (LOO): fixed 121 labels. Loss 0.01799. Accuracy 0.996.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013883283
Train loss (w/o reg) on all data: 0.0058600814
Test loss (w/o reg) on all data: 0.015899975
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.810707e-07
Norm of the params: 12.66744
                Loss: fixed 119 labels. Loss 0.01590. Accuracy 0.996.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21659449
Train loss (w/o reg) on all data: 0.20815712
Test loss (w/o reg) on all data: 0.13293797
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0957621e-05
Norm of the params: 12.990279
              Random: fixed  20 labels. Loss 0.13294. Accuracy 0.981.
### Flips: 260, rs: 36, checks: 312
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010489633
Train loss (w/o reg) on all data: 0.005539453
Test loss (w/o reg) on all data: 0.013845848
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9355932e-06
Norm of the params: 9.950055
     Influence (LOO): fixed 124 labels. Loss 0.01385. Accuracy 0.989.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010344868
Train loss (w/o reg) on all data: 0.004095234
Test loss (w/o reg) on all data: 0.013821603
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4073754e-07
Norm of the params: 11.180013
                Loss: fixed 121 labels. Loss 0.01382. Accuracy 0.992.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2058234
Train loss (w/o reg) on all data: 0.19714297
Test loss (w/o reg) on all data: 0.12852071
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7741213e-05
Norm of the params: 13.176066
              Random: fixed  26 labels. Loss 0.12852. Accuracy 0.977.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24842304
Train loss (w/o reg) on all data: 0.24126194
Test loss (w/o reg) on all data: 0.17696252
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.677664e-05
Norm of the params: 11.967532
Flipped loss: 0.17696. Accuracy: 0.966
### Flips: 260, rs: 37, checks: 52
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17848223
Train loss (w/o reg) on all data: 0.16762334
Test loss (w/o reg) on all data: 0.131627
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2501062e-05
Norm of the params: 14.736957
     Influence (LOO): fixed  38 labels. Loss 0.13163. Accuracy 0.973.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13007265
Train loss (w/o reg) on all data: 0.11608403
Test loss (w/o reg) on all data: 0.13263865
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.7115826e-05
Norm of the params: 16.726402
                Loss: fixed  52 labels. Loss 0.13264. Accuracy 0.947.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2472761
Train loss (w/o reg) on all data: 0.24005762
Test loss (w/o reg) on all data: 0.17494194
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.8472417e-05
Norm of the params: 12.015396
              Random: fixed   2 labels. Loss 0.17494. Accuracy 0.966.
### Flips: 260, rs: 37, checks: 104
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13107619
Train loss (w/o reg) on all data: 0.11878145
Test loss (w/o reg) on all data: 0.107081205
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0788788e-05
Norm of the params: 15.681037
     Influence (LOO): fixed  66 labels. Loss 0.10708. Accuracy 0.985.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041347492
Train loss (w/o reg) on all data: 0.027328834
Test loss (w/o reg) on all data: 0.040239174
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.3172628e-06
Norm of the params: 16.744349
                Loss: fixed 101 labels. Loss 0.04024. Accuracy 0.989.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2400284
Train loss (w/o reg) on all data: 0.23277867
Test loss (w/o reg) on all data: 0.16216108
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1416005e-05
Norm of the params: 12.041371
              Random: fixed   9 labels. Loss 0.16216. Accuracy 0.985.
### Flips: 260, rs: 37, checks: 156
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08677064
Train loss (w/o reg) on all data: 0.075135626
Test loss (w/o reg) on all data: 0.07967568
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1821819e-05
Norm of the params: 15.254515
     Influence (LOO): fixed  93 labels. Loss 0.07968. Accuracy 0.989.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018634442
Train loss (w/o reg) on all data: 0.00844843
Test loss (w/o reg) on all data: 0.027551519
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.1054745e-07
Norm of the params: 14.27306
                Loss: fixed 117 labels. Loss 0.02755. Accuracy 0.989.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23377052
Train loss (w/o reg) on all data: 0.22647393
Test loss (w/o reg) on all data: 0.14461486
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.532308e-05
Norm of the params: 12.080224
              Random: fixed  16 labels. Loss 0.14461. Accuracy 0.992.
### Flips: 260, rs: 37, checks: 208
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04771938
Train loss (w/o reg) on all data: 0.037547328
Test loss (w/o reg) on all data: 0.059265897
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1817212e-06
Norm of the params: 14.263278
     Influence (LOO): fixed 110 labels. Loss 0.05927. Accuracy 0.989.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013132798
Train loss (w/o reg) on all data: 0.005689028
Test loss (w/o reg) on all data: 0.021235991
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2623957e-07
Norm of the params: 12.20145
                Loss: fixed 121 labels. Loss 0.02124. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22639875
Train loss (w/o reg) on all data: 0.21888539
Test loss (w/o reg) on all data: 0.13750796
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.44815585e-05
Norm of the params: 12.258354
              Random: fixed  21 labels. Loss 0.13751. Accuracy 0.992.
### Flips: 260, rs: 37, checks: 260
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0225142
Train loss (w/o reg) on all data: 0.01511942
Test loss (w/o reg) on all data: 0.026616586
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6855179e-06
Norm of the params: 12.161233
     Influence (LOO): fixed 120 labels. Loss 0.02662. Accuracy 0.992.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010560147
Train loss (w/o reg) on all data: 0.0043152818
Test loss (w/o reg) on all data: 0.019345691
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3265132e-07
Norm of the params: 11.175747
                Loss: fixed 122 labels. Loss 0.01935. Accuracy 0.992.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21440955
Train loss (w/o reg) on all data: 0.20643485
Test loss (w/o reg) on all data: 0.12523161
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.115022e-06
Norm of the params: 12.629087
              Random: fixed  29 labels. Loss 0.12523. Accuracy 0.989.
### Flips: 260, rs: 37, checks: 312
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013053302
Train loss (w/o reg) on all data: 0.007583172
Test loss (w/o reg) on all data: 0.017070571
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.71233e-07
Norm of the params: 10.459571
     Influence (LOO): fixed 123 labels. Loss 0.01707. Accuracy 0.996.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010560146
Train loss (w/o reg) on all data: 0.0043156813
Test loss (w/o reg) on all data: 0.019345101
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0027065e-06
Norm of the params: 11.175387
                Loss: fixed 122 labels. Loss 0.01935. Accuracy 0.992.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19781688
Train loss (w/o reg) on all data: 0.1899447
Test loss (w/o reg) on all data: 0.1073238
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.818522e-05
Norm of the params: 12.547652
              Random: fixed  41 labels. Loss 0.10732. Accuracy 0.992.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25110495
Train loss (w/o reg) on all data: 0.24407344
Test loss (w/o reg) on all data: 0.16523361
Train acc on all data:  0.889207258834766
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 6.904499e-05
Norm of the params: 11.858755
Flipped loss: 0.16523. Accuracy: 0.950
### Flips: 260, rs: 38, checks: 52
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19415012
Train loss (w/o reg) on all data: 0.18386343
Test loss (w/o reg) on all data: 0.13171852
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.4800644e-05
Norm of the params: 14.34342
     Influence (LOO): fixed  35 labels. Loss 0.13172. Accuracy 0.966.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14355534
Train loss (w/o reg) on all data: 0.13049139
Test loss (w/o reg) on all data: 0.11718579
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.8354523e-05
Norm of the params: 16.164133
                Loss: fixed  51 labels. Loss 0.11719. Accuracy 0.947.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24010943
Train loss (w/o reg) on all data: 0.23310228
Test loss (w/o reg) on all data: 0.15393224
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.9864952e-05
Norm of the params: 11.8382015
              Random: fixed  10 labels. Loss 0.15393. Accuracy 0.962.
### Flips: 260, rs: 38, checks: 104
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14018036
Train loss (w/o reg) on all data: 0.12819439
Test loss (w/o reg) on all data: 0.096420504
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1356575e-05
Norm of the params: 15.482878
     Influence (LOO): fixed  66 labels. Loss 0.09642. Accuracy 0.977.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066020645
Train loss (w/o reg) on all data: 0.05128428
Test loss (w/o reg) on all data: 0.069906265
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.58436e-06
Norm of the params: 17.167622
                Loss: fixed  95 labels. Loss 0.06991. Accuracy 0.977.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23003903
Train loss (w/o reg) on all data: 0.22294118
Test loss (w/o reg) on all data: 0.15168186
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.7665172e-05
Norm of the params: 11.914581
              Random: fixed  18 labels. Loss 0.15168. Accuracy 0.950.
### Flips: 260, rs: 38, checks: 156
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097343184
Train loss (w/o reg) on all data: 0.08661184
Test loss (w/o reg) on all data: 0.04775898
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0060912e-05
Norm of the params: 14.650152
     Influence (LOO): fixed  93 labels. Loss 0.04776. Accuracy 0.989.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02583248
Train loss (w/o reg) on all data: 0.014878708
Test loss (w/o reg) on all data: 0.01767786
Train acc on all data:  0.9980897803247374
Test acc on all data:   1.0
Norm of the mean of gradients: 4.962627e-06
Norm of the params: 14.801197
                Loss: fixed 119 labels. Loss 0.01768. Accuracy 1.000.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2164012
Train loss (w/o reg) on all data: 0.20878766
Test loss (w/o reg) on all data: 0.13061027
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.6365785e-05
Norm of the params: 12.339811
              Random: fixed  29 labels. Loss 0.13061. Accuracy 0.973.
### Flips: 260, rs: 38, checks: 208
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0488242
Train loss (w/o reg) on all data: 0.039659493
Test loss (w/o reg) on all data: 0.019364353
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.8792e-06
Norm of the params: 13.538617
     Influence (LOO): fixed 114 labels. Loss 0.01936. Accuracy 0.996.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017542556
Train loss (w/o reg) on all data: 0.008829087
Test loss (w/o reg) on all data: 0.0074497145
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 5.4207777e-07
Norm of the params: 13.201112
                Loss: fixed 125 labels. Loss 0.00745. Accuracy 1.000.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20792806
Train loss (w/o reg) on all data: 0.20032816
Test loss (w/o reg) on all data: 0.11900802
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.7174521e-05
Norm of the params: 12.32875
              Random: fixed  36 labels. Loss 0.11901. Accuracy 0.989.
### Flips: 260, rs: 38, checks: 260
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024930308
Train loss (w/o reg) on all data: 0.018073585
Test loss (w/o reg) on all data: 0.015044935
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9794408e-06
Norm of the params: 11.710443
     Influence (LOO): fixed 127 labels. Loss 0.01504. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013716025
Train loss (w/o reg) on all data: 0.006343135
Test loss (w/o reg) on all data: 0.0067364494
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 1.287038e-06
Norm of the params: 12.143221
                Loss: fixed 128 labels. Loss 0.00674. Accuracy 1.000.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20387602
Train loss (w/o reg) on all data: 0.19627455
Test loss (w/o reg) on all data: 0.113143556
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1364322e-05
Norm of the params: 12.330019
              Random: fixed  40 labels. Loss 0.11314. Accuracy 0.989.
### Flips: 260, rs: 38, checks: 312
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016603706
Train loss (w/o reg) on all data: 0.010852234
Test loss (w/o reg) on all data: 0.013591761
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.6061638e-07
Norm of the params: 10.725178
     Influence (LOO): fixed 130 labels. Loss 0.01359. Accuracy 0.996.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009581836
Train loss (w/o reg) on all data: 0.0040776054
Test loss (w/o reg) on all data: 0.006312444
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 1.5573217e-07
Norm of the params: 10.492122
                Loss: fixed 130 labels. Loss 0.00631. Accuracy 1.000.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19843735
Train loss (w/o reg) on all data: 0.19061
Test loss (w/o reg) on all data: 0.10657629
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0845265e-05
Norm of the params: 12.511867
              Random: fixed  45 labels. Loss 0.10658. Accuracy 0.989.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25208217
Train loss (w/o reg) on all data: 0.2438098
Test loss (w/o reg) on all data: 0.17685589
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8460032e-05
Norm of the params: 12.862643
Flipped loss: 0.17686. Accuracy: 0.966
### Flips: 260, rs: 39, checks: 52
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18981677
Train loss (w/o reg) on all data: 0.17924425
Test loss (w/o reg) on all data: 0.12720689
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5328016e-05
Norm of the params: 14.541342
     Influence (LOO): fixed  34 labels. Loss 0.12721. Accuracy 0.973.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14046302
Train loss (w/o reg) on all data: 0.12569271
Test loss (w/o reg) on all data: 0.13298154
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 6.4261076e-06
Norm of the params: 17.187386
                Loss: fixed  52 labels. Loss 0.13298. Accuracy 0.947.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24926646
Train loss (w/o reg) on all data: 0.24117179
Test loss (w/o reg) on all data: 0.17328222
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.4767054e-05
Norm of the params: 12.723735
              Random: fixed   3 labels. Loss 0.17328. Accuracy 0.969.
### Flips: 260, rs: 39, checks: 104
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14262283
Train loss (w/o reg) on all data: 0.13276164
Test loss (w/o reg) on all data: 0.08572332
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0334897e-05
Norm of the params: 14.0436325
     Influence (LOO): fixed  65 labels. Loss 0.08572. Accuracy 0.985.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053691376
Train loss (w/o reg) on all data: 0.038394883
Test loss (w/o reg) on all data: 0.06431039
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.509355e-06
Norm of the params: 17.49085
                Loss: fixed  99 labels. Loss 0.06431. Accuracy 0.973.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23962526
Train loss (w/o reg) on all data: 0.23097427
Test loss (w/o reg) on all data: 0.16665709
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.5379348e-05
Norm of the params: 13.153693
              Random: fixed  10 labels. Loss 0.16666. Accuracy 0.962.
### Flips: 260, rs: 39, checks: 156
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09063938
Train loss (w/o reg) on all data: 0.080354735
Test loss (w/o reg) on all data: 0.051428527
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.3779063e-06
Norm of the params: 14.341998
     Influence (LOO): fixed  90 labels. Loss 0.05143. Accuracy 0.989.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020509394
Train loss (w/o reg) on all data: 0.011690803
Test loss (w/o reg) on all data: 0.013862869
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.07697e-07
Norm of the params: 13.280504
                Loss: fixed 121 labels. Loss 0.01386. Accuracy 0.996.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22672667
Train loss (w/o reg) on all data: 0.21827917
Test loss (w/o reg) on all data: 0.15127668
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.434494e-05
Norm of the params: 12.998074
              Random: fixed  22 labels. Loss 0.15128. Accuracy 0.966.
### Flips: 260, rs: 39, checks: 208
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056681383
Train loss (w/o reg) on all data: 0.047940597
Test loss (w/o reg) on all data: 0.03054813
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4262537e-05
Norm of the params: 13.22179
     Influence (LOO): fixed 108 labels. Loss 0.03055. Accuracy 0.992.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01466153
Train loss (w/o reg) on all data: 0.006773559
Test loss (w/o reg) on all data: 0.010939482
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2156842e-06
Norm of the params: 12.560233
                Loss: fixed 123 labels. Loss 0.01094. Accuracy 0.996.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21779795
Train loss (w/o reg) on all data: 0.20934062
Test loss (w/o reg) on all data: 0.14204544
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.8635707e-05
Norm of the params: 13.005635
              Random: fixed  28 labels. Loss 0.14205. Accuracy 0.962.
### Flips: 260, rs: 39, checks: 260
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028152222
Train loss (w/o reg) on all data: 0.021306364
Test loss (w/o reg) on all data: 0.02064221
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9839206e-06
Norm of the params: 11.701161
     Influence (LOO): fixed 119 labels. Loss 0.02064. Accuracy 0.992.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014661532
Train loss (w/o reg) on all data: 0.0067740907
Test loss (w/o reg) on all data: 0.010939394
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.6822704e-07
Norm of the params: 12.55981
                Loss: fixed 123 labels. Loss 0.01094. Accuracy 0.996.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20963512
Train loss (w/o reg) on all data: 0.20115611
Test loss (w/o reg) on all data: 0.14061426
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.878492e-05
Norm of the params: 13.022298
              Random: fixed  35 labels. Loss 0.14061. Accuracy 0.973.
### Flips: 260, rs: 39, checks: 312
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015056492
Train loss (w/o reg) on all data: 0.009467522
Test loss (w/o reg) on all data: 0.014700204
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.528434e-07
Norm of the params: 10.572577
     Influence (LOO): fixed 123 labels. Loss 0.01470. Accuracy 0.992.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009598746
Train loss (w/o reg) on all data: 0.0038187637
Test loss (w/o reg) on all data: 0.009981651
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9462075e-07
Norm of the params: 10.751728
                Loss: fixed 125 labels. Loss 0.00998. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19999173
Train loss (w/o reg) on all data: 0.19105923
Test loss (w/o reg) on all data: 0.13243714
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5487556e-05
Norm of the params: 13.366
              Random: fixed  43 labels. Loss 0.13244. Accuracy 0.985.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2544987
Train loss (w/o reg) on all data: 0.24698648
Test loss (w/o reg) on all data: 0.20819962
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.911807e-05
Norm of the params: 12.25741
Flipped loss: 0.20820. Accuracy: 0.939
### Flips: 312, rs: 0, checks: 52
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19993502
Train loss (w/o reg) on all data: 0.19017725
Test loss (w/o reg) on all data: 0.1647756
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 0.00010634333
Norm of the params: 13.969807
     Influence (LOO): fixed  35 labels. Loss 0.16478. Accuracy 0.939.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14254859
Train loss (w/o reg) on all data: 0.12977193
Test loss (w/o reg) on all data: 0.18843012
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 4.657936e-05
Norm of the params: 15.985408
                Loss: fixed  51 labels. Loss 0.18843. Accuracy 0.912.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24885246
Train loss (w/o reg) on all data: 0.24159548
Test loss (w/o reg) on all data: 0.20116276
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.3938006e-05
Norm of the params: 12.047393
              Random: fixed   6 labels. Loss 0.20116. Accuracy 0.943.
### Flips: 312, rs: 0, checks: 104
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15088679
Train loss (w/o reg) on all data: 0.13918483
Test loss (w/o reg) on all data: 0.12373965
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4318544e-05
Norm of the params: 15.29834
     Influence (LOO): fixed  63 labels. Loss 0.12374. Accuracy 0.958.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05891238
Train loss (w/o reg) on all data: 0.04127959
Test loss (w/o reg) on all data: 0.12190136
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1113243e-05
Norm of the params: 18.779133
                Loss: fixed  96 labels. Loss 0.12190. Accuracy 0.966.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24898191
Train loss (w/o reg) on all data: 0.24178363
Test loss (w/o reg) on all data: 0.20054202
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.46885395e-05
Norm of the params: 11.998561
              Random: fixed   7 labels. Loss 0.20054. Accuracy 0.939.
### Flips: 312, rs: 0, checks: 156
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112249665
Train loss (w/o reg) on all data: 0.10181057
Test loss (w/o reg) on all data: 0.08692023
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.097373e-05
Norm of the params: 14.449286
     Influence (LOO): fixed  90 labels. Loss 0.08692. Accuracy 0.973.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033276826
Train loss (w/o reg) on all data: 0.019659977
Test loss (w/o reg) on all data: 0.061146844
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9750427e-06
Norm of the params: 16.502636
                Loss: fixed 118 labels. Loss 0.06115. Accuracy 0.977.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24076766
Train loss (w/o reg) on all data: 0.23338251
Test loss (w/o reg) on all data: 0.18834744
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.8423324e-05
Norm of the params: 12.153316
              Random: fixed  14 labels. Loss 0.18835. Accuracy 0.943.
### Flips: 312, rs: 0, checks: 208
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07485509
Train loss (w/o reg) on all data: 0.065122046
Test loss (w/o reg) on all data: 0.04402381
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.6122195e-06
Norm of the params: 13.952089
     Influence (LOO): fixed 115 labels. Loss 0.04402. Accuracy 0.989.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02695021
Train loss (w/o reg) on all data: 0.015534661
Test loss (w/o reg) on all data: 0.046931796
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2770502e-06
Norm of the params: 15.109963
                Loss: fixed 127 labels. Loss 0.04693. Accuracy 0.989.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22572522
Train loss (w/o reg) on all data: 0.21772835
Test loss (w/o reg) on all data: 0.18450421
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.0573885e-05
Norm of the params: 12.646632
              Random: fixed  23 labels. Loss 0.18450. Accuracy 0.931.
### Flips: 312, rs: 0, checks: 260
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039075773
Train loss (w/o reg) on all data: 0.030791992
Test loss (w/o reg) on all data: 0.01669434
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.932507e-06
Norm of the params: 12.871505
     Influence (LOO): fixed 130 labels. Loss 0.01669. Accuracy 0.996.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01853087
Train loss (w/o reg) on all data: 0.009797673
Test loss (w/o reg) on all data: 0.031886294
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3941864e-06
Norm of the params: 13.216048
                Loss: fixed 132 labels. Loss 0.03189. Accuracy 0.989.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2163901
Train loss (w/o reg) on all data: 0.20829448
Test loss (w/o reg) on all data: 0.16388483
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.599246e-05
Norm of the params: 12.724484
              Random: fixed  31 labels. Loss 0.16388. Accuracy 0.943.
### Flips: 312, rs: 0, checks: 312
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012864319
Train loss (w/o reg) on all data: 0.0072640856
Test loss (w/o reg) on all data: 0.010810892
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.844036e-07
Norm of the params: 10.583225
     Influence (LOO): fixed 139 labels. Loss 0.01081. Accuracy 0.996.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01667212
Train loss (w/o reg) on all data: 0.008153443
Test loss (w/o reg) on all data: 0.03552345
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1656068e-06
Norm of the params: 13.052722
                Loss: fixed 133 labels. Loss 0.03552. Accuracy 0.989.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20829013
Train loss (w/o reg) on all data: 0.19976395
Test loss (w/o reg) on all data: 0.13835728
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.5193316e-05
Norm of the params: 13.058471
              Random: fixed  39 labels. Loss 0.13836. Accuracy 0.962.
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2583847
Train loss (w/o reg) on all data: 0.25188887
Test loss (w/o reg) on all data: 0.19775154
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 0.0001331363
Norm of the params: 11.398094
Flipped loss: 0.19775. Accuracy: 0.950
### Flips: 312, rs: 1, checks: 52
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20463753
Train loss (w/o reg) on all data: 0.1940978
Test loss (w/o reg) on all data: 0.16433679
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.5762546e-05
Norm of the params: 14.518759
     Influence (LOO): fixed  32 labels. Loss 0.16434. Accuracy 0.966.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14258757
Train loss (w/o reg) on all data: 0.12960565
Test loss (w/o reg) on all data: 0.1309439
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.6631291e-05
Norm of the params: 16.1133
                Loss: fixed  52 labels. Loss 0.13094. Accuracy 0.939.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25638828
Train loss (w/o reg) on all data: 0.25001258
Test loss (w/o reg) on all data: 0.19183476
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.111804e-05
Norm of the params: 11.292221
              Random: fixed   3 labels. Loss 0.19183. Accuracy 0.954.
### Flips: 312, rs: 1, checks: 104
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15926087
Train loss (w/o reg) on all data: 0.14707848
Test loss (w/o reg) on all data: 0.12852597
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.4825824e-05
Norm of the params: 15.609221
     Influence (LOO): fixed  60 labels. Loss 0.12853. Accuracy 0.969.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06282484
Train loss (w/o reg) on all data: 0.047276333
Test loss (w/o reg) on all data: 0.08938258
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.2283137e-06
Norm of the params: 17.634344
                Loss: fixed 100 labels. Loss 0.08938. Accuracy 0.962.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25523686
Train loss (w/o reg) on all data: 0.24898756
Test loss (w/o reg) on all data: 0.18687977
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.153489e-05
Norm of the params: 11.179716
              Random: fixed   7 labels. Loss 0.18688. Accuracy 0.958.
### Flips: 312, rs: 1, checks: 156
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120895125
Train loss (w/o reg) on all data: 0.10929475
Test loss (w/o reg) on all data: 0.09309117
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.3458635e-06
Norm of the params: 15.231794
     Influence (LOO): fixed  89 labels. Loss 0.09309. Accuracy 0.981.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02954714
Train loss (w/o reg) on all data: 0.01761224
Test loss (w/o reg) on all data: 0.048534423
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.4487796e-06
Norm of the params: 15.449854
                Loss: fixed 122 labels. Loss 0.04853. Accuracy 0.981.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24159384
Train loss (w/o reg) on all data: 0.23538211
Test loss (w/o reg) on all data: 0.1618219
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4281965e-05
Norm of the params: 11.146059
              Random: fixed  21 labels. Loss 0.16182. Accuracy 0.966.
### Flips: 312, rs: 1, checks: 208
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060567934
Train loss (w/o reg) on all data: 0.050394908
Test loss (w/o reg) on all data: 0.033047058
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.040501e-05
Norm of the params: 14.26396
     Influence (LOO): fixed 118 labels. Loss 0.03305. Accuracy 0.985.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0160231
Train loss (w/o reg) on all data: 0.0076487283
Test loss (w/o reg) on all data: 0.0143195875
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.294063e-06
Norm of the params: 12.941694
                Loss: fixed 133 labels. Loss 0.01432. Accuracy 0.996.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23120444
Train loss (w/o reg) on all data: 0.22516327
Test loss (w/o reg) on all data: 0.14908631
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3323895e-05
Norm of the params: 10.991968
              Random: fixed  29 labels. Loss 0.14909. Accuracy 0.969.
### Flips: 312, rs: 1, checks: 260
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041226257
Train loss (w/o reg) on all data: 0.0315683
Test loss (w/o reg) on all data: 0.022353338
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1147722e-05
Norm of the params: 13.8981695
     Influence (LOO): fixed 127 labels. Loss 0.02235. Accuracy 0.996.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0160231
Train loss (w/o reg) on all data: 0.0076487693
Test loss (w/o reg) on all data: 0.014318543
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4796184e-06
Norm of the params: 12.941662
                Loss: fixed 133 labels. Loss 0.01432. Accuracy 0.996.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2239765
Train loss (w/o reg) on all data: 0.21732911
Test loss (w/o reg) on all data: 0.14420232
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.4541404e-05
Norm of the params: 11.530288
              Random: fixed  37 labels. Loss 0.14420. Accuracy 0.989.
### Flips: 312, rs: 1, checks: 312
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024597362
Train loss (w/o reg) on all data: 0.017002404
Test loss (w/o reg) on all data: 0.013701943
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.17212e-07
Norm of the params: 12.324738
     Influence (LOO): fixed 134 labels. Loss 0.01370. Accuracy 0.996.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012558041
Train loss (w/o reg) on all data: 0.005363771
Test loss (w/o reg) on all data: 0.010839369
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0653315e-06
Norm of the params: 11.995224
                Loss: fixed 136 labels. Loss 0.01084. Accuracy 0.996.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21653375
Train loss (w/o reg) on all data: 0.20986427
Test loss (w/o reg) on all data: 0.13223569
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7204994e-05
Norm of the params: 11.549435
              Random: fixed  43 labels. Loss 0.13224. Accuracy 0.992.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24918748
Train loss (w/o reg) on all data: 0.24195696
Test loss (w/o reg) on all data: 0.16883993
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.7958347e-05
Norm of the params: 12.025402
Flipped loss: 0.16884. Accuracy: 0.958
### Flips: 312, rs: 2, checks: 52
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19134685
Train loss (w/o reg) on all data: 0.18262404
Test loss (w/o reg) on all data: 0.119586505
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.9020692e-05
Norm of the params: 13.208182
     Influence (LOO): fixed  39 labels. Loss 0.11959. Accuracy 0.962.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13293321
Train loss (w/o reg) on all data: 0.118347086
Test loss (w/o reg) on all data: 0.11749531
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.0275518e-05
Norm of the params: 17.07989
                Loss: fixed  51 labels. Loss 0.11750. Accuracy 0.947.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24506995
Train loss (w/o reg) on all data: 0.23775476
Test loss (w/o reg) on all data: 0.16274628
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.3053064e-05
Norm of the params: 12.095615
              Random: fixed   5 labels. Loss 0.16275. Accuracy 0.969.
### Flips: 312, rs: 2, checks: 104
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13787019
Train loss (w/o reg) on all data: 0.12850736
Test loss (w/o reg) on all data: 0.08132725
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.7549286e-06
Norm of the params: 13.684178
     Influence (LOO): fixed  72 labels. Loss 0.08133. Accuracy 0.985.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045343235
Train loss (w/o reg) on all data: 0.028241456
Test loss (w/o reg) on all data: 0.0693337
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.0311857e-06
Norm of the params: 18.494205
                Loss: fixed  96 labels. Loss 0.06933. Accuracy 0.969.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23764512
Train loss (w/o reg) on all data: 0.22984901
Test loss (w/o reg) on all data: 0.15998821
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.9052564e-05
Norm of the params: 12.486877
              Random: fixed  12 labels. Loss 0.15999. Accuracy 0.969.
### Flips: 312, rs: 2, checks: 156
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086922884
Train loss (w/o reg) on all data: 0.07705194
Test loss (w/o reg) on all data: 0.055218954
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.4599295e-06
Norm of the params: 14.050586
     Influence (LOO): fixed  97 labels. Loss 0.05522. Accuracy 0.985.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028920833
Train loss (w/o reg) on all data: 0.016392438
Test loss (w/o reg) on all data: 0.028795384
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4954621e-06
Norm of the params: 15.829337
                Loss: fixed 114 labels. Loss 0.02880. Accuracy 0.992.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22912969
Train loss (w/o reg) on all data: 0.22099294
Test loss (w/o reg) on all data: 0.15794016
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.1253369e-05
Norm of the params: 12.756766
              Random: fixed  18 labels. Loss 0.15794. Accuracy 0.954.
### Flips: 312, rs: 2, checks: 208
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04174959
Train loss (w/o reg) on all data: 0.03295604
Test loss (w/o reg) on all data: 0.032472976
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0148061e-06
Norm of the params: 13.261636
     Influence (LOO): fixed 117 labels. Loss 0.03247. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017870136
Train loss (w/o reg) on all data: 0.008549655
Test loss (w/o reg) on all data: 0.014079402
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.168766e-07
Norm of the params: 13.65319
                Loss: fixed 123 labels. Loss 0.01408. Accuracy 0.996.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21863958
Train loss (w/o reg) on all data: 0.21033178
Test loss (w/o reg) on all data: 0.14361608
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5918162e-05
Norm of the params: 12.890154
              Random: fixed  27 labels. Loss 0.14362. Accuracy 0.973.
### Flips: 312, rs: 2, checks: 260
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025033034
Train loss (w/o reg) on all data: 0.017852176
Test loss (w/o reg) on all data: 0.02153564
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4269646e-06
Norm of the params: 11.984039
     Influence (LOO): fixed 128 labels. Loss 0.02154. Accuracy 0.989.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016201835
Train loss (w/o reg) on all data: 0.007537632
Test loss (w/o reg) on all data: 0.021508854
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0094748e-07
Norm of the params: 13.163739
                Loss: fixed 125 labels. Loss 0.02151. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20934905
Train loss (w/o reg) on all data: 0.20141031
Test loss (w/o reg) on all data: 0.13859276
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.2395626e-05
Norm of the params: 12.60059
              Random: fixed  36 labels. Loss 0.13859. Accuracy 0.969.
### Flips: 312, rs: 2, checks: 312
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021325603
Train loss (w/o reg) on all data: 0.014980978
Test loss (w/o reg) on all data: 0.020504283
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7055866e-06
Norm of the params: 11.264656
     Influence (LOO): fixed 130 labels. Loss 0.02050. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01219276
Train loss (w/o reg) on all data: 0.0051125507
Test loss (w/o reg) on all data: 0.016321272
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9564807e-06
Norm of the params: 11.899756
                Loss: fixed 129 labels. Loss 0.01632. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20305622
Train loss (w/o reg) on all data: 0.19483191
Test loss (w/o reg) on all data: 0.13343246
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.52180155e-05
Norm of the params: 12.825221
              Random: fixed  41 labels. Loss 0.13343. Accuracy 0.977.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26269287
Train loss (w/o reg) on all data: 0.25543162
Test loss (w/o reg) on all data: 0.2102596
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.9335233e-05
Norm of the params: 12.050937
Flipped loss: 0.21026. Accuracy: 0.920
### Flips: 312, rs: 3, checks: 52
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21712056
Train loss (w/o reg) on all data: 0.20690443
Test loss (w/o reg) on all data: 0.18676883
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 3.288734e-05
Norm of the params: 14.294151
     Influence (LOO): fixed  32 labels. Loss 0.18677. Accuracy 0.927.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16107202
Train loss (w/o reg) on all data: 0.14834616
Test loss (w/o reg) on all data: 0.18775696
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 5.2775136e-05
Norm of the params: 15.953593
                Loss: fixed  52 labels. Loss 0.18776. Accuracy 0.916.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25467128
Train loss (w/o reg) on all data: 0.2477752
Test loss (w/o reg) on all data: 0.19520941
Train acc on all data:  0.8720152817574021
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.2846623e-05
Norm of the params: 11.744001
              Random: fixed   9 labels. Loss 0.19521. Accuracy 0.920.
### Flips: 312, rs: 3, checks: 104
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1651108
Train loss (w/o reg) on all data: 0.15387836
Test loss (w/o reg) on all data: 0.14655706
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.0544416e-05
Norm of the params: 14.988288
     Influence (LOO): fixed  63 labels. Loss 0.14656. Accuracy 0.950.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07763894
Train loss (w/o reg) on all data: 0.059647787
Test loss (w/o reg) on all data: 0.17759775
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.2879377e-05
Norm of the params: 18.969004
                Loss: fixed  95 labels. Loss 0.17760. Accuracy 0.943.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25125414
Train loss (w/o reg) on all data: 0.24423242
Test loss (w/o reg) on all data: 0.19160825
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.5415163e-05
Norm of the params: 11.850508
              Random: fixed  14 labels. Loss 0.19161. Accuracy 0.920.
### Flips: 312, rs: 3, checks: 156
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13079046
Train loss (w/o reg) on all data: 0.11891339
Test loss (w/o reg) on all data: 0.12749968
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.6434114e-05
Norm of the params: 15.412378
     Influence (LOO): fixed  83 labels. Loss 0.12750. Accuracy 0.954.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042766366
Train loss (w/o reg) on all data: 0.027664179
Test loss (w/o reg) on all data: 0.08759359
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.8378236e-06
Norm of the params: 17.379406
                Loss: fixed 121 labels. Loss 0.08759. Accuracy 0.958.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24707748
Train loss (w/o reg) on all data: 0.24004476
Test loss (w/o reg) on all data: 0.1925807
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 8.716283e-05
Norm of the params: 11.859781
              Random: fixed  20 labels. Loss 0.19258. Accuracy 0.912.
### Flips: 312, rs: 3, checks: 208
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10162612
Train loss (w/o reg) on all data: 0.09076788
Test loss (w/o reg) on all data: 0.103783764
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.2384119e-05
Norm of the params: 14.73651
     Influence (LOO): fixed 106 labels. Loss 0.10378. Accuracy 0.950.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028200455
Train loss (w/o reg) on all data: 0.016739216
Test loss (w/o reg) on all data: 0.033536658
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9579234e-06
Norm of the params: 15.140171
                Loss: fixed 136 labels. Loss 0.03354. Accuracy 0.985.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24215941
Train loss (w/o reg) on all data: 0.2353521
Test loss (w/o reg) on all data: 0.1748514
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.35148985e-05
Norm of the params: 11.668169
              Random: fixed  27 labels. Loss 0.17485. Accuracy 0.924.
### Flips: 312, rs: 3, checks: 260
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07047552
Train loss (w/o reg) on all data: 0.06113398
Test loss (w/o reg) on all data: 0.07403423
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0393806e-05
Norm of the params: 13.668603
     Influence (LOO): fixed 123 labels. Loss 0.07403. Accuracy 0.969.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018621791
Train loss (w/o reg) on all data: 0.009311851
Test loss (w/o reg) on all data: 0.029361991
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7555667e-06
Norm of the params: 13.645468
                Loss: fixed 143 labels. Loss 0.02936. Accuracy 0.985.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22993292
Train loss (w/o reg) on all data: 0.22289228
Test loss (w/o reg) on all data: 0.1551691
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.4828354e-05
Norm of the params: 11.866456
              Random: fixed  38 labels. Loss 0.15517. Accuracy 0.947.
### Flips: 312, rs: 3, checks: 312
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037168045
Train loss (w/o reg) on all data: 0.028111944
Test loss (w/o reg) on all data: 0.040089358
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7260899e-06
Norm of the params: 13.458158
     Influence (LOO): fixed 140 labels. Loss 0.04009. Accuracy 0.992.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016287182
Train loss (w/o reg) on all data: 0.0077985064
Test loss (w/o reg) on all data: 0.027872253
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0367437e-06
Norm of the params: 13.029716
                Loss: fixed 147 labels. Loss 0.02787. Accuracy 0.989.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2189573
Train loss (w/o reg) on all data: 0.21152383
Test loss (w/o reg) on all data: 0.15316379
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.37568695e-05
Norm of the params: 12.19301
              Random: fixed  46 labels. Loss 0.15316. Accuracy 0.943.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2760364
Train loss (w/o reg) on all data: 0.26968762
Test loss (w/o reg) on all data: 0.22836469
Train acc on all data:  0.8653295128939829
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 9.3863906e-05
Norm of the params: 11.268352
Flipped loss: 0.22836. Accuracy: 0.924
### Flips: 312, rs: 4, checks: 52
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22749431
Train loss (w/o reg) on all data: 0.21756889
Test loss (w/o reg) on all data: 0.18868248
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.4191966e-05
Norm of the params: 14.0893
     Influence (LOO): fixed  29 labels. Loss 0.18868. Accuracy 0.939.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18129417
Train loss (w/o reg) on all data: 0.16928037
Test loss (w/o reg) on all data: 0.17459868
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.4887137e-05
Norm of the params: 15.500849
                Loss: fixed  50 labels. Loss 0.17460. Accuracy 0.947.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26412618
Train loss (w/o reg) on all data: 0.25726604
Test loss (w/o reg) on all data: 0.20755932
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.411609e-05
Norm of the params: 11.713358
              Random: fixed  12 labels. Loss 0.20756. Accuracy 0.939.
### Flips: 312, rs: 4, checks: 104
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19260572
Train loss (w/o reg) on all data: 0.18212569
Test loss (w/o reg) on all data: 0.14103504
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.171651e-05
Norm of the params: 14.4775915
     Influence (LOO): fixed  55 labels. Loss 0.14104. Accuracy 0.954.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09694559
Train loss (w/o reg) on all data: 0.08052767
Test loss (w/o reg) on all data: 0.13173041
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.597349e-05
Norm of the params: 18.120663
                Loss: fixed  98 labels. Loss 0.13173. Accuracy 0.947.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25595683
Train loss (w/o reg) on all data: 0.24897484
Test loss (w/o reg) on all data: 0.19997302
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.407565e-05
Norm of the params: 11.816927
              Random: fixed  22 labels. Loss 0.19997. Accuracy 0.954.
### Flips: 312, rs: 4, checks: 156
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14141595
Train loss (w/o reg) on all data: 0.12976682
Test loss (w/o reg) on all data: 0.10781787
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.1659711e-05
Norm of the params: 15.263769
     Influence (LOO): fixed  87 labels. Loss 0.10782. Accuracy 0.962.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05154613
Train loss (w/o reg) on all data: 0.03534011
Test loss (w/o reg) on all data: 0.09311806
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.3496662e-06
Norm of the params: 18.003344
                Loss: fixed 127 labels. Loss 0.09312. Accuracy 0.966.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25251892
Train loss (w/o reg) on all data: 0.24546652
Test loss (w/o reg) on all data: 0.18861604
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.768722e-05
Norm of the params: 11.876365
              Random: fixed  27 labels. Loss 0.18862. Accuracy 0.958.
### Flips: 312, rs: 4, checks: 208
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0965658
Train loss (w/o reg) on all data: 0.083395354
Test loss (w/o reg) on all data: 0.089978784
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.541647e-06
Norm of the params: 16.229876
     Influence (LOO): fixed 111 labels. Loss 0.08998. Accuracy 0.981.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037003834
Train loss (w/o reg) on all data: 0.023617385
Test loss (w/o reg) on all data: 0.07158605
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5321808e-06
Norm of the params: 16.362427
                Loss: fixed 137 labels. Loss 0.07159. Accuracy 0.973.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23592642
Train loss (w/o reg) on all data: 0.22777155
Test loss (w/o reg) on all data: 0.18148771
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 5.0018323e-05
Norm of the params: 12.770955
              Random: fixed  35 labels. Loss 0.18149. Accuracy 0.939.
### Flips: 312, rs: 4, checks: 260
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06470217
Train loss (w/o reg) on all data: 0.05472599
Test loss (w/o reg) on all data: 0.04679584
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.165714e-06
Norm of the params: 14.125278
     Influence (LOO): fixed 129 labels. Loss 0.04680. Accuracy 0.985.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029511943
Train loss (w/o reg) on all data: 0.018162934
Test loss (w/o reg) on all data: 0.05814221
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.5207717e-06
Norm of the params: 15.065862
                Loss: fixed 142 labels. Loss 0.05814. Accuracy 0.985.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23478983
Train loss (w/o reg) on all data: 0.22681448
Test loss (w/o reg) on all data: 0.17473434
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.1209334e-05
Norm of the params: 12.629613
              Random: fixed  38 labels. Loss 0.17473. Accuracy 0.950.
### Flips: 312, rs: 4, checks: 312
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04250917
Train loss (w/o reg) on all data: 0.032524414
Test loss (w/o reg) on all data: 0.034512274
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5211473e-06
Norm of the params: 14.13135
     Influence (LOO): fixed 141 labels. Loss 0.03451. Accuracy 0.985.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020558914
Train loss (w/o reg) on all data: 0.01084574
Test loss (w/o reg) on all data: 0.035405215
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2543112e-06
Norm of the params: 13.937843
                Loss: fixed 148 labels. Loss 0.03541. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22483821
Train loss (w/o reg) on all data: 0.21687159
Test loss (w/o reg) on all data: 0.16442156
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.4699159e-05
Norm of the params: 12.622691
              Random: fixed  46 labels. Loss 0.16442. Accuracy 0.954.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26185206
Train loss (w/o reg) on all data: 0.25418672
Test loss (w/o reg) on all data: 0.2197846
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 5.5572695e-05
Norm of the params: 12.381706
Flipped loss: 0.21978. Accuracy: 0.920
### Flips: 312, rs: 5, checks: 52
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20969376
Train loss (w/o reg) on all data: 0.19972736
Test loss (w/o reg) on all data: 0.17699684
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.4150166e-05
Norm of the params: 14.118356
     Influence (LOO): fixed  36 labels. Loss 0.17700. Accuracy 0.939.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15278447
Train loss (w/o reg) on all data: 0.13836996
Test loss (w/o reg) on all data: 0.21225663
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.09096345e-05
Norm of the params: 16.97911
                Loss: fixed  52 labels. Loss 0.21226. Accuracy 0.908.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2586819
Train loss (w/o reg) on all data: 0.2510232
Test loss (w/o reg) on all data: 0.21581964
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.7798606e-05
Norm of the params: 12.3763485
              Random: fixed   4 labels. Loss 0.21582. Accuracy 0.920.
### Flips: 312, rs: 5, checks: 104
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16575083
Train loss (w/o reg) on all data: 0.15326785
Test loss (w/o reg) on all data: 0.15933208
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.3014734e-05
Norm of the params: 15.800629
     Influence (LOO): fixed  62 labels. Loss 0.15933. Accuracy 0.935.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084249444
Train loss (w/o reg) on all data: 0.06533907
Test loss (w/o reg) on all data: 0.14631888
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.602553e-06
Norm of the params: 19.447556
                Loss: fixed  90 labels. Loss 0.14632. Accuracy 0.943.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25481382
Train loss (w/o reg) on all data: 0.24704647
Test loss (w/o reg) on all data: 0.21509503
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 6.325315e-05
Norm of the params: 12.463821
              Random: fixed   8 labels. Loss 0.21510. Accuracy 0.920.
### Flips: 312, rs: 5, checks: 156
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10942136
Train loss (w/o reg) on all data: 0.09552203
Test loss (w/o reg) on all data: 0.12297284
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.7212276e-05
Norm of the params: 16.672928
     Influence (LOO): fixed  94 labels. Loss 0.12297. Accuracy 0.962.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046055242
Train loss (w/o reg) on all data: 0.029766906
Test loss (w/o reg) on all data: 0.07312972
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.264477e-05
Norm of the params: 18.049011
                Loss: fixed 119 labels. Loss 0.07313. Accuracy 0.973.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25049335
Train loss (w/o reg) on all data: 0.24290763
Test loss (w/o reg) on all data: 0.20362195
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 7.431305e-05
Norm of the params: 12.317247
              Random: fixed  13 labels. Loss 0.20362. Accuracy 0.920.
### Flips: 312, rs: 5, checks: 208
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082038954
Train loss (w/o reg) on all data: 0.070440836
Test loss (w/o reg) on all data: 0.08063718
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1304779e-05
Norm of the params: 15.230312
     Influence (LOO): fixed 114 labels. Loss 0.08064. Accuracy 0.966.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027974319
Train loss (w/o reg) on all data: 0.015445545
Test loss (w/o reg) on all data: 0.03188955
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.05537e-06
Norm of the params: 15.829576
                Loss: fixed 132 labels. Loss 0.03189. Accuracy 0.989.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24383366
Train loss (w/o reg) on all data: 0.23594765
Test loss (w/o reg) on all data: 0.19296633
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 7.592924e-05
Norm of the params: 12.558663
              Random: fixed  21 labels. Loss 0.19297. Accuracy 0.939.
### Flips: 312, rs: 5, checks: 260
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053523254
Train loss (w/o reg) on all data: 0.043145254
Test loss (w/o reg) on all data: 0.052717544
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.888393e-06
Norm of the params: 14.406943
     Influence (LOO): fixed 129 labels. Loss 0.05272. Accuracy 0.981.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021742351
Train loss (w/o reg) on all data: 0.010490095
Test loss (w/o reg) on all data: 0.02575475
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.3583326e-06
Norm of the params: 15.001503
                Loss: fixed 137 labels. Loss 0.02575. Accuracy 0.989.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23396988
Train loss (w/o reg) on all data: 0.22595271
Test loss (w/o reg) on all data: 0.1865246
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.9751664e-05
Norm of the params: 12.662671
              Random: fixed  29 labels. Loss 0.18652. Accuracy 0.943.
### Flips: 312, rs: 5, checks: 312
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034540005
Train loss (w/o reg) on all data: 0.025298145
Test loss (w/o reg) on all data: 0.023754537
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7862484e-06
Norm of the params: 13.595484
     Influence (LOO): fixed 139 labels. Loss 0.02375. Accuracy 0.992.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017377982
Train loss (w/o reg) on all data: 0.008087743
Test loss (w/o reg) on all data: 0.020933112
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.633906e-07
Norm of the params: 13.631022
                Loss: fixed 141 labels. Loss 0.02093. Accuracy 0.992.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22486313
Train loss (w/o reg) on all data: 0.21638307
Test loss (w/o reg) on all data: 0.17000312
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 0.00011848664
Norm of the params: 13.023098
              Random: fixed  37 labels. Loss 0.17000. Accuracy 0.943.
Using normal model
LBFGS training took [328] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25838166
Train loss (w/o reg) on all data: 0.2515811
Test loss (w/o reg) on all data: 0.19268718
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.2074434e-05
Norm of the params: 11.662376
Flipped loss: 0.19269. Accuracy: 0.958
### Flips: 312, rs: 6, checks: 52
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2070826
Train loss (w/o reg) on all data: 0.19837342
Test loss (w/o reg) on all data: 0.1644717
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.9327788e-05
Norm of the params: 13.197864
     Influence (LOO): fixed  32 labels. Loss 0.16447. Accuracy 0.958.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14550693
Train loss (w/o reg) on all data: 0.13145933
Test loss (w/o reg) on all data: 0.16678849
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.4571876e-05
Norm of the params: 16.761625
                Loss: fixed  49 labels. Loss 0.16679. Accuracy 0.924.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24804208
Train loss (w/o reg) on all data: 0.24134947
Test loss (w/o reg) on all data: 0.18441778
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.7367014e-05
Norm of the params: 11.56944
              Random: fixed   9 labels. Loss 0.18442. Accuracy 0.943.
### Flips: 312, rs: 6, checks: 104
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15416086
Train loss (w/o reg) on all data: 0.1428564
Test loss (w/o reg) on all data: 0.14093748
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 7.2234657e-06
Norm of the params: 15.036256
     Influence (LOO): fixed  65 labels. Loss 0.14094. Accuracy 0.947.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076273106
Train loss (w/o reg) on all data: 0.05694092
Test loss (w/o reg) on all data: 0.09829088
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4532109e-05
Norm of the params: 19.663258
                Loss: fixed  91 labels. Loss 0.09829. Accuracy 0.969.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24168044
Train loss (w/o reg) on all data: 0.23495233
Test loss (w/o reg) on all data: 0.16219056
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.2033466e-05
Norm of the params: 11.600091
              Random: fixed  17 labels. Loss 0.16219. Accuracy 0.962.
### Flips: 312, rs: 6, checks: 156
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1078452
Train loss (w/o reg) on all data: 0.096661955
Test loss (w/o reg) on all data: 0.08605018
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.314628e-06
Norm of the params: 14.955434
     Influence (LOO): fixed 100 labels. Loss 0.08605. Accuracy 0.981.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03724721
Train loss (w/o reg) on all data: 0.022227187
Test loss (w/o reg) on all data: 0.037776437
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.768698e-06
Norm of the params: 17.332066
                Loss: fixed 118 labels. Loss 0.03778. Accuracy 0.985.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23496121
Train loss (w/o reg) on all data: 0.22819841
Test loss (w/o reg) on all data: 0.15450293
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.7991027e-05
Norm of the params: 11.629969
              Random: fixed  24 labels. Loss 0.15450. Accuracy 0.962.
### Flips: 312, rs: 6, checks: 208
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0616915
Train loss (w/o reg) on all data: 0.0516295
Test loss (w/o reg) on all data: 0.044617467
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.444164e-06
Norm of the params: 14.185911
     Influence (LOO): fixed 124 labels. Loss 0.04462. Accuracy 0.989.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027496766
Train loss (w/o reg) on all data: 0.014742606
Test loss (w/o reg) on all data: 0.029464839
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6683751e-06
Norm of the params: 15.971325
                Loss: fixed 128 labels. Loss 0.02946. Accuracy 0.989.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22163264
Train loss (w/o reg) on all data: 0.21508102
Test loss (w/o reg) on all data: 0.14362942
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.7873208e-05
Norm of the params: 11.4469385
              Random: fixed  35 labels. Loss 0.14363. Accuracy 0.954.
### Flips: 312, rs: 6, checks: 260
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032733455
Train loss (w/o reg) on all data: 0.022968313
Test loss (w/o reg) on all data: 0.027130684
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.543945e-06
Norm of the params: 13.9750805
     Influence (LOO): fixed 136 labels. Loss 0.02713. Accuracy 0.989.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020276913
Train loss (w/o reg) on all data: 0.010476901
Test loss (w/o reg) on all data: 0.023388408
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1829325e-06
Norm of the params: 14.000009
                Loss: fixed 137 labels. Loss 0.02339. Accuracy 0.985.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21228269
Train loss (w/o reg) on all data: 0.20551895
Test loss (w/o reg) on all data: 0.13251503
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.250714e-06
Norm of the params: 11.630775
              Random: fixed  42 labels. Loss 0.13252. Accuracy 0.969.
### Flips: 312, rs: 6, checks: 312
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015054223
Train loss (w/o reg) on all data: 0.008667203
Test loss (w/o reg) on all data: 0.0116762305
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4629578e-06
Norm of the params: 11.302231
     Influence (LOO): fixed 144 labels. Loss 0.01168. Accuracy 0.996.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015450183
Train loss (w/o reg) on all data: 0.0073123574
Test loss (w/o reg) on all data: 0.020927535
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.134784e-06
Norm of the params: 12.7576065
                Loss: fixed 141 labels. Loss 0.02093. Accuracy 0.989.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20506158
Train loss (w/o reg) on all data: 0.19798653
Test loss (w/o reg) on all data: 0.12037321
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0398619e-05
Norm of the params: 11.89543
              Random: fixed  49 labels. Loss 0.12037. Accuracy 0.992.
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26930723
Train loss (w/o reg) on all data: 0.26239428
Test loss (w/o reg) on all data: 0.23156236
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 4.5274417e-05
Norm of the params: 11.758366
Flipped loss: 0.23156. Accuracy: 0.924
### Flips: 312, rs: 7, checks: 52
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22043179
Train loss (w/o reg) on all data: 0.21032621
Test loss (w/o reg) on all data: 0.19874096
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.440284e-05
Norm of the params: 14.2165985
     Influence (LOO): fixed  33 labels. Loss 0.19874. Accuracy 0.935.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16557465
Train loss (w/o reg) on all data: 0.15303735
Test loss (w/o reg) on all data: 0.18307737
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.5465472e-05
Norm of the params: 15.834959
                Loss: fixed  52 labels. Loss 0.18308. Accuracy 0.920.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26676017
Train loss (w/o reg) on all data: 0.2598982
Test loss (w/o reg) on all data: 0.2173939
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.3324523e-05
Norm of the params: 11.714948
              Random: fixed   9 labels. Loss 0.21739. Accuracy 0.939.
### Flips: 312, rs: 7, checks: 104
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17529207
Train loss (w/o reg) on all data: 0.16434604
Test loss (w/o reg) on all data: 0.16327862
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.4954749e-05
Norm of the params: 14.795969
     Influence (LOO): fixed  65 labels. Loss 0.16328. Accuracy 0.943.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091575496
Train loss (w/o reg) on all data: 0.07507667
Test loss (w/o reg) on all data: 0.118884295
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 5.1647103e-06
Norm of the params: 18.165257
                Loss: fixed  92 labels. Loss 0.11888. Accuracy 0.943.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2599448
Train loss (w/o reg) on all data: 0.252874
Test loss (w/o reg) on all data: 0.19582252
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.7503538e-05
Norm of the params: 11.891853
              Random: fixed  18 labels. Loss 0.19582. Accuracy 0.947.
### Flips: 312, rs: 7, checks: 156
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13508259
Train loss (w/o reg) on all data: 0.12348043
Test loss (w/o reg) on all data: 0.13076305
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.9758943e-05
Norm of the params: 15.232961
     Influence (LOO): fixed  91 labels. Loss 0.13076. Accuracy 0.962.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054392323
Train loss (w/o reg) on all data: 0.036777273
Test loss (w/o reg) on all data: 0.085779056
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.1999947e-06
Norm of the params: 18.769682
                Loss: fixed 120 labels. Loss 0.08578. Accuracy 0.966.
Using normal model
LBFGS training took [341] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25447482
Train loss (w/o reg) on all data: 0.24682565
Test loss (w/o reg) on all data: 0.19693504
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.9395766e-05
Norm of the params: 12.36864
              Random: fixed  23 labels. Loss 0.19694. Accuracy 0.950.
### Flips: 312, rs: 7, checks: 208
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107188046
Train loss (w/o reg) on all data: 0.096697345
Test loss (w/o reg) on all data: 0.09806472
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.6938317e-05
Norm of the params: 14.48496
     Influence (LOO): fixed 110 labels. Loss 0.09806. Accuracy 0.962.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036648013
Train loss (w/o reg) on all data: 0.02153531
Test loss (w/o reg) on all data: 0.047455978
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6452342e-06
Norm of the params: 17.385454
                Loss: fixed 134 labels. Loss 0.04746. Accuracy 0.985.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24481644
Train loss (w/o reg) on all data: 0.23725843
Test loss (w/o reg) on all data: 0.17719853
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.079558e-05
Norm of the params: 12.29472
              Random: fixed  33 labels. Loss 0.17720. Accuracy 0.962.
### Flips: 312, rs: 7, checks: 260
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07198415
Train loss (w/o reg) on all data: 0.061936583
Test loss (w/o reg) on all data: 0.05288302
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1872163e-06
Norm of the params: 14.175729
     Influence (LOO): fixed 131 labels. Loss 0.05288. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025281347
Train loss (w/o reg) on all data: 0.013383419
Test loss (w/o reg) on all data: 0.029057726
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9533977e-06
Norm of the params: 15.425906
                Loss: fixed 142 labels. Loss 0.02906. Accuracy 0.989.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22490473
Train loss (w/o reg) on all data: 0.21628313
Test loss (w/o reg) on all data: 0.15912622
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.5696594e-05
Norm of the params: 13.131334
              Random: fixed  46 labels. Loss 0.15913. Accuracy 0.973.
### Flips: 312, rs: 7, checks: 312
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042063497
Train loss (w/o reg) on all data: 0.033029616
Test loss (w/o reg) on all data: 0.029964749
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.725368e-06
Norm of the params: 13.441639
     Influence (LOO): fixed 145 labels. Loss 0.02996. Accuracy 0.992.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020298455
Train loss (w/o reg) on all data: 0.009904985
Test loss (w/o reg) on all data: 0.024658661
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.0976744e-07
Norm of the params: 14.417677
                Loss: fixed 146 labels. Loss 0.02466. Accuracy 0.989.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21990846
Train loss (w/o reg) on all data: 0.21148771
Test loss (w/o reg) on all data: 0.1454032
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.0002313e-05
Norm of the params: 12.977478
              Random: fixed  51 labels. Loss 0.14540. Accuracy 0.973.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25021106
Train loss (w/o reg) on all data: 0.24313943
Test loss (w/o reg) on all data: 0.17939104
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.9609265e-05
Norm of the params: 11.892544
Flipped loss: 0.17939. Accuracy: 0.935
### Flips: 312, rs: 8, checks: 52
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20506427
Train loss (w/o reg) on all data: 0.19487797
Test loss (w/o reg) on all data: 0.14848322
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.6994947e-05
Norm of the params: 14.273266
     Influence (LOO): fixed  32 labels. Loss 0.14848. Accuracy 0.943.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1411029
Train loss (w/o reg) on all data: 0.12655342
Test loss (w/o reg) on all data: 0.13092126
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 6.518728e-06
Norm of the params: 17.058418
                Loss: fixed  52 labels. Loss 0.13092. Accuracy 0.931.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2489925
Train loss (w/o reg) on all data: 0.24192593
Test loss (w/o reg) on all data: 0.16997151
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.9356808e-05
Norm of the params: 11.8883
              Random: fixed   3 labels. Loss 0.16997. Accuracy 0.950.
### Flips: 312, rs: 8, checks: 104
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1472269
Train loss (w/o reg) on all data: 0.13603163
Test loss (w/o reg) on all data: 0.10652922
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.420556e-06
Norm of the params: 14.963469
     Influence (LOO): fixed  66 labels. Loss 0.10653. Accuracy 0.966.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063643545
Train loss (w/o reg) on all data: 0.046229072
Test loss (w/o reg) on all data: 0.07000137
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.262746e-06
Norm of the params: 18.662514
                Loss: fixed  98 labels. Loss 0.07000. Accuracy 0.969.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23610163
Train loss (w/o reg) on all data: 0.22857411
Test loss (w/o reg) on all data: 0.1503822
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.2395604e-05
Norm of the params: 12.269898
              Random: fixed  14 labels. Loss 0.15038. Accuracy 0.973.
### Flips: 312, rs: 8, checks: 156
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09557429
Train loss (w/o reg) on all data: 0.083095536
Test loss (w/o reg) on all data: 0.07931616
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.3676968e-05
Norm of the params: 15.797943
     Influence (LOO): fixed  94 labels. Loss 0.07932. Accuracy 0.962.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03723266
Train loss (w/o reg) on all data: 0.022768488
Test loss (w/o reg) on all data: 0.039217185
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.746232e-06
Norm of the params: 17.008335
                Loss: fixed 116 labels. Loss 0.03922. Accuracy 0.989.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2238288
Train loss (w/o reg) on all data: 0.21641421
Test loss (w/o reg) on all data: 0.1295471
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5167887e-05
Norm of the params: 12.177503
              Random: fixed  26 labels. Loss 0.12955. Accuracy 0.985.
### Flips: 312, rs: 8, checks: 208
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06633183
Train loss (w/o reg) on all data: 0.05516291
Test loss (w/o reg) on all data: 0.049754106
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.918885e-06
Norm of the params: 14.94585
     Influence (LOO): fixed 112 labels. Loss 0.04975. Accuracy 0.981.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024329847
Train loss (w/o reg) on all data: 0.013113217
Test loss (w/o reg) on all data: 0.02955056
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3373346e-06
Norm of the params: 14.977736
                Loss: fixed 125 labels. Loss 0.02955. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21526764
Train loss (w/o reg) on all data: 0.20712799
Test loss (w/o reg) on all data: 0.123372115
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2610314e-05
Norm of the params: 12.759035
              Random: fixed  32 labels. Loss 0.12337. Accuracy 0.977.
### Flips: 312, rs: 8, checks: 260
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040294394
Train loss (w/o reg) on all data: 0.031440534
Test loss (w/o reg) on all data: 0.03129793
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5033797e-06
Norm of the params: 13.307036
     Influence (LOO): fixed 126 labels. Loss 0.03130. Accuracy 0.996.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018171906
Train loss (w/o reg) on all data: 0.00879342
Test loss (w/o reg) on all data: 0.019311467
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0691457e-06
Norm of the params: 13.695609
                Loss: fixed 129 labels. Loss 0.01931. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20549147
Train loss (w/o reg) on all data: 0.19754273
Test loss (w/o reg) on all data: 0.11560766
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.9885715e-05
Norm of the params: 12.608521
              Random: fixed  41 labels. Loss 0.11561. Accuracy 0.989.
### Flips: 312, rs: 8, checks: 312
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02646513
Train loss (w/o reg) on all data: 0.019314634
Test loss (w/o reg) on all data: 0.016318154
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.553397e-07
Norm of the params: 11.958676
     Influence (LOO): fixed 132 labels. Loss 0.01632. Accuracy 0.996.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012964828
Train loss (w/o reg) on all data: 0.005483545
Test loss (w/o reg) on all data: 0.015671453
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1304718e-06
Norm of the params: 12.232158
                Loss: fixed 132 labels. Loss 0.01567. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19838838
Train loss (w/o reg) on all data: 0.19088519
Test loss (w/o reg) on all data: 0.11124418
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.717932e-05
Norm of the params: 12.250059
              Random: fixed  48 labels. Loss 0.11124. Accuracy 0.989.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2612924
Train loss (w/o reg) on all data: 0.25429502
Test loss (w/o reg) on all data: 0.21289676
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 0.00010616138
Norm of the params: 11.8299465
Flipped loss: 0.21290. Accuracy: 0.931
### Flips: 312, rs: 9, checks: 52
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20593975
Train loss (w/o reg) on all data: 0.19667439
Test loss (w/o reg) on all data: 0.17919426
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 3.5115005e-05
Norm of the params: 13.612758
     Influence (LOO): fixed  34 labels. Loss 0.17919. Accuracy 0.912.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15937664
Train loss (w/o reg) on all data: 0.14740483
Test loss (w/o reg) on all data: 0.18670066
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 9.479938e-06
Norm of the params: 15.4737215
                Loss: fixed  50 labels. Loss 0.18670. Accuracy 0.908.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2585892
Train loss (w/o reg) on all data: 0.25159582
Test loss (w/o reg) on all data: 0.20545697
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 4.690449e-05
Norm of the params: 11.8265705
              Random: fixed   3 labels. Loss 0.20546. Accuracy 0.935.
### Flips: 312, rs: 9, checks: 104
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1671964
Train loss (w/o reg) on all data: 0.1574575
Test loss (w/o reg) on all data: 0.14945254
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.005903e-05
Norm of the params: 13.956282
     Influence (LOO): fixed  58 labels. Loss 0.14945. Accuracy 0.935.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07962277
Train loss (w/o reg) on all data: 0.06409287
Test loss (w/o reg) on all data: 0.082035795
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.0332087e-06
Norm of the params: 17.62379
                Loss: fixed  94 labels. Loss 0.08204. Accuracy 0.973.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25436738
Train loss (w/o reg) on all data: 0.24686112
Test loss (w/o reg) on all data: 0.19545421
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3373676e-05
Norm of the params: 12.252569
              Random: fixed  10 labels. Loss 0.19545. Accuracy 0.962.
### Flips: 312, rs: 9, checks: 156
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12128779
Train loss (w/o reg) on all data: 0.11075254
Test loss (w/o reg) on all data: 0.10216963
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.1272135e-05
Norm of the params: 14.515685
     Influence (LOO): fixed  89 labels. Loss 0.10217. Accuracy 0.962.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043561246
Train loss (w/o reg) on all data: 0.029771462
Test loss (w/o reg) on all data: 0.038072538
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.3710394e-06
Norm of the params: 16.607098
                Loss: fixed 118 labels. Loss 0.03807. Accuracy 0.989.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24872921
Train loss (w/o reg) on all data: 0.24114004
Test loss (w/o reg) on all data: 0.18538043
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.716518e-05
Norm of the params: 12.32004
              Random: fixed  16 labels. Loss 0.18538. Accuracy 0.966.
### Flips: 312, rs: 9, checks: 208
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08592751
Train loss (w/o reg) on all data: 0.07653997
Test loss (w/o reg) on all data: 0.063102536
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.945155e-05
Norm of the params: 13.702219
     Influence (LOO): fixed 111 labels. Loss 0.06310. Accuracy 0.989.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024032652
Train loss (w/o reg) on all data: 0.013306674
Test loss (w/o reg) on all data: 0.031224193
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.31612e-06
Norm of the params: 14.646487
                Loss: fixed 131 labels. Loss 0.03122. Accuracy 0.985.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23613343
Train loss (w/o reg) on all data: 0.22800049
Test loss (w/o reg) on all data: 0.17596579
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.913523e-05
Norm of the params: 12.7537775
              Random: fixed  25 labels. Loss 0.17597. Accuracy 0.962.
### Flips: 312, rs: 9, checks: 260
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049917184
Train loss (w/o reg) on all data: 0.040040426
Test loss (w/o reg) on all data: 0.04621797
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.7620715e-06
Norm of the params: 14.05472
     Influence (LOO): fixed 128 labels. Loss 0.04622. Accuracy 0.985.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018102195
Train loss (w/o reg) on all data: 0.008292376
Test loss (w/o reg) on all data: 0.020741735
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8033456e-06
Norm of the params: 14.007011
                Loss: fixed 137 labels. Loss 0.02074. Accuracy 0.989.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22765492
Train loss (w/o reg) on all data: 0.21976769
Test loss (w/o reg) on all data: 0.16835429
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.0331692e-05
Norm of the params: 12.559647
              Random: fixed  32 labels. Loss 0.16835. Accuracy 0.950.
### Flips: 312, rs: 9, checks: 312
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018490793
Train loss (w/o reg) on all data: 0.011254376
Test loss (w/o reg) on all data: 0.02007941
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1637245e-06
Norm of the params: 12.030311
     Influence (LOO): fixed 141 labels. Loss 0.02008. Accuracy 0.996.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015241178
Train loss (w/o reg) on all data: 0.0069046323
Test loss (w/o reg) on all data: 0.02364551
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0174514e-07
Norm of the params: 12.912432
                Loss: fixed 139 labels. Loss 0.02365. Accuracy 0.989.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22096212
Train loss (w/o reg) on all data: 0.2132286
Test loss (w/o reg) on all data: 0.15036297
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.1364072e-05
Norm of the params: 12.436666
              Random: fixed  39 labels. Loss 0.15036. Accuracy 0.966.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24645472
Train loss (w/o reg) on all data: 0.23849785
Test loss (w/o reg) on all data: 0.2016659
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 3.579603e-05
Norm of the params: 12.614962
Flipped loss: 0.20167. Accuracy: 0.924
### Flips: 312, rs: 10, checks: 52
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1981203
Train loss (w/o reg) on all data: 0.18885492
Test loss (w/o reg) on all data: 0.17580262
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.6383088e-05
Norm of the params: 13.612768
     Influence (LOO): fixed  31 labels. Loss 0.17580. Accuracy 0.939.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13906313
Train loss (w/o reg) on all data: 0.12689003
Test loss (w/o reg) on all data: 0.16785905
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.0019637e-05
Norm of the params: 15.6032715
                Loss: fixed  49 labels. Loss 0.16786. Accuracy 0.920.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23460469
Train loss (w/o reg) on all data: 0.22688505
Test loss (w/o reg) on all data: 0.18042555
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.17102445e-05
Norm of the params: 12.425482
              Random: fixed  13 labels. Loss 0.18043. Accuracy 0.943.
### Flips: 312, rs: 10, checks: 104
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14352226
Train loss (w/o reg) on all data: 0.13318129
Test loss (w/o reg) on all data: 0.1497395
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.0932655e-05
Norm of the params: 14.381222
     Influence (LOO): fixed  59 labels. Loss 0.14974. Accuracy 0.950.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056768145
Train loss (w/o reg) on all data: 0.042486873
Test loss (w/o reg) on all data: 0.09790044
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.4122268e-06
Norm of the params: 16.900457
                Loss: fixed  94 labels. Loss 0.09790. Accuracy 0.962.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22621365
Train loss (w/o reg) on all data: 0.21830967
Test loss (w/o reg) on all data: 0.16043207
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.343479e-05
Norm of the params: 12.572965
              Random: fixed  21 labels. Loss 0.16043. Accuracy 0.966.
### Flips: 312, rs: 10, checks: 156
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09438985
Train loss (w/o reg) on all data: 0.081887625
Test loss (w/o reg) on all data: 0.105968624
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.192957e-05
Norm of the params: 15.812797
     Influence (LOO): fixed  85 labels. Loss 0.10597. Accuracy 0.969.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034383316
Train loss (w/o reg) on all data: 0.023404613
Test loss (w/o reg) on all data: 0.03566939
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.5571155e-06
Norm of the params: 14.818032
                Loss: fixed 114 labels. Loss 0.03567. Accuracy 0.996.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22085808
Train loss (w/o reg) on all data: 0.2131156
Test loss (w/o reg) on all data: 0.15482688
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.1878055e-05
Norm of the params: 12.443852
              Random: fixed  26 labels. Loss 0.15483. Accuracy 0.962.
### Flips: 312, rs: 10, checks: 208
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04732346
Train loss (w/o reg) on all data: 0.03453724
Test loss (w/o reg) on all data: 0.070863396
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.044357e-06
Norm of the params: 15.991386
     Influence (LOO): fixed 108 labels. Loss 0.07086. Accuracy 0.977.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018018957
Train loss (w/o reg) on all data: 0.0094214175
Test loss (w/o reg) on all data: 0.02109648
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4600427e-06
Norm of the params: 13.113
                Loss: fixed 123 labels. Loss 0.02110. Accuracy 0.992.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20863229
Train loss (w/o reg) on all data: 0.20060642
Test loss (w/o reg) on all data: 0.14183976
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.3464094e-05
Norm of the params: 12.669547
              Random: fixed  36 labels. Loss 0.14184. Accuracy 0.962.
### Flips: 312, rs: 10, checks: 260
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030740226
Train loss (w/o reg) on all data: 0.021465624
Test loss (w/o reg) on all data: 0.042933065
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8717884e-06
Norm of the params: 13.619546
     Influence (LOO): fixed 120 labels. Loss 0.04293. Accuracy 0.973.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010907302
Train loss (w/o reg) on all data: 0.004607092
Test loss (w/o reg) on all data: 0.014508272
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.605293e-07
Norm of the params: 11.225159
                Loss: fixed 127 labels. Loss 0.01451. Accuracy 0.996.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20008188
Train loss (w/o reg) on all data: 0.19210276
Test loss (w/o reg) on all data: 0.13637476
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.958492e-06
Norm of the params: 12.632592
              Random: fixed  41 labels. Loss 0.13637. Accuracy 0.966.
### Flips: 312, rs: 10, checks: 312
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013616028
Train loss (w/o reg) on all data: 0.0074465796
Test loss (w/o reg) on all data: 0.011247954
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.029261e-06
Norm of the params: 11.108059
     Influence (LOO): fixed 129 labels. Loss 0.01125. Accuracy 0.996.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009952482
Train loss (w/o reg) on all data: 0.0040762373
Test loss (w/o reg) on all data: 0.012442396
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9490275e-07
Norm of the params: 10.840891
                Loss: fixed 128 labels. Loss 0.01244. Accuracy 0.996.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18499145
Train loss (w/o reg) on all data: 0.1768292
Test loss (w/o reg) on all data: 0.1323128
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.9247393e-05
Norm of the params: 12.776738
              Random: fixed  49 labels. Loss 0.13231. Accuracy 0.962.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2604372
Train loss (w/o reg) on all data: 0.2522322
Test loss (w/o reg) on all data: 0.25062308
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.5394535e-05
Norm of the params: 12.810154
Flipped loss: 0.25062. Accuracy: 0.920
### Flips: 312, rs: 11, checks: 52
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20505759
Train loss (w/o reg) on all data: 0.19343144
Test loss (w/o reg) on all data: 0.2177516
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.6552798e-05
Norm of the params: 15.248711
     Influence (LOO): fixed  36 labels. Loss 0.21775. Accuracy 0.939.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15789115
Train loss (w/o reg) on all data: 0.14048836
Test loss (w/o reg) on all data: 0.24203795
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 9.423116e-06
Norm of the params: 18.65626
                Loss: fixed  50 labels. Loss 0.24204. Accuracy 0.912.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25276002
Train loss (w/o reg) on all data: 0.24491195
Test loss (w/o reg) on all data: 0.22721489
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.0253994e-05
Norm of the params: 12.52843
              Random: fixed  11 labels. Loss 0.22721. Accuracy 0.935.
### Flips: 312, rs: 11, checks: 104
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15351756
Train loss (w/o reg) on all data: 0.13823791
Test loss (w/o reg) on all data: 0.21753381
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.1272827e-05
Norm of the params: 17.481218
     Influence (LOO): fixed  63 labels. Loss 0.21753. Accuracy 0.916.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083293825
Train loss (w/o reg) on all data: 0.06700312
Test loss (w/o reg) on all data: 0.17829242
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.3088238e-05
Norm of the params: 18.05032
                Loss: fixed  95 labels. Loss 0.17829. Accuracy 0.943.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24857648
Train loss (w/o reg) on all data: 0.24051213
Test loss (w/o reg) on all data: 0.21357828
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.9231178e-05
Norm of the params: 12.699877
              Random: fixed  17 labels. Loss 0.21358. Accuracy 0.950.
### Flips: 312, rs: 11, checks: 156
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1295386
Train loss (w/o reg) on all data: 0.11673268
Test loss (w/o reg) on all data: 0.17858294
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.2596352e-05
Norm of the params: 16.003695
     Influence (LOO): fixed  81 labels. Loss 0.17858. Accuracy 0.931.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040482167
Train loss (w/o reg) on all data: 0.027658347
Test loss (w/o reg) on all data: 0.12480379
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.846652e-06
Norm of the params: 16.014881
                Loss: fixed 124 labels. Loss 0.12480. Accuracy 0.954.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24290554
Train loss (w/o reg) on all data: 0.23474698
Test loss (w/o reg) on all data: 0.20185675
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.3736027e-05
Norm of the params: 12.7738495
              Random: fixed  23 labels. Loss 0.20186. Accuracy 0.954.
### Flips: 312, rs: 11, checks: 208
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09181023
Train loss (w/o reg) on all data: 0.07881163
Test loss (w/o reg) on all data: 0.11892285
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.350259e-05
Norm of the params: 16.123644
     Influence (LOO): fixed 106 labels. Loss 0.11892. Accuracy 0.958.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028371267
Train loss (w/o reg) on all data: 0.016522711
Test loss (w/o reg) on all data: 0.081536986
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8854944e-06
Norm of the params: 15.393867
                Loss: fixed 133 labels. Loss 0.08154. Accuracy 0.973.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23352239
Train loss (w/o reg) on all data: 0.22515784
Test loss (w/o reg) on all data: 0.1928116
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.7778908e-05
Norm of the params: 12.9341
              Random: fixed  31 labels. Loss 0.19281. Accuracy 0.958.
### Flips: 312, rs: 11, checks: 260
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060017586
Train loss (w/o reg) on all data: 0.048671745
Test loss (w/o reg) on all data: 0.055305
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8614666e-05
Norm of the params: 15.06376
     Influence (LOO): fixed 128 labels. Loss 0.05531. Accuracy 0.989.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019605579
Train loss (w/o reg) on all data: 0.009708554
Test loss (w/o reg) on all data: 0.03995294
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0481315e-06
Norm of the params: 14.069134
                Loss: fixed 139 labels. Loss 0.03995. Accuracy 0.989.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22549112
Train loss (w/o reg) on all data: 0.21742912
Test loss (w/o reg) on all data: 0.18283346
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.6000065e-05
Norm of the params: 12.698037
              Random: fixed  39 labels. Loss 0.18283. Accuracy 0.962.
### Flips: 312, rs: 11, checks: 312
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025626577
Train loss (w/o reg) on all data: 0.018350748
Test loss (w/o reg) on all data: 0.02640867
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.851248e-06
Norm of the params: 12.063026
     Influence (LOO): fixed 145 labels. Loss 0.02641. Accuracy 0.996.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019482478
Train loss (w/o reg) on all data: 0.009765221
Test loss (w/o reg) on all data: 0.036161166
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6877525e-06
Norm of the params: 13.940774
                Loss: fixed 140 labels. Loss 0.03616. Accuracy 0.989.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21346235
Train loss (w/o reg) on all data: 0.20536128
Test loss (w/o reg) on all data: 0.16170406
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.9113355e-06
Norm of the params: 12.728765
              Random: fixed  50 labels. Loss 0.16170. Accuracy 0.973.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26247686
Train loss (w/o reg) on all data: 0.25651705
Test loss (w/o reg) on all data: 0.22763908
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 4.749853e-05
Norm of the params: 10.917693
Flipped loss: 0.22764. Accuracy: 0.901
### Flips: 312, rs: 12, checks: 52
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22243099
Train loss (w/o reg) on all data: 0.21446764
Test loss (w/o reg) on all data: 0.18278112
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.506736e-05
Norm of the params: 12.620097
     Influence (LOO): fixed  30 labels. Loss 0.18278. Accuracy 0.920.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1659126
Train loss (w/o reg) on all data: 0.15458249
Test loss (w/o reg) on all data: 0.19333747
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.0088853e-05
Norm of the params: 15.05331
                Loss: fixed  47 labels. Loss 0.19334. Accuracy 0.908.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26059112
Train loss (w/o reg) on all data: 0.25447124
Test loss (w/o reg) on all data: 0.22228576
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 2.9206481e-05
Norm of the params: 11.063348
              Random: fixed   3 labels. Loss 0.22229. Accuracy 0.905.
### Flips: 312, rs: 12, checks: 104
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16984145
Train loss (w/o reg) on all data: 0.16112165
Test loss (w/o reg) on all data: 0.14865327
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.1680653e-05
Norm of the params: 13.20591
     Influence (LOO): fixed  65 labels. Loss 0.14865. Accuracy 0.927.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090974346
Train loss (w/o reg) on all data: 0.07504131
Test loss (w/o reg) on all data: 0.15005739
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 4.5450033e-06
Norm of the params: 17.851072
                Loss: fixed  90 labels. Loss 0.15006. Accuracy 0.939.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25220755
Train loss (w/o reg) on all data: 0.24568872
Test loss (w/o reg) on all data: 0.20596524
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.486134e-05
Norm of the params: 11.418248
              Random: fixed  12 labels. Loss 0.20597. Accuracy 0.935.
### Flips: 312, rs: 12, checks: 156
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12256773
Train loss (w/o reg) on all data: 0.11245231
Test loss (w/o reg) on all data: 0.124475755
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.686897e-06
Norm of the params: 14.22351
     Influence (LOO): fixed  93 labels. Loss 0.12448. Accuracy 0.966.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04496532
Train loss (w/o reg) on all data: 0.03172589
Test loss (w/o reg) on all data: 0.10801799
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.686181e-06
Norm of the params: 16.272326
                Loss: fixed 119 labels. Loss 0.10802. Accuracy 0.962.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24175417
Train loss (w/o reg) on all data: 0.23494515
Test loss (w/o reg) on all data: 0.18373276
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.1328859e-05
Norm of the params: 11.669642
              Random: fixed  22 labels. Loss 0.18373. Accuracy 0.950.
### Flips: 312, rs: 12, checks: 208
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07881927
Train loss (w/o reg) on all data: 0.06833388
Test loss (w/o reg) on all data: 0.081074715
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.048762e-06
Norm of the params: 14.481292
     Influence (LOO): fixed 119 labels. Loss 0.08107. Accuracy 0.981.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030447096
Train loss (w/o reg) on all data: 0.019047292
Test loss (w/o reg) on all data: 0.06334197
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1428673e-06
Norm of the params: 15.099539
                Loss: fixed 133 labels. Loss 0.06334. Accuracy 0.977.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23872645
Train loss (w/o reg) on all data: 0.232172
Test loss (w/o reg) on all data: 0.17339435
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.6606866e-05
Norm of the params: 11.449414
              Random: fixed  26 labels. Loss 0.17339. Accuracy 0.966.
### Flips: 312, rs: 12, checks: 260
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03043672
Train loss (w/o reg) on all data: 0.022733832
Test loss (w/o reg) on all data: 0.037219282
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7080612e-06
Norm of the params: 12.412003
     Influence (LOO): fixed 141 labels. Loss 0.03722. Accuracy 0.989.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017919905
Train loss (w/o reg) on all data: 0.009418947
Test loss (w/o reg) on all data: 0.035203293
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6344649e-06
Norm of the params: 13.039141
                Loss: fixed 141 labels. Loss 0.03520. Accuracy 0.985.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23261032
Train loss (w/o reg) on all data: 0.22662804
Test loss (w/o reg) on all data: 0.15921678
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.759538e-06
Norm of the params: 10.938264
              Random: fixed  33 labels. Loss 0.15922. Accuracy 0.962.
### Flips: 312, rs: 12, checks: 312
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014498441
Train loss (w/o reg) on all data: 0.00834416
Test loss (w/o reg) on all data: 0.0220849
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3390655e-06
Norm of the params: 11.094396
     Influence (LOO): fixed 146 labels. Loss 0.02208. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009413425
Train loss (w/o reg) on all data: 0.0037069912
Test loss (w/o reg) on all data: 0.01521272
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.656739e-07
Norm of the params: 10.683104
                Loss: fixed 147 labels. Loss 0.01521. Accuracy 0.992.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22054161
Train loss (w/o reg) on all data: 0.2139901
Test loss (w/o reg) on all data: 0.13717239
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5513615e-05
Norm of the params: 11.446843
              Random: fixed  43 labels. Loss 0.13717. Accuracy 0.969.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2632055
Train loss (w/o reg) on all data: 0.25642827
Test loss (w/o reg) on all data: 0.20937999
Train acc on all data:  0.8662846227316141
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 2.3933078e-05
Norm of the params: 11.642355
Flipped loss: 0.20938. Accuracy: 0.908
### Flips: 312, rs: 13, checks: 52
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20641313
Train loss (w/o reg) on all data: 0.19672617
Test loss (w/o reg) on all data: 0.18302126
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.4663967e-05
Norm of the params: 13.919025
     Influence (LOO): fixed  33 labels. Loss 0.18302. Accuracy 0.927.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15473926
Train loss (w/o reg) on all data: 0.13995312
Test loss (w/o reg) on all data: 0.18617377
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.5148609e-05
Norm of the params: 17.196589
                Loss: fixed  51 labels. Loss 0.18617. Accuracy 0.920.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26296628
Train loss (w/o reg) on all data: 0.2561768
Test loss (w/o reg) on all data: 0.20555708
Train acc on all data:  0.8662846227316141
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 4.694528e-05
Norm of the params: 11.652871
              Random: fixed   2 labels. Loss 0.20556. Accuracy 0.924.
### Flips: 312, rs: 13, checks: 104
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15228772
Train loss (w/o reg) on all data: 0.1405759
Test loss (w/o reg) on all data: 0.1563807
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.1158125e-05
Norm of the params: 15.304784
     Influence (LOO): fixed  65 labels. Loss 0.15638. Accuracy 0.943.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09155687
Train loss (w/o reg) on all data: 0.07413909
Test loss (w/o reg) on all data: 0.12990756
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.0557988e-05
Norm of the params: 18.66429
                Loss: fixed  92 labels. Loss 0.12991. Accuracy 0.935.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25923982
Train loss (w/o reg) on all data: 0.25255513
Test loss (w/o reg) on all data: 0.1978805
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 5.8022393e-05
Norm of the params: 11.562598
              Random: fixed   9 labels. Loss 0.19788. Accuracy 0.931.
### Flips: 312, rs: 13, checks: 156
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11488016
Train loss (w/o reg) on all data: 0.10197797
Test loss (w/o reg) on all data: 0.123160526
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.1672737e-05
Norm of the params: 16.063744
     Influence (LOO): fixed  86 labels. Loss 0.12316. Accuracy 0.954.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040156573
Train loss (w/o reg) on all data: 0.024952237
Test loss (w/o reg) on all data: 0.052788187
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.738605e-06
Norm of the params: 17.438082
                Loss: fixed 124 labels. Loss 0.05279. Accuracy 0.977.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25053078
Train loss (w/o reg) on all data: 0.24421099
Test loss (w/o reg) on all data: 0.17842536
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.7737503e-05
Norm of the params: 11.242582
              Random: fixed  21 labels. Loss 0.17843. Accuracy 0.954.
### Flips: 312, rs: 13, checks: 208
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08599819
Train loss (w/o reg) on all data: 0.07265177
Test loss (w/o reg) on all data: 0.09620566
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.6686613e-06
Norm of the params: 16.337942
     Influence (LOO): fixed 107 labels. Loss 0.09621. Accuracy 0.969.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025234027
Train loss (w/o reg) on all data: 0.013006519
Test loss (w/o reg) on all data: 0.034173436
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1745759e-06
Norm of the params: 15.6381
                Loss: fixed 136 labels. Loss 0.03417. Accuracy 0.989.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24461986
Train loss (w/o reg) on all data: 0.23814575
Test loss (w/o reg) on all data: 0.16906452
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.2289227e-05
Norm of the params: 11.379027
              Random: fixed  28 labels. Loss 0.16906. Accuracy 0.958.
### Flips: 312, rs: 13, checks: 260
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063908346
Train loss (w/o reg) on all data: 0.052660305
Test loss (w/o reg) on all data: 0.055705648
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0701497e-06
Norm of the params: 14.998695
     Influence (LOO): fixed 125 labels. Loss 0.05571. Accuracy 0.992.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022131816
Train loss (w/o reg) on all data: 0.011077303
Test loss (w/o reg) on all data: 0.030354952
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.6241313e-07
Norm of the params: 14.869104
                Loss: fixed 141 labels. Loss 0.03035. Accuracy 0.989.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23583187
Train loss (w/o reg) on all data: 0.22926855
Test loss (w/o reg) on all data: 0.15306413
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.4886347e-05
Norm of the params: 11.457152
              Random: fixed  38 labels. Loss 0.15306. Accuracy 0.969.
### Flips: 312, rs: 13, checks: 312
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038653456
Train loss (w/o reg) on all data: 0.029418008
Test loss (w/o reg) on all data: 0.033834595
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.195091e-06
Norm of the params: 13.590769
     Influence (LOO): fixed 139 labels. Loss 0.03383. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013594532
Train loss (w/o reg) on all data: 0.0058170473
Test loss (w/o reg) on all data: 0.02069935
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3059467e-07
Norm of the params: 12.471956
                Loss: fixed 145 labels. Loss 0.02070. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22836453
Train loss (w/o reg) on all data: 0.22175941
Test loss (w/o reg) on all data: 0.14280836
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1533925e-05
Norm of the params: 11.493577
              Random: fixed  44 labels. Loss 0.14281. Accuracy 0.973.
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25714502
Train loss (w/o reg) on all data: 0.24984446
Test loss (w/o reg) on all data: 0.17444867
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.3503473e-05
Norm of the params: 12.083499
Flipped loss: 0.17445. Accuracy: 0.943
### Flips: 312, rs: 14, checks: 52
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19869599
Train loss (w/o reg) on all data: 0.18794292
Test loss (w/o reg) on all data: 0.13165374
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 9.649397e-06
Norm of the params: 14.664972
     Influence (LOO): fixed  36 labels. Loss 0.13165. Accuracy 0.958.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14834696
Train loss (w/o reg) on all data: 0.13196516
Test loss (w/o reg) on all data: 0.12654923
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.6837033e-05
Norm of the params: 18.100721
                Loss: fixed  51 labels. Loss 0.12655. Accuracy 0.935.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25044167
Train loss (w/o reg) on all data: 0.24293785
Test loss (w/o reg) on all data: 0.17324962
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.3461998e-05
Norm of the params: 12.250559
              Random: fixed   6 labels. Loss 0.17325. Accuracy 0.943.
### Flips: 312, rs: 14, checks: 104
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15082401
Train loss (w/o reg) on all data: 0.13788182
Test loss (w/o reg) on all data: 0.095630206
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2182861e-05
Norm of the params: 16.08863
     Influence (LOO): fixed  62 labels. Loss 0.09563. Accuracy 0.966.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068114825
Train loss (w/o reg) on all data: 0.049559362
Test loss (w/o reg) on all data: 0.08541524
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.582256e-06
Norm of the params: 19.264194
                Loss: fixed  97 labels. Loss 0.08542. Accuracy 0.962.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24608496
Train loss (w/o reg) on all data: 0.23832606
Test loss (w/o reg) on all data: 0.16632353
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.6436574e-05
Norm of the params: 12.45705
              Random: fixed  10 labels. Loss 0.16632. Accuracy 0.939.
### Flips: 312, rs: 14, checks: 156
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10165446
Train loss (w/o reg) on all data: 0.08912609
Test loss (w/o reg) on all data: 0.059231997
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.9951777e-06
Norm of the params: 15.829322
     Influence (LOO): fixed  92 labels. Loss 0.05923. Accuracy 0.981.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037283085
Train loss (w/o reg) on all data: 0.022235211
Test loss (w/o reg) on all data: 0.047299255
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.49436e-06
Norm of the params: 17.348125
                Loss: fixed 117 labels. Loss 0.04730. Accuracy 0.977.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23743543
Train loss (w/o reg) on all data: 0.22969575
Test loss (w/o reg) on all data: 0.15455744
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.048027e-05
Norm of the params: 12.441601
              Random: fixed  16 labels. Loss 0.15456. Accuracy 0.954.
### Flips: 312, rs: 14, checks: 208
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060304876
Train loss (w/o reg) on all data: 0.049068015
Test loss (w/o reg) on all data: 0.03679809
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.5933865e-06
Norm of the params: 14.991238
     Influence (LOO): fixed 115 labels. Loss 0.03680. Accuracy 0.989.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02108334
Train loss (w/o reg) on all data: 0.010936221
Test loss (w/o reg) on all data: 0.018779367
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.793398e-06
Norm of the params: 14.245786
                Loss: fixed 131 labels. Loss 0.01878. Accuracy 0.996.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23068208
Train loss (w/o reg) on all data: 0.22240037
Test loss (w/o reg) on all data: 0.14916486
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.993455e-05
Norm of the params: 12.869897
              Random: fixed  21 labels. Loss 0.14916. Accuracy 0.954.
### Flips: 312, rs: 14, checks: 260
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046217315
Train loss (w/o reg) on all data: 0.03614901
Test loss (w/o reg) on all data: 0.033988427
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.361592e-06
Norm of the params: 14.190351
     Influence (LOO): fixed 124 labels. Loss 0.03399. Accuracy 0.989.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015895002
Train loss (w/o reg) on all data: 0.007711302
Test loss (w/o reg) on all data: 0.016035397
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.809098e-07
Norm of the params: 12.793514
                Loss: fixed 134 labels. Loss 0.01604. Accuracy 0.992.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22631574
Train loss (w/o reg) on all data: 0.21800566
Test loss (w/o reg) on all data: 0.1431666
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.77693e-05
Norm of the params: 12.891921
              Random: fixed  25 labels. Loss 0.14317. Accuracy 0.969.
### Flips: 312, rs: 14, checks: 312
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018176753
Train loss (w/o reg) on all data: 0.0109782
Test loss (w/o reg) on all data: 0.02342117
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.226983e-07
Norm of the params: 11.998795
     Influence (LOO): fixed 137 labels. Loss 0.02342. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015161058
Train loss (w/o reg) on all data: 0.007223976
Test loss (w/o reg) on all data: 0.016767677
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0762507e-07
Norm of the params: 12.599272
                Loss: fixed 135 labels. Loss 0.01677. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21675643
Train loss (w/o reg) on all data: 0.20818561
Test loss (w/o reg) on all data: 0.1289282
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.056384e-05
Norm of the params: 13.092611
              Random: fixed  34 labels. Loss 0.12893. Accuracy 0.973.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27154332
Train loss (w/o reg) on all data: 0.26533476
Test loss (w/o reg) on all data: 0.22513272
Train acc on all data:  0.8586437440305635
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.4267641e-05
Norm of the params: 11.143213
Flipped loss: 0.22513. Accuracy: 0.924
### Flips: 312, rs: 15, checks: 52
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23148702
Train loss (w/o reg) on all data: 0.22310165
Test loss (w/o reg) on all data: 0.19052313
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.1976712e-05
Norm of the params: 12.95019
     Influence (LOO): fixed  28 labels. Loss 0.19052. Accuracy 0.931.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17564268
Train loss (w/o reg) on all data: 0.16388188
Test loss (w/o reg) on all data: 0.18858616
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.5507103e-05
Norm of the params: 15.336752
                Loss: fixed  52 labels. Loss 0.18859. Accuracy 0.924.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26456833
Train loss (w/o reg) on all data: 0.25807667
Test loss (w/o reg) on all data: 0.21175641
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 0.00010081481
Norm of the params: 11.394431
              Random: fixed  10 labels. Loss 0.21176. Accuracy 0.931.
### Flips: 312, rs: 15, checks: 104
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19867985
Train loss (w/o reg) on all data: 0.18883663
Test loss (w/o reg) on all data: 0.16621158
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.0741805e-05
Norm of the params: 14.030829
     Influence (LOO): fixed  51 labels. Loss 0.16621. Accuracy 0.958.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10409495
Train loss (w/o reg) on all data: 0.08780744
Test loss (w/o reg) on all data: 0.15819263
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 4.959711e-06
Norm of the params: 18.048552
                Loss: fixed  90 labels. Loss 0.15819. Accuracy 0.935.
Using normal model
LBFGS training took [318] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2631311
Train loss (w/o reg) on all data: 0.2565066
Test loss (w/o reg) on all data: 0.20187345
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.9740264e-05
Norm of the params: 11.510452
              Random: fixed  16 labels. Loss 0.20187. Accuracy 0.931.
### Flips: 312, rs: 15, checks: 156
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.158163
Train loss (w/o reg) on all data: 0.14903359
Test loss (w/o reg) on all data: 0.13864145
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.3046926e-05
Norm of the params: 13.512521
     Influence (LOO): fixed  80 labels. Loss 0.13864. Accuracy 0.947.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069567606
Train loss (w/o reg) on all data: 0.05371342
Test loss (w/o reg) on all data: 0.12133
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.2025071e-05
Norm of the params: 17.806845
                Loss: fixed 116 labels. Loss 0.12133. Accuracy 0.943.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25629675
Train loss (w/o reg) on all data: 0.24961933
Test loss (w/o reg) on all data: 0.18363373
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 8.076626e-05
Norm of the params: 11.556324
              Random: fixed  24 labels. Loss 0.18363. Accuracy 0.947.
### Flips: 312, rs: 15, checks: 208
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114169925
Train loss (w/o reg) on all data: 0.10384097
Test loss (w/o reg) on all data: 0.112291686
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.950638e-05
Norm of the params: 14.372862
     Influence (LOO): fixed 103 labels. Loss 0.11229. Accuracy 0.954.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03831001
Train loss (w/o reg) on all data: 0.024329377
Test loss (w/o reg) on all data: 0.048357226
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.913824e-06
Norm of the params: 16.721622
                Loss: fixed 139 labels. Loss 0.04836. Accuracy 0.985.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24427989
Train loss (w/o reg) on all data: 0.2375139
Test loss (w/o reg) on all data: 0.16523273
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.6376986e-05
Norm of the params: 11.632703
              Random: fixed  36 labels. Loss 0.16523. Accuracy 0.950.
### Flips: 312, rs: 15, checks: 260
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084560454
Train loss (w/o reg) on all data: 0.074774325
Test loss (w/o reg) on all data: 0.06860539
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3793043e-05
Norm of the params: 13.99009
     Influence (LOO): fixed 125 labels. Loss 0.06861. Accuracy 0.977.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025649505
Train loss (w/o reg) on all data: 0.015961017
Test loss (w/o reg) on all data: 0.028713938
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.2841056e-06
Norm of the params: 13.920119
                Loss: fixed 150 labels. Loss 0.02871. Accuracy 0.985.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23891488
Train loss (w/o reg) on all data: 0.23215511
Test loss (w/o reg) on all data: 0.14831339
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.380299e-05
Norm of the params: 11.627349
              Random: fixed  42 labels. Loss 0.14831. Accuracy 0.958.
### Flips: 312, rs: 15, checks: 312
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047923148
Train loss (w/o reg) on all data: 0.038365368
Test loss (w/o reg) on all data: 0.037957788
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.802302e-06
Norm of the params: 13.825902
     Influence (LOO): fixed 144 labels. Loss 0.03796. Accuracy 0.989.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018501803
Train loss (w/o reg) on all data: 0.010549015
Test loss (w/o reg) on all data: 0.024037234
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.1458143e-06
Norm of the params: 12.6117325
                Loss: fixed 154 labels. Loss 0.02404. Accuracy 0.985.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23021533
Train loss (w/o reg) on all data: 0.22362052
Test loss (w/o reg) on all data: 0.13442764
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.4427506e-05
Norm of the params: 11.484599
              Random: fixed  50 labels. Loss 0.13443. Accuracy 0.966.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25859657
Train loss (w/o reg) on all data: 0.2512338
Test loss (w/o reg) on all data: 0.17938815
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.1439236e-05
Norm of the params: 12.134895
Flipped loss: 0.17939. Accuracy: 0.958
### Flips: 312, rs: 16, checks: 52
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20118031
Train loss (w/o reg) on all data: 0.19187436
Test loss (w/o reg) on all data: 0.13504787
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.6807767e-05
Norm of the params: 13.642546
     Influence (LOO): fixed  36 labels. Loss 0.13505. Accuracy 0.962.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15562856
Train loss (w/o reg) on all data: 0.1433538
Test loss (w/o reg) on all data: 0.13901535
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.3867371e-05
Norm of the params: 15.668282
                Loss: fixed  51 labels. Loss 0.13902. Accuracy 0.958.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2554307
Train loss (w/o reg) on all data: 0.24845244
Test loss (w/o reg) on all data: 0.16009143
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.8375965e-05
Norm of the params: 11.813775
              Random: fixed   7 labels. Loss 0.16009. Accuracy 0.962.
### Flips: 312, rs: 16, checks: 104
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14494084
Train loss (w/o reg) on all data: 0.13496901
Test loss (w/o reg) on all data: 0.09865748
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.062258e-05
Norm of the params: 14.122197
     Influence (LOO): fixed  69 labels. Loss 0.09866. Accuracy 0.966.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07786945
Train loss (w/o reg) on all data: 0.062249344
Test loss (w/o reg) on all data: 0.06893997
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.032209e-05
Norm of the params: 17.674902
                Loss: fixed 100 labels. Loss 0.06894. Accuracy 0.973.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24349737
Train loss (w/o reg) on all data: 0.23662479
Test loss (w/o reg) on all data: 0.14317124
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.8117574e-05
Norm of the params: 11.723976
              Random: fixed  17 labels. Loss 0.14317. Accuracy 0.981.
### Flips: 312, rs: 16, checks: 156
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101291135
Train loss (w/o reg) on all data: 0.09008056
Test loss (w/o reg) on all data: 0.09610635
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.432763e-05
Norm of the params: 14.973697
     Influence (LOO): fixed  91 labels. Loss 0.09611. Accuracy 0.966.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023588493
Train loss (w/o reg) on all data: 0.013165151
Test loss (w/o reg) on all data: 0.022385994
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9328622e-06
Norm of the params: 14.438382
                Loss: fixed 128 labels. Loss 0.02239. Accuracy 0.992.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23664822
Train loss (w/o reg) on all data: 0.22987096
Test loss (w/o reg) on all data: 0.13371892
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9042123e-05
Norm of the params: 11.642384
              Random: fixed  25 labels. Loss 0.13372. Accuracy 0.977.
### Flips: 312, rs: 16, checks: 208
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07196315
Train loss (w/o reg) on all data: 0.061477937
Test loss (w/o reg) on all data: 0.07183449
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.49341395e-05
Norm of the params: 14.481168
     Influence (LOO): fixed 110 labels. Loss 0.07183. Accuracy 0.969.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014785012
Train loss (w/o reg) on all data: 0.006663296
Test loss (w/o reg) on all data: 0.022250991
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2551976e-07
Norm of the params: 12.744972
                Loss: fixed 133 labels. Loss 0.02225. Accuracy 0.992.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22941647
Train loss (w/o reg) on all data: 0.222447
Test loss (w/o reg) on all data: 0.13132855
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.836002e-05
Norm of the params: 11.806344
              Random: fixed  30 labels. Loss 0.13133. Accuracy 0.973.
### Flips: 312, rs: 16, checks: 260
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037189215
Train loss (w/o reg) on all data: 0.029885964
Test loss (w/o reg) on all data: 0.028354414
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.111649e-06
Norm of the params: 12.085737
     Influence (LOO): fixed 129 labels. Loss 0.02835. Accuracy 0.996.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013465665
Train loss (w/o reg) on all data: 0.005969311
Test loss (w/o reg) on all data: 0.013221537
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6392682e-06
Norm of the params: 12.2444725
                Loss: fixed 134 labels. Loss 0.01322. Accuracy 0.996.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21426022
Train loss (w/o reg) on all data: 0.2067248
Test loss (w/o reg) on all data: 0.11974166
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.078586e-05
Norm of the params: 12.276344
              Random: fixed  38 labels. Loss 0.11974. Accuracy 0.977.
### Flips: 312, rs: 16, checks: 312
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01881413
Train loss (w/o reg) on all data: 0.013496295
Test loss (w/o reg) on all data: 0.018709099
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2288436e-07
Norm of the params: 10.312939
     Influence (LOO): fixed 137 labels. Loss 0.01871. Accuracy 0.992.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012418412
Train loss (w/o reg) on all data: 0.0052838977
Test loss (w/o reg) on all data: 0.014180657
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8994918e-06
Norm of the params: 11.945304
                Loss: fixed 135 labels. Loss 0.01418. Accuracy 0.996.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20703508
Train loss (w/o reg) on all data: 0.20034441
Test loss (w/o reg) on all data: 0.110272214
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.3125805e-05
Norm of the params: 11.567776
              Random: fixed  45 labels. Loss 0.11027. Accuracy 0.981.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25828493
Train loss (w/o reg) on all data: 0.25018737
Test loss (w/o reg) on all data: 0.19702224
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.651698e-05
Norm of the params: 12.726016
Flipped loss: 0.19702. Accuracy: 0.931
### Flips: 312, rs: 17, checks: 52
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2048451
Train loss (w/o reg) on all data: 0.1948577
Test loss (w/o reg) on all data: 0.14899094
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.4669127e-05
Norm of the params: 14.133225
     Influence (LOO): fixed  36 labels. Loss 0.14899. Accuracy 0.977.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14806913
Train loss (w/o reg) on all data: 0.13299724
Test loss (w/o reg) on all data: 0.12801372
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.5319201e-05
Norm of the params: 17.361961
                Loss: fixed  52 labels. Loss 0.12801. Accuracy 0.943.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25503743
Train loss (w/o reg) on all data: 0.24696381
Test loss (w/o reg) on all data: 0.17606935
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.72334e-05
Norm of the params: 12.707185
              Random: fixed   8 labels. Loss 0.17607. Accuracy 0.962.
### Flips: 312, rs: 17, checks: 104
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15257482
Train loss (w/o reg) on all data: 0.14107196
Test loss (w/o reg) on all data: 0.1162346
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.441177e-05
Norm of the params: 15.167639
     Influence (LOO): fixed  68 labels. Loss 0.11623. Accuracy 0.969.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068462
Train loss (w/o reg) on all data: 0.050079178
Test loss (w/o reg) on all data: 0.05686094
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0933709e-05
Norm of the params: 19.17437
                Loss: fixed 100 labels. Loss 0.05686. Accuracy 0.985.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24197215
Train loss (w/o reg) on all data: 0.23366933
Test loss (w/o reg) on all data: 0.15435538
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6034312e-05
Norm of the params: 12.886287
              Random: fixed  21 labels. Loss 0.15436. Accuracy 0.969.
### Flips: 312, rs: 17, checks: 156
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11090599
Train loss (w/o reg) on all data: 0.09915265
Test loss (w/o reg) on all data: 0.08250842
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.593659e-05
Norm of the params: 15.331892
     Influence (LOO): fixed  93 labels. Loss 0.08251. Accuracy 0.977.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038124666
Train loss (w/o reg) on all data: 0.023116698
Test loss (w/o reg) on all data: 0.023405729
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9096882e-06
Norm of the params: 17.32511
                Loss: fixed 123 labels. Loss 0.02341. Accuracy 0.996.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23082957
Train loss (w/o reg) on all data: 0.22231172
Test loss (w/o reg) on all data: 0.14058915
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.1663978e-05
Norm of the params: 13.052083
              Random: fixed  30 labels. Loss 0.14059. Accuracy 0.977.
### Flips: 312, rs: 17, checks: 208
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07600889
Train loss (w/o reg) on all data: 0.06633861
Test loss (w/o reg) on all data: 0.053185083
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2824106e-05
Norm of the params: 13.907035
     Influence (LOO): fixed 116 labels. Loss 0.05319. Accuracy 0.985.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02753837
Train loss (w/o reg) on all data: 0.014495444
Test loss (w/o reg) on all data: 0.017562209
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9427648e-06
Norm of the params: 16.151115
                Loss: fixed 129 labels. Loss 0.01756. Accuracy 0.996.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22389436
Train loss (w/o reg) on all data: 0.21565881
Test loss (w/o reg) on all data: 0.13262092
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.9579793e-05
Norm of the params: 12.833971
              Random: fixed  37 labels. Loss 0.13262. Accuracy 0.962.
### Flips: 312, rs: 17, checks: 260
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048374385
Train loss (w/o reg) on all data: 0.039957494
Test loss (w/o reg) on all data: 0.032324683
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.08463e-06
Norm of the params: 12.974506
     Influence (LOO): fixed 129 labels. Loss 0.03232. Accuracy 0.992.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02158361
Train loss (w/o reg) on all data: 0.010575848
Test loss (w/o reg) on all data: 0.014414864
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0378933e-06
Norm of the params: 14.837629
                Loss: fixed 133 labels. Loss 0.01441. Accuracy 0.996.
Using normal model
LBFGS training took [316] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21710476
Train loss (w/o reg) on all data: 0.20911309
Test loss (w/o reg) on all data: 0.12859617
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.0512463e-05
Norm of the params: 12.642523
              Random: fixed  45 labels. Loss 0.12860. Accuracy 0.962.
### Flips: 312, rs: 17, checks: 312
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021128528
Train loss (w/o reg) on all data: 0.014800302
Test loss (w/o reg) on all data: 0.019834604
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.3003545e-07
Norm of the params: 11.250091
     Influence (LOO): fixed 140 labels. Loss 0.01983. Accuracy 0.992.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0178141
Train loss (w/o reg) on all data: 0.008177135
Test loss (w/o reg) on all data: 0.014883643
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.664671e-07
Norm of the params: 13.883057
                Loss: fixed 136 labels. Loss 0.01488. Accuracy 0.996.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2087577
Train loss (w/o reg) on all data: 0.20091255
Test loss (w/o reg) on all data: 0.11868809
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.088093e-06
Norm of the params: 12.526094
              Random: fixed  53 labels. Loss 0.11869. Accuracy 0.973.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26209566
Train loss (w/o reg) on all data: 0.25533727
Test loss (w/o reg) on all data: 0.21476004
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 7.543952e-05
Norm of the params: 11.626167
Flipped loss: 0.21476. Accuracy: 0.916
### Flips: 312, rs: 18, checks: 52
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19922914
Train loss (w/o reg) on all data: 0.1873288
Test loss (w/o reg) on all data: 0.17974472
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 8.628233e-06
Norm of the params: 15.427465
     Influence (LOO): fixed  33 labels. Loss 0.17974. Accuracy 0.935.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15644085
Train loss (w/o reg) on all data: 0.14249767
Test loss (w/o reg) on all data: 0.16129525
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.1751629e-05
Norm of the params: 16.699207
                Loss: fixed  50 labels. Loss 0.16130. Accuracy 0.947.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2581489
Train loss (w/o reg) on all data: 0.25130752
Test loss (w/o reg) on all data: 0.20146309
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 0.00016947862
Norm of the params: 11.697352
              Random: fixed   8 labels. Loss 0.20146. Accuracy 0.935.
### Flips: 312, rs: 18, checks: 104
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16447312
Train loss (w/o reg) on all data: 0.1525986
Test loss (w/o reg) on all data: 0.14282314
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 9.3320905e-06
Norm of the params: 15.41072
     Influence (LOO): fixed  61 labels. Loss 0.14282. Accuracy 0.950.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09022391
Train loss (w/o reg) on all data: 0.07256813
Test loss (w/o reg) on all data: 0.104319245
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.488677e-06
Norm of the params: 18.791367
                Loss: fixed  90 labels. Loss 0.10432. Accuracy 0.954.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2507762
Train loss (w/o reg) on all data: 0.24377339
Test loss (w/o reg) on all data: 0.19170888
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.11312e-05
Norm of the params: 11.834526
              Random: fixed  16 labels. Loss 0.19171. Accuracy 0.947.
### Flips: 312, rs: 18, checks: 156
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12934716
Train loss (w/o reg) on all data: 0.116783574
Test loss (w/o reg) on all data: 0.10585493
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.939614e-06
Norm of the params: 15.851549
     Influence (LOO): fixed  84 labels. Loss 0.10585. Accuracy 0.966.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050549347
Train loss (w/o reg) on all data: 0.035840183
Test loss (w/o reg) on all data: 0.053801015
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.4385594e-06
Norm of the params: 17.151772
                Loss: fixed 119 labels. Loss 0.05380. Accuracy 0.981.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24352428
Train loss (w/o reg) on all data: 0.23600139
Test loss (w/o reg) on all data: 0.18389909
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.4398025e-05
Norm of the params: 12.266134
              Random: fixed  22 labels. Loss 0.18390. Accuracy 0.947.
### Flips: 312, rs: 18, checks: 208
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08733848
Train loss (w/o reg) on all data: 0.0757778
Test loss (w/o reg) on all data: 0.078135625
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.2557145e-05
Norm of the params: 15.205709
     Influence (LOO): fixed 110 labels. Loss 0.07814. Accuracy 0.981.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030742884
Train loss (w/o reg) on all data: 0.017965212
Test loss (w/o reg) on all data: 0.04520071
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.863513e-06
Norm of the params: 15.986038
                Loss: fixed 133 labels. Loss 0.04520. Accuracy 0.985.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23773463
Train loss (w/o reg) on all data: 0.23035356
Test loss (w/o reg) on all data: 0.17523228
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.443051e-05
Norm of the params: 12.14995
              Random: fixed  30 labels. Loss 0.17523. Accuracy 0.958.
### Flips: 312, rs: 18, checks: 260
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055618554
Train loss (w/o reg) on all data: 0.04584683
Test loss (w/o reg) on all data: 0.041478448
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2788132e-05
Norm of the params: 13.979787
     Influence (LOO): fixed 126 labels. Loss 0.04148. Accuracy 0.989.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022901617
Train loss (w/o reg) on all data: 0.012143253
Test loss (w/o reg) on all data: 0.02787004
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2563551e-06
Norm of the params: 14.668581
                Loss: fixed 138 labels. Loss 0.02787. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2371852
Train loss (w/o reg) on all data: 0.2299693
Test loss (w/o reg) on all data: 0.16502845
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2947921e-05
Norm of the params: 12.013244
              Random: fixed  34 labels. Loss 0.16503. Accuracy 0.966.
### Flips: 312, rs: 18, checks: 312
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04057779
Train loss (w/o reg) on all data: 0.03133931
Test loss (w/o reg) on all data: 0.037799817
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1967988e-06
Norm of the params: 13.592998
     Influence (LOO): fixed 136 labels. Loss 0.03780. Accuracy 0.992.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017797519
Train loss (w/o reg) on all data: 0.008468402
Test loss (w/o reg) on all data: 0.016799806
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.561873e-07
Norm of the params: 13.659515
                Loss: fixed 142 labels. Loss 0.01680. Accuracy 0.992.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22606851
Train loss (w/o reg) on all data: 0.21897557
Test loss (w/o reg) on all data: 0.15113908
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8215189e-05
Norm of the params: 11.910444
              Random: fixed  44 labels. Loss 0.15114. Accuracy 0.973.
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24691242
Train loss (w/o reg) on all data: 0.2389741
Test loss (w/o reg) on all data: 0.18266478
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.5501918e-05
Norm of the params: 12.600253
Flipped loss: 0.18266. Accuracy: 0.958
### Flips: 312, rs: 19, checks: 52
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18549287
Train loss (w/o reg) on all data: 0.17499687
Test loss (w/o reg) on all data: 0.14112467
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.4680792e-05
Norm of the params: 14.488619
     Influence (LOO): fixed  37 labels. Loss 0.14112. Accuracy 0.947.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13644557
Train loss (w/o reg) on all data: 0.12057409
Test loss (w/o reg) on all data: 0.13554296
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.4935176e-05
Norm of the params: 17.816557
                Loss: fixed  52 labels. Loss 0.13554. Accuracy 0.943.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2436054
Train loss (w/o reg) on all data: 0.23577589
Test loss (w/o reg) on all data: 0.17740567
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.660272e-05
Norm of the params: 12.513611
              Random: fixed   8 labels. Loss 0.17741. Accuracy 0.947.
### Flips: 312, rs: 19, checks: 104
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13170424
Train loss (w/o reg) on all data: 0.11973114
Test loss (w/o reg) on all data: 0.109243676
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.1061236e-05
Norm of the params: 15.474564
     Influence (LOO): fixed  65 labels. Loss 0.10924. Accuracy 0.969.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056830153
Train loss (w/o reg) on all data: 0.039914086
Test loss (w/o reg) on all data: 0.061505705
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6789489e-05
Norm of the params: 18.393513
                Loss: fixed  97 labels. Loss 0.06151. Accuracy 0.973.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.240282
Train loss (w/o reg) on all data: 0.2323402
Test loss (w/o reg) on all data: 0.17080578
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.2566634e-05
Norm of the params: 12.603017
              Random: fixed  12 labels. Loss 0.17081. Accuracy 0.947.
### Flips: 312, rs: 19, checks: 156
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09415227
Train loss (w/o reg) on all data: 0.08264651
Test loss (w/o reg) on all data: 0.08317499
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.4260533e-05
Norm of the params: 15.169549
     Influence (LOO): fixed  87 labels. Loss 0.08317. Accuracy 0.969.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0326456
Train loss (w/o reg) on all data: 0.020118507
Test loss (w/o reg) on all data: 0.026755158
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.615608e-06
Norm of the params: 15.828515
                Loss: fixed 117 labels. Loss 0.02676. Accuracy 0.989.
Using normal model
LBFGS training took [313] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23502362
Train loss (w/o reg) on all data: 0.22727132
Test loss (w/o reg) on all data: 0.15639295
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.9933604e-05
Norm of the params: 12.451746
              Random: fixed  17 labels. Loss 0.15639. Accuracy 0.969.
### Flips: 312, rs: 19, checks: 208
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050859503
Train loss (w/o reg) on all data: 0.040640764
Test loss (w/o reg) on all data: 0.044598013
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.636177e-06
Norm of the params: 14.295973
     Influence (LOO): fixed 111 labels. Loss 0.04460. Accuracy 0.992.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025309805
Train loss (w/o reg) on all data: 0.013957892
Test loss (w/o reg) on all data: 0.022842677
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0106747e-06
Norm of the params: 15.067789
                Loss: fixed 121 labels. Loss 0.02284. Accuracy 0.992.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2273296
Train loss (w/o reg) on all data: 0.21894713
Test loss (w/o reg) on all data: 0.15288664
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2363623e-05
Norm of the params: 12.947946
              Random: fixed  21 labels. Loss 0.15289. Accuracy 0.962.
### Flips: 312, rs: 19, checks: 260
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022844505
Train loss (w/o reg) on all data: 0.01447619
Test loss (w/o reg) on all data: 0.021246683
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8616123e-06
Norm of the params: 12.937013
     Influence (LOO): fixed 124 labels. Loss 0.02125. Accuracy 0.992.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018232638
Train loss (w/o reg) on all data: 0.0088843275
Test loss (w/o reg) on all data: 0.021480964
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.393738e-07
Norm of the params: 13.673559
                Loss: fixed 124 labels. Loss 0.02148. Accuracy 0.992.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21588294
Train loss (w/o reg) on all data: 0.207438
Test loss (w/o reg) on all data: 0.13044034
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4254281e-05
Norm of the params: 12.9961
              Random: fixed  32 labels. Loss 0.13044. Accuracy 0.977.
### Flips: 312, rs: 19, checks: 312
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014129015
Train loss (w/o reg) on all data: 0.0073818844
Test loss (w/o reg) on all data: 0.013940076
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2580693e-07
Norm of the params: 11.61648
     Influence (LOO): fixed 130 labels. Loss 0.01394. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013827702
Train loss (w/o reg) on all data: 0.006676116
Test loss (w/o reg) on all data: 0.018275714
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0263581e-06
Norm of the params: 11.959587
                Loss: fixed 129 labels. Loss 0.01828. Accuracy 0.992.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21202557
Train loss (w/o reg) on all data: 0.20376943
Test loss (w/o reg) on all data: 0.12705825
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.080573e-05
Norm of the params: 12.850004
              Random: fixed  36 labels. Loss 0.12706. Accuracy 0.977.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2783451
Train loss (w/o reg) on all data: 0.27143112
Test loss (w/o reg) on all data: 0.21270269
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.048553e-05
Norm of the params: 11.759247
Flipped loss: 0.21270. Accuracy: 0.954
### Flips: 312, rs: 20, checks: 52
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23489855
Train loss (w/o reg) on all data: 0.22517806
Test loss (w/o reg) on all data: 0.17728755
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 8.5713145e-05
Norm of the params: 13.943089
     Influence (LOO): fixed  31 labels. Loss 0.17729. Accuracy 0.954.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17083485
Train loss (w/o reg) on all data: 0.15934122
Test loss (w/o reg) on all data: 0.18594436
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 7.302112e-06
Norm of the params: 15.161552
                Loss: fixed  52 labels. Loss 0.18594. Accuracy 0.920.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26566657
Train loss (w/o reg) on all data: 0.25811365
Test loss (w/o reg) on all data: 0.19713898
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.4681386e-05
Norm of the params: 12.290595
              Random: fixed  12 labels. Loss 0.19714. Accuracy 0.966.
### Flips: 312, rs: 20, checks: 104
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17740417
Train loss (w/o reg) on all data: 0.16577765
Test loss (w/o reg) on all data: 0.15526997
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.7948134e-05
Norm of the params: 15.248945
     Influence (LOO): fixed  65 labels. Loss 0.15527. Accuracy 0.950.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08160094
Train loss (w/o reg) on all data: 0.06492869
Test loss (w/o reg) on all data: 0.124378234
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.185849e-05
Norm of the params: 18.260479
                Loss: fixed 100 labels. Loss 0.12438. Accuracy 0.950.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25891963
Train loss (w/o reg) on all data: 0.25088912
Test loss (w/o reg) on all data: 0.1911677
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.4611156e-05
Norm of the params: 12.673202
              Random: fixed  18 labels. Loss 0.19117. Accuracy 0.962.
### Flips: 312, rs: 20, checks: 156
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1242443
Train loss (w/o reg) on all data: 0.11191494
Test loss (w/o reg) on all data: 0.11699093
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 7.2604566e-06
Norm of the params: 15.703096
     Influence (LOO): fixed  93 labels. Loss 0.11699. Accuracy 0.954.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033905305
Train loss (w/o reg) on all data: 0.02047785
Test loss (w/o reg) on all data: 0.029407399
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.03291e-06
Norm of the params: 16.387468
                Loss: fixed 132 labels. Loss 0.02941. Accuracy 0.992.
Using normal model
LBFGS training took [318] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25645882
Train loss (w/o reg) on all data: 0.24852175
Test loss (w/o reg) on all data: 0.1788981
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6349457e-05
Norm of the params: 12.599253
              Random: fixed  23 labels. Loss 0.17890. Accuracy 0.973.
### Flips: 312, rs: 20, checks: 208
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08862732
Train loss (w/o reg) on all data: 0.077017345
Test loss (w/o reg) on all data: 0.084963866
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.0215146e-06
Norm of the params: 15.238095
     Influence (LOO): fixed 115 labels. Loss 0.08496. Accuracy 0.969.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017691433
Train loss (w/o reg) on all data: 0.008420849
Test loss (w/o reg) on all data: 0.015210315
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.1925133e-06
Norm of the params: 13.616595
                Loss: fixed 146 labels. Loss 0.01521. Accuracy 0.996.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24663095
Train loss (w/o reg) on all data: 0.23816054
Test loss (w/o reg) on all data: 0.15450686
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.624266e-06
Norm of the params: 13.015695
              Random: fixed  32 labels. Loss 0.15451. Accuracy 0.985.
### Flips: 312, rs: 20, checks: 260
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05935903
Train loss (w/o reg) on all data: 0.04885881
Test loss (w/o reg) on all data: 0.060823046
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.6690855e-06
Norm of the params: 14.4915285
     Influence (LOO): fixed 135 labels. Loss 0.06082. Accuracy 0.989.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013404092
Train loss (w/o reg) on all data: 0.005799268
Test loss (w/o reg) on all data: 0.017907556
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.892357e-07
Norm of the params: 12.33274
                Loss: fixed 150 labels. Loss 0.01791. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23965341
Train loss (w/o reg) on all data: 0.23173591
Test loss (w/o reg) on all data: 0.13841788
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3245688e-05
Norm of the params: 12.583715
              Random: fixed  38 labels. Loss 0.13842. Accuracy 0.996.
### Flips: 312, rs: 20, checks: 312
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032546565
Train loss (w/o reg) on all data: 0.024339842
Test loss (w/o reg) on all data: 0.033328746
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.983833e-06
Norm of the params: 12.811499
     Influence (LOO): fixed 147 labels. Loss 0.03333. Accuracy 0.992.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012456065
Train loss (w/o reg) on all data: 0.0053732004
Test loss (w/o reg) on all data: 0.019562397
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9444254e-07
Norm of the params: 11.901987
                Loss: fixed 152 labels. Loss 0.01956. Accuracy 0.992.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22872724
Train loss (w/o reg) on all data: 0.22126268
Test loss (w/o reg) on all data: 0.12763217
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.958543e-05
Norm of the params: 12.218472
              Random: fixed  48 labels. Loss 0.12763. Accuracy 0.996.
Using normal model
LBFGS training took [334] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2715519
Train loss (w/o reg) on all data: 0.26342416
Test loss (w/o reg) on all data: 0.23286127
Train acc on all data:  0.8672397325692455
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 4.948951e-05
Norm of the params: 12.749711
Flipped loss: 0.23286. Accuracy: 0.927
### Flips: 312, rs: 21, checks: 52
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23145123
Train loss (w/o reg) on all data: 0.22157793
Test loss (w/o reg) on all data: 0.21099912
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.5276797e-05
Norm of the params: 14.052262
     Influence (LOO): fixed  33 labels. Loss 0.21100. Accuracy 0.943.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17269324
Train loss (w/o reg) on all data: 0.15867601
Test loss (w/o reg) on all data: 0.17772458
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.1301915e-05
Norm of the params: 16.743486
                Loss: fixed  51 labels. Loss 0.17772. Accuracy 0.935.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26248842
Train loss (w/o reg) on all data: 0.25431252
Test loss (w/o reg) on all data: 0.22401994
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.468522e-05
Norm of the params: 12.787422
              Random: fixed  10 labels. Loss 0.22402. Accuracy 0.935.
### Flips: 312, rs: 21, checks: 104
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1823289
Train loss (w/o reg) on all data: 0.17092788
Test loss (w/o reg) on all data: 0.17471455
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.8770254e-05
Norm of the params: 15.10034
     Influence (LOO): fixed  63 labels. Loss 0.17471. Accuracy 0.950.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10457395
Train loss (w/o reg) on all data: 0.08772781
Test loss (w/o reg) on all data: 0.13886131
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.5997385e-05
Norm of the params: 18.355457
                Loss: fixed  95 labels. Loss 0.13886. Accuracy 0.954.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25787023
Train loss (w/o reg) on all data: 0.24948604
Test loss (w/o reg) on all data: 0.22246495
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 5.2983836e-05
Norm of the params: 12.949278
              Random: fixed  14 labels. Loss 0.22246. Accuracy 0.931.
### Flips: 312, rs: 21, checks: 156
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14554459
Train loss (w/o reg) on all data: 0.13322423
Test loss (w/o reg) on all data: 0.1543019
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.8575098e-05
Norm of the params: 15.697363
     Influence (LOO): fixed  84 labels. Loss 0.15430. Accuracy 0.950.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054702062
Train loss (w/o reg) on all data: 0.040035993
Test loss (w/o reg) on all data: 0.08404369
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.4847936e-06
Norm of the params: 17.126629
                Loss: fixed 125 labels. Loss 0.08404. Accuracy 0.977.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24862725
Train loss (w/o reg) on all data: 0.23980683
Test loss (w/o reg) on all data: 0.20396307
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.6817934e-05
Norm of the params: 13.281875
              Random: fixed  24 labels. Loss 0.20396. Accuracy 0.947.
### Flips: 312, rs: 21, checks: 208
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10147643
Train loss (w/o reg) on all data: 0.090157844
Test loss (w/o reg) on all data: 0.10007561
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2612151e-05
Norm of the params: 15.045654
     Influence (LOO): fixed 113 labels. Loss 0.10008. Accuracy 0.973.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03407188
Train loss (w/o reg) on all data: 0.021463767
Test loss (w/o reg) on all data: 0.05002931
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5565253e-06
Norm of the params: 15.879619
                Loss: fixed 139 labels. Loss 0.05003. Accuracy 0.985.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24803144
Train loss (w/o reg) on all data: 0.23955324
Test loss (w/o reg) on all data: 0.19517136
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.7509266e-05
Norm of the params: 13.021668
              Random: fixed  28 labels. Loss 0.19517. Accuracy 0.962.
### Flips: 312, rs: 21, checks: 260
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07264057
Train loss (w/o reg) on all data: 0.06258091
Test loss (w/o reg) on all data: 0.07031267
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.893793e-05
Norm of the params: 14.184254
     Influence (LOO): fixed 130 labels. Loss 0.07031. Accuracy 0.989.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023781046
Train loss (w/o reg) on all data: 0.013164527
Test loss (w/o reg) on all data: 0.02107261
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.866737e-06
Norm of the params: 14.571561
                Loss: fixed 147 labels. Loss 0.02107. Accuracy 0.996.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23806453
Train loss (w/o reg) on all data: 0.22997922
Test loss (w/o reg) on all data: 0.1782817
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.222634e-05
Norm of the params: 12.7163725
              Random: fixed  39 labels. Loss 0.17828. Accuracy 0.950.
### Flips: 312, rs: 21, checks: 312
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037140172
Train loss (w/o reg) on all data: 0.029770106
Test loss (w/o reg) on all data: 0.03547132
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2020333e-06
Norm of the params: 12.140895
     Influence (LOO): fixed 147 labels. Loss 0.03547. Accuracy 0.992.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018584106
Train loss (w/o reg) on all data: 0.009129908
Test loss (w/o reg) on all data: 0.017698755
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.092874e-06
Norm of the params: 13.750781
                Loss: fixed 149 labels. Loss 0.01770. Accuracy 0.996.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22354817
Train loss (w/o reg) on all data: 0.21492787
Test loss (w/o reg) on all data: 0.16382128
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.4188994e-05
Norm of the params: 13.130354
              Random: fixed  50 labels. Loss 0.16382. Accuracy 0.950.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2733101
Train loss (w/o reg) on all data: 0.2661215
Test loss (w/o reg) on all data: 0.21043454
Train acc on all data:  0.8595988538681948
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.649826e-05
Norm of the params: 11.990488
Flipped loss: 0.21043. Accuracy: 0.939
### Flips: 312, rs: 22, checks: 52
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2337338
Train loss (w/o reg) on all data: 0.22404017
Test loss (w/o reg) on all data: 0.16474955
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.8489543e-05
Norm of the params: 13.923824
     Influence (LOO): fixed  30 labels. Loss 0.16475. Accuracy 0.954.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17758098
Train loss (w/o reg) on all data: 0.16494149
Test loss (w/o reg) on all data: 0.14499405
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 4.3284763e-05
Norm of the params: 15.899363
                Loss: fixed  52 labels. Loss 0.14499. Accuracy 0.935.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27160963
Train loss (w/o reg) on all data: 0.2647646
Test loss (w/o reg) on all data: 0.19398871
Train acc on all data:  0.8634192932187201
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.5457088e-05
Norm of the params: 11.700451
              Random: fixed   6 labels. Loss 0.19399. Accuracy 0.954.
### Flips: 312, rs: 22, checks: 104
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19208789
Train loss (w/o reg) on all data: 0.18198568
Test loss (w/o reg) on all data: 0.13100623
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.4943608e-05
Norm of the params: 14.214225
     Influence (LOO): fixed  58 labels. Loss 0.13101. Accuracy 0.962.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10643976
Train loss (w/o reg) on all data: 0.08800161
Test loss (w/o reg) on all data: 0.09114162
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.253115e-06
Norm of the params: 19.203205
                Loss: fixed  96 labels. Loss 0.09114. Accuracy 0.962.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2676796
Train loss (w/o reg) on all data: 0.26086575
Test loss (w/o reg) on all data: 0.18870305
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.8481012e-05
Norm of the params: 11.67378
              Random: fixed  12 labels. Loss 0.18870. Accuracy 0.947.
### Flips: 312, rs: 22, checks: 156
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1422457
Train loss (w/o reg) on all data: 0.13115162
Test loss (w/o reg) on all data: 0.0802381
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2006633e-05
Norm of the params: 14.895692
     Influence (LOO): fixed  91 labels. Loss 0.08024. Accuracy 0.981.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063718095
Train loss (w/o reg) on all data: 0.046615608
Test loss (w/o reg) on all data: 0.05465359
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.784139e-06
Norm of the params: 18.494587
                Loss: fixed 123 labels. Loss 0.05465. Accuracy 0.981.
Using normal model
LBFGS training took [324] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26280302
Train loss (w/o reg) on all data: 0.25567865
Test loss (w/o reg) on all data: 0.18301697
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.9089004e-05
Norm of the params: 11.936799
              Random: fixed  18 labels. Loss 0.18302. Accuracy 0.966.
### Flips: 312, rs: 22, checks: 208
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0898432
Train loss (w/o reg) on all data: 0.07794261
Test loss (w/o reg) on all data: 0.06754918
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.48553e-06
Norm of the params: 15.427628
     Influence (LOO): fixed 120 labels. Loss 0.06755. Accuracy 0.977.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037220366
Train loss (w/o reg) on all data: 0.023015065
Test loss (w/o reg) on all data: 0.018329173
Train acc on all data:  0.9923591212989494
Test acc on all data:   1.0
Norm of the mean of gradients: 1.8332664e-06
Norm of the params: 16.855444
                Loss: fixed 142 labels. Loss 0.01833. Accuracy 1.000.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25278616
Train loss (w/o reg) on all data: 0.24522835
Test loss (w/o reg) on all data: 0.17925294
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.7942339e-05
Norm of the params: 12.294563
              Random: fixed  26 labels. Loss 0.17925. Accuracy 0.954.
### Flips: 312, rs: 22, checks: 260
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07323178
Train loss (w/o reg) on all data: 0.06214496
Test loss (w/o reg) on all data: 0.061325233
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.151007e-05
Norm of the params: 14.890813
     Influence (LOO): fixed 132 labels. Loss 0.06133. Accuracy 0.981.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031392846
Train loss (w/o reg) on all data: 0.018184433
Test loss (w/o reg) on all data: 0.018296283
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.6214647e-06
Norm of the params: 16.253254
                Loss: fixed 145 labels. Loss 0.01830. Accuracy 0.996.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24531454
Train loss (w/o reg) on all data: 0.23781729
Test loss (w/o reg) on all data: 0.1608208
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.374037e-05
Norm of the params: 12.245203
              Random: fixed  35 labels. Loss 0.16082. Accuracy 0.977.
### Flips: 312, rs: 22, checks: 312
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050614834
Train loss (w/o reg) on all data: 0.041655138
Test loss (w/o reg) on all data: 0.036908783
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.052429e-06
Norm of the params: 13.3863325
     Influence (LOO): fixed 145 labels. Loss 0.03691. Accuracy 0.992.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020398825
Train loss (w/o reg) on all data: 0.010137698
Test loss (w/o reg) on all data: 0.011204323
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1879202e-06
Norm of the params: 14.325589
                Loss: fixed 151 labels. Loss 0.01120. Accuracy 0.996.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2300615
Train loss (w/o reg) on all data: 0.22214973
Test loss (w/o reg) on all data: 0.14563061
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6492313e-05
Norm of the params: 12.57917
              Random: fixed  47 labels. Loss 0.14563. Accuracy 0.969.
Using normal model
LBFGS training took [326] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27119416
Train loss (w/o reg) on all data: 0.26538837
Test loss (w/o reg) on all data: 0.18954165
Train acc on all data:  0.8662846227316141
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.163485e-05
Norm of the params: 10.775719
Flipped loss: 0.18954. Accuracy: 0.973
### Flips: 312, rs: 23, checks: 52
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21034732
Train loss (w/o reg) on all data: 0.2000166
Test loss (w/o reg) on all data: 0.15494792
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2591704e-05
Norm of the params: 14.374084
     Influence (LOO): fixed  33 labels. Loss 0.15495. Accuracy 0.962.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16510257
Train loss (w/o reg) on all data: 0.15249161
Test loss (w/o reg) on all data: 0.11582365
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.3116585e-05
Norm of the params: 15.881411
                Loss: fixed  52 labels. Loss 0.11582. Accuracy 0.954.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26682904
Train loss (w/o reg) on all data: 0.2606987
Test loss (w/o reg) on all data: 0.18184581
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.332723e-05
Norm of the params: 11.072804
              Random: fixed   5 labels. Loss 0.18185. Accuracy 0.973.
### Flips: 312, rs: 23, checks: 104
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16969898
Train loss (w/o reg) on all data: 0.15944077
Test loss (w/o reg) on all data: 0.12939388
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.810794e-05
Norm of the params: 14.323551
     Influence (LOO): fixed  61 labels. Loss 0.12939. Accuracy 0.973.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08561134
Train loss (w/o reg) on all data: 0.06831846
Test loss (w/o reg) on all data: 0.07721941
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.703351e-06
Norm of the params: 18.597248
                Loss: fixed  99 labels. Loss 0.07722. Accuracy 0.966.
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262151
Train loss (w/o reg) on all data: 0.2559944
Test loss (w/o reg) on all data: 0.1739603
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.2137725e-05
Norm of the params: 11.096476
              Random: fixed  10 labels. Loss 0.17396. Accuracy 0.977.
### Flips: 312, rs: 23, checks: 156
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1230709
Train loss (w/o reg) on all data: 0.11144071
Test loss (w/o reg) on all data: 0.092448555
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6436573e-05
Norm of the params: 15.251354
     Influence (LOO): fixed  85 labels. Loss 0.09245. Accuracy 0.985.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021728331
Train loss (w/o reg) on all data: 0.011980799
Test loss (w/o reg) on all data: 0.033270925
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.5987404e-06
Norm of the params: 13.962472
                Loss: fixed 133 labels. Loss 0.03327. Accuracy 0.989.
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26026273
Train loss (w/o reg) on all data: 0.25416452
Test loss (w/o reg) on all data: 0.16915353
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6561109e-05
Norm of the params: 11.043745
              Random: fixed  14 labels. Loss 0.16915. Accuracy 0.985.
### Flips: 312, rs: 23, checks: 208
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.092049114
Train loss (w/o reg) on all data: 0.08186118
Test loss (w/o reg) on all data: 0.06325099
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.6353007e-06
Norm of the params: 14.274406
     Influence (LOO): fixed 104 labels. Loss 0.06325. Accuracy 0.985.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016511787
Train loss (w/o reg) on all data: 0.007848806
Test loss (w/o reg) on all data: 0.024986263
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.078075e-07
Norm of the params: 13.162811
                Loss: fixed 139 labels. Loss 0.02499. Accuracy 0.992.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24861136
Train loss (w/o reg) on all data: 0.24253508
Test loss (w/o reg) on all data: 0.15170492
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0816744e-05
Norm of the params: 11.023859
              Random: fixed  28 labels. Loss 0.15170. Accuracy 0.989.
### Flips: 312, rs: 23, checks: 260
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047833018
Train loss (w/o reg) on all data: 0.03858234
Test loss (w/o reg) on all data: 0.03322106
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2838573e-06
Norm of the params: 13.601969
     Influence (LOO): fixed 128 labels. Loss 0.03322. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0095543265
Train loss (w/o reg) on all data: 0.003986976
Test loss (w/o reg) on all data: 0.017191468
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8660409e-07
Norm of the params: 10.55211
                Loss: fixed 144 labels. Loss 0.01719. Accuracy 0.992.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23909843
Train loss (w/o reg) on all data: 0.23287603
Test loss (w/o reg) on all data: 0.1458483
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.0200964e-05
Norm of the params: 11.155625
              Random: fixed  33 labels. Loss 0.14585. Accuracy 0.992.
### Flips: 312, rs: 23, checks: 312
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034561567
Train loss (w/o reg) on all data: 0.026133884
Test loss (w/o reg) on all data: 0.018967237
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.7814121e-06
Norm of the params: 12.982822
     Influence (LOO): fixed 136 labels. Loss 0.01897. Accuracy 0.996.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729344
Test loss (w/o reg) on all data: 0.012055012
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.153757e-07
Norm of the params: 9.153235
                Loss: fixed 146 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2209917
Train loss (w/o reg) on all data: 0.21451783
Test loss (w/o reg) on all data: 0.12679896
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.5378082e-05
Norm of the params: 11.378809
              Random: fixed  47 labels. Loss 0.12680. Accuracy 0.996.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2739402
Train loss (w/o reg) on all data: 0.26696208
Test loss (w/o reg) on all data: 0.20352438
Train acc on all data:  0.8605539637058262
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.1881233e-05
Norm of the params: 11.813656
Flipped loss: 0.20352. Accuracy: 0.954
### Flips: 312, rs: 24, checks: 52
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2395802
Train loss (w/o reg) on all data: 0.23090902
Test loss (w/o reg) on all data: 0.17875323
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.808986e-05
Norm of the params: 13.169042
     Influence (LOO): fixed  26 labels. Loss 0.17875. Accuracy 0.947.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18708608
Train loss (w/o reg) on all data: 0.17451732
Test loss (w/o reg) on all data: 0.14481775
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.968296e-06
Norm of the params: 15.854812
                Loss: fixed  49 labels. Loss 0.14482. Accuracy 0.958.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26860613
Train loss (w/o reg) on all data: 0.26176336
Test loss (w/o reg) on all data: 0.20363419
Train acc on all data:  0.8653295128939829
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 8.853951e-05
Norm of the params: 11.698506
              Random: fixed   5 labels. Loss 0.20363. Accuracy 0.954.
### Flips: 312, rs: 24, checks: 104
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19170758
Train loss (w/o reg) on all data: 0.1816423
Test loss (w/o reg) on all data: 0.1391366
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.1627245e-05
Norm of the params: 14.188223
     Influence (LOO): fixed  59 labels. Loss 0.13914. Accuracy 0.962.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11331588
Train loss (w/o reg) on all data: 0.09710229
Test loss (w/o reg) on all data: 0.08668548
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1307163e-05
Norm of the params: 18.007545
                Loss: fixed  96 labels. Loss 0.08669. Accuracy 0.973.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26334924
Train loss (w/o reg) on all data: 0.25679588
Test loss (w/o reg) on all data: 0.18866596
Train acc on all data:  0.8672397325692455
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.4550376e-05
Norm of the params: 11.448445
              Random: fixed  15 labels. Loss 0.18867. Accuracy 0.962.
### Flips: 312, rs: 24, checks: 156
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13904478
Train loss (w/o reg) on all data: 0.1296485
Test loss (w/o reg) on all data: 0.10219035
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.7449693e-05
Norm of the params: 13.708586
     Influence (LOO): fixed  93 labels. Loss 0.10219. Accuracy 0.962.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056988906
Train loss (w/o reg) on all data: 0.040170603
Test loss (w/o reg) on all data: 0.051070787
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0906304e-05
Norm of the params: 18.340284
                Loss: fixed 127 labels. Loss 0.05107. Accuracy 0.977.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2620111
Train loss (w/o reg) on all data: 0.25561255
Test loss (w/o reg) on all data: 0.18278658
Train acc on all data:  0.8720152817574021
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.655427e-05
Norm of the params: 11.312445
              Random: fixed  19 labels. Loss 0.18279. Accuracy 0.962.
### Flips: 312, rs: 24, checks: 208
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09395124
Train loss (w/o reg) on all data: 0.08490382
Test loss (w/o reg) on all data: 0.06648936
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1339476e-06
Norm of the params: 13.451704
     Influence (LOO): fixed 117 labels. Loss 0.06649. Accuracy 0.977.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036587607
Train loss (w/o reg) on all data: 0.02210488
Test loss (w/o reg) on all data: 0.04259
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3470097e-06
Norm of the params: 17.01924
                Loss: fixed 140 labels. Loss 0.04259. Accuracy 0.977.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25846153
Train loss (w/o reg) on all data: 0.2519916
Test loss (w/o reg) on all data: 0.17871909
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.2414304e-05
Norm of the params: 11.375354
              Random: fixed  23 labels. Loss 0.17872. Accuracy 0.962.
### Flips: 312, rs: 24, checks: 260
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065868326
Train loss (w/o reg) on all data: 0.056657948
Test loss (w/o reg) on all data: 0.04770102
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.187731e-06
Norm of the params: 13.57231
     Influence (LOO): fixed 134 labels. Loss 0.04770. Accuracy 0.985.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021255683
Train loss (w/o reg) on all data: 0.009894489
Test loss (w/o reg) on all data: 0.027909765
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2671625e-06
Norm of the params: 15.073948
                Loss: fixed 150 labels. Loss 0.02791. Accuracy 0.985.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25243914
Train loss (w/o reg) on all data: 0.24575369
Test loss (w/o reg) on all data: 0.16931456
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.0330128e-05
Norm of the params: 11.563263
              Random: fixed  29 labels. Loss 0.16931. Accuracy 0.962.
### Flips: 312, rs: 24, checks: 312
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027443415
Train loss (w/o reg) on all data: 0.020436784
Test loss (w/o reg) on all data: 0.01899824
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.206677e-06
Norm of the params: 11.837762
     Influence (LOO): fixed 152 labels. Loss 0.01900. Accuracy 0.992.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018711466
Train loss (w/o reg) on all data: 0.008355817
Test loss (w/o reg) on all data: 0.025821883
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2858941e-06
Norm of the params: 14.391422
                Loss: fixed 152 labels. Loss 0.02582. Accuracy 0.985.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24799198
Train loss (w/o reg) on all data: 0.2422019
Test loss (w/o reg) on all data: 0.15760037
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 0.00021085056
Norm of the params: 10.761121
              Random: fixed  37 labels. Loss 0.15760. Accuracy 0.966.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26719025
Train loss (w/o reg) on all data: 0.25870332
Test loss (w/o reg) on all data: 0.2117939
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.1510623e-05
Norm of the params: 13.028385
Flipped loss: 0.21179. Accuracy: 0.954
### Flips: 312, rs: 25, checks: 52
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21224724
Train loss (w/o reg) on all data: 0.20007287
Test loss (w/o reg) on all data: 0.18849242
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 3.745943e-05
Norm of the params: 15.604077
     Influence (LOO): fixed  32 labels. Loss 0.18849. Accuracy 0.924.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16758417
Train loss (w/o reg) on all data: 0.15397474
Test loss (w/o reg) on all data: 0.15545706
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.345379e-05
Norm of the params: 16.498135
                Loss: fixed  50 labels. Loss 0.15546. Accuracy 0.927.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2596691
Train loss (w/o reg) on all data: 0.25122553
Test loss (w/o reg) on all data: 0.20552094
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.9004185e-05
Norm of the params: 12.995045
              Random: fixed   8 labels. Loss 0.20552. Accuracy 0.950.
### Flips: 312, rs: 25, checks: 104
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16722202
Train loss (w/o reg) on all data: 0.15340088
Test loss (w/o reg) on all data: 0.15415238
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 8.824747e-06
Norm of the params: 16.62597
     Influence (LOO): fixed  58 labels. Loss 0.15415. Accuracy 0.947.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09415403
Train loss (w/o reg) on all data: 0.07931982
Test loss (w/o reg) on all data: 0.12686823
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.0707765e-05
Norm of the params: 17.224522
                Loss: fixed  93 labels. Loss 0.12687. Accuracy 0.954.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25533018
Train loss (w/o reg) on all data: 0.24671002
Test loss (w/o reg) on all data: 0.19246545
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.6094445e-05
Norm of the params: 13.130231
              Random: fixed  14 labels. Loss 0.19247. Accuracy 0.962.
### Flips: 312, rs: 25, checks: 156
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.136299
Train loss (w/o reg) on all data: 0.12456501
Test loss (w/o reg) on all data: 0.105202354
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.3304216e-05
Norm of the params: 15.319257
     Influence (LOO): fixed  83 labels. Loss 0.10520. Accuracy 0.966.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053200696
Train loss (w/o reg) on all data: 0.038630556
Test loss (w/o reg) on all data: 0.08435073
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 9.2156615e-06
Norm of the params: 17.070524
                Loss: fixed 122 labels. Loss 0.08435. Accuracy 0.962.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24504232
Train loss (w/o reg) on all data: 0.23631622
Test loss (w/o reg) on all data: 0.18034416
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 9.807288e-05
Norm of the params: 13.210676
              Random: fixed  23 labels. Loss 0.18034. Accuracy 0.958.
### Flips: 312, rs: 25, checks: 208
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097531505
Train loss (w/o reg) on all data: 0.08474059
Test loss (w/o reg) on all data: 0.08036812
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.304296e-06
Norm of the params: 15.994324
     Influence (LOO): fixed 105 labels. Loss 0.08037. Accuracy 0.977.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026999973
Train loss (w/o reg) on all data: 0.0150249675
Test loss (w/o reg) on all data: 0.064736515
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.423896e-06
Norm of the params: 15.475792
                Loss: fixed 141 labels. Loss 0.06474. Accuracy 0.977.
Using normal model
LBFGS training took [295] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24213465
Train loss (w/o reg) on all data: 0.23391569
Test loss (w/o reg) on all data: 0.1602553
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.1188942e-05
Norm of the params: 12.821042
              Random: fixed  31 labels. Loss 0.16026. Accuracy 0.958.
### Flips: 312, rs: 25, checks: 260
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07142942
Train loss (w/o reg) on all data: 0.058644745
Test loss (w/o reg) on all data: 0.054956354
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.4107223e-06
Norm of the params: 15.990418
     Influence (LOO): fixed 122 labels. Loss 0.05496. Accuracy 0.989.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015457557
Train loss (w/o reg) on all data: 0.006528578
Test loss (w/o reg) on all data: 0.0385184
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0705928e-06
Norm of the params: 13.363367
                Loss: fixed 146 labels. Loss 0.03852. Accuracy 0.985.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23644558
Train loss (w/o reg) on all data: 0.22819231
Test loss (w/o reg) on all data: 0.15627432
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.240561e-05
Norm of the params: 12.847774
              Random: fixed  36 labels. Loss 0.15627. Accuracy 0.958.
### Flips: 312, rs: 25, checks: 312
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047983665
Train loss (w/o reg) on all data: 0.03887327
Test loss (w/o reg) on all data: 0.028166585
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1860772e-06
Norm of the params: 13.498442
     Influence (LOO): fixed 139 labels. Loss 0.02817. Accuracy 0.992.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015923409
Train loss (w/o reg) on all data: 0.0069890735
Test loss (w/o reg) on all data: 0.035434976
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.9907145e-07
Norm of the params: 13.367374
                Loss: fixed 147 labels. Loss 0.03543. Accuracy 0.985.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23112717
Train loss (w/o reg) on all data: 0.22335294
Test loss (w/o reg) on all data: 0.15014765
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.1204518e-05
Norm of the params: 12.46935
              Random: fixed  43 labels. Loss 0.15015. Accuracy 0.969.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27552545
Train loss (w/o reg) on all data: 0.26827005
Test loss (w/o reg) on all data: 0.2048179
Train acc on all data:  0.8605539637058262
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.4963039e-05
Norm of the params: 12.046093
Flipped loss: 0.20482. Accuracy: 0.947
### Flips: 312, rs: 26, checks: 52
Using normal model
LBFGS training took [321] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22173455
Train loss (w/o reg) on all data: 0.21064197
Test loss (w/o reg) on all data: 0.1903017
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 3.9309703e-05
Norm of the params: 14.89469
     Influence (LOO): fixed  33 labels. Loss 0.19030. Accuracy 0.931.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18003055
Train loss (w/o reg) on all data: 0.16529553
Test loss (w/o reg) on all data: 0.15315707
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.3052574e-05
Norm of the params: 17.166843
                Loss: fixed  50 labels. Loss 0.15316. Accuracy 0.947.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26467866
Train loss (w/o reg) on all data: 0.2567977
Test loss (w/o reg) on all data: 0.19429132
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.167635e-05
Norm of the params: 12.554636
              Random: fixed  12 labels. Loss 0.19429. Accuracy 0.954.
### Flips: 312, rs: 26, checks: 104
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19185081
Train loss (w/o reg) on all data: 0.1802089
Test loss (w/o reg) on all data: 0.14064251
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.7993396e-05
Norm of the params: 15.259037
     Influence (LOO): fixed  59 labels. Loss 0.14064. Accuracy 0.966.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10695933
Train loss (w/o reg) on all data: 0.08956285
Test loss (w/o reg) on all data: 0.092808284
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.5419278e-05
Norm of the params: 18.65287
                Loss: fixed  97 labels. Loss 0.09281. Accuracy 0.966.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25851792
Train loss (w/o reg) on all data: 0.25093243
Test loss (w/o reg) on all data: 0.18519312
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4946663e-05
Norm of the params: 12.317053
              Random: fixed  19 labels. Loss 0.18519. Accuracy 0.958.
### Flips: 312, rs: 26, checks: 156
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14181492
Train loss (w/o reg) on all data: 0.1298634
Test loss (w/o reg) on all data: 0.083423585
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5604624e-05
Norm of the params: 15.460606
     Influence (LOO): fixed  90 labels. Loss 0.08342. Accuracy 0.989.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06680851
Train loss (w/o reg) on all data: 0.05023543
Test loss (w/o reg) on all data: 0.06757276
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.8845046e-06
Norm of the params: 18.206081
                Loss: fixed 122 labels. Loss 0.06757. Accuracy 0.977.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25417474
Train loss (w/o reg) on all data: 0.24645396
Test loss (w/o reg) on all data: 0.17667541
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.7349564e-05
Norm of the params: 12.426405
              Random: fixed  27 labels. Loss 0.17668. Accuracy 0.969.
### Flips: 312, rs: 26, checks: 208
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121717736
Train loss (w/o reg) on all data: 0.111084886
Test loss (w/o reg) on all data: 0.05868363
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.879248e-06
Norm of the params: 14.582767
     Influence (LOO): fixed 105 labels. Loss 0.05868. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03360753
Train loss (w/o reg) on all data: 0.020802913
Test loss (w/o reg) on all data: 0.05091874
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4006074e-06
Norm of the params: 16.002884
                Loss: fixed 139 labels. Loss 0.05092. Accuracy 0.985.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24563964
Train loss (w/o reg) on all data: 0.23782189
Test loss (w/o reg) on all data: 0.1679074
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2226037e-05
Norm of the params: 12.504197
              Random: fixed  34 labels. Loss 0.16791. Accuracy 0.962.
### Flips: 312, rs: 26, checks: 260
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07921379
Train loss (w/o reg) on all data: 0.06962013
Test loss (w/o reg) on all data: 0.03703197
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8437427e-06
Norm of the params: 13.851828
     Influence (LOO): fixed 130 labels. Loss 0.03703. Accuracy 0.989.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01777616
Train loss (w/o reg) on all data: 0.00939399
Test loss (w/o reg) on all data: 0.02311317
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5340461e-06
Norm of the params: 12.947718
                Loss: fixed 152 labels. Loss 0.02311. Accuracy 0.992.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23722021
Train loss (w/o reg) on all data: 0.22943313
Test loss (w/o reg) on all data: 0.15106066
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.8249751e-05
Norm of the params: 12.479649
              Random: fixed  42 labels. Loss 0.15106. Accuracy 0.969.
### Flips: 312, rs: 26, checks: 312
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047022197
Train loss (w/o reg) on all data: 0.038500827
Test loss (w/o reg) on all data: 0.024552152
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.2052092e-06
Norm of the params: 13.054787
     Influence (LOO): fixed 146 labels. Loss 0.02455. Accuracy 0.996.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012467487
Train loss (w/o reg) on all data: 0.0056643584
Test loss (w/o reg) on all data: 0.01985541
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0140442e-06
Norm of the params: 11.664586
                Loss: fixed 156 labels. Loss 0.01986. Accuracy 0.992.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2317882
Train loss (w/o reg) on all data: 0.2244909
Test loss (w/o reg) on all data: 0.14224838
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 0.00011001619
Norm of the params: 12.080823
              Random: fixed  49 labels. Loss 0.14225. Accuracy 0.977.
Using normal model
LBFGS training took [293] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26100075
Train loss (w/o reg) on all data: 0.2536879
Test loss (w/o reg) on all data: 0.20170967
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.4402503e-05
Norm of the params: 12.093698
Flipped loss: 0.20171. Accuracy: 0.950
### Flips: 312, rs: 27, checks: 52
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2197269
Train loss (w/o reg) on all data: 0.21156573
Test loss (w/o reg) on all data: 0.15613568
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.31428e-05
Norm of the params: 12.775894
     Influence (LOO): fixed  33 labels. Loss 0.15614. Accuracy 0.962.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16290969
Train loss (w/o reg) on all data: 0.15206055
Test loss (w/o reg) on all data: 0.1508267
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 3.3437143e-05
Norm of the params: 14.730337
                Loss: fixed  51 labels. Loss 0.15083. Accuracy 0.931.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25736427
Train loss (w/o reg) on all data: 0.2502369
Test loss (w/o reg) on all data: 0.19341795
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.060401e-05
Norm of the params: 11.939334
              Random: fixed   5 labels. Loss 0.19342. Accuracy 0.947.
### Flips: 312, rs: 27, checks: 104
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17213385
Train loss (w/o reg) on all data: 0.1631675
Test loss (w/o reg) on all data: 0.12286997
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.784874e-05
Norm of the params: 13.391299
     Influence (LOO): fixed  65 labels. Loss 0.12287. Accuracy 0.962.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08345133
Train loss (w/o reg) on all data: 0.06740064
Test loss (w/o reg) on all data: 0.093576245
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.200183e-06
Norm of the params: 17.916859
                Loss: fixed  96 labels. Loss 0.09358. Accuracy 0.954.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25369912
Train loss (w/o reg) on all data: 0.24680462
Test loss (w/o reg) on all data: 0.18934642
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.8216391e-05
Norm of the params: 11.742661
              Random: fixed  12 labels. Loss 0.18935. Accuracy 0.954.
### Flips: 312, rs: 27, checks: 156
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11742036
Train loss (w/o reg) on all data: 0.10771281
Test loss (w/o reg) on all data: 0.08870449
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.6825496e-06
Norm of the params: 13.933809
     Influence (LOO): fixed  93 labels. Loss 0.08870. Accuracy 0.962.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03563939
Train loss (w/o reg) on all data: 0.021985108
Test loss (w/o reg) on all data: 0.054761212
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0240243e-06
Norm of the params: 16.525305
                Loss: fixed 126 labels. Loss 0.05476. Accuracy 0.985.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24700461
Train loss (w/o reg) on all data: 0.2403955
Test loss (w/o reg) on all data: 0.18352413
Train acc on all data:  0.87774594078319
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 9.2609174e-05
Norm of the params: 11.497047
              Random: fixed  18 labels. Loss 0.18352. Accuracy 0.950.
### Flips: 312, rs: 27, checks: 208
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0867879
Train loss (w/o reg) on all data: 0.07791098
Test loss (w/o reg) on all data: 0.06560422
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6289725e-05
Norm of the params: 13.324351
     Influence (LOO): fixed 110 labels. Loss 0.06560. Accuracy 0.981.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020118345
Train loss (w/o reg) on all data: 0.010467792
Test loss (w/o reg) on all data: 0.037275344
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.302421e-07
Norm of the params: 13.892841
                Loss: fixed 138 labels. Loss 0.03728. Accuracy 0.985.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23847176
Train loss (w/o reg) on all data: 0.23197392
Test loss (w/o reg) on all data: 0.16181402
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.561866e-05
Norm of the params: 11.39987
              Random: fixed  27 labels. Loss 0.16181. Accuracy 0.985.
### Flips: 312, rs: 27, checks: 260
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055214137
Train loss (w/o reg) on all data: 0.045916777
Test loss (w/o reg) on all data: 0.05705803
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.542035e-06
Norm of the params: 13.636246
     Influence (LOO): fixed 126 labels. Loss 0.05706. Accuracy 0.981.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016844045
Train loss (w/o reg) on all data: 0.008211198
Test loss (w/o reg) on all data: 0.027724978
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.043827e-07
Norm of the params: 13.139898
                Loss: fixed 140 labels. Loss 0.02772. Accuracy 0.989.
Using normal model
LBFGS training took [315] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23445699
Train loss (w/o reg) on all data: 0.22804916
Test loss (w/o reg) on all data: 0.1614667
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.087787e-06
Norm of the params: 11.320627
              Random: fixed  31 labels. Loss 0.16147. Accuracy 0.981.
### Flips: 312, rs: 27, checks: 312
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02873273
Train loss (w/o reg) on all data: 0.01967421
Test loss (w/o reg) on all data: 0.027263956
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2456156e-06
Norm of the params: 13.459956
     Influence (LOO): fixed 138 labels. Loss 0.02726. Accuracy 0.996.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012368919
Train loss (w/o reg) on all data: 0.0050373697
Test loss (w/o reg) on all data: 0.01932659
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.924954e-06
Norm of the params: 12.109129
                Loss: fixed 142 labels. Loss 0.01933. Accuracy 0.992.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22587664
Train loss (w/o reg) on all data: 0.21923909
Test loss (w/o reg) on all data: 0.16382934
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1452485e-05
Norm of the params: 11.5217705
              Random: fixed  36 labels. Loss 0.16383. Accuracy 0.981.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27145505
Train loss (w/o reg) on all data: 0.26310313
Test loss (w/o reg) on all data: 0.22921488
Train acc on all data:  0.8672397325692455
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 3.228234e-05
Norm of the params: 12.924348
Flipped loss: 0.22921. Accuracy: 0.931
### Flips: 312, rs: 28, checks: 52
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23492089
Train loss (w/o reg) on all data: 0.22548291
Test loss (w/o reg) on all data: 0.19411246
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.1470258e-05
Norm of the params: 13.738985
     Influence (LOO): fixed  28 labels. Loss 0.19411. Accuracy 0.920.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18020815
Train loss (w/o reg) on all data: 0.16702792
Test loss (w/o reg) on all data: 0.1923886
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.0096998e-05
Norm of the params: 16.235903
                Loss: fixed  50 labels. Loss 0.19239. Accuracy 0.931.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26874405
Train loss (w/o reg) on all data: 0.2605449
Test loss (w/o reg) on all data: 0.21760479
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 0.00013996148
Norm of the params: 12.805584
              Random: fixed   5 labels. Loss 0.21760. Accuracy 0.943.
### Flips: 312, rs: 28, checks: 104
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19357027
Train loss (w/o reg) on all data: 0.1837281
Test loss (w/o reg) on all data: 0.15303198
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.1720936e-05
Norm of the params: 14.030095
     Influence (LOO): fixed  57 labels. Loss 0.15303. Accuracy 0.958.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10305193
Train loss (w/o reg) on all data: 0.08552093
Test loss (w/o reg) on all data: 0.11633838
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.093864e-05
Norm of the params: 18.72485
                Loss: fixed  96 labels. Loss 0.11634. Accuracy 0.954.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26500562
Train loss (w/o reg) on all data: 0.25656825
Test loss (w/o reg) on all data: 0.20504472
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 6.707659e-05
Norm of the params: 12.990267
              Random: fixed   9 labels. Loss 0.20504. Accuracy 0.943.
### Flips: 312, rs: 28, checks: 156
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13762009
Train loss (w/o reg) on all data: 0.12717496
Test loss (w/o reg) on all data: 0.108920455
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.6040247e-05
Norm of the params: 14.453467
     Influence (LOO): fixed  93 labels. Loss 0.10892. Accuracy 0.958.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051464744
Train loss (w/o reg) on all data: 0.03691792
Test loss (w/o reg) on all data: 0.060674224
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.7673642e-06
Norm of the params: 17.05686
                Loss: fixed 126 labels. Loss 0.06067. Accuracy 0.977.
Using normal model
LBFGS training took [319] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2551489
Train loss (w/o reg) on all data: 0.24623182
Test loss (w/o reg) on all data: 0.19673198
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.4381572e-05
Norm of the params: 13.354454
              Random: fixed  18 labels. Loss 0.19673. Accuracy 0.950.
### Flips: 312, rs: 28, checks: 208
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08533533
Train loss (w/o reg) on all data: 0.07341391
Test loss (w/o reg) on all data: 0.08109191
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2952987e-05
Norm of the params: 15.441128
     Influence (LOO): fixed 121 labels. Loss 0.08109. Accuracy 0.966.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036261484
Train loss (w/o reg) on all data: 0.022281013
Test loss (w/o reg) on all data: 0.048286512
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.181637e-06
Norm of the params: 16.721525
                Loss: fixed 135 labels. Loss 0.04829. Accuracy 0.985.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24530128
Train loss (w/o reg) on all data: 0.23637244
Test loss (w/o reg) on all data: 0.18143946
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.9093025e-05
Norm of the params: 13.363261
              Random: fixed  28 labels. Loss 0.18144. Accuracy 0.954.
### Flips: 312, rs: 28, checks: 260
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05923322
Train loss (w/o reg) on all data: 0.04929211
Test loss (w/o reg) on all data: 0.05851887
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.683231e-06
Norm of the params: 14.100432
     Influence (LOO): fixed 138 labels. Loss 0.05852. Accuracy 0.985.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028844085
Train loss (w/o reg) on all data: 0.01646624
Test loss (w/o reg) on all data: 0.03834538
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7588188e-06
Norm of the params: 15.733942
                Loss: fixed 141 labels. Loss 0.03835. Accuracy 0.992.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23097974
Train loss (w/o reg) on all data: 0.22215383
Test loss (w/o reg) on all data: 0.17092395
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.6468275e-05
Norm of the params: 13.286017
              Random: fixed  37 labels. Loss 0.17092. Accuracy 0.954.
### Flips: 312, rs: 28, checks: 312
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038700536
Train loss (w/o reg) on all data: 0.030391818
Test loss (w/o reg) on all data: 0.037619594
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4346892e-06
Norm of the params: 12.890862
     Influence (LOO): fixed 149 labels. Loss 0.03762. Accuracy 0.992.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018351384
Train loss (w/o reg) on all data: 0.008846904
Test loss (w/o reg) on all data: 0.009623632
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 2.0774899e-06
Norm of the params: 13.787299
                Loss: fixed 149 labels. Loss 0.00962. Accuracy 1.000.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21825324
Train loss (w/o reg) on all data: 0.2093407
Test loss (w/o reg) on all data: 0.14764032
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.6966812e-05
Norm of the params: 13.351054
              Random: fixed  46 labels. Loss 0.14764. Accuracy 0.966.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26828083
Train loss (w/o reg) on all data: 0.26022133
Test loss (w/o reg) on all data: 0.23219961
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.7666669e-05
Norm of the params: 12.696054
Flipped loss: 0.23220. Accuracy: 0.920
### Flips: 312, rs: 29, checks: 52
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22008148
Train loss (w/o reg) on all data: 0.20942603
Test loss (w/o reg) on all data: 0.19499296
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.2069673e-05
Norm of the params: 14.598249
     Influence (LOO): fixed  36 labels. Loss 0.19499. Accuracy 0.931.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1715243
Train loss (w/o reg) on all data: 0.15620853
Test loss (w/o reg) on all data: 0.19964355
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 5.011527e-05
Norm of the params: 17.501867
                Loss: fixed  50 labels. Loss 0.19964. Accuracy 0.916.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2579253
Train loss (w/o reg) on all data: 0.24955074
Test loss (w/o reg) on all data: 0.22094424
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.9235495e-05
Norm of the params: 12.941839
              Random: fixed  10 labels. Loss 0.22094. Accuracy 0.924.
### Flips: 312, rs: 29, checks: 104
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18685602
Train loss (w/o reg) on all data: 0.17644833
Test loss (w/o reg) on all data: 0.16283105
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.888439e-05
Norm of the params: 14.427531
     Influence (LOO): fixed  59 labels. Loss 0.16283. Accuracy 0.947.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09006638
Train loss (w/o reg) on all data: 0.07101747
Test loss (w/o reg) on all data: 0.14586563
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.8705864e-06
Norm of the params: 19.518665
                Loss: fixed  97 labels. Loss 0.14587. Accuracy 0.935.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2515422
Train loss (w/o reg) on all data: 0.24368316
Test loss (w/o reg) on all data: 0.21022888
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.3457272e-05
Norm of the params: 12.537195
              Random: fixed  19 labels. Loss 0.21023. Accuracy 0.920.
### Flips: 312, rs: 29, checks: 156
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14247614
Train loss (w/o reg) on all data: 0.13117254
Test loss (w/o reg) on all data: 0.12431685
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.010596e-05
Norm of the params: 15.035699
     Influence (LOO): fixed  87 labels. Loss 0.12432. Accuracy 0.966.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049138773
Train loss (w/o reg) on all data: 0.033676025
Test loss (w/o reg) on all data: 0.081493825
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.9925135e-06
Norm of the params: 17.585648
                Loss: fixed 124 labels. Loss 0.08149. Accuracy 0.977.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24782081
Train loss (w/o reg) on all data: 0.24019858
Test loss (w/o reg) on all data: 0.19073498
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.520557e-05
Norm of the params: 12.346844
              Random: fixed  27 labels. Loss 0.19073. Accuracy 0.943.
### Flips: 312, rs: 29, checks: 208
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10442924
Train loss (w/o reg) on all data: 0.09376888
Test loss (w/o reg) on all data: 0.09792064
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5777283e-05
Norm of the params: 14.601613
     Influence (LOO): fixed 113 labels. Loss 0.09792. Accuracy 0.973.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036711775
Train loss (w/o reg) on all data: 0.022191478
Test loss (w/o reg) on all data: 0.043559995
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5184569e-06
Norm of the params: 17.041302
                Loss: fixed 136 labels. Loss 0.04356. Accuracy 0.985.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23597828
Train loss (w/o reg) on all data: 0.22793232
Test loss (w/o reg) on all data: 0.17085145
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.661926e-05
Norm of the params: 12.685394
              Random: fixed  38 labels. Loss 0.17085. Accuracy 0.958.
### Flips: 312, rs: 29, checks: 260
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068096854
Train loss (w/o reg) on all data: 0.05675378
Test loss (w/o reg) on all data: 0.069165565
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.1815265e-06
Norm of the params: 15.061923
     Influence (LOO): fixed 131 labels. Loss 0.06917. Accuracy 0.969.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027044512
Train loss (w/o reg) on all data: 0.014580041
Test loss (w/o reg) on all data: 0.041681316
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4910373e-06
Norm of the params: 15.788902
                Loss: fixed 143 labels. Loss 0.04168. Accuracy 0.985.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23353906
Train loss (w/o reg) on all data: 0.22563316
Test loss (w/o reg) on all data: 0.16239615
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.559599e-05
Norm of the params: 12.574494
              Random: fixed  42 labels. Loss 0.16240. Accuracy 0.962.
### Flips: 312, rs: 29, checks: 312
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04401165
Train loss (w/o reg) on all data: 0.036029898
Test loss (w/o reg) on all data: 0.03029058
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.806486e-06
Norm of the params: 12.634675
     Influence (LOO): fixed 148 labels. Loss 0.03029. Accuracy 0.996.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020025883
Train loss (w/o reg) on all data: 0.009433583
Test loss (w/o reg) on all data: 0.040458184
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.3099957e-07
Norm of the params: 14.55493
                Loss: fixed 148 labels. Loss 0.04046. Accuracy 0.985.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2284737
Train loss (w/o reg) on all data: 0.22055496
Test loss (w/o reg) on all data: 0.1613816
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.467053e-06
Norm of the params: 12.584697
              Random: fixed  46 labels. Loss 0.16138. Accuracy 0.958.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25300616
Train loss (w/o reg) on all data: 0.24417286
Test loss (w/o reg) on all data: 0.18112762
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.2216576e-05
Norm of the params: 13.29158
Flipped loss: 0.18113. Accuracy: 0.966
### Flips: 312, rs: 30, checks: 52
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19898817
Train loss (w/o reg) on all data: 0.18903823
Test loss (w/o reg) on all data: 0.15761535
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 9.422352e-06
Norm of the params: 14.106693
     Influence (LOO): fixed  33 labels. Loss 0.15762. Accuracy 0.950.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15205726
Train loss (w/o reg) on all data: 0.13713841
Test loss (w/o reg) on all data: 0.11776534
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.5374767e-05
Norm of the params: 17.273592
                Loss: fixed  50 labels. Loss 0.11777. Accuracy 0.958.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24590269
Train loss (w/o reg) on all data: 0.23688707
Test loss (w/o reg) on all data: 0.17150122
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.9990926e-05
Norm of the params: 13.428043
              Random: fixed   7 labels. Loss 0.17150. Accuracy 0.969.
### Flips: 312, rs: 30, checks: 104
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15460098
Train loss (w/o reg) on all data: 0.14301567
Test loss (w/o reg) on all data: 0.12790711
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.6593223e-05
Norm of the params: 15.221898
     Influence (LOO): fixed  58 labels. Loss 0.12791. Accuracy 0.966.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08628719
Train loss (w/o reg) on all data: 0.07152734
Test loss (w/o reg) on all data: 0.062188737
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6321626e-05
Norm of the params: 17.181301
                Loss: fixed  91 labels. Loss 0.06219. Accuracy 0.977.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2385674
Train loss (w/o reg) on all data: 0.22918233
Test loss (w/o reg) on all data: 0.1634463
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.454696e-05
Norm of the params: 13.700412
              Random: fixed  13 labels. Loss 0.16345. Accuracy 0.969.
### Flips: 312, rs: 30, checks: 156
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10739902
Train loss (w/o reg) on all data: 0.09689303
Test loss (w/o reg) on all data: 0.08486359
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.065806e-06
Norm of the params: 14.495508
     Influence (LOO): fixed  87 labels. Loss 0.08486. Accuracy 0.969.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03519681
Train loss (w/o reg) on all data: 0.024026291
Test loss (w/o reg) on all data: 0.028545292
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.673264e-06
Norm of the params: 14.9469185
                Loss: fixed 120 labels. Loss 0.02855. Accuracy 0.989.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23645149
Train loss (w/o reg) on all data: 0.22693035
Test loss (w/o reg) on all data: 0.16230394
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.94371e-05
Norm of the params: 13.799382
              Random: fixed  17 labels. Loss 0.16230. Accuracy 0.969.
### Flips: 312, rs: 30, checks: 208
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08043206
Train loss (w/o reg) on all data: 0.07045744
Test loss (w/o reg) on all data: 0.065642916
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.2540695e-06
Norm of the params: 14.124171
     Influence (LOO): fixed 104 labels. Loss 0.06564. Accuracy 0.981.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012044642
Train loss (w/o reg) on all data: 0.0052879946
Test loss (w/o reg) on all data: 0.01344369
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3969112e-06
Norm of the params: 11.624671
                Loss: fixed 135 labels. Loss 0.01344. Accuracy 0.992.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23275022
Train loss (w/o reg) on all data: 0.22308832
Test loss (w/o reg) on all data: 0.15992038
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.1563761e-05
Norm of the params: 13.901001
              Random: fixed  21 labels. Loss 0.15992. Accuracy 0.973.
### Flips: 312, rs: 30, checks: 260
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04628995
Train loss (w/o reg) on all data: 0.038267154
Test loss (w/o reg) on all data: 0.034629107
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.3341157e-06
Norm of the params: 12.667121
     Influence (LOO): fixed 124 labels. Loss 0.03463. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073947757
Train loss (w/o reg) on all data: 0.0026357349
Test loss (w/o reg) on all data: 0.013665508
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.252718e-07
Norm of the params: 9.756065
                Loss: fixed 137 labels. Loss 0.01367. Accuracy 0.992.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2266752
Train loss (w/o reg) on all data: 0.21765502
Test loss (w/o reg) on all data: 0.15290076
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.441323e-05
Norm of the params: 13.4314375
              Random: fixed  26 labels. Loss 0.15290. Accuracy 0.981.
### Flips: 312, rs: 30, checks: 312
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019120183
Train loss (w/o reg) on all data: 0.013723878
Test loss (w/o reg) on all data: 0.019027632
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9918543e-06
Norm of the params: 10.388748
     Influence (LOO): fixed 135 labels. Loss 0.01903. Accuracy 0.996.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007394774
Train loss (w/o reg) on all data: 0.0026357304
Test loss (w/o reg) on all data: 0.013663151
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6724309e-06
Norm of the params: 9.756069
                Loss: fixed 137 labels. Loss 0.01366. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2216321
Train loss (w/o reg) on all data: 0.21272786
Test loss (w/o reg) on all data: 0.14690629
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4016694e-05
Norm of the params: 13.344839
              Random: fixed  30 labels. Loss 0.14691. Accuracy 0.981.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25921088
Train loss (w/o reg) on all data: 0.2511086
Test loss (w/o reg) on all data: 0.18815404
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.4546504e-05
Norm of the params: 12.729733
Flipped loss: 0.18815. Accuracy: 0.962
### Flips: 312, rs: 31, checks: 52
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20665652
Train loss (w/o reg) on all data: 0.19408536
Test loss (w/o reg) on all data: 0.15467368
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.280848e-05
Norm of the params: 15.856329
     Influence (LOO): fixed  33 labels. Loss 0.15467. Accuracy 0.958.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15580694
Train loss (w/o reg) on all data: 0.1409291
Test loss (w/o reg) on all data: 0.14354546
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.04308e-05
Norm of the params: 17.249834
                Loss: fixed  49 labels. Loss 0.14355. Accuracy 0.939.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25528383
Train loss (w/o reg) on all data: 0.2473064
Test loss (w/o reg) on all data: 0.170132
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 0.00011005892
Norm of the params: 12.631251
              Random: fixed  11 labels. Loss 0.17013. Accuracy 0.973.
### Flips: 312, rs: 31, checks: 104
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17696038
Train loss (w/o reg) on all data: 0.16380738
Test loss (w/o reg) on all data: 0.14793968
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.941641e-06
Norm of the params: 16.219124
     Influence (LOO): fixed  55 labels. Loss 0.14794. Accuracy 0.977.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09040147
Train loss (w/o reg) on all data: 0.0730483
Test loss (w/o reg) on all data: 0.098424315
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.054772e-06
Norm of the params: 18.629639
                Loss: fixed  91 labels. Loss 0.09842. Accuracy 0.969.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24458201
Train loss (w/o reg) on all data: 0.23587185
Test loss (w/o reg) on all data: 0.15954319
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4723302e-05
Norm of the params: 13.198603
              Random: fixed  19 labels. Loss 0.15954. Accuracy 0.973.
### Flips: 312, rs: 31, checks: 156
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12648343
Train loss (w/o reg) on all data: 0.112769075
Test loss (w/o reg) on all data: 0.10315021
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2082627e-05
Norm of the params: 16.561617
     Influence (LOO): fixed  87 labels. Loss 0.10315. Accuracy 0.985.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05397165
Train loss (w/o reg) on all data: 0.038863204
Test loss (w/o reg) on all data: 0.06323414
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.4846e-06
Norm of the params: 17.383005
                Loss: fixed 115 labels. Loss 0.06323. Accuracy 0.973.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23974803
Train loss (w/o reg) on all data: 0.23115876
Test loss (w/o reg) on all data: 0.15164968
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.3404133e-05
Norm of the params: 13.106687
              Random: fixed  25 labels. Loss 0.15165. Accuracy 0.966.
### Flips: 312, rs: 31, checks: 208
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09178236
Train loss (w/o reg) on all data: 0.08073131
Test loss (w/o reg) on all data: 0.07360065
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.748191e-06
Norm of the params: 14.8667755
     Influence (LOO): fixed 112 labels. Loss 0.07360. Accuracy 0.985.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034132365
Train loss (w/o reg) on all data: 0.021485923
Test loss (w/o reg) on all data: 0.03392467
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.207898e-06
Norm of the params: 15.903736
                Loss: fixed 130 labels. Loss 0.03392. Accuracy 0.985.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23230313
Train loss (w/o reg) on all data: 0.22404289
Test loss (w/o reg) on all data: 0.13906129
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.3380136e-05
Norm of the params: 12.853194
              Random: fixed  32 labels. Loss 0.13906. Accuracy 0.977.
### Flips: 312, rs: 31, checks: 260
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048232395
Train loss (w/o reg) on all data: 0.038712
Test loss (w/o reg) on all data: 0.043424487
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4260474e-06
Norm of the params: 13.798839
     Influence (LOO): fixed 134 labels. Loss 0.04342. Accuracy 0.992.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021126825
Train loss (w/o reg) on all data: 0.011276985
Test loss (w/o reg) on all data: 0.02688388
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0428852e-06
Norm of the params: 14.035557
                Loss: fixed 140 labels. Loss 0.02688. Accuracy 0.989.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22720847
Train loss (w/o reg) on all data: 0.21884286
Test loss (w/o reg) on all data: 0.13292417
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.7252825e-05
Norm of the params: 12.934911
              Random: fixed  40 labels. Loss 0.13292. Accuracy 0.977.
### Flips: 312, rs: 31, checks: 312
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026618082
Train loss (w/o reg) on all data: 0.018564897
Test loss (w/o reg) on all data: 0.02460417
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0020068e-06
Norm of the params: 12.691087
     Influence (LOO): fixed 143 labels. Loss 0.02460. Accuracy 0.992.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01704679
Train loss (w/o reg) on all data: 0.008458179
Test loss (w/o reg) on all data: 0.021485753
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5857477e-06
Norm of the params: 13.106189
                Loss: fixed 143 labels. Loss 0.02149. Accuracy 0.992.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21230942
Train loss (w/o reg) on all data: 0.204156
Test loss (w/o reg) on all data: 0.12055124
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.853041e-05
Norm of the params: 12.769827
              Random: fixed  49 labels. Loss 0.12055. Accuracy 0.977.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2635347
Train loss (w/o reg) on all data: 0.25725588
Test loss (w/o reg) on all data: 0.19560881
Train acc on all data:  0.8720152817574021
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.521818e-05
Norm of the params: 11.206075
Flipped loss: 0.19561. Accuracy: 0.954
### Flips: 312, rs: 32, checks: 52
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2173336
Train loss (w/o reg) on all data: 0.20812199
Test loss (w/o reg) on all data: 0.16708533
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3418117e-05
Norm of the params: 13.573224
     Influence (LOO): fixed  30 labels. Loss 0.16709. Accuracy 0.962.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16538015
Train loss (w/o reg) on all data: 0.15504058
Test loss (w/o reg) on all data: 0.13696347
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.497586e-05
Norm of the params: 14.380241
                Loss: fixed  50 labels. Loss 0.13696. Accuracy 0.962.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25971183
Train loss (w/o reg) on all data: 0.25348574
Test loss (w/o reg) on all data: 0.18921487
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.7233744e-05
Norm of the params: 11.158929
              Random: fixed   5 labels. Loss 0.18921. Accuracy 0.962.
### Flips: 312, rs: 32, checks: 104
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16497643
Train loss (w/o reg) on all data: 0.15485531
Test loss (w/o reg) on all data: 0.12350612
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5156669e-05
Norm of the params: 14.227524
     Influence (LOO): fixed  63 labels. Loss 0.12351. Accuracy 0.969.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084212065
Train loss (w/o reg) on all data: 0.07007482
Test loss (w/o reg) on all data: 0.081290066
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.5970538e-06
Norm of the params: 16.815022
                Loss: fixed  94 labels. Loss 0.08129. Accuracy 0.985.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2520361
Train loss (w/o reg) on all data: 0.24586692
Test loss (w/o reg) on all data: 0.1706439
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.648439e-05
Norm of the params: 11.107811
              Random: fixed  13 labels. Loss 0.17064. Accuracy 0.969.
### Flips: 312, rs: 32, checks: 156
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13065052
Train loss (w/o reg) on all data: 0.12054734
Test loss (w/o reg) on all data: 0.088202
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2645483e-05
Norm of the params: 14.214905
     Influence (LOO): fixed  86 labels. Loss 0.08820. Accuracy 0.981.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029116532
Train loss (w/o reg) on all data: 0.017237017
Test loss (w/o reg) on all data: 0.051697213
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.1137578e-06
Norm of the params: 15.413965
                Loss: fixed 126 labels. Loss 0.05170. Accuracy 0.981.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2451414
Train loss (w/o reg) on all data: 0.23907866
Test loss (w/o reg) on all data: 0.15320174
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.487937e-06
Norm of the params: 11.011579
              Random: fixed  23 labels. Loss 0.15320. Accuracy 0.985.
### Flips: 312, rs: 32, checks: 208
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08904385
Train loss (w/o reg) on all data: 0.08045548
Test loss (w/o reg) on all data: 0.050055224
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1783197e-05
Norm of the params: 13.106002
     Influence (LOO): fixed 110 labels. Loss 0.05006. Accuracy 0.992.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017785508
Train loss (w/o reg) on all data: 0.009136601
Test loss (w/o reg) on all data: 0.04287156
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4020807e-06
Norm of the params: 13.152115
                Loss: fixed 135 labels. Loss 0.04287. Accuracy 0.992.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23604168
Train loss (w/o reg) on all data: 0.22952087
Test loss (w/o reg) on all data: 0.14395237
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.6663346e-05
Norm of the params: 11.419985
              Random: fixed  33 labels. Loss 0.14395. Accuracy 0.985.
### Flips: 312, rs: 32, checks: 260
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03858594
Train loss (w/o reg) on all data: 0.030035643
Test loss (w/o reg) on all data: 0.037644964
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4780204e-06
Norm of the params: 13.076923
     Influence (LOO): fixed 130 labels. Loss 0.03764. Accuracy 0.985.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015598588
Train loss (w/o reg) on all data: 0.007792754
Test loss (w/o reg) on all data: 0.037615716
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3198836e-06
Norm of the params: 12.494665
                Loss: fixed 138 labels. Loss 0.03762. Accuracy 0.989.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22807209
Train loss (w/o reg) on all data: 0.2219138
Test loss (w/o reg) on all data: 0.13698669
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6627615e-05
Norm of the params: 11.098017
              Random: fixed  40 labels. Loss 0.13699. Accuracy 0.985.
### Flips: 312, rs: 32, checks: 312
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020757684
Train loss (w/o reg) on all data: 0.0147515675
Test loss (w/o reg) on all data: 0.023590073
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.818131e-07
Norm of the params: 10.960034
     Influence (LOO): fixed 139 labels. Loss 0.02359. Accuracy 0.989.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008834364
Train loss (w/o reg) on all data: 0.0035714149
Test loss (w/o reg) on all data: 0.024815775
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3213645e-06
Norm of the params: 10.259581
                Loss: fixed 141 labels. Loss 0.02482. Accuracy 0.989.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21609776
Train loss (w/o reg) on all data: 0.20985281
Test loss (w/o reg) on all data: 0.12404666
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5755975e-05
Norm of the params: 11.175817
              Random: fixed  50 labels. Loss 0.12405. Accuracy 0.981.
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27393287
Train loss (w/o reg) on all data: 0.26783526
Test loss (w/o reg) on all data: 0.22615902
Train acc on all data:  0.874880611270296
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 4.9796003e-05
Norm of the params: 11.043199
Flipped loss: 0.22616. Accuracy: 0.924
### Flips: 312, rs: 33, checks: 52
Using normal model
LBFGS training took [310] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21889883
Train loss (w/o reg) on all data: 0.20970146
Test loss (w/o reg) on all data: 0.18399562
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.4826942e-05
Norm of the params: 13.562724
     Influence (LOO): fixed  36 labels. Loss 0.18400. Accuracy 0.939.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1647749
Train loss (w/o reg) on all data: 0.15266833
Test loss (w/o reg) on all data: 0.19670686
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.7453749e-05
Norm of the params: 15.560575
                Loss: fixed  51 labels. Loss 0.19671. Accuracy 0.908.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26560256
Train loss (w/o reg) on all data: 0.2596322
Test loss (w/o reg) on all data: 0.20760705
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.6992904e-05
Norm of the params: 10.92736
              Random: fixed  11 labels. Loss 0.20761. Accuracy 0.935.
### Flips: 312, rs: 33, checks: 104
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18432055
Train loss (w/o reg) on all data: 0.17368951
Test loss (w/o reg) on all data: 0.15432699
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.565958e-06
Norm of the params: 14.581519
     Influence (LOO): fixed  62 labels. Loss 0.15433. Accuracy 0.966.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08484451
Train loss (w/o reg) on all data: 0.06895625
Test loss (w/o reg) on all data: 0.1269394
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.621942e-05
Norm of the params: 17.825968
                Loss: fixed  97 labels. Loss 0.12694. Accuracy 0.950.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2505279
Train loss (w/o reg) on all data: 0.24379237
Test loss (w/o reg) on all data: 0.20273414
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 7.6432916e-05
Norm of the params: 11.606482
              Random: fixed  22 labels. Loss 0.20273. Accuracy 0.927.
### Flips: 312, rs: 33, checks: 156
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13351376
Train loss (w/o reg) on all data: 0.121847466
Test loss (w/o reg) on all data: 0.09972476
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.4346892e-05
Norm of the params: 15.275011
     Influence (LOO): fixed  93 labels. Loss 0.09972. Accuracy 0.985.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04282748
Train loss (w/o reg) on all data: 0.028956994
Test loss (w/o reg) on all data: 0.06519603
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.1145375e-06
Norm of the params: 16.655619
                Loss: fixed 125 labels. Loss 0.06520. Accuracy 0.973.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24208587
Train loss (w/o reg) on all data: 0.23506235
Test loss (w/o reg) on all data: 0.18981156
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 4.4942204e-05
Norm of the params: 11.852031
              Random: fixed  30 labels. Loss 0.18981. Accuracy 0.935.
### Flips: 312, rs: 33, checks: 208
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08726113
Train loss (w/o reg) on all data: 0.07530011
Test loss (w/o reg) on all data: 0.07821112
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.5964506e-06
Norm of the params: 15.466754
     Influence (LOO): fixed 118 labels. Loss 0.07821. Accuracy 0.981.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03059447
Train loss (w/o reg) on all data: 0.019107737
Test loss (w/o reg) on all data: 0.03388468
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8968523e-06
Norm of the params: 15.1570015
                Loss: fixed 135 labels. Loss 0.03388. Accuracy 0.989.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24015279
Train loss (w/o reg) on all data: 0.23304854
Test loss (w/o reg) on all data: 0.18641567
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 6.5148975e-05
Norm of the params: 11.91994
              Random: fixed  32 labels. Loss 0.18642. Accuracy 0.939.
### Flips: 312, rs: 33, checks: 260
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042182297
Train loss (w/o reg) on all data: 0.032087047
Test loss (w/o reg) on all data: 0.036467023
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.781175e-06
Norm of the params: 14.209328
     Influence (LOO): fixed 138 labels. Loss 0.03647. Accuracy 0.992.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01448534
Train loss (w/o reg) on all data: 0.006656911
Test loss (w/o reg) on all data: 0.018546235
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.7863306e-07
Norm of the params: 12.512736
                Loss: fixed 144 labels. Loss 0.01855. Accuracy 0.989.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22918095
Train loss (w/o reg) on all data: 0.22174121
Test loss (w/o reg) on all data: 0.18045242
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.8427765e-05
Norm of the params: 12.19814
              Random: fixed  39 labels. Loss 0.18045. Accuracy 0.950.
### Flips: 312, rs: 33, checks: 312
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029422652
Train loss (w/o reg) on all data: 0.021050844
Test loss (w/o reg) on all data: 0.023752624
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0332817e-06
Norm of the params: 12.939713
     Influence (LOO): fixed 143 labels. Loss 0.02375. Accuracy 0.992.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012158865
Train loss (w/o reg) on all data: 0.005460445
Test loss (w/o reg) on all data: 0.016374463
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.551827e-07
Norm of the params: 11.574471
                Loss: fixed 146 labels. Loss 0.01637. Accuracy 0.992.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21238962
Train loss (w/o reg) on all data: 0.20455122
Test loss (w/o reg) on all data: 0.15980959
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.519361e-05
Norm of the params: 12.520705
              Random: fixed  50 labels. Loss 0.15981. Accuracy 0.954.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25906315
Train loss (w/o reg) on all data: 0.25044745
Test loss (w/o reg) on all data: 0.22664906
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.7211037e-05
Norm of the params: 13.126841
Flipped loss: 0.22665. Accuracy: 0.931
### Flips: 312, rs: 34, checks: 52
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20915207
Train loss (w/o reg) on all data: 0.19878381
Test loss (w/o reg) on all data: 0.20031328
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.0757477e-05
Norm of the params: 14.400181
     Influence (LOO): fixed  32 labels. Loss 0.20031. Accuracy 0.920.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14824045
Train loss (w/o reg) on all data: 0.13385004
Test loss (w/o reg) on all data: 0.21767594
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 1.2201187e-05
Norm of the params: 16.964914
                Loss: fixed  52 labels. Loss 0.21768. Accuracy 0.912.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25045973
Train loss (w/o reg) on all data: 0.24239643
Test loss (w/o reg) on all data: 0.20326321
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.698431e-05
Norm of the params: 12.699068
              Random: fixed  11 labels. Loss 0.20326. Accuracy 0.950.
### Flips: 312, rs: 34, checks: 104
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15808058
Train loss (w/o reg) on all data: 0.14611551
Test loss (w/o reg) on all data: 0.17932947
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.600717e-05
Norm of the params: 15.469366
     Influence (LOO): fixed  59 labels. Loss 0.17933. Accuracy 0.943.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07649097
Train loss (w/o reg) on all data: 0.059190687
Test loss (w/o reg) on all data: 0.13271621
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 6.0445864e-06
Norm of the params: 18.601225
                Loss: fixed  99 labels. Loss 0.13272. Accuracy 0.931.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24294306
Train loss (w/o reg) on all data: 0.2347926
Test loss (w/o reg) on all data: 0.19280113
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.4473175e-05
Norm of the params: 12.767505
              Random: fixed  17 labels. Loss 0.19280. Accuracy 0.950.
### Flips: 312, rs: 34, checks: 156
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10991311
Train loss (w/o reg) on all data: 0.09652401
Test loss (w/o reg) on all data: 0.13103746
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2065196e-05
Norm of the params: 16.36405
     Influence (LOO): fixed  87 labels. Loss 0.13104. Accuracy 0.962.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04300636
Train loss (w/o reg) on all data: 0.027268413
Test loss (w/o reg) on all data: 0.093903564
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.8635764e-06
Norm of the params: 17.741447
                Loss: fixed 121 labels. Loss 0.09390. Accuracy 0.958.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23048566
Train loss (w/o reg) on all data: 0.22238132
Test loss (w/o reg) on all data: 0.18096083
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.147821e-05
Norm of the params: 12.731333
              Random: fixed  29 labels. Loss 0.18096. Accuracy 0.962.
### Flips: 312, rs: 34, checks: 208
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064052425
Train loss (w/o reg) on all data: 0.051544517
Test loss (w/o reg) on all data: 0.091053255
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.1097245e-05
Norm of the params: 15.816389
     Influence (LOO): fixed 110 labels. Loss 0.09105. Accuracy 0.969.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023087945
Train loss (w/o reg) on all data: 0.012670302
Test loss (w/o reg) on all data: 0.028080918
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1169044e-06
Norm of the params: 14.434434
                Loss: fixed 132 labels. Loss 0.02808. Accuracy 0.989.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22420646
Train loss (w/o reg) on all data: 0.21568708
Test loss (w/o reg) on all data: 0.17132825
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.869147e-05
Norm of the params: 13.053264
              Random: fixed  34 labels. Loss 0.17133. Accuracy 0.962.
### Flips: 312, rs: 34, checks: 260
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04859216
Train loss (w/o reg) on all data: 0.037378263
Test loss (w/o reg) on all data: 0.06847094
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.9033844e-06
Norm of the params: 14.975913
     Influence (LOO): fixed 123 labels. Loss 0.06847. Accuracy 0.969.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014835926
Train loss (w/o reg) on all data: 0.006533825
Test loss (w/o reg) on all data: 0.014185671
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.055173e-06
Norm of the params: 12.88573
                Loss: fixed 137 labels. Loss 0.01419. Accuracy 0.992.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2168916
Train loss (w/o reg) on all data: 0.20838384
Test loss (w/o reg) on all data: 0.16859317
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.8315852e-05
Norm of the params: 13.044353
              Random: fixed  38 labels. Loss 0.16859. Accuracy 0.958.
### Flips: 312, rs: 34, checks: 312
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02847538
Train loss (w/o reg) on all data: 0.018909173
Test loss (w/o reg) on all data: 0.031503517
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.5119987e-06
Norm of the params: 13.831997
     Influence (LOO): fixed 136 labels. Loss 0.03150. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012077501
Train loss (w/o reg) on all data: 0.004837786
Test loss (w/o reg) on all data: 0.01572002
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5667086e-07
Norm of the params: 12.033051
                Loss: fixed 139 labels. Loss 0.01572. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2129449
Train loss (w/o reg) on all data: 0.20460653
Test loss (w/o reg) on all data: 0.15744413
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.779568e-05
Norm of the params: 12.913839
              Random: fixed  44 labels. Loss 0.15744. Accuracy 0.969.
Using normal model
LBFGS training took [323] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27312544
Train loss (w/o reg) on all data: 0.2651221
Test loss (w/o reg) on all data: 0.2690193
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 7.710405e-05
Norm of the params: 12.651769
Flipped loss: 0.26902. Accuracy: 0.901
### Flips: 312, rs: 35, checks: 52
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22881994
Train loss (w/o reg) on all data: 0.2184979
Test loss (w/o reg) on all data: 0.21378143
Train acc on all data:  0.89207258834766
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.6822542e-05
Norm of the params: 14.368041
     Influence (LOO): fixed  33 labels. Loss 0.21378. Accuracy 0.916.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17259204
Train loss (w/o reg) on all data: 0.15734382
Test loss (w/o reg) on all data: 0.24993038
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 9.275413e-06
Norm of the params: 17.463232
                Loss: fixed  51 labels. Loss 0.24993. Accuracy 0.905.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26375484
Train loss (w/o reg) on all data: 0.2554572
Test loss (w/o reg) on all data: 0.24285616
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 3.7713937e-05
Norm of the params: 12.882281
              Random: fixed  11 labels. Loss 0.24286. Accuracy 0.905.
### Flips: 312, rs: 35, checks: 104
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18399902
Train loss (w/o reg) on all data: 0.17197053
Test loss (w/o reg) on all data: 0.19420198
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 8.477692e-06
Norm of the params: 15.510307
     Influence (LOO): fixed  59 labels. Loss 0.19420. Accuracy 0.927.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111965165
Train loss (w/o reg) on all data: 0.09350392
Test loss (w/o reg) on all data: 0.17960563
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.6946065e-05
Norm of the params: 19.215225
                Loss: fixed  87 labels. Loss 0.17961. Accuracy 0.935.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2581762
Train loss (w/o reg) on all data: 0.24974519
Test loss (w/o reg) on all data: 0.23718978
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 3.696526e-05
Norm of the params: 12.985392
              Random: fixed  19 labels. Loss 0.23719. Accuracy 0.905.
### Flips: 312, rs: 35, checks: 156
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14072207
Train loss (w/o reg) on all data: 0.12793252
Test loss (w/o reg) on all data: 0.12285742
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.9256191e-05
Norm of the params: 15.993465
     Influence (LOO): fixed  89 labels. Loss 0.12286. Accuracy 0.954.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0734878
Train loss (w/o reg) on all data: 0.055337
Test loss (w/o reg) on all data: 0.1414097
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.41527e-06
Norm of the params: 19.05298
                Loss: fixed 116 labels. Loss 0.14141. Accuracy 0.954.
Using normal model
LBFGS training took [309] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25678882
Train loss (w/o reg) on all data: 0.24843745
Test loss (w/o reg) on all data: 0.21905887
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.768996e-05
Norm of the params: 12.923902
              Random: fixed  24 labels. Loss 0.21906. Accuracy 0.927.
### Flips: 312, rs: 35, checks: 208
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10213909
Train loss (w/o reg) on all data: 0.08917589
Test loss (w/o reg) on all data: 0.090033375
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2696672e-05
Norm of the params: 16.10168
     Influence (LOO): fixed 113 labels. Loss 0.09003. Accuracy 0.981.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0557205
Train loss (w/o reg) on all data: 0.038253043
Test loss (w/o reg) on all data: 0.10998467
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.2154804e-06
Norm of the params: 18.690886
                Loss: fixed 131 labels. Loss 0.10998. Accuracy 0.954.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24997349
Train loss (w/o reg) on all data: 0.2415634
Test loss (w/o reg) on all data: 0.19577424
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.5203564e-05
Norm of the params: 12.969264
              Random: fixed  34 labels. Loss 0.19577. Accuracy 0.943.
### Flips: 312, rs: 35, checks: 260
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06658872
Train loss (w/o reg) on all data: 0.05436748
Test loss (w/o reg) on all data: 0.05624105
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.731618e-06
Norm of the params: 15.634091
     Influence (LOO): fixed 133 labels. Loss 0.05624. Accuracy 0.977.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037208524
Train loss (w/o reg) on all data: 0.022021255
Test loss (w/o reg) on all data: 0.05526387
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.638009e-06
Norm of the params: 17.428291
                Loss: fixed 147 labels. Loss 0.05526. Accuracy 0.977.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24014397
Train loss (w/o reg) on all data: 0.23163323
Test loss (w/o reg) on all data: 0.17373283
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.39175945e-05
Norm of the params: 13.04664
              Random: fixed  44 labels. Loss 0.17373. Accuracy 0.958.
### Flips: 312, rs: 35, checks: 312
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050017428
Train loss (w/o reg) on all data: 0.039258547
Test loss (w/o reg) on all data: 0.039563306
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.209597e-06
Norm of the params: 14.668934
     Influence (LOO): fixed 148 labels. Loss 0.03956. Accuracy 0.992.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031756002
Train loss (w/o reg) on all data: 0.017994016
Test loss (w/o reg) on all data: 0.041095667
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.936185e-07
Norm of the params: 16.590351
                Loss: fixed 153 labels. Loss 0.04110. Accuracy 0.985.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23041518
Train loss (w/o reg) on all data: 0.22255807
Test loss (w/o reg) on all data: 0.15423119
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.901579e-05
Norm of the params: 12.535644
              Random: fixed  55 labels. Loss 0.15423. Accuracy 0.969.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24206604
Train loss (w/o reg) on all data: 0.23375031
Test loss (w/o reg) on all data: 0.18929383
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 5.1541734e-05
Norm of the params: 12.896299
Flipped loss: 0.18929. Accuracy: 0.924
### Flips: 312, rs: 36, checks: 52
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1868457
Train loss (w/o reg) on all data: 0.17581446
Test loss (w/o reg) on all data: 0.1656781
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.3958063e-05
Norm of the params: 14.853441
     Influence (LOO): fixed  34 labels. Loss 0.16568. Accuracy 0.931.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14185762
Train loss (w/o reg) on all data: 0.12703042
Test loss (w/o reg) on all data: 0.15543085
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 9.552926e-06
Norm of the params: 17.220457
                Loss: fixed  48 labels. Loss 0.15543. Accuracy 0.931.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23626578
Train loss (w/o reg) on all data: 0.22740954
Test loss (w/o reg) on all data: 0.18643674
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 1.6810369e-05
Norm of the params: 13.308816
              Random: fixed   5 labels. Loss 0.18644. Accuracy 0.912.
### Flips: 312, rs: 36, checks: 104
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14319418
Train loss (w/o reg) on all data: 0.13002601
Test loss (w/o reg) on all data: 0.1453176
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 5.7219208e-05
Norm of the params: 16.22848
     Influence (LOO): fixed  56 labels. Loss 0.14532. Accuracy 0.931.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08614026
Train loss (w/o reg) on all data: 0.06799338
Test loss (w/o reg) on all data: 0.10697409
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.9105362e-05
Norm of the params: 19.05092
                Loss: fixed  82 labels. Loss 0.10697. Accuracy 0.947.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22745602
Train loss (w/o reg) on all data: 0.21832372
Test loss (w/o reg) on all data: 0.17495781
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.1529786e-05
Norm of the params: 13.514661
              Random: fixed  14 labels. Loss 0.17496. Accuracy 0.931.
### Flips: 312, rs: 36, checks: 156
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11352203
Train loss (w/o reg) on all data: 0.099218324
Test loss (w/o reg) on all data: 0.118319824
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.9323274e-06
Norm of the params: 16.913727
     Influence (LOO): fixed  79 labels. Loss 0.11832. Accuracy 0.954.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049357884
Train loss (w/o reg) on all data: 0.033032063
Test loss (w/o reg) on all data: 0.04636271
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4863675e-06
Norm of the params: 18.069765
                Loss: fixed 109 labels. Loss 0.04636. Accuracy 0.985.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22335361
Train loss (w/o reg) on all data: 0.21422997
Test loss (w/o reg) on all data: 0.16629125
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.2641083e-05
Norm of the params: 13.508252
              Random: fixed  20 labels. Loss 0.16629. Accuracy 0.939.
### Flips: 312, rs: 36, checks: 208
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07726815
Train loss (w/o reg) on all data: 0.06542428
Test loss (w/o reg) on all data: 0.076164044
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.5893231e-06
Norm of the params: 15.390824
     Influence (LOO): fixed 100 labels. Loss 0.07616. Accuracy 0.962.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029486585
Train loss (w/o reg) on all data: 0.016969576
Test loss (w/o reg) on all data: 0.016853195
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.737873e-06
Norm of the params: 15.822142
                Loss: fixed 127 labels. Loss 0.01685. Accuracy 0.992.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22032328
Train loss (w/o reg) on all data: 0.21140687
Test loss (w/o reg) on all data: 0.15855208
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.469924e-05
Norm of the params: 13.353952
              Random: fixed  25 labels. Loss 0.15855. Accuracy 0.958.
### Flips: 312, rs: 36, checks: 260
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039086007
Train loss (w/o reg) on all data: 0.028798472
Test loss (w/o reg) on all data: 0.045694504
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.9785793e-06
Norm of the params: 14.344014
     Influence (LOO): fixed 124 labels. Loss 0.04569. Accuracy 0.977.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023355722
Train loss (w/o reg) on all data: 0.012990161
Test loss (w/o reg) on all data: 0.017199
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4750545e-06
Norm of the params: 14.398307
                Loss: fixed 131 labels. Loss 0.01720. Accuracy 0.992.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21077055
Train loss (w/o reg) on all data: 0.20127197
Test loss (w/o reg) on all data: 0.15155913
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.5728298e-05
Norm of the params: 13.78302
              Random: fixed  33 labels. Loss 0.15156. Accuracy 0.962.
### Flips: 312, rs: 36, checks: 312
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024882656
Train loss (w/o reg) on all data: 0.016502757
Test loss (w/o reg) on all data: 0.022748776
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.9862134e-06
Norm of the params: 12.945966
     Influence (LOO): fixed 131 labels. Loss 0.02275. Accuracy 0.989.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017952394
Train loss (w/o reg) on all data: 0.008924208
Test loss (w/o reg) on all data: 0.012661899
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0364336e-06
Norm of the params: 13.4374
                Loss: fixed 133 labels. Loss 0.01266. Accuracy 0.996.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20409998
Train loss (w/o reg) on all data: 0.19437924
Test loss (w/o reg) on all data: 0.1422575
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.0532043e-05
Norm of the params: 13.9432745
              Random: fixed  38 labels. Loss 0.14226. Accuracy 0.943.
Using normal model
LBFGS training took [305] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26621655
Train loss (w/o reg) on all data: 0.25709435
Test loss (w/o reg) on all data: 0.23549335
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 4.2691896e-05
Norm of the params: 13.507185
Flipped loss: 0.23549. Accuracy: 0.924
### Flips: 312, rs: 37, checks: 52
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21856977
Train loss (w/o reg) on all data: 0.20579177
Test loss (w/o reg) on all data: 0.20804532
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 5.348271e-05
Norm of the params: 15.986245
     Influence (LOO): fixed  31 labels. Loss 0.20805. Accuracy 0.920.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17322513
Train loss (w/o reg) on all data: 0.15764749
Test loss (w/o reg) on all data: 0.1918982
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 7.236242e-06
Norm of the params: 17.650862
                Loss: fixed  52 labels. Loss 0.19190. Accuracy 0.939.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25956297
Train loss (w/o reg) on all data: 0.24999744
Test loss (w/o reg) on all data: 0.2311708
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 5.995074e-05
Norm of the params: 13.831503
              Random: fixed   5 labels. Loss 0.23117. Accuracy 0.927.
### Flips: 312, rs: 37, checks: 104
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18018946
Train loss (w/o reg) on all data: 0.16593595
Test loss (w/o reg) on all data: 0.17766505
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 6.6016037e-06
Norm of the params: 16.884027
     Influence (LOO): fixed  57 labels. Loss 0.17767. Accuracy 0.939.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10466865
Train loss (w/o reg) on all data: 0.085125774
Test loss (w/o reg) on all data: 0.13306616
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 5.7033635e-06
Norm of the params: 19.770117
                Loss: fixed  94 labels. Loss 0.13307. Accuracy 0.943.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25877112
Train loss (w/o reg) on all data: 0.2492544
Test loss (w/o reg) on all data: 0.2239708
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 7.972429e-05
Norm of the params: 13.796166
              Random: fixed   9 labels. Loss 0.22397. Accuracy 0.931.
### Flips: 312, rs: 37, checks: 156
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13952674
Train loss (w/o reg) on all data: 0.12658304
Test loss (w/o reg) on all data: 0.14757682
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.721726e-06
Norm of the params: 16.089565
     Influence (LOO): fixed  83 labels. Loss 0.14758. Accuracy 0.950.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068039514
Train loss (w/o reg) on all data: 0.05196271
Test loss (w/o reg) on all data: 0.085760005
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.8373945e-06
Norm of the params: 17.931427
                Loss: fixed 122 labels. Loss 0.08576. Accuracy 0.973.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25545695
Train loss (w/o reg) on all data: 0.24602461
Test loss (w/o reg) on all data: 0.2148957
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.4962454e-05
Norm of the params: 13.734882
              Random: fixed  13 labels. Loss 0.21490. Accuracy 0.954.
### Flips: 312, rs: 37, checks: 208
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09980068
Train loss (w/o reg) on all data: 0.08696676
Test loss (w/o reg) on all data: 0.10593036
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.831607e-06
Norm of the params: 16.021189
     Influence (LOO): fixed 105 labels. Loss 0.10593. Accuracy 0.969.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037027426
Train loss (w/o reg) on all data: 0.023807835
Test loss (w/o reg) on all data: 0.047790222
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.669429e-06
Norm of the params: 16.260132
                Loss: fixed 138 labels. Loss 0.04779. Accuracy 0.989.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25045973
Train loss (w/o reg) on all data: 0.24132895
Test loss (w/o reg) on all data: 0.20497087
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4315826e-05
Norm of the params: 13.513541
              Random: fixed  20 labels. Loss 0.20497. Accuracy 0.969.
### Flips: 312, rs: 37, checks: 260
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068725176
Train loss (w/o reg) on all data: 0.05757479
Test loss (w/o reg) on all data: 0.063742585
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.6138463e-06
Norm of the params: 14.933444
     Influence (LOO): fixed 127 labels. Loss 0.06374. Accuracy 0.981.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019139756
Train loss (w/o reg) on all data: 0.010250682
Test loss (w/o reg) on all data: 0.029012782
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.039237e-06
Norm of the params: 13.333472
                Loss: fixed 147 labels. Loss 0.02901. Accuracy 0.989.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2391145
Train loss (w/o reg) on all data: 0.22890726
Test loss (w/o reg) on all data: 0.20155609
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.8139962e-05
Norm of the params: 14.287923
              Random: fixed  28 labels. Loss 0.20156. Accuracy 0.962.
### Flips: 312, rs: 37, checks: 312
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049424667
Train loss (w/o reg) on all data: 0.039485134
Test loss (w/o reg) on all data: 0.044288438
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.4601647e-06
Norm of the params: 14.099314
     Influence (LOO): fixed 138 labels. Loss 0.04429. Accuracy 0.981.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015856538
Train loss (w/o reg) on all data: 0.00737323
Test loss (w/o reg) on all data: 0.028750798
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.715317e-07
Norm of the params: 13.025596
                Loss: fixed 149 labels. Loss 0.02875. Accuracy 0.989.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23009571
Train loss (w/o reg) on all data: 0.2195107
Test loss (w/o reg) on all data: 0.19091335
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3301338e-05
Norm of the params: 14.549916
              Random: fixed  37 labels. Loss 0.19091. Accuracy 0.969.
Using normal model
LBFGS training took [336] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27535048
Train loss (w/o reg) on all data: 0.26703498
Test loss (w/o reg) on all data: 0.21136144
Train acc on all data:  0.8615090735434575
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.4588945e-05
Norm of the params: 12.896117
Flipped loss: 0.21136. Accuracy: 0.947
### Flips: 312, rs: 38, checks: 52
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23444366
Train loss (w/o reg) on all data: 0.22312231
Test loss (w/o reg) on all data: 0.17905766
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 9.3937066e-05
Norm of the params: 15.047491
     Influence (LOO): fixed  27 labels. Loss 0.17906. Accuracy 0.947.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18331476
Train loss (w/o reg) on all data: 0.17033051
Test loss (w/o reg) on all data: 0.15357453
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.5351041e-05
Norm of the params: 16.11474
                Loss: fixed  48 labels. Loss 0.15357. Accuracy 0.950.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27302718
Train loss (w/o reg) on all data: 0.2646145
Test loss (w/o reg) on all data: 0.20533843
Train acc on all data:  0.8653295128939829
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.5289657e-05
Norm of the params: 12.971259
              Random: fixed   5 labels. Loss 0.20534. Accuracy 0.943.
### Flips: 312, rs: 38, checks: 104
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19305643
Train loss (w/o reg) on all data: 0.181054
Test loss (w/o reg) on all data: 0.15125349
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.8817354e-05
Norm of the params: 15.493509
     Influence (LOO): fixed  55 labels. Loss 0.15125. Accuracy 0.947.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10349432
Train loss (w/o reg) on all data: 0.08889394
Test loss (w/o reg) on all data: 0.10785818
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.0770825e-05
Norm of the params: 17.088228
                Loss: fixed  96 labels. Loss 0.10786. Accuracy 0.962.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27118233
Train loss (w/o reg) on all data: 0.26300418
Test loss (w/o reg) on all data: 0.19571011
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 6.747393e-05
Norm of the params: 12.789178
              Random: fixed  10 labels. Loss 0.19571. Accuracy 0.943.
### Flips: 312, rs: 38, checks: 156
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15032364
Train loss (w/o reg) on all data: 0.1380754
Test loss (w/o reg) on all data: 0.120379925
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.6008522e-05
Norm of the params: 15.651357
     Influence (LOO): fixed  82 labels. Loss 0.12038. Accuracy 0.966.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06321551
Train loss (w/o reg) on all data: 0.046591926
Test loss (w/o reg) on all data: 0.08384796
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.95894e-06
Norm of the params: 18.233805
                Loss: fixed 125 labels. Loss 0.08385. Accuracy 0.958.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26900208
Train loss (w/o reg) on all data: 0.26066482
Test loss (w/o reg) on all data: 0.1933713
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.340163e-05
Norm of the params: 12.912991
              Random: fixed  14 labels. Loss 0.19337. Accuracy 0.950.
### Flips: 312, rs: 38, checks: 208
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11492422
Train loss (w/o reg) on all data: 0.10093389
Test loss (w/o reg) on all data: 0.0886814
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.199655e-06
Norm of the params: 16.727425
     Influence (LOO): fixed 105 labels. Loss 0.08868. Accuracy 0.966.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042331092
Train loss (w/o reg) on all data: 0.026882503
Test loss (w/o reg) on all data: 0.058508568
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3370532e-05
Norm of the params: 17.577593
                Loss: fixed 137 labels. Loss 0.05851. Accuracy 0.981.
Using normal model
LBFGS training took [331] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26357305
Train loss (w/o reg) on all data: 0.25467736
Test loss (w/o reg) on all data: 0.17973556
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.270297e-05
Norm of the params: 13.338443
              Random: fixed  22 labels. Loss 0.17974. Accuracy 0.962.
### Flips: 312, rs: 38, checks: 260
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09279557
Train loss (w/o reg) on all data: 0.081522435
Test loss (w/o reg) on all data: 0.07281804
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2459553e-05
Norm of the params: 15.01542
     Influence (LOO): fixed 122 labels. Loss 0.07282. Accuracy 0.973.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032237217
Train loss (w/o reg) on all data: 0.019668007
Test loss (w/o reg) on all data: 0.036057606
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1083009e-06
Norm of the params: 15.855101
                Loss: fixed 148 labels. Loss 0.03606. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25837323
Train loss (w/o reg) on all data: 0.24922532
Test loss (w/o reg) on all data: 0.17474934
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.3519977e-05
Norm of the params: 13.526204
              Random: fixed  28 labels. Loss 0.17475. Accuracy 0.962.
### Flips: 312, rs: 38, checks: 312
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06411696
Train loss (w/o reg) on all data: 0.054345183
Test loss (w/o reg) on all data: 0.048715197
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.22868705e-05
Norm of the params: 13.979828
     Influence (LOO): fixed 139 labels. Loss 0.04872. Accuracy 0.981.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023903184
Train loss (w/o reg) on all data: 0.0128830485
Test loss (w/o reg) on all data: 0.027248934
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9704935e-06
Norm of the params: 14.845965
                Loss: fixed 154 labels. Loss 0.02725. Accuracy 0.992.
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24876916
Train loss (w/o reg) on all data: 0.24016336
Test loss (w/o reg) on all data: 0.15804005
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.0082075e-05
Norm of the params: 13.119301
              Random: fixed  40 labels. Loss 0.15804. Accuracy 0.962.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2581156
Train loss (w/o reg) on all data: 0.25174794
Test loss (w/o reg) on all data: 0.20974618
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.5675709e-05
Norm of the params: 11.28509
Flipped loss: 0.20975. Accuracy: 0.939
### Flips: 312, rs: 39, checks: 52
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20395437
Train loss (w/o reg) on all data: 0.19473976
Test loss (w/o reg) on all data: 0.17094053
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.618959e-05
Norm of the params: 13.575428
     Influence (LOO): fixed  35 labels. Loss 0.17094. Accuracy 0.947.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15128426
Train loss (w/o reg) on all data: 0.1369013
Test loss (w/o reg) on all data: 0.18452463
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.0397172e-05
Norm of the params: 16.960516
                Loss: fixed  49 labels. Loss 0.18452. Accuracy 0.943.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25003856
Train loss (w/o reg) on all data: 0.24342494
Test loss (w/o reg) on all data: 0.18809342
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.8788447e-05
Norm of the params: 11.500976
              Random: fixed  10 labels. Loss 0.18809. Accuracy 0.947.
### Flips: 312, rs: 39, checks: 104
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15558216
Train loss (w/o reg) on all data: 0.14574064
Test loss (w/o reg) on all data: 0.12709162
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.7737779e-05
Norm of the params: 14.029624
     Influence (LOO): fixed  66 labels. Loss 0.12709. Accuracy 0.954.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086018175
Train loss (w/o reg) on all data: 0.068710946
Test loss (w/o reg) on all data: 0.1478683
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.2360307e-05
Norm of the params: 18.604961
                Loss: fixed  90 labels. Loss 0.14787. Accuracy 0.943.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24547556
Train loss (w/o reg) on all data: 0.23865786
Test loss (w/o reg) on all data: 0.18344556
Train acc on all data:  0.894937917860554
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.30488e-05
Norm of the params: 11.677075
              Random: fixed  16 labels. Loss 0.18345. Accuracy 0.950.
### Flips: 312, rs: 39, checks: 156
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10737635
Train loss (w/o reg) on all data: 0.09637576
Test loss (w/o reg) on all data: 0.09861665
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.1612191e-05
Norm of the params: 14.832795
     Influence (LOO): fixed  94 labels. Loss 0.09862. Accuracy 0.947.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04873398
Train loss (w/o reg) on all data: 0.03304104
Test loss (w/o reg) on all data: 0.08432951
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.5150232e-06
Norm of the params: 17.716059
                Loss: fixed 118 labels. Loss 0.08433. Accuracy 0.958.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23583008
Train loss (w/o reg) on all data: 0.22932918
Test loss (w/o reg) on all data: 0.1565523
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.400912e-05
Norm of the params: 11.402542
              Random: fixed  26 labels. Loss 0.15655. Accuracy 0.958.
### Flips: 312, rs: 39, checks: 208
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07969345
Train loss (w/o reg) on all data: 0.0682987
Test loss (w/o reg) on all data: 0.09826515
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.9342076e-05
Norm of the params: 15.096195
     Influence (LOO): fixed 114 labels. Loss 0.09827. Accuracy 0.966.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029209414
Train loss (w/o reg) on all data: 0.01689945
Test loss (w/o reg) on all data: 0.037964817
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.921961e-06
Norm of the params: 15.69074
                Loss: fixed 131 labels. Loss 0.03796. Accuracy 0.985.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2307616
Train loss (w/o reg) on all data: 0.22455621
Test loss (w/o reg) on all data: 0.14562687
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1992059e-05
Norm of the params: 11.140369
              Random: fixed  33 labels. Loss 0.14563. Accuracy 0.973.
### Flips: 312, rs: 39, checks: 260
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049315333
Train loss (w/o reg) on all data: 0.039709188
Test loss (w/o reg) on all data: 0.057990268
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.2866805e-06
Norm of the params: 13.86084
     Influence (LOO): fixed 129 labels. Loss 0.05799. Accuracy 0.969.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019803343
Train loss (w/o reg) on all data: 0.009304962
Test loss (w/o reg) on all data: 0.014897642
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.64418e-06
Norm of the params: 14.490259
                Loss: fixed 136 labels. Loss 0.01490. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22233637
Train loss (w/o reg) on all data: 0.21604995
Test loss (w/o reg) on all data: 0.13669857
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.7013322e-05
Norm of the params: 11.212864
              Random: fixed  40 labels. Loss 0.13670. Accuracy 0.969.
### Flips: 312, rs: 39, checks: 312
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037199758
Train loss (w/o reg) on all data: 0.027839325
Test loss (w/o reg) on all data: 0.044610213
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9404422e-06
Norm of the params: 13.682423
     Influence (LOO): fixed 138 labels. Loss 0.04461. Accuracy 0.981.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019095073
Train loss (w/o reg) on all data: 0.009493991
Test loss (w/o reg) on all data: 0.012836792
Train acc on all data:  0.9990448901623686
Test acc on all data:   1.0
Norm of the mean of gradients: 1.4251702e-06
Norm of the params: 13.857186
                Loss: fixed 140 labels. Loss 0.01284. Accuracy 1.000.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21308444
Train loss (w/o reg) on all data: 0.20656788
Test loss (w/o reg) on all data: 0.1308184
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1693342e-05
Norm of the params: 11.416271
              Random: fixed  50 labels. Loss 0.13082. Accuracy 0.973.
