/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-02-07 11:08:41.345780: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-07 11:08:55.907493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:07:00.0
totalMemory: 11.17GiB freeMemory: 460.12MiB
2018-02-07 11:08:55.907543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)
/home/lily/jd2392/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Total number of parameters: 121
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012054949
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.01469375e-07
Norm of the params: 9.153184
Orig loss: 0.01205. Accuracy: 0.992
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05725093
Train loss (w/o reg) on all data: 0.045219745
Test loss (w/o reg) on all data: 0.055148087
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.5853135e-06
Norm of the params: 15.512051
Flipped loss: 0.05515. Accuracy: 0.981
### Flips: 52, rs: 0, checks: 52
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008055277
Train loss (w/o reg) on all data: 0.0030825357
Test loss (w/o reg) on all data: 0.014463902
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7390935e-06
Norm of the params: 9.972704
     Influence (LOO): fixed  22 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008055277
Train loss (w/o reg) on all data: 0.0030825362
Test loss (w/o reg) on all data: 0.014463662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.579274e-07
Norm of the params: 9.972704
                Loss: fixed  22 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053919565
Train loss (w/o reg) on all data: 0.042305503
Test loss (w/o reg) on all data: 0.06086048
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.880423e-06
Norm of the params: 15.240776
              Random: fixed   1 labels. Loss 0.06086. Accuracy 0.977.
### Flips: 52, rs: 0, checks: 104
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172928
Test loss (w/o reg) on all data: 0.01205493
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2680697e-07
Norm of the params: 9.153241
     Influence (LOO): fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729288
Test loss (w/o reg) on all data: 0.012054892
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.2463956e-08
Norm of the params: 9.15324
                Loss: fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051319133
Train loss (w/o reg) on all data: 0.039938625
Test loss (w/o reg) on all data: 0.055863272
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.3141113e-06
Norm of the params: 15.086755
              Random: fixed   3 labels. Loss 0.05586. Accuracy 0.977.
### Flips: 52, rs: 0, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729001
Test loss (w/o reg) on all data: 0.012054992
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2210698e-07
Norm of the params: 9.153273
     Influence (LOO): fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729015
Test loss (w/o reg) on all data: 0.012054916
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3297038e-07
Norm of the params: 9.153271
                Loss: fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049613137
Train loss (w/o reg) on all data: 0.03869438
Test loss (w/o reg) on all data: 0.05302587
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.9483193e-06
Norm of the params: 14.77752
              Random: fixed   4 labels. Loss 0.05303. Accuracy 0.981.
### Flips: 52, rs: 0, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002173014
Test loss (w/o reg) on all data: 0.0120551195
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6162803e-07
Norm of the params: 9.153145
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730151
Test loss (w/o reg) on all data: 0.012055051
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.305607e-07
Norm of the params: 9.153146
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04516237
Train loss (w/o reg) on all data: 0.034047052
Test loss (w/o reg) on all data: 0.04968756
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7421772e-06
Norm of the params: 14.90994
              Random: fixed   5 labels. Loss 0.04969. Accuracy 0.981.
### Flips: 52, rs: 0, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730638
Test loss (w/o reg) on all data: 0.012054611
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.664907e-07
Norm of the params: 9.153091
     Influence (LOO): fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [2] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729365
Test loss (w/o reg) on all data: 0.012055042
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7210385e-07
Norm of the params: 9.153233
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041769244
Train loss (w/o reg) on all data: 0.031906437
Test loss (w/o reg) on all data: 0.04007685
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.41531e-06
Norm of the params: 14.044791
              Random: fixed   8 labels. Loss 0.04008. Accuracy 0.985.
### Flips: 52, rs: 0, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728396
Test loss (w/o reg) on all data: 0.012055654
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.581859e-07
Norm of the params: 9.153337
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728422
Test loss (w/o reg) on all data: 0.01205547
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3591172e-07
Norm of the params: 9.153336
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041769244
Train loss (w/o reg) on all data: 0.03190699
Test loss (w/o reg) on all data: 0.040074047
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3789473e-06
Norm of the params: 14.044399
              Random: fixed   8 labels. Loss 0.04007. Accuracy 0.985.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07679798
Train loss (w/o reg) on all data: 0.06757323
Test loss (w/o reg) on all data: 0.06655551
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0020684e-05
Norm of the params: 13.582892
Flipped loss: 0.06656. Accuracy: 0.989
### Flips: 52, rs: 1, checks: 52
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729344
Test loss (w/o reg) on all data: 0.012054719
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9169451e-07
Norm of the params: 9.1532345
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172936
Test loss (w/o reg) on all data: 0.012054655
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9302654e-07
Norm of the params: 9.153234
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07279645
Train loss (w/o reg) on all data: 0.06333853
Test loss (w/o reg) on all data: 0.061559502
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6131224e-05
Norm of the params: 13.753487
              Random: fixed   2 labels. Loss 0.06156. Accuracy 0.985.
### Flips: 52, rs: 1, checks: 104
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730724
Test loss (w/o reg) on all data: 0.012054845
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9526577e-07
Norm of the params: 9.153082
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730661
Test loss (w/o reg) on all data: 0.012054798
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.157407e-07
Norm of the params: 9.153089
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06997348
Train loss (w/o reg) on all data: 0.060244326
Test loss (w/o reg) on all data: 0.059637003
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.9954865e-06
Norm of the params: 13.949308
              Random: fixed   3 labels. Loss 0.05964. Accuracy 0.989.
### Flips: 52, rs: 1, checks: 156
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729735
Test loss (w/o reg) on all data: 0.012054772
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3172923e-07
Norm of the params: 9.15319
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729744
Test loss (w/o reg) on all data: 0.012054863
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9089165e-07
Norm of the params: 9.15319
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06720215
Train loss (w/o reg) on all data: 0.05821084
Test loss (w/o reg) on all data: 0.05807235
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3130478e-05
Norm of the params: 13.409932
              Random: fixed   4 labels. Loss 0.05807. Accuracy 0.985.
### Flips: 52, rs: 1, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021730638
Test loss (w/o reg) on all data: 0.01205458
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7261043e-07
Norm of the params: 9.15309
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730643
Test loss (w/o reg) on all data: 0.01205467
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6398297e-07
Norm of the params: 9.1530905
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06720216
Train loss (w/o reg) on all data: 0.05821131
Test loss (w/o reg) on all data: 0.058063354
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.219168e-06
Norm of the params: 13.409584
              Random: fixed   4 labels. Loss 0.05806. Accuracy 0.985.
### Flips: 52, rs: 1, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729555
Test loss (w/o reg) on all data: 0.012054933
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.373882e-07
Norm of the params: 9.153211
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729548
Test loss (w/o reg) on all data: 0.012054871
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4092359e-07
Norm of the params: 9.153212
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063783675
Train loss (w/o reg) on all data: 0.055009805
Test loss (w/o reg) on all data: 0.054269746
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.603922e-05
Norm of the params: 13.246788
              Random: fixed   6 labels. Loss 0.05427. Accuracy 0.985.
### Flips: 52, rs: 1, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012054473
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6234858e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729632
Test loss (w/o reg) on all data: 0.012054458
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5487264e-07
Norm of the params: 9.153204
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06378367
Train loss (w/o reg) on all data: 0.05500476
Test loss (w/o reg) on all data: 0.054283183
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.6610843e-06
Norm of the params: 13.25059
              Random: fixed   6 labels. Loss 0.05428. Accuracy 0.985.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064729296
Train loss (w/o reg) on all data: 0.055584893
Test loss (w/o reg) on all data: 0.047980074
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.062202e-06
Norm of the params: 13.52361
Flipped loss: 0.04798. Accuracy: 0.985
### Flips: 52, rs: 2, checks: 52
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006805391
Train loss (w/o reg) on all data: 0.0024353105
Test loss (w/o reg) on all data: 0.010979051
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.060123e-07
Norm of the params: 9.348883
     Influence (LOO): fixed  26 labels. Loss 0.01098. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729553
Test loss (w/o reg) on all data: 0.012054864
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6964125e-07
Norm of the params: 9.153212
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0628457
Train loss (w/o reg) on all data: 0.053954743
Test loss (w/o reg) on all data: 0.044061307
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.5053965e-06
Norm of the params: 13.334884
              Random: fixed   1 labels. Loss 0.04406. Accuracy 0.985.
### Flips: 52, rs: 2, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172895
Test loss (w/o reg) on all data: 0.012054221
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0571924e-07
Norm of the params: 9.153276
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021728957
Test loss (w/o reg) on all data: 0.012054131
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.724026e-07
Norm of the params: 9.153274
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06201228
Train loss (w/o reg) on all data: 0.053422254
Test loss (w/o reg) on all data: 0.044663034
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.126523e-06
Norm of the params: 13.10727
              Random: fixed   2 labels. Loss 0.04466. Accuracy 0.981.
### Flips: 52, rs: 2, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729977
Test loss (w/o reg) on all data: 0.01205577
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8839237e-07
Norm of the params: 9.153165
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729975
Test loss (w/o reg) on all data: 0.012055604
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.121478e-07
Norm of the params: 9.153165
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05923102
Train loss (w/o reg) on all data: 0.05044096
Test loss (w/o reg) on all data: 0.04491525
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.1177087e-06
Norm of the params: 13.259005
              Random: fixed   4 labels. Loss 0.04492. Accuracy 0.981.
### Flips: 52, rs: 2, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729078
Test loss (w/o reg) on all data: 0.012054855
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.7544595e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172908
Test loss (w/o reg) on all data: 0.012055032
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1477193e-07
Norm of the params: 9.153263
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047259346
Train loss (w/o reg) on all data: 0.037045382
Test loss (w/o reg) on all data: 0.0370045
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6047755e-06
Norm of the params: 14.292631
              Random: fixed   7 labels. Loss 0.03700. Accuracy 0.985.
### Flips: 52, rs: 2, checks: 260
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729677
Test loss (w/o reg) on all data: 0.0120553095
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.201742e-08
Norm of the params: 9.153196
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172969
Test loss (w/o reg) on all data: 0.012055339
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0306595e-07
Norm of the params: 9.153196
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04725935
Train loss (w/o reg) on all data: 0.0370443
Test loss (w/o reg) on all data: 0.037011277
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.5804044e-06
Norm of the params: 14.293387
              Random: fixed   7 labels. Loss 0.03701. Accuracy 0.985.
### Flips: 52, rs: 2, checks: 312
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729274
Test loss (w/o reg) on all data: 0.0120553365
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2779795e-07
Norm of the params: 9.153241
     Influence (LOO): fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.01205526
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3463686e-07
Norm of the params: 9.153237
                Loss: fixed  27 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03743462
Train loss (w/o reg) on all data: 0.027737673
Test loss (w/o reg) on all data: 0.036810413
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.765446e-06
Norm of the params: 13.926195
              Random: fixed  11 labels. Loss 0.03681. Accuracy 0.989.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070763424
Train loss (w/o reg) on all data: 0.06197766
Test loss (w/o reg) on all data: 0.04870795
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4288185e-05
Norm of the params: 13.255766
Flipped loss: 0.04871. Accuracy: 0.981
### Flips: 52, rs: 3, checks: 52
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172887
Test loss (w/o reg) on all data: 0.012054202
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3900265e-07
Norm of the params: 9.153286
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728894
Test loss (w/o reg) on all data: 0.012054272
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.959535e-07
Norm of the params: 9.153284
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06738268
Train loss (w/o reg) on all data: 0.0582863
Test loss (w/o reg) on all data: 0.04779386
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7402886e-06
Norm of the params: 13.4880495
              Random: fixed   1 labels. Loss 0.04779. Accuracy 0.977.
### Flips: 52, rs: 3, checks: 104
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728957
Test loss (w/o reg) on all data: 0.012053931
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7709078e-07
Norm of the params: 9.153276
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729067
Test loss (w/o reg) on all data: 0.012054306
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0224422e-06
Norm of the params: 9.153264
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065650634
Train loss (w/o reg) on all data: 0.05657624
Test loss (w/o reg) on all data: 0.047646776
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.047592e-06
Norm of the params: 13.471743
              Random: fixed   2 labels. Loss 0.04765. Accuracy 0.977.
### Flips: 52, rs: 3, checks: 156
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729432
Test loss (w/o reg) on all data: 0.012054797
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.8495e-08
Norm of the params: 9.153224
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.012054565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.00501985e-07
Norm of the params: 9.153195
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06565063
Train loss (w/o reg) on all data: 0.05657762
Test loss (w/o reg) on all data: 0.04765211
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4302637e-05
Norm of the params: 13.470717
              Random: fixed   2 labels. Loss 0.04765. Accuracy 0.977.
### Flips: 52, rs: 3, checks: 208
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729777
Test loss (w/o reg) on all data: 0.012054742
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1778541e-07
Norm of the params: 9.153187
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729767
Test loss (w/o reg) on all data: 0.012054796
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.09597416e-07
Norm of the params: 9.153187
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065650634
Train loss (w/o reg) on all data: 0.05657647
Test loss (w/o reg) on all data: 0.047650542
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0674329e-05
Norm of the params: 13.471571
              Random: fixed   2 labels. Loss 0.04765. Accuracy 0.977.
### Flips: 52, rs: 3, checks: 260
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729309
Test loss (w/o reg) on all data: 0.012054571
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.523597e-08
Norm of the params: 9.153239
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729306
Test loss (w/o reg) on all data: 0.012054615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6052688e-07
Norm of the params: 9.153238
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059014995
Train loss (w/o reg) on all data: 0.04989045
Test loss (w/o reg) on all data: 0.038609996
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.76433e-06
Norm of the params: 13.508917
              Random: fixed   4 labels. Loss 0.03861. Accuracy 0.985.
### Flips: 52, rs: 3, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172964
Test loss (w/o reg) on all data: 0.012054886
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3741397e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012054976
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3173374e-07
Norm of the params: 9.153203
                Loss: fixed  24 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05735518
Train loss (w/o reg) on all data: 0.04838915
Test loss (w/o reg) on all data: 0.04155299
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.935749e-06
Norm of the params: 13.391063
              Random: fixed   5 labels. Loss 0.04155. Accuracy 0.981.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08335395
Train loss (w/o reg) on all data: 0.07465396
Test loss (w/o reg) on all data: 0.05427363
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4310041e-05
Norm of the params: 13.190902
Flipped loss: 0.05427. Accuracy: 0.985
### Flips: 52, rs: 4, checks: 52
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729814
Test loss (w/o reg) on all data: 0.012055037
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1126296e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012055121
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4509257e-07
Norm of the params: 9.153183
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078576446
Train loss (w/o reg) on all data: 0.06982956
Test loss (w/o reg) on all data: 0.05375865
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.233841e-05
Norm of the params: 13.2264
              Random: fixed   2 labels. Loss 0.05376. Accuracy 0.985.
### Flips: 52, rs: 4, checks: 104
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729853
Test loss (w/o reg) on all data: 0.012054879
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3683997e-07
Norm of the params: 9.153176
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172985
Test loss (w/o reg) on all data: 0.012054917
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6670838e-07
Norm of the params: 9.153176
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066762984
Train loss (w/o reg) on all data: 0.056985993
Test loss (w/o reg) on all data: 0.05633145
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.9542e-06
Norm of the params: 13.983555
              Random: fixed   5 labels. Loss 0.05633. Accuracy 0.981.
### Flips: 52, rs: 4, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728857
Test loss (w/o reg) on all data: 0.012054733
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4204612e-06
Norm of the params: 9.153286
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728901
Test loss (w/o reg) on all data: 0.012054518
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3227165e-07
Norm of the params: 9.153282
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056744047
Train loss (w/o reg) on all data: 0.046341103
Test loss (w/o reg) on all data: 0.05669131
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.415287e-06
Norm of the params: 14.424245
              Random: fixed   8 labels. Loss 0.05669. Accuracy 0.985.
### Flips: 52, rs: 4, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728498
Test loss (w/o reg) on all data: 0.012054002
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.422495e-07
Norm of the params: 9.153327
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728503
Test loss (w/o reg) on all data: 0.012054158
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.749888e-07
Norm of the params: 9.153326
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052501954
Train loss (w/o reg) on all data: 0.04213126
Test loss (w/o reg) on all data: 0.049659926
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.487098e-06
Norm of the params: 14.40187
              Random: fixed  10 labels. Loss 0.04966. Accuracy 0.989.
### Flips: 52, rs: 4, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172969
Test loss (w/o reg) on all data: 0.012054538
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6410463e-07
Norm of the params: 9.153195
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729684
Test loss (w/o reg) on all data: 0.012054785
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.499532e-07
Norm of the params: 9.153197
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044488773
Train loss (w/o reg) on all data: 0.033687998
Test loss (w/o reg) on all data: 0.0573961
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.4731013e-06
Norm of the params: 14.697468
              Random: fixed  12 labels. Loss 0.05740. Accuracy 0.981.
### Flips: 52, rs: 4, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729507
Test loss (w/o reg) on all data: 0.012054515
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9211228e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.002172952
Test loss (w/o reg) on all data: 0.012054488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.477836e-08
Norm of the params: 9.153216
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04342767
Train loss (w/o reg) on all data: 0.032788716
Test loss (w/o reg) on all data: 0.052852057
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.65959e-06
Norm of the params: 14.586949
              Random: fixed  13 labels. Loss 0.05285. Accuracy 0.981.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060939748
Train loss (w/o reg) on all data: 0.04905793
Test loss (w/o reg) on all data: 0.0498195
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1519273e-06
Norm of the params: 15.415459
Flipped loss: 0.04982. Accuracy: 0.989
### Flips: 52, rs: 5, checks: 52
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007034419
Train loss (w/o reg) on all data: 0.0026642089
Test loss (w/o reg) on all data: 0.01218408
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0187367e-07
Norm of the params: 9.349022
     Influence (LOO): fixed  21 labels. Loss 0.01218. Accuracy 0.992.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012054958
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.806523e-08
Norm of the params: 9.153171
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06093975
Train loss (w/o reg) on all data: 0.04905773
Test loss (w/o reg) on all data: 0.04981984
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.2305174e-06
Norm of the params: 15.415589
              Random: fixed   0 labels. Loss 0.04982. Accuracy 0.989.
### Flips: 52, rs: 5, checks: 104
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.012055184
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1964329e-07
Norm of the params: 9.153182
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729772
Test loss (w/o reg) on all data: 0.0120552955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3987793e-07
Norm of the params: 9.153187
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05887538
Train loss (w/o reg) on all data: 0.047447175
Test loss (w/o reg) on all data: 0.051879656
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.0265444e-06
Norm of the params: 15.118336
              Random: fixed   1 labels. Loss 0.05188. Accuracy 0.985.
### Flips: 52, rs: 5, checks: 156
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730727
Test loss (w/o reg) on all data: 0.012055109
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.239955e-07
Norm of the params: 9.153082
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173072
Test loss (w/o reg) on all data: 0.012055218
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9668182e-07
Norm of the params: 9.153083
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058875382
Train loss (w/o reg) on all data: 0.04744677
Test loss (w/o reg) on all data: 0.05187415
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4429582e-05
Norm of the params: 15.118607
              Random: fixed   1 labels. Loss 0.05187. Accuracy 0.985.
### Flips: 52, rs: 5, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729246
Test loss (w/o reg) on all data: 0.012055321
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.359801e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729246
Test loss (w/o reg) on all data: 0.012055247
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4052935e-07
Norm of the params: 9.153244
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058875374
Train loss (w/o reg) on all data: 0.04745063
Test loss (w/o reg) on all data: 0.051876705
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.083957e-06
Norm of the params: 15.116048
              Random: fixed   1 labels. Loss 0.05188. Accuracy 0.985.
### Flips: 52, rs: 5, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730117
Test loss (w/o reg) on all data: 0.012054859
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8759968e-07
Norm of the params: 9.153149
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730112
Test loss (w/o reg) on all data: 0.012054925
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1046577e-07
Norm of the params: 9.15315
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058875382
Train loss (w/o reg) on all data: 0.04744686
Test loss (w/o reg) on all data: 0.051881548
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.1684216e-06
Norm of the params: 15.1185465
              Random: fixed   1 labels. Loss 0.05188. Accuracy 0.985.
### Flips: 52, rs: 5, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172907
Test loss (w/o reg) on all data: 0.012055144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.3681464e-08
Norm of the params: 9.153264
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729218
Test loss (w/o reg) on all data: 0.0120551875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6308336e-07
Norm of the params: 9.153248
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052415267
Train loss (w/o reg) on all data: 0.040975254
Test loss (w/o reg) on all data: 0.059120193
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0794852e-05
Norm of the params: 15.126144
              Random: fixed   3 labels. Loss 0.05912. Accuracy 0.981.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081892826
Train loss (w/o reg) on all data: 0.07245672
Test loss (w/o reg) on all data: 0.05993779
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9258503e-06
Norm of the params: 13.737617
Flipped loss: 0.05994. Accuracy: 0.992
### Flips: 52, rs: 6, checks: 52
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729337
Test loss (w/o reg) on all data: 0.012054477
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3854705e-07
Norm of the params: 9.1532345
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729346
Test loss (w/o reg) on all data: 0.012054516
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4594569e-07
Norm of the params: 9.153234
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072146006
Train loss (w/o reg) on all data: 0.062279016
Test loss (w/o reg) on all data: 0.052236143
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9447067e-06
Norm of the params: 14.047769
              Random: fixed   3 labels. Loss 0.05224. Accuracy 0.992.
### Flips: 52, rs: 6, checks: 104
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729914
Test loss (w/o reg) on all data: 0.012054613
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.37607e-07
Norm of the params: 9.153171
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729772
Test loss (w/o reg) on all data: 0.012054489
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5131706e-07
Norm of the params: 9.153188
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066252545
Train loss (w/o reg) on all data: 0.05562974
Test loss (w/o reg) on all data: 0.05079964
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.5990694e-06
Norm of the params: 14.5758705
              Random: fixed   4 labels. Loss 0.05080. Accuracy 0.989.
### Flips: 52, rs: 6, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172851
Test loss (w/o reg) on all data: 0.012054979
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.938981e-07
Norm of the params: 9.153325
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728552
Test loss (w/o reg) on all data: 0.012054935
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.308406e-07
Norm of the params: 9.15332
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06246243
Train loss (w/o reg) on all data: 0.052374505
Test loss (w/o reg) on all data: 0.049330957
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.564574e-06
Norm of the params: 14.204174
              Random: fixed   6 labels. Loss 0.04933. Accuracy 0.992.
### Flips: 52, rs: 6, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173014
Test loss (w/o reg) on all data: 0.012054702
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6214553e-07
Norm of the params: 9.153147
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730147
Test loss (w/o reg) on all data: 0.012054789
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3013269e-07
Norm of the params: 9.153147
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05726669
Train loss (w/o reg) on all data: 0.046422247
Test loss (w/o reg) on all data: 0.045374732
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8633237e-06
Norm of the params: 14.727147
              Random: fixed   8 labels. Loss 0.04537. Accuracy 0.992.
### Flips: 52, rs: 6, checks: 260
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012055103
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9320136e-07
Norm of the params: 9.153202
     Influence (LOO): fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729628
Test loss (w/o reg) on all data: 0.0120550115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2882408e-07
Norm of the params: 9.153203
                Loss: fixed  29 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05529605
Train loss (w/o reg) on all data: 0.045023542
Test loss (w/o reg) on all data: 0.046574987
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.727833e-06
Norm of the params: 14.333533
              Random: fixed   9 labels. Loss 0.04657. Accuracy 0.992.
### Flips: 52, rs: 6, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012054496
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4188458e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729814
Test loss (w/o reg) on all data: 0.012054575
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9334084e-07
Norm of the params: 9.153184
                Loss: fixed  29 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050496355
Train loss (w/o reg) on all data: 0.04055035
Test loss (w/o reg) on all data: 0.045041066
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.123907e-06
Norm of the params: 14.103902
              Random: fixed  11 labels. Loss 0.04504. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086100385
Train loss (w/o reg) on all data: 0.07736872
Test loss (w/o reg) on all data: 0.04932679
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0748772e-05
Norm of the params: 13.214891
Flipped loss: 0.04933. Accuracy: 0.989
### Flips: 52, rs: 7, checks: 52
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730436
Test loss (w/o reg) on all data: 0.012053373
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3549164e-06
Norm of the params: 9.153115
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [5] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729334
Test loss (w/o reg) on all data: 0.0120544825
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3665093e-07
Norm of the params: 9.1532345
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08413285
Train loss (w/o reg) on all data: 0.07552338
Test loss (w/o reg) on all data: 0.046587624
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.5913085e-06
Norm of the params: 13.122097
              Random: fixed   1 labels. Loss 0.04659. Accuracy 0.989.
### Flips: 52, rs: 7, checks: 104
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730233
Test loss (w/o reg) on all data: 0.012053968
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.075348e-07
Norm of the params: 9.153137
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730226
Test loss (w/o reg) on all data: 0.01205407
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.650101e-07
Norm of the params: 9.153138
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07409199
Train loss (w/o reg) on all data: 0.0649941
Test loss (w/o reg) on all data: 0.0483735
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6022193e-06
Norm of the params: 13.489179
              Random: fixed   3 labels. Loss 0.04837. Accuracy 0.985.
### Flips: 52, rs: 7, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729274
Test loss (w/o reg) on all data: 0.012054435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9872152e-07
Norm of the params: 9.153242
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729944
Test loss (w/o reg) on all data: 0.012055253
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5067943e-07
Norm of the params: 9.153168
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06552105
Train loss (w/o reg) on all data: 0.056989435
Test loss (w/o reg) on all data: 0.033775263
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.770882e-06
Norm of the params: 13.062629
              Random: fixed   7 labels. Loss 0.03378. Accuracy 0.996.
### Flips: 52, rs: 7, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729213
Test loss (w/o reg) on all data: 0.012055179
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9816887e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729225
Test loss (w/o reg) on all data: 0.012055071
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6244422e-07
Norm of the params: 9.153248
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063786075
Train loss (w/o reg) on all data: 0.055851206
Test loss (w/o reg) on all data: 0.033671908
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3128164e-06
Norm of the params: 12.597514
              Random: fixed   8 labels. Loss 0.03367. Accuracy 0.996.
### Flips: 52, rs: 7, checks: 260
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729644
Test loss (w/o reg) on all data: 0.012054815
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.396218e-08
Norm of the params: 9.153201
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.002172966
Test loss (w/o reg) on all data: 0.012054856
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0339273e-07
Norm of the params: 9.153201
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063786075
Train loss (w/o reg) on all data: 0.05585089
Test loss (w/o reg) on all data: 0.03366863
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9868978e-06
Norm of the params: 12.597766
              Random: fixed   8 labels. Loss 0.03367. Accuracy 0.996.
### Flips: 52, rs: 7, checks: 312
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.012054903
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3903587e-08
Norm of the params: 9.153194
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729714
Test loss (w/o reg) on all data: 0.012054951
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7198671e-07
Norm of the params: 9.153194
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06106551
Train loss (w/o reg) on all data: 0.053175207
Test loss (w/o reg) on all data: 0.030780189
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0185719e-05
Norm of the params: 12.56209
              Random: fixed   9 labels. Loss 0.03078. Accuracy 0.996.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055638704
Train loss (w/o reg) on all data: 0.045149915
Test loss (w/o reg) on all data: 0.029061913
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.80481e-06
Norm of the params: 14.483639
Flipped loss: 0.02906. Accuracy: 0.992
### Flips: 52, rs: 8, checks: 52
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172958
Test loss (w/o reg) on all data: 0.012055024
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.3807074e-07
Norm of the params: 9.153206
     Influence (LOO): fixed  18 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729593
Test loss (w/o reg) on all data: 0.012054853
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.476117e-07
Norm of the params: 9.153207
                Loss: fixed  18 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055638704
Train loss (w/o reg) on all data: 0.04515086
Test loss (w/o reg) on all data: 0.02905662
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.098851e-06
Norm of the params: 14.482985
              Random: fixed   0 labels. Loss 0.02906. Accuracy 0.992.
### Flips: 52, rs: 8, checks: 104
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172916
Test loss (w/o reg) on all data: 0.012054855
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.160101e-07
Norm of the params: 9.153254
     Influence (LOO): fixed  18 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729153
Test loss (w/o reg) on all data: 0.012054756
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3615985e-07
Norm of the params: 9.153254
                Loss: fixed  18 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043189503
Train loss (w/o reg) on all data: 0.03173734
Test loss (w/o reg) on all data: 0.024336018
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.472046e-06
Norm of the params: 15.134177
              Random: fixed   3 labels. Loss 0.02434. Accuracy 0.992.
### Flips: 52, rs: 8, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.012054544
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.864289e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  18 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729353
Test loss (w/o reg) on all data: 0.012054875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.975857e-07
Norm of the params: 9.153234
                Loss: fixed  18 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043189496
Train loss (w/o reg) on all data: 0.031734746
Test loss (w/o reg) on all data: 0.024336804
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.208774e-06
Norm of the params: 15.135884
              Random: fixed   3 labels. Loss 0.02434. Accuracy 0.992.
### Flips: 52, rs: 8, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729039
Test loss (w/o reg) on all data: 0.012054483
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0642085e-06
Norm of the params: 9.153267
     Influence (LOO): fixed  18 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729046
Test loss (w/o reg) on all data: 0.012054695
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3861966e-07
Norm of the params: 9.153266
                Loss: fixed  18 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04026933
Train loss (w/o reg) on all data: 0.029212976
Test loss (w/o reg) on all data: 0.029334702
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.916462e-06
Norm of the params: 14.870345
              Random: fixed   4 labels. Loss 0.02933. Accuracy 0.992.
### Flips: 52, rs: 8, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729735
Test loss (w/o reg) on all data: 0.012054966
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.234981e-08
Norm of the params: 9.153191
     Influence (LOO): fixed  18 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172974
Test loss (w/o reg) on all data: 0.012054991
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7023063e-07
Norm of the params: 9.153192
                Loss: fixed  18 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03756409
Train loss (w/o reg) on all data: 0.02660043
Test loss (w/o reg) on all data: 0.03190283
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.15146e-06
Norm of the params: 14.807876
              Random: fixed   5 labels. Loss 0.03190. Accuracy 0.989.
### Flips: 52, rs: 8, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729374
Test loss (w/o reg) on all data: 0.01205545
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4680104e-07
Norm of the params: 9.15323
     Influence (LOO): fixed  18 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729397
Test loss (w/o reg) on all data: 0.012055263
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.903402e-07
Norm of the params: 9.153227
                Loss: fixed  18 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037564088
Train loss (w/o reg) on all data: 0.026599474
Test loss (w/o reg) on all data: 0.03190411
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.402099e-06
Norm of the params: 14.808521
              Random: fixed   5 labels. Loss 0.03190. Accuracy 0.989.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09185513
Train loss (w/o reg) on all data: 0.0829332
Test loss (w/o reg) on all data: 0.0479521
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.6265527e-06
Norm of the params: 13.358091
Flipped loss: 0.04795. Accuracy: 0.989
### Flips: 52, rs: 9, checks: 52
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0068443445
Train loss (w/o reg) on all data: 0.002449068
Test loss (w/o reg) on all data: 0.010202652
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8519312e-07
Norm of the params: 9.375796
     Influence (LOO): fixed  31 labels. Loss 0.01020. Accuracy 0.992.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007836757
Train loss (w/o reg) on all data: 0.0030057994
Test loss (w/o reg) on all data: 0.01041982
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.7504228e-07
Norm of the params: 9.829504
                Loss: fixed  30 labels. Loss 0.01042. Accuracy 0.996.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09185513
Train loss (w/o reg) on all data: 0.08293076
Test loss (w/o reg) on all data: 0.04795984
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8641173e-05
Norm of the params: 13.35992
              Random: fixed   0 labels. Loss 0.04796. Accuracy 0.989.
### Flips: 52, rs: 9, checks: 104
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172881
Test loss (w/o reg) on all data: 0.012054889
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.113149e-07
Norm of the params: 9.153291
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172883
Test loss (w/o reg) on all data: 0.012054735
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6519323e-07
Norm of the params: 9.15329
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08882427
Train loss (w/o reg) on all data: 0.079751365
Test loss (w/o reg) on all data: 0.04531835
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1265452e-05
Norm of the params: 13.470642
              Random: fixed   1 labels. Loss 0.04532. Accuracy 0.989.
### Flips: 52, rs: 9, checks: 156
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730266
Test loss (w/o reg) on all data: 0.01205599
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.476297e-07
Norm of the params: 9.1531315
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730254
Test loss (w/o reg) on all data: 0.012055868
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9748434e-07
Norm of the params: 9.153133
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08450024
Train loss (w/o reg) on all data: 0.07550129
Test loss (w/o reg) on all data: 0.044701092
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.031289e-06
Norm of the params: 13.415622
              Random: fixed   3 labels. Loss 0.04470. Accuracy 0.989.
### Flips: 52, rs: 9, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729236
Test loss (w/o reg) on all data: 0.012055244
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.14796755e-07
Norm of the params: 9.153246
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729404
Test loss (w/o reg) on all data: 0.012055082
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5390045e-07
Norm of the params: 9.15323
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08450024
Train loss (w/o reg) on all data: 0.07550096
Test loss (w/o reg) on all data: 0.04469753
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0273617e-06
Norm of the params: 13.415874
              Random: fixed   3 labels. Loss 0.04470. Accuracy 0.989.
### Flips: 52, rs: 9, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.00217299
Test loss (w/o reg) on all data: 0.0120556075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7618952e-07
Norm of the params: 9.153174
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012055787
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.340267e-07
Norm of the params: 9.15318
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08450025
Train loss (w/o reg) on all data: 0.07550464
Test loss (w/o reg) on all data: 0.04469231
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.421111e-06
Norm of the params: 13.4131365
              Random: fixed   3 labels. Loss 0.04469. Accuracy 0.989.
### Flips: 52, rs: 9, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730445
Test loss (w/o reg) on all data: 0.012055188
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5318524e-07
Norm of the params: 9.153112
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730422
Test loss (w/o reg) on all data: 0.0120552555
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0916425e-07
Norm of the params: 9.153116
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07939179
Train loss (w/o reg) on all data: 0.070440814
Test loss (w/o reg) on all data: 0.03972977
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.3671495e-06
Norm of the params: 13.379823
              Random: fixed   5 labels. Loss 0.03973. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061163884
Train loss (w/o reg) on all data: 0.050624862
Test loss (w/o reg) on all data: 0.044892535
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.160862e-05
Norm of the params: 14.518281
Flipped loss: 0.04489. Accuracy: 0.989
### Flips: 52, rs: 10, checks: 52
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730012
Test loss (w/o reg) on all data: 0.012055361
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5992372e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730023
Test loss (w/o reg) on all data: 0.012055382
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8821574e-07
Norm of the params: 9.15316
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061163884
Train loss (w/o reg) on all data: 0.05062845
Test loss (w/o reg) on all data: 0.044879545
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.2403864e-06
Norm of the params: 14.515808
              Random: fixed   0 labels. Loss 0.04488. Accuracy 0.989.
### Flips: 52, rs: 10, checks: 104
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730477
Test loss (w/o reg) on all data: 0.012055182
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6367977e-07
Norm of the params: 9.1531105
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173048
Test loss (w/o reg) on all data: 0.012055063
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4141932e-07
Norm of the params: 9.1531105
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06116388
Train loss (w/o reg) on all data: 0.050625272
Test loss (w/o reg) on all data: 0.04488965
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8573988e-06
Norm of the params: 14.517995
              Random: fixed   0 labels. Loss 0.04489. Accuracy 0.989.
### Flips: 52, rs: 10, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021730177
Test loss (w/o reg) on all data: 0.012055008
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1742424e-07
Norm of the params: 9.153145
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730124
Test loss (w/o reg) on all data: 0.012055059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.882818e-07
Norm of the params: 9.153148
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05932915
Train loss (w/o reg) on all data: 0.0490946
Test loss (w/o reg) on all data: 0.042526577
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.5642994e-06
Norm of the params: 14.307026
              Random: fixed   1 labels. Loss 0.04253. Accuracy 0.989.
### Flips: 52, rs: 10, checks: 208
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730117
Test loss (w/o reg) on all data: 0.01205477
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2492482e-07
Norm of the params: 9.153149
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002173012
Test loss (w/o reg) on all data: 0.012054701
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3350698e-07
Norm of the params: 9.153151
                Loss: fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052602615
Train loss (w/o reg) on all data: 0.041436475
Test loss (w/o reg) on all data: 0.04235636
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.936629e-06
Norm of the params: 14.943989
              Random: fixed   2 labels. Loss 0.04236. Accuracy 0.985.
### Flips: 52, rs: 10, checks: 260
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173009
Test loss (w/o reg) on all data: 0.012054794
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8586503e-07
Norm of the params: 9.153152
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729958
Test loss (w/o reg) on all data: 0.012055078
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.6783397e-07
Norm of the params: 9.153167
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0469342
Train loss (w/o reg) on all data: 0.035804816
Test loss (w/o reg) on all data: 0.040308032
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8783593e-06
Norm of the params: 14.919372
              Random: fixed   4 labels. Loss 0.04031. Accuracy 0.985.
### Flips: 52, rs: 10, checks: 312
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021728333
Test loss (w/o reg) on all data: 0.012055554
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3742332e-07
Norm of the params: 9.153346
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172833
Test loss (w/o reg) on all data: 0.012055445
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4380236e-07
Norm of the params: 9.153344
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044552922
Train loss (w/o reg) on all data: 0.03325677
Test loss (w/o reg) on all data: 0.0374673
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.213259e-06
Norm of the params: 15.030736
              Random: fixed   5 labels. Loss 0.03747. Accuracy 0.985.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07350602
Train loss (w/o reg) on all data: 0.06680599
Test loss (w/o reg) on all data: 0.02290946
Train acc on all data:  0.9818529130850048
Test acc on all data:   1.0
Norm of the mean of gradients: 2.5983682e-06
Norm of the params: 11.575865
Flipped loss: 0.02291. Accuracy: 1.000
### Flips: 52, rs: 11, checks: 52
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729558
Test loss (w/o reg) on all data: 0.01205435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.341366e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729562
Test loss (w/o reg) on all data: 0.012054278
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4980034e-07
Norm of the params: 9.153212
                Loss: fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070713304
Train loss (w/o reg) on all data: 0.064077206
Test loss (w/o reg) on all data: 0.020296684
Train acc on all data:  0.9818529130850048
Test acc on all data:   1.0
Norm of the mean of gradients: 8.037573e-06
Norm of the params: 11.520499
              Random: fixed   1 labels. Loss 0.02030. Accuracy 1.000.
### Flips: 52, rs: 11, checks: 104
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173038
Test loss (w/o reg) on all data: 0.012055781
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8288946e-07
Norm of the params: 9.15312
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730347
Test loss (w/o reg) on all data: 0.012055874
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8165296e-07
Norm of the params: 9.153123
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0707133
Train loss (w/o reg) on all data: 0.06407771
Test loss (w/o reg) on all data: 0.020293318
Train acc on all data:  0.9818529130850048
Test acc on all data:   1.0
Norm of the mean of gradients: 3.5201108e-06
Norm of the params: 11.520058
              Random: fixed   1 labels. Loss 0.02029. Accuracy 1.000.
### Flips: 52, rs: 11, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.00217302
Test loss (w/o reg) on all data: 0.012055704
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.8787684e-07
Norm of the params: 9.153141
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730186
Test loss (w/o reg) on all data: 0.012055533
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9113573e-07
Norm of the params: 9.153141
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06683874
Train loss (w/o reg) on all data: 0.06034345
Test loss (w/o reg) on all data: 0.019236
Train acc on all data:  0.9837631327602674
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8801205e-06
Norm of the params: 11.397622
              Random: fixed   2 labels. Loss 0.01924. Accuracy 1.000.
### Flips: 52, rs: 11, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.002172959
Test loss (w/o reg) on all data: 0.012054792
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0090124e-07
Norm of the params: 9.153209
     Influence (LOO): fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620214
Train loss (w/o reg) on all data: 0.00217296
Test loss (w/o reg) on all data: 0.012054841
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4273836e-07
Norm of the params: 9.153209
                Loss: fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06683875
Train loss (w/o reg) on all data: 0.060343646
Test loss (w/o reg) on all data: 0.019237308
Train acc on all data:  0.9837631327602674
Test acc on all data:   1.0
Norm of the mean of gradients: 7.845421e-06
Norm of the params: 11.397462
              Random: fixed   2 labels. Loss 0.01924. Accuracy 1.000.
### Flips: 52, rs: 11, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172998
Test loss (w/o reg) on all data: 0.012054528
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6142529e-07
Norm of the params: 9.153165
     Influence (LOO): fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172995
Test loss (w/o reg) on all data: 0.012054697
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5822327e-07
Norm of the params: 9.153167
                Loss: fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060626883
Train loss (w/o reg) on all data: 0.053905442
Test loss (w/o reg) on all data: 0.018281965
Train acc on all data:  0.9856733524355301
Test acc on all data:   1.0
Norm of the mean of gradients: 1.40355705e-05
Norm of the params: 11.594344
              Random: fixed   4 labels. Loss 0.01828. Accuracy 1.000.
### Flips: 52, rs: 11, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729842
Test loss (w/o reg) on all data: 0.012054613
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3872553e-07
Norm of the params: 9.153179
     Influence (LOO): fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172983
Test loss (w/o reg) on all data: 0.012054551
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1624629e-07
Norm of the params: 9.15318
                Loss: fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059469063
Train loss (w/o reg) on all data: 0.05294276
Test loss (w/o reg) on all data: 0.018232746
Train acc on all data:  0.9856733524355301
Test acc on all data:   1.0
Norm of the mean of gradients: 2.8047273e-06
Norm of the params: 11.4248
              Random: fixed   5 labels. Loss 0.01823. Accuracy 1.000.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055408783
Train loss (w/o reg) on all data: 0.044823006
Test loss (w/o reg) on all data: 0.04386202
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0338213e-06
Norm of the params: 14.550447
Flipped loss: 0.04386. Accuracy: 0.989
### Flips: 52, rs: 12, checks: 52
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728922
Test loss (w/o reg) on all data: 0.012055126
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.309865e-07
Norm of the params: 9.15328
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728945
Test loss (w/o reg) on all data: 0.012055078
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3641919e-07
Norm of the params: 9.153277
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054413505
Train loss (w/o reg) on all data: 0.044219002
Test loss (w/o reg) on all data: 0.0405867
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4607916e-05
Norm of the params: 14.279006
              Random: fixed   1 labels. Loss 0.04059. Accuracy 0.992.
### Flips: 52, rs: 12, checks: 104
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730321
Test loss (w/o reg) on all data: 0.012054994
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.373537e-07
Norm of the params: 9.153127
     Influence (LOO): fixed  20 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730296
Test loss (w/o reg) on all data: 0.012055036
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6003479e-07
Norm of the params: 9.153128
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054413505
Train loss (w/o reg) on all data: 0.04422129
Test loss (w/o reg) on all data: 0.040570937
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9416784e-06
Norm of the params: 14.277405
              Random: fixed   1 labels. Loss 0.04057. Accuracy 0.992.
### Flips: 52, rs: 12, checks: 156
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002173047
Test loss (w/o reg) on all data: 0.012055393
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1353005e-07
Norm of the params: 9.153111
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730459
Test loss (w/o reg) on all data: 0.012055477
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.427298e-07
Norm of the params: 9.153112
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051762544
Train loss (w/o reg) on all data: 0.041416403
Test loss (w/o reg) on all data: 0.03777264
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2828402e-05
Norm of the params: 14.384811
              Random: fixed   2 labels. Loss 0.03777. Accuracy 0.992.
### Flips: 52, rs: 12, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729055
Test loss (w/o reg) on all data: 0.012055255
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3992272e-07
Norm of the params: 9.153265
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172931
Test loss (w/o reg) on all data: 0.012055257
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.027946e-07
Norm of the params: 9.153237
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051762532
Train loss (w/o reg) on all data: 0.04141726
Test loss (w/o reg) on all data: 0.037773546
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7299523e-06
Norm of the params: 14.384209
              Random: fixed   2 labels. Loss 0.03777. Accuracy 0.992.
### Flips: 52, rs: 12, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728363
Test loss (w/o reg) on all data: 0.0120564215
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8387633e-07
Norm of the params: 9.153342
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172837
Test loss (w/o reg) on all data: 0.012056275
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8143153e-07
Norm of the params: 9.153339
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051762536
Train loss (w/o reg) on all data: 0.04141813
Test loss (w/o reg) on all data: 0.0377713
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.206574e-06
Norm of the params: 14.383606
              Random: fixed   2 labels. Loss 0.03777. Accuracy 0.992.
### Flips: 52, rs: 12, checks: 312
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729253
Test loss (w/o reg) on all data: 0.012055493
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.789389e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729274
Test loss (w/o reg) on all data: 0.012055443
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.166503e-08
Norm of the params: 9.153243
                Loss: fixed  20 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051762544
Train loss (w/o reg) on all data: 0.04141619
Test loss (w/o reg) on all data: 0.03777025
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2012702e-05
Norm of the params: 14.384959
              Random: fixed   2 labels. Loss 0.03777. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07351773
Train loss (w/o reg) on all data: 0.06289024
Test loss (w/o reg) on all data: 0.060466982
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1330747e-05
Norm of the params: 14.579089
Flipped loss: 0.06047. Accuracy: 0.985
### Flips: 52, rs: 13, checks: 52
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730044
Test loss (w/o reg) on all data: 0.012054828
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9216156e-06
Norm of the params: 9.153156
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730035
Test loss (w/o reg) on all data: 0.012054588
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.529186e-07
Norm of the params: 9.153158
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06864564
Train loss (w/o reg) on all data: 0.057373986
Test loss (w/o reg) on all data: 0.06340792
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.502235e-06
Norm of the params: 15.014428
              Random: fixed   1 labels. Loss 0.06341. Accuracy 0.981.
### Flips: 52, rs: 13, checks: 104
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729523
Test loss (w/o reg) on all data: 0.012055328
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2375386e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172954
Test loss (w/o reg) on all data: 0.012055128
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0549887e-07
Norm of the params: 9.153213
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06670667
Train loss (w/o reg) on all data: 0.05485123
Test loss (w/o reg) on all data: 0.057380978
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.0151386e-06
Norm of the params: 15.39834
              Random: fixed   2 labels. Loss 0.05738. Accuracy 0.977.
### Flips: 52, rs: 13, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729057
Test loss (w/o reg) on all data: 0.012055622
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.471507e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729064
Test loss (w/o reg) on all data: 0.012055542
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.942893e-07
Norm of the params: 9.153264
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056624234
Train loss (w/o reg) on all data: 0.044882663
Test loss (w/o reg) on all data: 0.057653595
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1913797e-06
Norm of the params: 15.324212
              Random: fixed   5 labels. Loss 0.05765. Accuracy 0.985.
### Flips: 52, rs: 13, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729458
Test loss (w/o reg) on all data: 0.012055906
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2417206e-07
Norm of the params: 9.153222
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172944
Test loss (w/o reg) on all data: 0.012055774
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9207627e-07
Norm of the params: 9.153222
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054612048
Train loss (w/o reg) on all data: 0.043179505
Test loss (w/o reg) on all data: 0.052389722
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.032463e-06
Norm of the params: 15.121205
              Random: fixed   6 labels. Loss 0.05239. Accuracy 0.985.
### Flips: 52, rs: 13, checks: 260
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730212
Test loss (w/o reg) on all data: 0.012055594
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4160605e-07
Norm of the params: 9.153139
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730196
Test loss (w/o reg) on all data: 0.012055554
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2713213e-07
Norm of the params: 9.153141
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05365741
Train loss (w/o reg) on all data: 0.04231413
Test loss (w/o reg) on all data: 0.052398156
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.949788e-06
Norm of the params: 15.0620575
              Random: fixed   7 labels. Loss 0.05240. Accuracy 0.985.
### Flips: 52, rs: 13, checks: 312
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729718
Test loss (w/o reg) on all data: 0.012055586
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5090816e-07
Norm of the params: 9.153193
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.01205554
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2791124e-07
Norm of the params: 9.153194
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04541503
Train loss (w/o reg) on all data: 0.03449612
Test loss (w/o reg) on all data: 0.050066195
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.210763e-06
Norm of the params: 14.777624
              Random: fixed  10 labels. Loss 0.05007. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07604462
Train loss (w/o reg) on all data: 0.066851616
Test loss (w/o reg) on all data: 0.072427064
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.6506474e-06
Norm of the params: 13.5595
Flipped loss: 0.07243. Accuracy: 0.981
### Flips: 52, rs: 14, checks: 52
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730352
Test loss (w/o reg) on all data: 0.012054679
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.741633e-07
Norm of the params: 9.153123
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730345
Test loss (w/o reg) on all data: 0.012054519
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7338874e-07
Norm of the params: 9.153124
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07132731
Train loss (w/o reg) on all data: 0.06216324
Test loss (w/o reg) on all data: 0.062322944
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9768364e-06
Norm of the params: 13.538141
              Random: fixed   2 labels. Loss 0.06232. Accuracy 0.985.
### Flips: 52, rs: 14, checks: 104
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729595
Test loss (w/o reg) on all data: 0.012054992
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.526581e-07
Norm of the params: 9.153209
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729583
Test loss (w/o reg) on all data: 0.0120550385
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3499934e-07
Norm of the params: 9.153208
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.068025656
Train loss (w/o reg) on all data: 0.059501898
Test loss (w/o reg) on all data: 0.058140785
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.2530886e-06
Norm of the params: 13.056617
              Random: fixed   5 labels. Loss 0.05814. Accuracy 0.985.
### Flips: 52, rs: 14, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730084
Test loss (w/o reg) on all data: 0.012055467
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1553768e-07
Norm of the params: 9.153152
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730023
Test loss (w/o reg) on all data: 0.0120552555
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1173265e-07
Norm of the params: 9.153159
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06534649
Train loss (w/o reg) on all data: 0.056402702
Test loss (w/o reg) on all data: 0.053849354
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.732162e-06
Norm of the params: 13.374444
              Random: fixed   6 labels. Loss 0.05385. Accuracy 0.985.
### Flips: 52, rs: 14, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730638
Test loss (w/o reg) on all data: 0.012054558
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7817288e-07
Norm of the params: 9.153093
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730238
Test loss (w/o reg) on all data: 0.012054619
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.116136e-07
Norm of the params: 9.153136
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062407732
Train loss (w/o reg) on all data: 0.05379743
Test loss (w/o reg) on all data: 0.055475686
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.990719e-06
Norm of the params: 13.122729
              Random: fixed   7 labels. Loss 0.05548. Accuracy 0.981.
### Flips: 52, rs: 14, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729514
Test loss (w/o reg) on all data: 0.0120550785
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6647894e-07
Norm of the params: 9.153215
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729511
Test loss (w/o reg) on all data: 0.0120551335
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5193062e-07
Norm of the params: 9.153215
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06240773
Train loss (w/o reg) on all data: 0.053794943
Test loss (w/o reg) on all data: 0.055476952
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.488895e-06
Norm of the params: 13.124623
              Random: fixed   7 labels. Loss 0.05548. Accuracy 0.981.
### Flips: 52, rs: 14, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730182
Test loss (w/o reg) on all data: 0.01205483
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6989536e-07
Norm of the params: 9.153141
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730182
Test loss (w/o reg) on all data: 0.012054911
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7047637e-07
Norm of the params: 9.153142
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06030152
Train loss (w/o reg) on all data: 0.051580958
Test loss (w/o reg) on all data: 0.0550012
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5867398e-06
Norm of the params: 13.206484
              Random: fixed   8 labels. Loss 0.05500. Accuracy 0.985.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07164
Train loss (w/o reg) on all data: 0.063443646
Test loss (w/o reg) on all data: 0.038943116
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0964597e-06
Norm of the params: 12.8034
Flipped loss: 0.03894. Accuracy: 0.989
### Flips: 52, rs: 15, checks: 52
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00796455
Train loss (w/o reg) on all data: 0.0030008624
Test loss (w/o reg) on all data: 0.012030813
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4836112e-06
Norm of the params: 9.963621
     Influence (LOO): fixed  25 labels. Loss 0.01203. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007964548
Train loss (w/o reg) on all data: 0.0030008606
Test loss (w/o reg) on all data: 0.012031086
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6684656e-07
Norm of the params: 9.963622
                Loss: fixed  25 labels. Loss 0.01203. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07087276
Train loss (w/o reg) on all data: 0.06277891
Test loss (w/o reg) on all data: 0.037141237
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6867147e-05
Norm of the params: 12.723088
              Random: fixed   1 labels. Loss 0.03714. Accuracy 0.989.
### Flips: 52, rs: 15, checks: 104
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729106
Test loss (w/o reg) on all data: 0.012055049
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7973932e-07
Norm of the params: 9.153261
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729125
Test loss (w/o reg) on all data: 0.01205503
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4036504e-07
Norm of the params: 9.153258
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06381883
Train loss (w/o reg) on all data: 0.055384137
Test loss (w/o reg) on all data: 0.037863042
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.033532e-06
Norm of the params: 12.988216
              Random: fixed   3 labels. Loss 0.03786. Accuracy 0.992.
### Flips: 52, rs: 15, checks: 156
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217296
Test loss (w/o reg) on all data: 0.012055035
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.688823e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172962
Test loss (w/o reg) on all data: 0.012055102
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6959538e-07
Norm of the params: 9.153204
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06061969
Train loss (w/o reg) on all data: 0.052029174
Test loss (w/o reg) on all data: 0.040325783
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8333125e-06
Norm of the params: 13.107642
              Random: fixed   4 labels. Loss 0.04033. Accuracy 0.989.
### Flips: 52, rs: 15, checks: 208
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729514
Test loss (w/o reg) on all data: 0.01205489
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4949165e-07
Norm of the params: 9.153216
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729493
Test loss (w/o reg) on all data: 0.012054874
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.287265e-08
Norm of the params: 9.153216
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06061969
Train loss (w/o reg) on all data: 0.052027013
Test loss (w/o reg) on all data: 0.040329598
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6786585e-06
Norm of the params: 13.109294
              Random: fixed   4 labels. Loss 0.04033. Accuracy 0.989.
### Flips: 52, rs: 15, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172966
Test loss (w/o reg) on all data: 0.012055162
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1382273e-07
Norm of the params: 9.1532
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729667
Test loss (w/o reg) on all data: 0.012055206
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.8667605e-08
Norm of the params: 9.1532
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05739828
Train loss (w/o reg) on all data: 0.04862479
Test loss (w/o reg) on all data: 0.043177336
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5842478e-06
Norm of the params: 13.246499
              Random: fixed   5 labels. Loss 0.04318. Accuracy 0.985.
### Flips: 52, rs: 15, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729753
Test loss (w/o reg) on all data: 0.012054678
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6767723e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729749
Test loss (w/o reg) on all data: 0.012054725
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4127802e-07
Norm of the params: 9.15319
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05574422
Train loss (w/o reg) on all data: 0.04806742
Test loss (w/o reg) on all data: 0.04636057
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2803016e-05
Norm of the params: 12.3909645
              Random: fixed   7 labels. Loss 0.04636. Accuracy 0.985.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07427042
Train loss (w/o reg) on all data: 0.06440906
Test loss (w/o reg) on all data: 0.05538923
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.291501e-06
Norm of the params: 14.043759
Flipped loss: 0.05539. Accuracy: 0.985
### Flips: 52, rs: 16, checks: 52
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172923
Test loss (w/o reg) on all data: 0.012054059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4021395e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729285
Test loss (w/o reg) on all data: 0.012053955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.346022e-07
Norm of the params: 9.153241
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07427042
Train loss (w/o reg) on all data: 0.06441004
Test loss (w/o reg) on all data: 0.055395085
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.8004748e-06
Norm of the params: 14.043064
              Random: fixed   0 labels. Loss 0.05540. Accuracy 0.985.
### Flips: 52, rs: 16, checks: 104
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729595
Test loss (w/o reg) on all data: 0.012054786
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.92074e-08
Norm of the params: 9.153207
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.012054675
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.397011e-07
Norm of the params: 9.153204
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07427041
Train loss (w/o reg) on all data: 0.064410046
Test loss (w/o reg) on all data: 0.055393685
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.6996724e-06
Norm of the params: 14.043055
              Random: fixed   0 labels. Loss 0.05539. Accuracy 0.985.
### Flips: 52, rs: 16, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728908
Test loss (w/o reg) on all data: 0.012054821
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5790323e-07
Norm of the params: 9.15328
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172892
Test loss (w/o reg) on all data: 0.012054867
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6929319e-07
Norm of the params: 9.153279
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07427043
Train loss (w/o reg) on all data: 0.06441048
Test loss (w/o reg) on all data: 0.05539614
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6668533e-05
Norm of the params: 14.042753
              Random: fixed   0 labels. Loss 0.05540. Accuracy 0.985.
### Flips: 52, rs: 16, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729502
Test loss (w/o reg) on all data: 0.012054127
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.005625e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217295
Test loss (w/o reg) on all data: 0.012054231
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2907675e-07
Norm of the params: 9.153217
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07345395
Train loss (w/o reg) on all data: 0.0638901
Test loss (w/o reg) on all data: 0.05399196
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.0398987e-06
Norm of the params: 13.830291
              Random: fixed   1 labels. Loss 0.05399. Accuracy 0.985.
### Flips: 52, rs: 16, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729558
Test loss (w/o reg) on all data: 0.012054783
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2472506e-07
Norm of the params: 9.153211
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012054821
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0406836e-07
Norm of the params: 9.153211
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06830854
Train loss (w/o reg) on all data: 0.057794895
Test loss (w/o reg) on all data: 0.053406328
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.978358e-06
Norm of the params: 14.500792
              Random: fixed   3 labels. Loss 0.05341. Accuracy 0.989.
### Flips: 52, rs: 16, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729644
Test loss (w/o reg) on all data: 0.012054877
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.491177e-07
Norm of the params: 9.1532
     Influence (LOO): fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.012054838
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8863883e-08
Norm of the params: 9.1532
                Loss: fixed  27 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061443187
Train loss (w/o reg) on all data: 0.051299978
Test loss (w/o reg) on all data: 0.048567694
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.748007e-06
Norm of the params: 14.243042
              Random: fixed   6 labels. Loss 0.04857. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073733345
Train loss (w/o reg) on all data: 0.063986175
Test loss (w/o reg) on all data: 0.033074763
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2388459e-05
Norm of the params: 13.962215
Flipped loss: 0.03307. Accuracy: 0.992
### Flips: 52, rs: 17, checks: 52
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729053
Test loss (w/o reg) on all data: 0.012055001
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0448459e-07
Norm of the params: 9.153267
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729064
Test loss (w/o reg) on all data: 0.012055035
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3104443e-07
Norm of the params: 9.153267
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.071981594
Train loss (w/o reg) on all data: 0.06237728
Test loss (w/o reg) on all data: 0.032536894
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.470265e-06
Norm of the params: 13.859515
              Random: fixed   1 labels. Loss 0.03254. Accuracy 0.992.
### Flips: 52, rs: 17, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172947
Test loss (w/o reg) on all data: 0.012056462
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.279878e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.00217295
Test loss (w/o reg) on all data: 0.012056567
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9945436e-07
Norm of the params: 9.153217
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06914311
Train loss (w/o reg) on all data: 0.0601877
Test loss (w/o reg) on all data: 0.030675793
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.8971843e-06
Norm of the params: 13.383128
              Random: fixed   3 labels. Loss 0.03068. Accuracy 0.996.
### Flips: 52, rs: 17, checks: 156
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021727574
Test loss (w/o reg) on all data: 0.012056149
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2102475e-07
Norm of the params: 9.153426
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021727658
Test loss (w/o reg) on all data: 0.012055847
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.9366364e-07
Norm of the params: 9.153419
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0691431
Train loss (w/o reg) on all data: 0.06018736
Test loss (w/o reg) on all data: 0.030676251
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4502289e-06
Norm of the params: 13.383379
              Random: fixed   3 labels. Loss 0.03068. Accuracy 0.996.
### Flips: 52, rs: 17, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729497
Test loss (w/o reg) on all data: 0.012055191
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0470005e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729516
Test loss (w/o reg) on all data: 0.01205512
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.677115e-07
Norm of the params: 9.153216
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069143094
Train loss (w/o reg) on all data: 0.060188964
Test loss (w/o reg) on all data: 0.030674286
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.37023835e-05
Norm of the params: 13.382176
              Random: fixed   3 labels. Loss 0.03067. Accuracy 0.996.
### Flips: 52, rs: 17, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729271
Test loss (w/o reg) on all data: 0.01205569
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.259369e-07
Norm of the params: 9.153241
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729285
Test loss (w/o reg) on all data: 0.012055605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.76992e-07
Norm of the params: 9.153241
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058790527
Train loss (w/o reg) on all data: 0.04900043
Test loss (w/o reg) on all data: 0.0290519
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5197156e-06
Norm of the params: 13.992924
              Random: fixed   6 labels. Loss 0.02905. Accuracy 0.992.
### Flips: 52, rs: 17, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728436
Test loss (w/o reg) on all data: 0.012056205
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2103215e-07
Norm of the params: 9.153334
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728445
Test loss (w/o reg) on all data: 0.012056273
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0928888e-07
Norm of the params: 9.153331
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058790535
Train loss (w/o reg) on all data: 0.049000762
Test loss (w/o reg) on all data: 0.02904883
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4267201e-06
Norm of the params: 13.992692
              Random: fixed   6 labels. Loss 0.02905. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086328626
Train loss (w/o reg) on all data: 0.07711542
Test loss (w/o reg) on all data: 0.059677776
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4136947e-05
Norm of the params: 13.574394
Flipped loss: 0.05968. Accuracy: 0.985
### Flips: 52, rs: 18, checks: 52
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007995633
Train loss (w/o reg) on all data: 0.0031796473
Test loss (w/o reg) on all data: 0.010465758
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0178092e-06
Norm of the params: 9.8142605
     Influence (LOO): fixed  31 labels. Loss 0.01047. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172987
Test loss (w/o reg) on all data: 0.012055111
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3824534e-07
Norm of the params: 9.153175
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08336727
Train loss (w/o reg) on all data: 0.073779956
Test loss (w/o reg) on all data: 0.05941116
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2688293e-05
Norm of the params: 13.84725
              Random: fixed   1 labels. Loss 0.05941. Accuracy 0.985.
### Flips: 52, rs: 18, checks: 104
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728782
Test loss (w/o reg) on all data: 0.01205408
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0405199e-07
Norm of the params: 9.1532955
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728848
Test loss (w/o reg) on all data: 0.012054246
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7938416e-07
Norm of the params: 9.153287
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079923615
Train loss (w/o reg) on all data: 0.07118505
Test loss (w/o reg) on all data: 0.047991037
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.396109e-06
Norm of the params: 13.220109
              Random: fixed   3 labels. Loss 0.04799. Accuracy 0.992.
### Flips: 52, rs: 18, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172933
Test loss (w/o reg) on all data: 0.012054631
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5080308e-07
Norm of the params: 9.153235
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729334
Test loss (w/o reg) on all data: 0.012054581
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5878904e-07
Norm of the params: 9.153235
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07992361
Train loss (w/o reg) on all data: 0.07118559
Test loss (w/o reg) on all data: 0.047983743
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.335285e-06
Norm of the params: 13.219698
              Random: fixed   3 labels. Loss 0.04798. Accuracy 0.992.
### Flips: 52, rs: 18, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728922
Test loss (w/o reg) on all data: 0.0120549
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8107274e-07
Norm of the params: 9.153278
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172895
Test loss (w/o reg) on all data: 0.012054833
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1826687e-07
Norm of the params: 9.153278
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077117614
Train loss (w/o reg) on all data: 0.068419956
Test loss (w/o reg) on all data: 0.044506088
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.043689e-06
Norm of the params: 13.189132
              Random: fixed   5 labels. Loss 0.04451. Accuracy 0.992.
### Flips: 52, rs: 18, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729134
Test loss (w/o reg) on all data: 0.012054549
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1825184e-07
Norm of the params: 9.153258
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729139
Test loss (w/o reg) on all data: 0.01205449
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2414885e-07
Norm of the params: 9.153257
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07460793
Train loss (w/o reg) on all data: 0.06602113
Test loss (w/o reg) on all data: 0.044226345
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.45739805e-05
Norm of the params: 13.104808
              Random: fixed   6 labels. Loss 0.04423. Accuracy 0.992.
### Flips: 52, rs: 18, checks: 312
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730491
Test loss (w/o reg) on all data: 0.012055514
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.099299e-07
Norm of the params: 9.153109
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730477
Test loss (w/o reg) on all data: 0.012055273
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4148145e-07
Norm of the params: 9.1531105
                Loss: fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06785675
Train loss (w/o reg) on all data: 0.058907386
Test loss (w/o reg) on all data: 0.0459401
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.318721e-06
Norm of the params: 13.378613
              Random: fixed   9 labels. Loss 0.04594. Accuracy 0.989.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06039442
Train loss (w/o reg) on all data: 0.050344273
Test loss (w/o reg) on all data: 0.046567842
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6549367e-06
Norm of the params: 14.177549
Flipped loss: 0.04657. Accuracy: 0.992
### Flips: 52, rs: 19, checks: 52
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012054565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3316296e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.0120546715
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.536959e-07
Norm of the params: 9.153182
                Loss: fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055204585
Train loss (w/o reg) on all data: 0.045343775
Test loss (w/o reg) on all data: 0.046212062
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8281388e-06
Norm of the params: 14.043371
              Random: fixed   2 labels. Loss 0.04621. Accuracy 0.989.
### Flips: 52, rs: 19, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.00217306
Test loss (w/o reg) on all data: 0.012055393
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.566725e-07
Norm of the params: 9.153094
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173042
Test loss (w/o reg) on all data: 0.012055261
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.715799e-07
Norm of the params: 9.153116
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049236212
Train loss (w/o reg) on all data: 0.0392663
Test loss (w/o reg) on all data: 0.04596684
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4721141e-06
Norm of the params: 14.120845
              Random: fixed   4 labels. Loss 0.04597. Accuracy 0.985.
### Flips: 52, rs: 19, checks: 156
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730682
Test loss (w/o reg) on all data: 0.012055273
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9948671e-07
Norm of the params: 9.153088
     Influence (LOO): fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730443
Test loss (w/o reg) on all data: 0.012054899
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0349693e-06
Norm of the params: 9.153112
                Loss: fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049236212
Train loss (w/o reg) on all data: 0.039267033
Test loss (w/o reg) on all data: 0.045965847
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.1201178e-06
Norm of the params: 14.120325
              Random: fixed   4 labels. Loss 0.04597. Accuracy 0.985.
### Flips: 52, rs: 19, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012054978
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8883719e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729677
Test loss (w/o reg) on all data: 0.012054851
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6794412e-07
Norm of the params: 9.153197
                Loss: fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04110671
Train loss (w/o reg) on all data: 0.031044308
Test loss (w/o reg) on all data: 0.043503348
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2367498e-05
Norm of the params: 14.186189
              Random: fixed   7 labels. Loss 0.04350. Accuracy 0.985.
### Flips: 52, rs: 19, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729486
Test loss (w/o reg) on all data: 0.012054924
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2549951e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729865
Test loss (w/o reg) on all data: 0.012055367
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5593729e-07
Norm of the params: 9.153175
                Loss: fixed  21 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0411067
Train loss (w/o reg) on all data: 0.031047158
Test loss (w/o reg) on all data: 0.043493323
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4893938e-06
Norm of the params: 14.184177
              Random: fixed   7 labels. Loss 0.04349. Accuracy 0.985.
### Flips: 52, rs: 19, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729765
Test loss (w/o reg) on all data: 0.012054764
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6722578e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729772
Test loss (w/o reg) on all data: 0.01205482
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.610048e-07
Norm of the params: 9.153189
                Loss: fixed  21 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03713069
Train loss (w/o reg) on all data: 0.027703146
Test loss (w/o reg) on all data: 0.038783908
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.8667523e-06
Norm of the params: 13.731384
              Random: fixed   9 labels. Loss 0.03878. Accuracy 0.989.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073955014
Train loss (w/o reg) on all data: 0.06465655
Test loss (w/o reg) on all data: 0.04775471
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7674562e-06
Norm of the params: 13.637059
Flipped loss: 0.04775. Accuracy: 0.985
### Flips: 52, rs: 20, checks: 52
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007984039
Train loss (w/o reg) on all data: 0.0031071662
Test loss (w/o reg) on all data: 0.014342133
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.1737257e-07
Norm of the params: 9.876106
     Influence (LOO): fixed  24 labels. Loss 0.01434. Accuracy 0.996.
Using normal model
LBFGS training took [25] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0070474558
Train loss (w/o reg) on all data: 0.0025446701
Test loss (w/o reg) on all data: 0.011712161
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.030621e-07
Norm of the params: 9.489769
                Loss: fixed  25 labels. Loss 0.01171. Accuracy 0.996.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073955014
Train loss (w/o reg) on all data: 0.064659506
Test loss (w/o reg) on all data: 0.047754858
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.891074e-05
Norm of the params: 13.634887
              Random: fixed   0 labels. Loss 0.04775. Accuracy 0.985.
### Flips: 52, rs: 20, checks: 104
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007535489
Train loss (w/o reg) on all data: 0.002880002
Test loss (w/o reg) on all data: 0.013884448
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2027507e-07
Norm of the params: 9.64934
     Influence (LOO): fixed  25 labels. Loss 0.01388. Accuracy 0.992.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730198
Test loss (w/o reg) on all data: 0.01205491
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2040584e-07
Norm of the params: 9.15314
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065345414
Train loss (w/o reg) on all data: 0.05554006
Test loss (w/o reg) on all data: 0.046305615
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.5897644e-06
Norm of the params: 14.003826
              Random: fixed   2 labels. Loss 0.04631. Accuracy 0.985.
### Flips: 52, rs: 20, checks: 156
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217299
Test loss (w/o reg) on all data: 0.01205498
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.114096e-07
Norm of the params: 9.153173
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729895
Test loss (w/o reg) on all data: 0.012054913
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0101671e-07
Norm of the params: 9.153173
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06087602
Train loss (w/o reg) on all data: 0.050983004
Test loss (w/o reg) on all data: 0.051146753
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.3778143e-06
Norm of the params: 14.06628
              Random: fixed   3 labels. Loss 0.05115. Accuracy 0.977.
### Flips: 52, rs: 20, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.012055022
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0826428e-07
Norm of the params: 9.153193
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012055059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0974627e-07
Norm of the params: 9.153193
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057761475
Train loss (w/o reg) on all data: 0.04757635
Test loss (w/o reg) on all data: 0.046553582
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0137088e-05
Norm of the params: 14.272438
              Random: fixed   4 labels. Loss 0.04655. Accuracy 0.985.
### Flips: 52, rs: 20, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021731104
Test loss (w/o reg) on all data: 0.012055743
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.570284e-07
Norm of the params: 9.153042
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730904
Test loss (w/o reg) on all data: 0.012055753
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.216107e-06
Norm of the params: 9.153065
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04801064
Train loss (w/o reg) on all data: 0.037445873
Test loss (w/o reg) on all data: 0.04694334
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0884794e-06
Norm of the params: 14.536003
              Random: fixed   7 labels. Loss 0.04694. Accuracy 0.989.
### Flips: 52, rs: 20, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173004
Test loss (w/o reg) on all data: 0.012054962
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2977906e-07
Norm of the params: 9.153157
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002173005
Test loss (w/o reg) on all data: 0.012055033
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5845708e-07
Norm of the params: 9.153158
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04801064
Train loss (w/o reg) on all data: 0.037443884
Test loss (w/o reg) on all data: 0.04694254
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4455467e-06
Norm of the params: 14.537369
              Random: fixed   7 labels. Loss 0.04694. Accuracy 0.989.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07224954
Train loss (w/o reg) on all data: 0.059723206
Test loss (w/o reg) on all data: 0.05555876
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1337256e-05
Norm of the params: 15.8280325
Flipped loss: 0.05556. Accuracy: 0.985
### Flips: 52, rs: 21, checks: 52
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009210212
Train loss (w/o reg) on all data: 0.0044515035
Test loss (w/o reg) on all data: 0.01335916
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0239862e-06
Norm of the params: 9.755725
     Influence (LOO): fixed  27 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021728957
Test loss (w/o reg) on all data: 0.012055959
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2105037e-07
Norm of the params: 9.153277
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07224953
Train loss (w/o reg) on all data: 0.059725612
Test loss (w/o reg) on all data: 0.055547204
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.485026e-06
Norm of the params: 15.826511
              Random: fixed   0 labels. Loss 0.05555. Accuracy 0.985.
### Flips: 52, rs: 21, checks: 104
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729816
Test loss (w/o reg) on all data: 0.012054209
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5618307e-06
Norm of the params: 9.153181
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172983
Test loss (w/o reg) on all data: 0.012054675
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.624202e-07
Norm of the params: 9.15318
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0705777
Train loss (w/o reg) on all data: 0.05858664
Test loss (w/o reg) on all data: 0.051469594
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.3478614e-06
Norm of the params: 15.486165
              Random: fixed   1 labels. Loss 0.05147. Accuracy 0.985.
### Flips: 52, rs: 21, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730422
Test loss (w/o reg) on all data: 0.012055229
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0905435e-07
Norm of the params: 9.153114
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730394
Test loss (w/o reg) on all data: 0.012055189
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4017327e-07
Norm of the params: 9.153116
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070577696
Train loss (w/o reg) on all data: 0.058583777
Test loss (w/o reg) on all data: 0.051478174
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0603237e-05
Norm of the params: 15.4880085
              Random: fixed   1 labels. Loss 0.05148. Accuracy 0.985.
### Flips: 52, rs: 21, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172901
Test loss (w/o reg) on all data: 0.012055447
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1709071e-07
Norm of the params: 9.153267
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729097
Test loss (w/o reg) on all data: 0.012055481
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8329767e-07
Norm of the params: 9.15326
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06807659
Train loss (w/o reg) on all data: 0.05647639
Test loss (w/o reg) on all data: 0.045053635
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.3191407e-06
Norm of the params: 15.231673
              Random: fixed   3 labels. Loss 0.04505. Accuracy 0.981.
### Flips: 52, rs: 21, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729532
Test loss (w/o reg) on all data: 0.012055289
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.019992e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729518
Test loss (w/o reg) on all data: 0.0120553365
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4376823e-07
Norm of the params: 9.153214
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06314259
Train loss (w/o reg) on all data: 0.051870294
Test loss (w/o reg) on all data: 0.040241797
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.4820035e-06
Norm of the params: 15.014855
              Random: fixed   5 labels. Loss 0.04024. Accuracy 0.981.
### Flips: 52, rs: 21, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.00217303
Test loss (w/o reg) on all data: 0.012055457
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0052076e-07
Norm of the params: 9.15313
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.00217303
Test loss (w/o reg) on all data: 0.012055495
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6394105e-07
Norm of the params: 9.1531315
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06314258
Train loss (w/o reg) on all data: 0.051871333
Test loss (w/o reg) on all data: 0.040241342
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.6090884e-06
Norm of the params: 15.014159
              Random: fixed   5 labels. Loss 0.04024. Accuracy 0.981.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07337413
Train loss (w/o reg) on all data: 0.06439422
Test loss (w/o reg) on all data: 0.07398974
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0897703e-05
Norm of the params: 13.401426
Flipped loss: 0.07399. Accuracy: 0.985
### Flips: 52, rs: 22, checks: 52
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173024
Test loss (w/o reg) on all data: 0.012054709
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9463494e-07
Norm of the params: 9.153135
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730224
Test loss (w/o reg) on all data: 0.012054655
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2115287e-07
Norm of the params: 9.153138
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07337413
Train loss (w/o reg) on all data: 0.064394444
Test loss (w/o reg) on all data: 0.073995076
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7333396e-06
Norm of the params: 13.401261
              Random: fixed   0 labels. Loss 0.07400. Accuracy 0.985.
### Flips: 52, rs: 22, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729323
Test loss (w/o reg) on all data: 0.012054466
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2748335e-07
Norm of the params: 9.1532345
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729344
Test loss (w/o reg) on all data: 0.012054412
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3770413e-07
Norm of the params: 9.1532345
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065384395
Train loss (w/o reg) on all data: 0.05668432
Test loss (w/o reg) on all data: 0.07411795
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7878049e-06
Norm of the params: 13.190966
              Random: fixed   3 labels. Loss 0.07412. Accuracy 0.985.
### Flips: 52, rs: 22, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021728831
Test loss (w/o reg) on all data: 0.01205421
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.017324e-07
Norm of the params: 9.153288
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728857
Test loss (w/o reg) on all data: 0.012054125
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.763059e-07
Norm of the params: 9.153287
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06550744
Train loss (w/o reg) on all data: 0.057177197
Test loss (w/o reg) on all data: 0.06383844
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.0872274e-06
Norm of the params: 12.90755
              Random: fixed   4 labels. Loss 0.06384. Accuracy 0.985.
### Flips: 52, rs: 22, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729218
Test loss (w/o reg) on all data: 0.012054192
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.023516e-07
Norm of the params: 9.15325
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729206
Test loss (w/o reg) on all data: 0.012054112
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1183427e-07
Norm of the params: 9.15325
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05594542
Train loss (w/o reg) on all data: 0.04699955
Test loss (w/o reg) on all data: 0.056469448
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.2797354e-06
Norm of the params: 13.3759985
              Random: fixed   8 labels. Loss 0.05647. Accuracy 0.981.
### Flips: 52, rs: 22, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729472
Test loss (w/o reg) on all data: 0.012055033
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6611605e-07
Norm of the params: 9.15322
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729479
Test loss (w/o reg) on all data: 0.012055083
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7134592e-07
Norm of the params: 9.153219
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05360257
Train loss (w/o reg) on all data: 0.044664875
Test loss (w/o reg) on all data: 0.056196336
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9004938e-05
Norm of the params: 13.369885
              Random: fixed   9 labels. Loss 0.05620. Accuracy 0.981.
### Flips: 52, rs: 22, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729819
Test loss (w/o reg) on all data: 0.012055128
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.860439e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729807
Test loss (w/o reg) on all data: 0.012055087
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0176177e-07
Norm of the params: 9.153182
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044649463
Train loss (w/o reg) on all data: 0.035301857
Test loss (w/o reg) on all data: 0.057651527
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.2650927e-06
Norm of the params: 13.673043
              Random: fixed  11 labels. Loss 0.05765. Accuracy 0.977.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08187116
Train loss (w/o reg) on all data: 0.07298961
Test loss (w/o reg) on all data: 0.04776695
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.739865e-06
Norm of the params: 13.327824
Flipped loss: 0.04777. Accuracy: 0.989
### Flips: 52, rs: 23, checks: 52
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007890079
Train loss (w/o reg) on all data: 0.002941445
Test loss (w/o reg) on all data: 0.011453407
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1237858e-07
Norm of the params: 9.948502
     Influence (LOO): fixed  31 labels. Loss 0.01145. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007890079
Train loss (w/o reg) on all data: 0.0029414454
Test loss (w/o reg) on all data: 0.01145335
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.510754e-07
Norm of the params: 9.948502
                Loss: fixed  31 labels. Loss 0.01145. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08187115
Train loss (w/o reg) on all data: 0.07299032
Test loss (w/o reg) on all data: 0.047764253
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.887015e-06
Norm of the params: 13.32729
              Random: fixed   0 labels. Loss 0.04776. Accuracy 0.989.
### Flips: 52, rs: 23, checks: 104
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007890079
Train loss (w/o reg) on all data: 0.002941488
Test loss (w/o reg) on all data: 0.011453426
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.8565194e-08
Norm of the params: 9.948459
     Influence (LOO): fixed  31 labels. Loss 0.01145. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007890079
Train loss (w/o reg) on all data: 0.002941488
Test loss (w/o reg) on all data: 0.01145354
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9587474e-07
Norm of the params: 9.948459
                Loss: fixed  31 labels. Loss 0.01145. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07579014
Train loss (w/o reg) on all data: 0.06661957
Test loss (w/o reg) on all data: 0.039743755
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.34247e-06
Norm of the params: 13.542946
              Random: fixed   2 labels. Loss 0.03974. Accuracy 0.992.
### Flips: 52, rs: 23, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007890077
Train loss (w/o reg) on all data: 0.0029414096
Test loss (w/o reg) on all data: 0.0114535475
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.285977e-07
Norm of the params: 9.948536
     Influence (LOO): fixed  31 labels. Loss 0.01145. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00789008
Train loss (w/o reg) on all data: 0.0029414124
Test loss (w/o reg) on all data: 0.011453428
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.443828e-07
Norm of the params: 9.948536
                Loss: fixed  31 labels. Loss 0.01145. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064505845
Train loss (w/o reg) on all data: 0.05494975
Test loss (w/o reg) on all data: 0.033814657
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.02788845e-05
Norm of the params: 13.824687
              Random: fixed   6 labels. Loss 0.03381. Accuracy 0.992.
### Flips: 52, rs: 23, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729707
Test loss (w/o reg) on all data: 0.012055053
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4460919e-07
Norm of the params: 9.153195
     Influence (LOO): fixed  32 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007890079
Train loss (w/o reg) on all data: 0.002941494
Test loss (w/o reg) on all data: 0.011453683
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8479968e-07
Norm of the params: 9.948452
                Loss: fixed  31 labels. Loss 0.01145. Accuracy 0.992.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064505845
Train loss (w/o reg) on all data: 0.054950766
Test loss (w/o reg) on all data: 0.033810515
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.795115e-06
Norm of the params: 13.823952
              Random: fixed   6 labels. Loss 0.03381. Accuracy 0.992.
### Flips: 52, rs: 23, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729365
Test loss (w/o reg) on all data: 0.012054827
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6530572e-07
Norm of the params: 9.153231
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729362
Test loss (w/o reg) on all data: 0.012054728
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.373587e-07
Norm of the params: 9.153229
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059578635
Train loss (w/o reg) on all data: 0.05080869
Test loss (w/o reg) on all data: 0.031923257
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.505299e-06
Norm of the params: 13.243823
              Random: fixed   8 labels. Loss 0.03192. Accuracy 0.992.
### Flips: 52, rs: 23, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.0120542245
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8781832e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.012054154
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7935094e-07
Norm of the params: 9.153198
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056211285
Train loss (w/o reg) on all data: 0.04777755
Test loss (w/o reg) on all data: 0.02881492
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.593711e-06
Norm of the params: 12.987483
              Random: fixed   9 labels. Loss 0.02881. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07063569
Train loss (w/o reg) on all data: 0.058569394
Test loss (w/o reg) on all data: 0.057694767
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.869797e-05
Norm of the params: 15.534669
Flipped loss: 0.05769. Accuracy: 0.977
### Flips: 52, rs: 24, checks: 52
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010996196
Train loss (w/o reg) on all data: 0.0048132795
Test loss (w/o reg) on all data: 0.017607993
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.223401e-07
Norm of the params: 11.120177
     Influence (LOO): fixed  25 labels. Loss 0.01761. Accuracy 0.992.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074153496
Train loss (w/o reg) on all data: 0.0026646669
Test loss (w/o reg) on all data: 0.016170904
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4603654e-07
Norm of the params: 9.747494
                Loss: fixed  27 labels. Loss 0.01617. Accuracy 0.992.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067070924
Train loss (w/o reg) on all data: 0.054760166
Test loss (w/o reg) on all data: 0.05963447
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1364619e-05
Norm of the params: 15.691244
              Random: fixed   1 labels. Loss 0.05963. Accuracy 0.973.
### Flips: 52, rs: 24, checks: 104
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172927
Test loss (w/o reg) on all data: 0.01205426
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0613555e-07
Norm of the params: 9.153243
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172927
Test loss (w/o reg) on all data: 0.012054334
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5325272e-07
Norm of the params: 9.153242
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060847618
Train loss (w/o reg) on all data: 0.048677064
Test loss (w/o reg) on all data: 0.05607696
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0490756e-05
Norm of the params: 15.601635
              Random: fixed   4 labels. Loss 0.05608. Accuracy 0.973.
### Flips: 52, rs: 24, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729658
Test loss (w/o reg) on all data: 0.012054196
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7103096e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729677
Test loss (w/o reg) on all data: 0.012054149
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.831515e-07
Norm of the params: 9.153197
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055358205
Train loss (w/o reg) on all data: 0.042913936
Test loss (w/o reg) on all data: 0.065838724
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.6676252e-06
Norm of the params: 15.776101
              Random: fixed   5 labels. Loss 0.06584. Accuracy 0.973.
### Flips: 52, rs: 24, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729544
Test loss (w/o reg) on all data: 0.012054337
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9296371e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729553
Test loss (w/o reg) on all data: 0.012054268
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.432594e-07
Norm of the params: 9.153211
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047723863
Train loss (w/o reg) on all data: 0.035389096
Test loss (w/o reg) on all data: 0.070363194
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5207362e-06
Norm of the params: 15.70654
              Random: fixed   8 labels. Loss 0.07036. Accuracy 0.973.
### Flips: 52, rs: 24, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730387
Test loss (w/o reg) on all data: 0.012053768
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.960223e-06
Norm of the params: 9.15312
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730345
Test loss (w/o reg) on all data: 0.012054098
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8156225e-07
Norm of the params: 9.153124
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04460459
Train loss (w/o reg) on all data: 0.03288054
Test loss (w/o reg) on all data: 0.06372715
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.3283295e-06
Norm of the params: 15.312772
              Random: fixed  10 labels. Loss 0.06373. Accuracy 0.981.
### Flips: 52, rs: 24, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729383
Test loss (w/o reg) on all data: 0.012054409
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.147237e-07
Norm of the params: 9.153231
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729388
Test loss (w/o reg) on all data: 0.0120543605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.06706814e-07
Norm of the params: 9.15323
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04189108
Train loss (w/o reg) on all data: 0.030402329
Test loss (w/o reg) on all data: 0.05787719
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.4540994e-07
Norm of the params: 15.158331
              Random: fixed  11 labels. Loss 0.05788. Accuracy 0.981.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06433459
Train loss (w/o reg) on all data: 0.053371757
Test loss (w/o reg) on all data: 0.03549252
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.107111e-06
Norm of the params: 14.807321
Flipped loss: 0.03549. Accuracy: 0.989
### Flips: 52, rs: 25, checks: 52
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729919
Test loss (w/o reg) on all data: 0.012055024
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0195034e-07
Norm of the params: 9.1531725
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729898
Test loss (w/o reg) on all data: 0.012054978
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1491582e-07
Norm of the params: 9.153173
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059805147
Train loss (w/o reg) on all data: 0.04933963
Test loss (w/o reg) on all data: 0.034386855
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2172903e-05
Norm of the params: 14.467563
              Random: fixed   1 labels. Loss 0.03439. Accuracy 0.992.
### Flips: 52, rs: 25, checks: 104
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002172954
Test loss (w/o reg) on all data: 0.0120556755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.127617e-07
Norm of the params: 9.153211
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729567
Test loss (w/o reg) on all data: 0.01205577
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3850452e-07
Norm of the params: 9.153211
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059805147
Train loss (w/o reg) on all data: 0.04934155
Test loss (w/o reg) on all data: 0.034386974
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3604678e-06
Norm of the params: 14.466235
              Random: fixed   1 labels. Loss 0.03439. Accuracy 0.992.
### Flips: 52, rs: 25, checks: 156
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729462
Test loss (w/o reg) on all data: 0.01205518
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9249095e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729441
Test loss (w/o reg) on all data: 0.012055313
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8361617e-07
Norm of the params: 9.153221
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05867812
Train loss (w/o reg) on all data: 0.04851319
Test loss (w/o reg) on all data: 0.03142184
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7709594e-05
Norm of the params: 14.258282
              Random: fixed   2 labels. Loss 0.03142. Accuracy 0.992.
### Flips: 52, rs: 25, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728803
Test loss (w/o reg) on all data: 0.012055446
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7719113e-07
Norm of the params: 9.153294
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728843
Test loss (w/o reg) on all data: 0.012055352
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2733087e-07
Norm of the params: 9.15329
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047624413
Train loss (w/o reg) on all data: 0.037458126
Test loss (w/o reg) on all data: 0.029755611
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6995417e-06
Norm of the params: 14.259234
              Random: fixed   6 labels. Loss 0.02976. Accuracy 0.989.
### Flips: 52, rs: 25, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729537
Test loss (w/o reg) on all data: 0.012055836
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9645137e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172959
Test loss (w/o reg) on all data: 0.012055497
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.138116e-07
Norm of the params: 9.153207
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04517588
Train loss (w/o reg) on all data: 0.034633107
Test loss (w/o reg) on all data: 0.032853227
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0021512e-06
Norm of the params: 14.520865
              Random: fixed   7 labels. Loss 0.03285. Accuracy 0.992.
### Flips: 52, rs: 25, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172985
Test loss (w/o reg) on all data: 0.012055257
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.080625e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729837
Test loss (w/o reg) on all data: 0.012055176
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4880257e-07
Norm of the params: 9.153177
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032854255
Train loss (w/o reg) on all data: 0.024328252
Test loss (w/o reg) on all data: 0.01838782
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.32258e-06
Norm of the params: 13.058333
              Random: fixed  11 labels. Loss 0.01839. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08060047
Train loss (w/o reg) on all data: 0.07130022
Test loss (w/o reg) on all data: 0.052982036
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.285572e-06
Norm of the params: 13.638366
Flipped loss: 0.05298. Accuracy: 0.989
### Flips: 52, rs: 26, checks: 52
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012053513
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.55107e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729684
Test loss (w/o reg) on all data: 0.012053367
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8698597e-07
Norm of the params: 9.153197
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08060048
Train loss (w/o reg) on all data: 0.07129781
Test loss (w/o reg) on all data: 0.05298363
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9483012e-06
Norm of the params: 13.640138
              Random: fixed   0 labels. Loss 0.05298. Accuracy 0.989.
### Flips: 52, rs: 26, checks: 104
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012055125
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.601695e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012055224
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4330145e-07
Norm of the params: 9.153184
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075880006
Train loss (w/o reg) on all data: 0.066592045
Test loss (w/o reg) on all data: 0.053044233
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.918526e-06
Norm of the params: 13.629352
              Random: fixed   2 labels. Loss 0.05304. Accuracy 0.981.
### Flips: 52, rs: 26, checks: 156
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729814
Test loss (w/o reg) on all data: 0.012055187
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9890167e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012055305
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9343133e-07
Norm of the params: 9.153184
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074029855
Train loss (w/o reg) on all data: 0.06511275
Test loss (w/o reg) on all data: 0.051880825
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.2241024e-06
Norm of the params: 13.354483
              Random: fixed   3 labels. Loss 0.05188. Accuracy 0.985.
### Flips: 52, rs: 26, checks: 208
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729413
Test loss (w/o reg) on all data: 0.0120552415
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.890976e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729416
Test loss (w/o reg) on all data: 0.0120552825
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2749797e-07
Norm of the params: 9.153227
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07402986
Train loss (w/o reg) on all data: 0.065114506
Test loss (w/o reg) on all data: 0.0518725
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5396105e-05
Norm of the params: 13.353169
              Random: fixed   3 labels. Loss 0.05187. Accuracy 0.985.
### Flips: 52, rs: 26, checks: 260
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.012054463
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0364794e-07
Norm of the params: 9.153195
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729697
Test loss (w/o reg) on all data: 0.0120544145
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.129097e-07
Norm of the params: 9.153194
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06413855
Train loss (w/o reg) on all data: 0.05612996
Test loss (w/o reg) on all data: 0.04339369
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.581153e-06
Norm of the params: 12.655898
              Random: fixed   7 labels. Loss 0.04339. Accuracy 0.989.
### Flips: 52, rs: 26, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729697
Test loss (w/o reg) on all data: 0.012054687
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5035928e-07
Norm of the params: 9.153196
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729688
Test loss (w/o reg) on all data: 0.012054634
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2805834e-07
Norm of the params: 9.153196
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056120608
Train loss (w/o reg) on all data: 0.04852554
Test loss (w/o reg) on all data: 0.0428635
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.538556e-06
Norm of the params: 12.324829
              Random: fixed  10 labels. Loss 0.04286. Accuracy 0.989.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090253085
Train loss (w/o reg) on all data: 0.081074424
Test loss (w/o reg) on all data: 0.05659106
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.437561e-06
Norm of the params: 13.548918
Flipped loss: 0.05659. Accuracy: 0.981
### Flips: 52, rs: 27, checks: 52
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021728803
Test loss (w/o reg) on all data: 0.012054749
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0839975e-07
Norm of the params: 9.1532955
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728817
Test loss (w/o reg) on all data: 0.012054815
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1009935e-07
Norm of the params: 9.153292
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08391587
Train loss (w/o reg) on all data: 0.07449336
Test loss (w/o reg) on all data: 0.05879759
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.3575363e-06
Norm of the params: 13.72771
              Random: fixed   2 labels. Loss 0.05880. Accuracy 0.981.
### Flips: 52, rs: 27, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729649
Test loss (w/o reg) on all data: 0.012054837
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9642383e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729649
Test loss (w/o reg) on all data: 0.012054975
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3596913e-07
Norm of the params: 9.153201
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075813636
Train loss (w/o reg) on all data: 0.06698642
Test loss (w/o reg) on all data: 0.05311555
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6836522e-05
Norm of the params: 13.286997
              Random: fixed   6 labels. Loss 0.05312. Accuracy 0.977.
### Flips: 52, rs: 27, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172959
Test loss (w/o reg) on all data: 0.012054905
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4691706e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729583
Test loss (w/o reg) on all data: 0.012054823
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7904812e-07
Norm of the params: 9.153208
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07581363
Train loss (w/o reg) on all data: 0.06698686
Test loss (w/o reg) on all data: 0.0531173
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.566188e-06
Norm of the params: 13.286661
              Random: fixed   6 labels. Loss 0.05312. Accuracy 0.977.
### Flips: 52, rs: 27, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729725
Test loss (w/o reg) on all data: 0.0120546995
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6415412e-07
Norm of the params: 9.153192
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729728
Test loss (w/o reg) on all data: 0.012054745
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6391004e-07
Norm of the params: 9.153192
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06529155
Train loss (w/o reg) on all data: 0.056037337
Test loss (w/o reg) on all data: 0.05026618
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.1221025e-06
Norm of the params: 13.604568
              Random: fixed   9 labels. Loss 0.05027. Accuracy 0.981.
### Flips: 52, rs: 27, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729309
Test loss (w/o reg) on all data: 0.012055221
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9578307e-07
Norm of the params: 9.153239
     Influence (LOO): fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729302
Test loss (w/o reg) on all data: 0.012055134
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1657532e-07
Norm of the params: 9.153238
                Loss: fixed  31 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06245017
Train loss (w/o reg) on all data: 0.05385357
Test loss (w/o reg) on all data: 0.047986627
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.3475753e-06
Norm of the params: 13.112283
              Random: fixed  11 labels. Loss 0.04799. Accuracy 0.981.
### Flips: 52, rs: 27, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730706
Test loss (w/o reg) on all data: 0.012054288
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.807305e-07
Norm of the params: 9.153085
     Influence (LOO): fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730703
Test loss (w/o reg) on all data: 0.012054412
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4906188e-07
Norm of the params: 9.153085
                Loss: fixed  31 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055892713
Train loss (w/o reg) on all data: 0.046484414
Test loss (w/o reg) on all data: 0.03882031
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.619495e-06
Norm of the params: 13.7173605
              Random: fixed  13 labels. Loss 0.03882. Accuracy 0.989.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07320327
Train loss (w/o reg) on all data: 0.061488796
Test loss (w/o reg) on all data: 0.07143547
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.8327394e-06
Norm of the params: 15.306521
Flipped loss: 0.07144. Accuracy: 0.981
### Flips: 52, rs: 28, checks: 52
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729572
Test loss (w/o reg) on all data: 0.012054311
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6304158e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076775756
Train loss (w/o reg) on all data: 0.002824792
Test loss (w/o reg) on all data: 0.014334082
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8629606e-07
Norm of the params: 9.851684
                Loss: fixed  27 labels. Loss 0.01433. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07320327
Train loss (w/o reg) on all data: 0.061490014
Test loss (w/o reg) on all data: 0.07143882
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.75174e-06
Norm of the params: 15.305723
              Random: fixed   0 labels. Loss 0.07144. Accuracy 0.981.
### Flips: 52, rs: 28, checks: 104
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172987
Test loss (w/o reg) on all data: 0.012054571
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3911758e-07
Norm of the params: 9.153176
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729853
Test loss (w/o reg) on all data: 0.012054623
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.12604404e-07
Norm of the params: 9.153177
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07143117
Train loss (w/o reg) on all data: 0.059917986
Test loss (w/o reg) on all data: 0.07213735
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.6179038e-06
Norm of the params: 15.174441
              Random: fixed   1 labels. Loss 0.07214. Accuracy 0.977.
### Flips: 52, rs: 28, checks: 156
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729595
Test loss (w/o reg) on all data: 0.012054878
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.373462e-07
Norm of the params: 9.153207
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012054893
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.308225e-07
Norm of the params: 9.153209
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07143117
Train loss (w/o reg) on all data: 0.05991623
Test loss (w/o reg) on all data: 0.07214435
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1452305e-05
Norm of the params: 15.175594
              Random: fixed   1 labels. Loss 0.07214. Accuracy 0.977.
### Flips: 52, rs: 28, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172999
Test loss (w/o reg) on all data: 0.012054632
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2952886e-07
Norm of the params: 9.153163
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729989
Test loss (w/o reg) on all data: 0.012054612
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.719713e-08
Norm of the params: 9.153164
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064045146
Train loss (w/o reg) on all data: 0.052421965
Test loss (w/o reg) on all data: 0.068155706
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.0657875e-06
Norm of the params: 15.246757
              Random: fixed   4 labels. Loss 0.06816. Accuracy 0.977.
### Flips: 52, rs: 28, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172948
Test loss (w/o reg) on all data: 0.012054822
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8043441e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172948
Test loss (w/o reg) on all data: 0.012054759
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1169886e-07
Norm of the params: 9.153217
                Loss: fixed  28 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064045146
Train loss (w/o reg) on all data: 0.0524183
Test loss (w/o reg) on all data: 0.0681626
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.1525545e-06
Norm of the params: 15.249162
              Random: fixed   4 labels. Loss 0.06816. Accuracy 0.977.
### Flips: 52, rs: 28, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172921
Test loss (w/o reg) on all data: 0.012055376
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3020205e-07
Norm of the params: 9.15325
     Influence (LOO): fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172922
Test loss (w/o reg) on all data: 0.012055261
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4401472e-07
Norm of the params: 9.153248
                Loss: fixed  28 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06283244
Train loss (w/o reg) on all data: 0.05182077
Test loss (w/o reg) on all data: 0.06765716
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0380537e-05
Norm of the params: 14.840262
              Random: fixed   5 labels. Loss 0.06766. Accuracy 0.981.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08799418
Train loss (w/o reg) on all data: 0.078820124
Test loss (w/o reg) on all data: 0.052366458
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.4848564e-06
Norm of the params: 13.545522
Flipped loss: 0.05237. Accuracy: 0.985
### Flips: 52, rs: 29, checks: 52
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729553
Test loss (w/o reg) on all data: 0.0120552825
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1472552e-07
Norm of the params: 9.15321
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729541
Test loss (w/o reg) on all data: 0.012055232
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.09363384e-07
Norm of the params: 9.153211
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086647026
Train loss (w/o reg) on all data: 0.07740467
Test loss (w/o reg) on all data: 0.049434494
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.2320203e-06
Norm of the params: 13.595846
              Random: fixed   1 labels. Loss 0.04943. Accuracy 0.985.
### Flips: 52, rs: 29, checks: 104
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172991
Test loss (w/o reg) on all data: 0.012054586
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2681776e-07
Norm of the params: 9.153171
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012054546
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.9649194e-07
Norm of the params: 9.153185
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08270877
Train loss (w/o reg) on all data: 0.07287559
Test loss (w/o reg) on all data: 0.043136727
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.595727e-06
Norm of the params: 14.023681
              Random: fixed   2 labels. Loss 0.04314. Accuracy 0.992.
### Flips: 52, rs: 29, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728824
Test loss (w/o reg) on all data: 0.012054779
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2868171e-07
Norm of the params: 9.153289
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728845
Test loss (w/o reg) on all data: 0.012054739
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8646637e-07
Norm of the params: 9.153287
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079534404
Train loss (w/o reg) on all data: 0.06953554
Test loss (w/o reg) on all data: 0.040756892
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3290546e-06
Norm of the params: 14.141332
              Random: fixed   3 labels. Loss 0.04076. Accuracy 0.992.
### Flips: 52, rs: 29, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729523
Test loss (w/o reg) on all data: 0.012054692
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.617567e-07
Norm of the params: 9.153215
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729516
Test loss (w/o reg) on all data: 0.012054541
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5510804e-07
Norm of the params: 9.153215
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074971884
Train loss (w/o reg) on all data: 0.06610301
Test loss (w/o reg) on all data: 0.039943404
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.49487e-05
Norm of the params: 13.3183155
              Random: fixed   5 labels. Loss 0.03994. Accuracy 0.992.
### Flips: 52, rs: 29, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729607
Test loss (w/o reg) on all data: 0.012055233
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9879723e-07
Norm of the params: 9.153205
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172961
Test loss (w/o reg) on all data: 0.012055148
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1246265e-07
Norm of the params: 9.153205
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06922668
Train loss (w/o reg) on all data: 0.060321696
Test loss (w/o reg) on all data: 0.039623138
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.5405173e-06
Norm of the params: 13.3454
              Random: fixed   7 labels. Loss 0.03962. Accuracy 0.989.
### Flips: 52, rs: 29, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172925
Test loss (w/o reg) on all data: 0.012054217
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5820835e-07
Norm of the params: 9.153244
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172924
Test loss (w/o reg) on all data: 0.012054326
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6903422e-07
Norm of the params: 9.153244
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06185545
Train loss (w/o reg) on all data: 0.05235675
Test loss (w/o reg) on all data: 0.03809167
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.974992e-06
Norm of the params: 13.783106
              Random: fixed   9 labels. Loss 0.03809. Accuracy 0.985.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07434978
Train loss (w/o reg) on all data: 0.06645126
Test loss (w/o reg) on all data: 0.046006825
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2387422e-06
Norm of the params: 12.568632
Flipped loss: 0.04601. Accuracy: 0.992
### Flips: 52, rs: 30, checks: 52
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172864
Test loss (w/o reg) on all data: 0.012055039
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.683491e-07
Norm of the params: 9.153311
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728636
Test loss (w/o reg) on all data: 0.012054947
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.107545e-07
Norm of the params: 9.153309
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06947669
Train loss (w/o reg) on all data: 0.061003406
Test loss (w/o reg) on all data: 0.047587916
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.2914394e-06
Norm of the params: 13.017895
              Random: fixed   1 labels. Loss 0.04759. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 104
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729467
Test loss (w/o reg) on all data: 0.01205472
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7754135e-07
Norm of the params: 9.15322
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729465
Test loss (w/o reg) on all data: 0.012054772
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4448291e-07
Norm of the params: 9.15322
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06210205
Train loss (w/o reg) on all data: 0.05320713
Test loss (w/o reg) on all data: 0.04487778
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1961205e-05
Norm of the params: 13.337858
              Random: fixed   4 labels. Loss 0.04488. Accuracy 0.989.
### Flips: 52, rs: 30, checks: 156
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730217
Test loss (w/o reg) on all data: 0.012055303
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9394994e-07
Norm of the params: 9.153138
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730198
Test loss (w/o reg) on all data: 0.012055216
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.926519e-07
Norm of the params: 9.15314
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055724345
Train loss (w/o reg) on all data: 0.046598066
Test loss (w/o reg) on all data: 0.041764397
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.4880486e-06
Norm of the params: 13.510203
              Random: fixed   6 labels. Loss 0.04176. Accuracy 0.981.
### Flips: 52, rs: 30, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728016
Test loss (w/o reg) on all data: 0.012055276
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5063545e-07
Norm of the params: 9.153378
     Influence (LOO): fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728303
Test loss (w/o reg) on all data: 0.012055057
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.0161257e-07
Norm of the params: 9.153346
                Loss: fixed  25 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055724338
Train loss (w/o reg) on all data: 0.04659624
Test loss (w/o reg) on all data: 0.041763384
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.5552716e-06
Norm of the params: 13.51155
              Random: fixed   6 labels. Loss 0.04176. Accuracy 0.981.
### Flips: 52, rs: 30, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729309
Test loss (w/o reg) on all data: 0.012054994
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.342488e-07
Norm of the params: 9.153238
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729304
Test loss (w/o reg) on all data: 0.01205497
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.72111e-08
Norm of the params: 9.153237
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0465718
Train loss (w/o reg) on all data: 0.037013132
Test loss (w/o reg) on all data: 0.041857444
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7632293e-06
Norm of the params: 13.826545
              Random: fixed   9 labels. Loss 0.04186. Accuracy 0.985.
### Flips: 52, rs: 30, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729055
Test loss (w/o reg) on all data: 0.012054894
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4631328e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172907
Test loss (w/o reg) on all data: 0.012054951
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5164828e-07
Norm of the params: 9.153264
                Loss: fixed  25 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04217568
Train loss (w/o reg) on all data: 0.032824032
Test loss (w/o reg) on all data: 0.036546946
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6281132e-06
Norm of the params: 13.675999
              Random: fixed  11 labels. Loss 0.03655. Accuracy 0.985.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08051206
Train loss (w/o reg) on all data: 0.07119796
Test loss (w/o reg) on all data: 0.05943061
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0494377e-05
Norm of the params: 13.648518
Flipped loss: 0.05943. Accuracy: 0.989
### Flips: 52, rs: 31, checks: 52
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069025187
Train loss (w/o reg) on all data: 0.0024108111
Test loss (w/o reg) on all data: 0.011627396
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2873821e-06
Norm of the params: 9.478088
     Influence (LOO): fixed  29 labels. Loss 0.01163. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073309587
Train loss (w/o reg) on all data: 0.0025893038
Test loss (w/o reg) on all data: 0.014859154
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9630835e-07
Norm of the params: 9.738229
                Loss: fixed  28 labels. Loss 0.01486. Accuracy 0.989.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08051207
Train loss (w/o reg) on all data: 0.07119692
Test loss (w/o reg) on all data: 0.05942599
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8188807e-06
Norm of the params: 13.649285
              Random: fixed   0 labels. Loss 0.05943. Accuracy 0.989.
### Flips: 52, rs: 31, checks: 104
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.012054831
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8719154e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069025178
Train loss (w/o reg) on all data: 0.002410669
Test loss (w/o reg) on all data: 0.011626613
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.391533e-08
Norm of the params: 9.478236
                Loss: fixed  29 labels. Loss 0.01163. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07515204
Train loss (w/o reg) on all data: 0.06659786
Test loss (w/o reg) on all data: 0.05468509
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5560536e-06
Norm of the params: 13.079892
              Random: fixed   2 labels. Loss 0.05469. Accuracy 0.981.
### Flips: 52, rs: 31, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729106
Test loss (w/o reg) on all data: 0.012055309
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.852239e-08
Norm of the params: 9.15326
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069025178
Train loss (w/o reg) on all data: 0.0024106586
Test loss (w/o reg) on all data: 0.011626637
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.404259e-08
Norm of the params: 9.478249
                Loss: fixed  29 labels. Loss 0.01163. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06869682
Train loss (w/o reg) on all data: 0.059832495
Test loss (w/o reg) on all data: 0.047767222
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5856457e-06
Norm of the params: 13.3148985
              Random: fixed   5 labels. Loss 0.04777. Accuracy 0.985.
### Flips: 52, rs: 31, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729067
Test loss (w/o reg) on all data: 0.0120551605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9318585e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729078
Test loss (w/o reg) on all data: 0.012055225
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.986477e-07
Norm of the params: 9.153263
                Loss: fixed  30 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06701946
Train loss (w/o reg) on all data: 0.058303576
Test loss (w/o reg) on all data: 0.045224547
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.183431e-05
Norm of the params: 13.202942
              Random: fixed   6 labels. Loss 0.04522. Accuracy 0.989.
### Flips: 52, rs: 31, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729658
Test loss (w/o reg) on all data: 0.012054935
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.141783e-08
Norm of the params: 9.153197
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729518
Test loss (w/o reg) on all data: 0.012054948
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.075119e-07
Norm of the params: 9.1532135
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06324979
Train loss (w/o reg) on all data: 0.054888565
Test loss (w/o reg) on all data: 0.036288265
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.2456685e-06
Norm of the params: 12.931531
              Random: fixed   8 labels. Loss 0.03629. Accuracy 0.989.
### Flips: 52, rs: 31, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173089
Test loss (w/o reg) on all data: 0.012054835
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0654288e-07
Norm of the params: 9.153065
     Influence (LOO): fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002173086
Test loss (w/o reg) on all data: 0.0120547665
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8759894e-07
Norm of the params: 9.153066
                Loss: fixed  30 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053894024
Train loss (w/o reg) on all data: 0.0448158
Test loss (w/o reg) on all data: 0.035251822
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.880049e-06
Norm of the params: 13.4745865
              Random: fixed   9 labels. Loss 0.03525. Accuracy 0.996.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091070585
Train loss (w/o reg) on all data: 0.083965935
Test loss (w/o reg) on all data: 0.056997053
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.967328e-06
Norm of the params: 11.920277
Flipped loss: 0.05700. Accuracy: 0.989
### Flips: 52, rs: 32, checks: 52
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008938424
Train loss (w/o reg) on all data: 0.0036617958
Test loss (w/o reg) on all data: 0.014991516
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.2510455e-07
Norm of the params: 10.272904
     Influence (LOO): fixed  30 labels. Loss 0.01499. Accuracy 0.989.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730717
Test loss (w/o reg) on all data: 0.012054415
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7711925e-07
Norm of the params: 9.153084
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0808089
Train loss (w/o reg) on all data: 0.07352682
Test loss (w/o reg) on all data: 0.049065635
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2664406e-05
Norm of the params: 12.068208
              Random: fixed   4 labels. Loss 0.04907. Accuracy 0.992.
### Flips: 52, rs: 32, checks: 104
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362022
Train loss (w/o reg) on all data: 0.0021730538
Test loss (w/o reg) on all data: 0.01205444
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2113224e-07
Norm of the params: 9.153107
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730494
Test loss (w/o reg) on all data: 0.012054573
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.658621e-07
Norm of the params: 9.153108
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077273525
Train loss (w/o reg) on all data: 0.06972569
Test loss (w/o reg) on all data: 0.05160787
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8769126e-05
Norm of the params: 12.286444
              Random: fixed   5 labels. Loss 0.05161. Accuracy 0.992.
### Flips: 52, rs: 32, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173069
Test loss (w/o reg) on all data: 0.012054404
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5280342e-07
Norm of the params: 9.153088
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730654
Test loss (w/o reg) on all data: 0.012054468
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8030877e-07
Norm of the params: 9.1530905
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.077273525
Train loss (w/o reg) on all data: 0.069724664
Test loss (w/o reg) on all data: 0.051615242
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.079493e-06
Norm of the params: 12.287281
              Random: fixed   5 labels. Loss 0.05162. Accuracy 0.992.
### Flips: 52, rs: 32, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729958
Test loss (w/o reg) on all data: 0.012053799
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9136777e-07
Norm of the params: 9.153167
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172979
Test loss (w/o reg) on all data: 0.012054575
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9700312e-07
Norm of the params: 9.153185
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.073338464
Train loss (w/o reg) on all data: 0.0655566
Test loss (w/o reg) on all data: 0.049247388
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8677339e-06
Norm of the params: 12.475468
              Random: fixed   7 labels. Loss 0.04925. Accuracy 0.989.
### Flips: 52, rs: 32, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172952
Test loss (w/o reg) on all data: 0.012054592
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6006139e-07
Norm of the params: 9.153215
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729525
Test loss (w/o reg) on all data: 0.012054631
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6081252e-07
Norm of the params: 9.153214
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06473221
Train loss (w/o reg) on all data: 0.05608838
Test loss (w/o reg) on all data: 0.045285754
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1197513e-06
Norm of the params: 13.148253
              Random: fixed   9 labels. Loss 0.04529. Accuracy 0.992.
### Flips: 52, rs: 32, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729432
Test loss (w/o reg) on all data: 0.01205436
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4571744e-07
Norm of the params: 9.153225
     Influence (LOO): fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620214
Train loss (w/o reg) on all data: 0.002172945
Test loss (w/o reg) on all data: 0.012054409
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0695823e-07
Norm of the params: 9.153225
                Loss: fixed  32 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05981326
Train loss (w/o reg) on all data: 0.0511966
Test loss (w/o reg) on all data: 0.042022735
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.0849835e-06
Norm of the params: 13.127575
              Random: fixed  11 labels. Loss 0.04202. Accuracy 0.992.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08097865
Train loss (w/o reg) on all data: 0.07234941
Test loss (w/o reg) on all data: 0.040789742
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.481815e-05
Norm of the params: 13.1371565
Flipped loss: 0.04079. Accuracy: 0.992
### Flips: 52, rs: 33, checks: 52
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729202
Test loss (w/o reg) on all data: 0.012055305
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9683227e-07
Norm of the params: 9.15325
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729216
Test loss (w/o reg) on all data: 0.012055253
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.418443e-07
Norm of the params: 9.153249
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07683955
Train loss (w/o reg) on all data: 0.06769944
Test loss (w/o reg) on all data: 0.042511772
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6256186e-05
Norm of the params: 13.52044
              Random: fixed   1 labels. Loss 0.04251. Accuracy 0.992.
### Flips: 52, rs: 33, checks: 104
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729965
Test loss (w/o reg) on all data: 0.0120554445
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7951935e-07
Norm of the params: 9.153166
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729968
Test loss (w/o reg) on all data: 0.012055372
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3493954e-07
Norm of the params: 9.153166
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07475568
Train loss (w/o reg) on all data: 0.06573602
Test loss (w/o reg) on all data: 0.04281378
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.591075e-06
Norm of the params: 13.431057
              Random: fixed   2 labels. Loss 0.04281. Accuracy 0.992.
### Flips: 52, rs: 33, checks: 156
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172948
Test loss (w/o reg) on all data: 0.012055845
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3543149e-06
Norm of the params: 9.153219
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172946
Test loss (w/o reg) on all data: 0.012055647
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3731583e-07
Norm of the params: 9.15322
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06549576
Train loss (w/o reg) on all data: 0.056979097
Test loss (w/o reg) on all data: 0.03970633
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.6922755e-06
Norm of the params: 13.051175
              Random: fixed   4 labels. Loss 0.03971. Accuracy 0.989.
### Flips: 52, rs: 33, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173043
Test loss (w/o reg) on all data: 0.012055622
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.627283e-06
Norm of the params: 9.153115
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.00217304
Test loss (w/o reg) on all data: 0.012055247
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.827903e-07
Norm of the params: 9.15312
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06549576
Train loss (w/o reg) on all data: 0.056981973
Test loss (w/o reg) on all data: 0.039699562
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.381874e-06
Norm of the params: 13.048974
              Random: fixed   4 labels. Loss 0.03970. Accuracy 0.989.
### Flips: 52, rs: 33, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729507
Test loss (w/o reg) on all data: 0.012055685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3079948e-07
Norm of the params: 9.153216
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729507
Test loss (w/o reg) on all data: 0.012055602
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.751534e-07
Norm of the params: 9.153216
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063361056
Train loss (w/o reg) on all data: 0.055417176
Test loss (w/o reg) on all data: 0.03791785
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5123193e-06
Norm of the params: 12.604668
              Random: fixed   6 labels. Loss 0.03792. Accuracy 0.992.
### Flips: 52, rs: 33, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172842
Test loss (w/o reg) on all data: 0.012055117
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.713057e-07
Norm of the params: 9.153333
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728436
Test loss (w/o reg) on all data: 0.012054983
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1174476e-07
Norm of the params: 9.153332
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057541624
Train loss (w/o reg) on all data: 0.049601406
Test loss (w/o reg) on all data: 0.037883524
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.469001e-06
Norm of the params: 12.60176
              Random: fixed   8 labels. Loss 0.03788. Accuracy 0.989.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06646196
Train loss (w/o reg) on all data: 0.0542756
Test loss (w/o reg) on all data: 0.07535315
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1854682e-05
Norm of the params: 15.611766
Flipped loss: 0.07535. Accuracy: 0.977
### Flips: 52, rs: 34, checks: 52
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730806
Test loss (w/o reg) on all data: 0.012054955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8644326e-07
Norm of the params: 9.153071
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730806
Test loss (w/o reg) on all data: 0.012054846
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0763767e-07
Norm of the params: 9.153073
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066461965
Train loss (w/o reg) on all data: 0.054277603
Test loss (w/o reg) on all data: 0.07533397
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2258031e-05
Norm of the params: 15.610483
              Random: fixed   0 labels. Loss 0.07533. Accuracy 0.977.
### Flips: 52, rs: 34, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730142
Test loss (w/o reg) on all data: 0.012054716
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9341365e-07
Norm of the params: 9.153146
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730142
Test loss (w/o reg) on all data: 0.012054662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7353008e-07
Norm of the params: 9.153148
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065486625
Train loss (w/o reg) on all data: 0.053868223
Test loss (w/o reg) on all data: 0.07048347
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.5855463e-06
Norm of the params: 15.243623
              Random: fixed   1 labels. Loss 0.07048. Accuracy 0.977.
### Flips: 52, rs: 34, checks: 156
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731276
Test loss (w/o reg) on all data: 0.012053435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4461306e-07
Norm of the params: 9.153022
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173127
Test loss (w/o reg) on all data: 0.012053639
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8860253e-07
Norm of the params: 9.153024
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06548663
Train loss (w/o reg) on all data: 0.053870242
Test loss (w/o reg) on all data: 0.070462525
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.940135e-06
Norm of the params: 15.242302
              Random: fixed   1 labels. Loss 0.07046. Accuracy 0.977.
### Flips: 52, rs: 34, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012053965
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.617958e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729933
Test loss (w/o reg) on all data: 0.012054088
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0869408e-07
Norm of the params: 9.15317
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061383024
Train loss (w/o reg) on all data: 0.050514508
Test loss (w/o reg) on all data: 0.06536461
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.031356e-06
Norm of the params: 14.743484
              Random: fixed   4 labels. Loss 0.06536. Accuracy 0.973.
### Flips: 52, rs: 34, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730093
Test loss (w/o reg) on all data: 0.012055052
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.307066e-07
Norm of the params: 9.1531515
     Influence (LOO): fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002173009
Test loss (w/o reg) on all data: 0.012055
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1957033e-07
Norm of the params: 9.153153
                Loss: fixed  26 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058513466
Train loss (w/o reg) on all data: 0.048630882
Test loss (w/o reg) on all data: 0.058249395
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.5038531e-06
Norm of the params: 14.058866
              Random: fixed   6 labels. Loss 0.05825. Accuracy 0.973.
### Flips: 52, rs: 34, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217295
Test loss (w/o reg) on all data: 0.012054336
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2788331e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729495
Test loss (w/o reg) on all data: 0.012054393
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3522128e-07
Norm of the params: 9.153216
                Loss: fixed  26 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058513466
Train loss (w/o reg) on all data: 0.04863366
Test loss (w/o reg) on all data: 0.058241755
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.3730042e-06
Norm of the params: 14.056888
              Random: fixed   6 labels. Loss 0.05824. Accuracy 0.973.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.098849304
Train loss (w/o reg) on all data: 0.08934649
Test loss (w/o reg) on all data: 0.055570997
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.625349e-06
Norm of the params: 13.786089
Flipped loss: 0.05557. Accuracy: 0.977
### Flips: 52, rs: 35, checks: 52
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729995
Test loss (w/o reg) on all data: 0.012055049
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.895352e-08
Norm of the params: 9.153162
     Influence (LOO): fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172992
Test loss (w/o reg) on all data: 0.01205507
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9722433e-07
Norm of the params: 9.153171
                Loss: fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0988493
Train loss (w/o reg) on all data: 0.08934849
Test loss (w/o reg) on all data: 0.055565014
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.971169e-06
Norm of the params: 13.784636
              Random: fixed   0 labels. Loss 0.05557. Accuracy 0.977.
### Flips: 52, rs: 35, checks: 104
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728822
Test loss (w/o reg) on all data: 0.0120549165
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.812767e-07
Norm of the params: 9.15329
     Influence (LOO): fixed  33 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728834
Test loss (w/o reg) on all data: 0.012055029
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.187939e-07
Norm of the params: 9.153289
                Loss: fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09857157
Train loss (w/o reg) on all data: 0.089363
Test loss (w/o reg) on all data: 0.052878927
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4888605e-05
Norm of the params: 13.570976
              Random: fixed   1 labels. Loss 0.05288. Accuracy 0.977.
### Flips: 52, rs: 35, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021732803
Test loss (w/o reg) on all data: 0.01205477
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4938634e-07
Norm of the params: 9.152857
     Influence (LOO): fixed  33 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021732752
Test loss (w/o reg) on all data: 0.0120549295
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0536315e-07
Norm of the params: 9.152861
                Loss: fixed  33 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096069485
Train loss (w/o reg) on all data: 0.086882584
Test loss (w/o reg) on all data: 0.051600464
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.132726e-06
Norm of the params: 13.554997
              Random: fixed   2 labels. Loss 0.05160. Accuracy 0.977.
### Flips: 52, rs: 35, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173037
Test loss (w/o reg) on all data: 0.012055358
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0300598e-07
Norm of the params: 9.153121
     Influence (LOO): fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172986
Test loss (w/o reg) on all data: 0.012054427
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5818733e-07
Norm of the params: 9.153178
                Loss: fixed  33 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09339841
Train loss (w/o reg) on all data: 0.083897255
Test loss (w/o reg) on all data: 0.055260323
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.063935e-06
Norm of the params: 13.784885
              Random: fixed   3 labels. Loss 0.05526. Accuracy 0.977.
### Flips: 52, rs: 35, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.012054372
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8009086e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  33 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729767
Test loss (w/o reg) on all data: 0.012054326
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5217244e-07
Norm of the params: 9.153188
                Loss: fixed  33 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09085221
Train loss (w/o reg) on all data: 0.08100306
Test loss (w/o reg) on all data: 0.055617973
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4302603e-05
Norm of the params: 14.035059
              Random: fixed   4 labels. Loss 0.05562. Accuracy 0.977.
### Flips: 52, rs: 35, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172946
Test loss (w/o reg) on all data: 0.012054707
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2095113e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  33 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729465
Test loss (w/o reg) on all data: 0.012054662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.842358e-08
Norm of the params: 9.153221
                Loss: fixed  33 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08073735
Train loss (w/o reg) on all data: 0.06997296
Test loss (w/o reg) on all data: 0.0538302
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2306432e-05
Norm of the params: 14.6726885
              Random: fixed   7 labels. Loss 0.05383. Accuracy 0.973.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043227687
Train loss (w/o reg) on all data: 0.032095376
Test loss (w/o reg) on all data: 0.05438745
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9526078e-06
Norm of the params: 14.921335
Flipped loss: 0.05439. Accuracy: 0.977
### Flips: 52, rs: 36, checks: 52
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009058047
Train loss (w/o reg) on all data: 0.0038678143
Test loss (w/o reg) on all data: 0.01371668
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3149078e-06
Norm of the params: 10.1884575
     Influence (LOO): fixed  16 labels. Loss 0.01372. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729018
Test loss (w/o reg) on all data: 0.01205503
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.534927e-07
Norm of the params: 9.15327
                Loss: fixed  17 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04322769
Train loss (w/o reg) on all data: 0.03209301
Test loss (w/o reg) on all data: 0.05440035
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.965044e-06
Norm of the params: 14.922922
              Random: fixed   0 labels. Loss 0.05440. Accuracy 0.977.
### Flips: 52, rs: 36, checks: 104
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729833
Test loss (w/o reg) on all data: 0.012055057
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0811324e-07
Norm of the params: 9.15318
     Influence (LOO): fixed  17 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729823
Test loss (w/o reg) on all data: 0.012055129
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5991381e-07
Norm of the params: 9.153181
                Loss: fixed  17 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041403063
Train loss (w/o reg) on all data: 0.03009965
Test loss (w/o reg) on all data: 0.049473975
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5380912e-05
Norm of the params: 15.035564
              Random: fixed   1 labels. Loss 0.04947. Accuracy 0.981.
### Flips: 52, rs: 36, checks: 156
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012055204
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6589436e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  17 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172982
Test loss (w/o reg) on all data: 0.0120551605
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2199695e-07
Norm of the params: 9.153181
                Loss: fixed  17 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041318968
Train loss (w/o reg) on all data: 0.030280186
Test loss (w/o reg) on all data: 0.0459773
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.7437314e-06
Norm of the params: 14.858521
              Random: fixed   2 labels. Loss 0.04598. Accuracy 0.981.
### Flips: 52, rs: 36, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728869
Test loss (w/o reg) on all data: 0.01205473
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.125539e-07
Norm of the params: 9.153286
     Influence (LOO): fixed  17 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728878
Test loss (w/o reg) on all data: 0.012054655
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7476391e-07
Norm of the params: 9.153285
                Loss: fixed  17 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04108209
Train loss (w/o reg) on all data: 0.03025264
Test loss (w/o reg) on all data: 0.04298971
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1903916e-06
Norm of the params: 14.716967
              Random: fixed   3 labels. Loss 0.04299. Accuracy 0.985.
### Flips: 52, rs: 36, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217289
Test loss (w/o reg) on all data: 0.012054928
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.3328374e-07
Norm of the params: 9.153283
     Influence (LOO): fixed  17 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728908
Test loss (w/o reg) on all data: 0.012054806
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.02782074e-07
Norm of the params: 9.153282
                Loss: fixed  17 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033294003
Train loss (w/o reg) on all data: 0.02315998
Test loss (w/o reg) on all data: 0.037320726
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2554184e-06
Norm of the params: 14.236588
              Random: fixed   5 labels. Loss 0.03732. Accuracy 0.989.
### Flips: 52, rs: 36, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730515
Test loss (w/o reg) on all data: 0.012054705
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.206764e-07
Norm of the params: 9.153106
     Influence (LOO): fixed  17 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730508
Test loss (w/o reg) on all data: 0.012054766
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8549464e-07
Norm of the params: 9.153107
                Loss: fixed  17 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03329401
Train loss (w/o reg) on all data: 0.023159064
Test loss (w/o reg) on all data: 0.037322007
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.0233847e-06
Norm of the params: 14.237237
              Random: fixed   5 labels. Loss 0.03732. Accuracy 0.989.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066826016
Train loss (w/o reg) on all data: 0.058384966
Test loss (w/o reg) on all data: 0.03746573
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1875237e-06
Norm of the params: 12.993112
Flipped loss: 0.03747. Accuracy: 0.992
### Flips: 52, rs: 37, checks: 52
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008603367
Train loss (w/o reg) on all data: 0.003686616
Test loss (w/o reg) on all data: 0.011063583
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5568004e-07
Norm of the params: 9.916402
     Influence (LOO): fixed  22 labels. Loss 0.01106. Accuracy 0.996.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172992
Test loss (w/o reg) on all data: 0.012054655
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3216037e-07
Norm of the params: 9.153171
                Loss: fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06169448
Train loss (w/o reg) on all data: 0.05343289
Test loss (w/o reg) on all data: 0.035013955
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3190267e-06
Norm of the params: 12.854253
              Random: fixed   2 labels. Loss 0.03501. Accuracy 0.992.
### Flips: 52, rs: 37, checks: 104
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730496
Test loss (w/o reg) on all data: 0.012054559
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2016656e-06
Norm of the params: 9.153109
     Influence (LOO): fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730498
Test loss (w/o reg) on all data: 0.012054348
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8855166e-07
Norm of the params: 9.153109
                Loss: fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056080334
Train loss (w/o reg) on all data: 0.046766683
Test loss (w/o reg) on all data: 0.03404208
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8645946e-06
Norm of the params: 13.648188
              Random: fixed   3 labels. Loss 0.03404. Accuracy 0.992.
### Flips: 52, rs: 37, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730142
Test loss (w/o reg) on all data: 0.012054648
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3608227e-07
Norm of the params: 9.153147
     Influence (LOO): fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730133
Test loss (w/o reg) on all data: 0.012054575
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1680442e-07
Norm of the params: 9.153147
                Loss: fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05608034
Train loss (w/o reg) on all data: 0.046765383
Test loss (w/o reg) on all data: 0.034042913
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7744442e-06
Norm of the params: 13.649145
              Random: fixed   3 labels. Loss 0.03404. Accuracy 0.992.
### Flips: 52, rs: 37, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729737
Test loss (w/o reg) on all data: 0.0120550115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.10863e-07
Norm of the params: 9.153191
     Influence (LOO): fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172974
Test loss (w/o reg) on all data: 0.012055066
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.9382936e-08
Norm of the params: 9.153191
                Loss: fixed  23 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0544646
Train loss (w/o reg) on all data: 0.04532633
Test loss (w/o reg) on all data: 0.034512065
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.32589e-06
Norm of the params: 13.519077
              Random: fixed   4 labels. Loss 0.03451. Accuracy 0.992.
### Flips: 52, rs: 37, checks: 260
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002172955
Test loss (w/o reg) on all data: 0.012054737
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1401112e-07
Norm of the params: 9.153209
     Influence (LOO): fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012054792
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.485359e-07
Norm of the params: 9.153209
                Loss: fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049109075
Train loss (w/o reg) on all data: 0.039057944
Test loss (w/o reg) on all data: 0.03509652
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8423162e-06
Norm of the params: 14.178245
              Random: fixed   5 labels. Loss 0.03510. Accuracy 0.992.
### Flips: 52, rs: 37, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729963
Test loss (w/o reg) on all data: 0.012054675
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5275052e-07
Norm of the params: 9.153166
     Influence (LOO): fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729954
Test loss (w/o reg) on all data: 0.012054719
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3005936e-07
Norm of the params: 9.153167
                Loss: fixed  23 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049109083
Train loss (w/o reg) on all data: 0.039056107
Test loss (w/o reg) on all data: 0.0350976
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3997058e-05
Norm of the params: 14.179545
              Random: fixed   5 labels. Loss 0.03510. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096855804
Train loss (w/o reg) on all data: 0.08762205
Test loss (w/o reg) on all data: 0.055189367
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5455667e-05
Norm of the params: 13.589526
Flipped loss: 0.05519. Accuracy: 0.992
### Flips: 52, rs: 38, checks: 52
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002173026
Test loss (w/o reg) on all data: 0.012055488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7175925e-07
Norm of the params: 9.153134
     Influence (LOO): fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730259
Test loss (w/o reg) on all data: 0.0120553905
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6482369e-07
Norm of the params: 9.153135
                Loss: fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09478367
Train loss (w/o reg) on all data: 0.08561166
Test loss (w/o reg) on all data: 0.05525133
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1737503e-05
Norm of the params: 13.544016
              Random: fixed   1 labels. Loss 0.05525. Accuracy 0.992.
### Flips: 52, rs: 38, checks: 104
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172998
Test loss (w/o reg) on all data: 0.0120550515
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.808027e-08
Norm of the params: 9.153165
     Influence (LOO): fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729977
Test loss (w/o reg) on all data: 0.012055084
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8103924e-07
Norm of the params: 9.153165
                Loss: fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09044607
Train loss (w/o reg) on all data: 0.08075281
Test loss (w/o reg) on all data: 0.05709815
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.490955e-06
Norm of the params: 13.923549
              Random: fixed   2 labels. Loss 0.05710. Accuracy 0.989.
### Flips: 52, rs: 38, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.01205521
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.292123e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172965
Test loss (w/o reg) on all data: 0.012055353
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.574796e-07
Norm of the params: 9.153199
                Loss: fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09044607
Train loss (w/o reg) on all data: 0.08075189
Test loss (w/o reg) on all data: 0.05710098
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.9823585e-06
Norm of the params: 13.924211
              Random: fixed   2 labels. Loss 0.05710. Accuracy 0.989.
### Flips: 52, rs: 38, checks: 208
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730391
Test loss (w/o reg) on all data: 0.012055155
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.419177e-07
Norm of the params: 9.153118
     Influence (LOO): fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173039
Test loss (w/o reg) on all data: 0.012055009
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.542057e-07
Norm of the params: 9.153118
                Loss: fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078424506
Train loss (w/o reg) on all data: 0.068435386
Test loss (w/o reg) on all data: 0.04829356
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.725581e-06
Norm of the params: 14.134441
              Random: fixed   6 labels. Loss 0.04829. Accuracy 0.992.
### Flips: 52, rs: 38, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730112
Test loss (w/o reg) on all data: 0.012056274
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0592082e-07
Norm of the params: 9.153151
     Influence (LOO): fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730072
Test loss (w/o reg) on all data: 0.01205609
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2831866e-07
Norm of the params: 9.153154
                Loss: fixed  33 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07345352
Train loss (w/o reg) on all data: 0.063074306
Test loss (w/o reg) on all data: 0.0469669
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.2110137e-06
Norm of the params: 14.407788
              Random: fixed   9 labels. Loss 0.04697. Accuracy 0.989.
### Flips: 52, rs: 38, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172934
Test loss (w/o reg) on all data: 0.012054622
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5933505e-07
Norm of the params: 9.153234
     Influence (LOO): fixed  33 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729365
Test loss (w/o reg) on all data: 0.012054538
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.441584e-07
Norm of the params: 9.153231
                Loss: fixed  33 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065950595
Train loss (w/o reg) on all data: 0.05554191
Test loss (w/o reg) on all data: 0.044588838
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5660127e-05
Norm of the params: 14.428227
              Random: fixed  12 labels. Loss 0.04459. Accuracy 0.985.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060634017
Train loss (w/o reg) on all data: 0.04822305
Test loss (w/o reg) on all data: 0.06102065
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.2416693e-06
Norm of the params: 15.75498
Flipped loss: 0.06102. Accuracy: 0.977
### Flips: 52, rs: 39, checks: 52
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728806
Test loss (w/o reg) on all data: 0.012055365
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0117095e-06
Norm of the params: 9.153292
     Influence (LOO): fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728815
Test loss (w/o reg) on all data: 0.012055172
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0676204e-07
Norm of the params: 9.153292
                Loss: fixed  22 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052242022
Train loss (w/o reg) on all data: 0.04084112
Test loss (w/o reg) on all data: 0.05811529
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.342903e-06
Norm of the params: 15.1002655
              Random: fixed   4 labels. Loss 0.05812. Accuracy 0.981.
### Flips: 52, rs: 39, checks: 104
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173052
Test loss (w/o reg) on all data: 0.012053203
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9936993e-07
Norm of the params: 9.153105
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620154
Train loss (w/o reg) on all data: 0.0021730487
Test loss (w/o reg) on all data: 0.012053376
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.193504e-07
Norm of the params: 9.153106
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052242026
Train loss (w/o reg) on all data: 0.040839817
Test loss (w/o reg) on all data: 0.058121894
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.5639554e-06
Norm of the params: 15.1011305
              Random: fixed   4 labels. Loss 0.05812. Accuracy 0.981.
### Flips: 52, rs: 39, checks: 156
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172893
Test loss (w/o reg) on all data: 0.012054626
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0008024e-07
Norm of the params: 9.153279
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728924
Test loss (w/o reg) on all data: 0.012054716
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3993115e-07
Norm of the params: 9.153278
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0477068
Train loss (w/o reg) on all data: 0.03564104
Test loss (w/o reg) on all data: 0.05705544
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.0670865e-06
Norm of the params: 15.534324
              Random: fixed   5 labels. Loss 0.05706. Accuracy 0.981.
### Flips: 52, rs: 39, checks: 208
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173045
Test loss (w/o reg) on all data: 0.012054625
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.792584e-07
Norm of the params: 9.153113
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730447
Test loss (w/o reg) on all data: 0.012054729
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7103517e-07
Norm of the params: 9.153113
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047706798
Train loss (w/o reg) on all data: 0.03564369
Test loss (w/o reg) on all data: 0.0570587
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4589984e-06
Norm of the params: 15.532617
              Random: fixed   5 labels. Loss 0.05706. Accuracy 0.981.
### Flips: 52, rs: 39, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172883
Test loss (w/o reg) on all data: 0.01205485
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0274267e-07
Norm of the params: 9.15329
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728848
Test loss (w/o reg) on all data: 0.012054769
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2503274e-07
Norm of the params: 9.153289
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04283394
Train loss (w/o reg) on all data: 0.031246364
Test loss (w/o reg) on all data: 0.05758589
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6531568e-06
Norm of the params: 15.223385
              Random: fixed   7 labels. Loss 0.05759. Accuracy 0.985.
### Flips: 52, rs: 39, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172969
Test loss (w/o reg) on all data: 0.012054711
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.942981e-08
Norm of the params: 9.153194
     Influence (LOO): fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729704
Test loss (w/o reg) on all data: 0.012054754
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7826571e-07
Norm of the params: 9.153194
                Loss: fixed  22 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029837288
Train loss (w/o reg) on all data: 0.020561932
Test loss (w/o reg) on all data: 0.033947345
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4548095e-06
Norm of the params: 13.620101
              Random: fixed  12 labels. Loss 0.03395. Accuracy 0.996.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13474138
Train loss (w/o reg) on all data: 0.12598397
Test loss (w/o reg) on all data: 0.07226719
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8534838e-05
Norm of the params: 13.234357
Flipped loss: 0.07227. Accuracy: 0.973
### Flips: 104, rs: 0, checks: 52
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02464204
Train loss (w/o reg) on all data: 0.016456876
Test loss (w/o reg) on all data: 0.032582574
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.461163e-07
Norm of the params: 12.794659
     Influence (LOO): fixed  43 labels. Loss 0.03258. Accuracy 0.985.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014799751
Train loss (w/o reg) on all data: 0.007440739
Test loss (w/o reg) on all data: 0.019799797
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0743164e-06
Norm of the params: 12.131788
                Loss: fixed  47 labels. Loss 0.01980. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13113517
Train loss (w/o reg) on all data: 0.122914806
Test loss (w/o reg) on all data: 0.06604339
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.058939e-05
Norm of the params: 12.822138
              Random: fixed   4 labels. Loss 0.06604. Accuracy 0.985.
### Flips: 104, rs: 0, checks: 104
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011643097
Train loss (w/o reg) on all data: 0.0054263747
Test loss (w/o reg) on all data: 0.014567611
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6957741e-06
Norm of the params: 11.150536
     Influence (LOO): fixed  50 labels. Loss 0.01457. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728792
Test loss (w/o reg) on all data: 0.012055133
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.823408e-07
Norm of the params: 9.153293
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12877509
Train loss (w/o reg) on all data: 0.12068989
Test loss (w/o reg) on all data: 0.06630931
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.908627e-06
Norm of the params: 12.716288
              Random: fixed   5 labels. Loss 0.06631. Accuracy 0.989.
### Flips: 104, rs: 0, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729234
Test loss (w/o reg) on all data: 0.012055014
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5186434e-07
Norm of the params: 9.153246
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729236
Test loss (w/o reg) on all data: 0.012055099
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7963528e-07
Norm of the params: 9.153244
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12877509
Train loss (w/o reg) on all data: 0.120688714
Test loss (w/o reg) on all data: 0.06630971
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4539777e-05
Norm of the params: 12.717212
              Random: fixed   5 labels. Loss 0.06631. Accuracy 0.989.
### Flips: 104, rs: 0, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728969
Test loss (w/o reg) on all data: 0.01205434
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.25235e-07
Norm of the params: 9.1532755
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728978
Test loss (w/o reg) on all data: 0.012054439
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4760316e-07
Norm of the params: 9.153273
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12877509
Train loss (w/o reg) on all data: 0.12068591
Test loss (w/o reg) on all data: 0.06629981
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8709661e-05
Norm of the params: 12.719419
              Random: fixed   5 labels. Loss 0.06630. Accuracy 0.989.
### Flips: 104, rs: 0, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729546
Test loss (w/o reg) on all data: 0.012055085
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5683163e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729576
Test loss (w/o reg) on all data: 0.012054815
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.625366e-07
Norm of the params: 9.153209
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117530234
Train loss (w/o reg) on all data: 0.10820599
Test loss (w/o reg) on all data: 0.070052944
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4513937e-05
Norm of the params: 13.655947
              Random: fixed   9 labels. Loss 0.07005. Accuracy 0.977.
### Flips: 104, rs: 0, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728585
Test loss (w/o reg) on all data: 0.01205452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.7801317e-07
Norm of the params: 9.153317
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172859
Test loss (w/o reg) on all data: 0.012054346
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.057442e-07
Norm of the params: 9.1533165
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10977947
Train loss (w/o reg) on all data: 0.09988584
Test loss (w/o reg) on all data: 0.067071296
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.869927e-06
Norm of the params: 14.066719
              Random: fixed  12 labels. Loss 0.06707. Accuracy 0.973.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13892175
Train loss (w/o reg) on all data: 0.1312741
Test loss (w/o reg) on all data: 0.07247404
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1617574e-05
Norm of the params: 12.367414
Flipped loss: 0.07247. Accuracy: 0.985
### Flips: 104, rs: 1, checks: 52
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029963478
Train loss (w/o reg) on all data: 0.02088549
Test loss (w/o reg) on all data: 0.026424587
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0721316e-06
Norm of the params: 13.474413
     Influence (LOO): fixed  43 labels. Loss 0.02642. Accuracy 0.985.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015629128
Train loss (w/o reg) on all data: 0.0073257526
Test loss (w/o reg) on all data: 0.029914033
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6377635e-06
Norm of the params: 12.886719
                Loss: fixed  48 labels. Loss 0.02991. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13492014
Train loss (w/o reg) on all data: 0.1274542
Test loss (w/o reg) on all data: 0.06745919
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.6351714e-06
Norm of the params: 12.219603
              Random: fixed   3 labels. Loss 0.06746. Accuracy 0.989.
### Flips: 104, rs: 1, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730151
Test loss (w/o reg) on all data: 0.012055107
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.042728e-07
Norm of the params: 9.153146
     Influence (LOO): fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.002173015
Test loss (w/o reg) on all data: 0.012055141
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4471788e-07
Norm of the params: 9.153148
                Loss: fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12727302
Train loss (w/o reg) on all data: 0.11929252
Test loss (w/o reg) on all data: 0.07400694
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.213274e-05
Norm of the params: 12.633688
              Random: fixed   6 labels. Loss 0.07401. Accuracy 0.981.
### Flips: 104, rs: 1, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729604
Test loss (w/o reg) on all data: 0.012055166
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9500995e-07
Norm of the params: 9.153206
     Influence (LOO): fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729602
Test loss (w/o reg) on all data: 0.01205503
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.289563e-07
Norm of the params: 9.153204
                Loss: fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12536757
Train loss (w/o reg) on all data: 0.11711877
Test loss (w/o reg) on all data: 0.07365597
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.923498e-06
Norm of the params: 12.844295
              Random: fixed   7 labels. Loss 0.07366. Accuracy 0.981.
### Flips: 104, rs: 1, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730566
Test loss (w/o reg) on all data: 0.012054709
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.3245584e-07
Norm of the params: 9.1531
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730557
Test loss (w/o reg) on all data: 0.012054876
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3177299e-07
Norm of the params: 9.153101
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11538001
Train loss (w/o reg) on all data: 0.10727487
Test loss (w/o reg) on all data: 0.07531606
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0076295e-05
Norm of the params: 12.731963
              Random: fixed  11 labels. Loss 0.07532. Accuracy 0.973.
### Flips: 104, rs: 1, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729982
Test loss (w/o reg) on all data: 0.012055302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6062868e-07
Norm of the params: 9.153165
     Influence (LOO): fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172996
Test loss (w/o reg) on all data: 0.012055397
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.029481e-07
Norm of the params: 9.153166
                Loss: fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10448043
Train loss (w/o reg) on all data: 0.096494555
Test loss (w/o reg) on all data: 0.06959334
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.3359895e-05
Norm of the params: 12.637937
              Random: fixed  16 labels. Loss 0.06959. Accuracy 0.985.
### Flips: 104, rs: 1, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172989
Test loss (w/o reg) on all data: 0.012054977
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.735171e-07
Norm of the params: 9.153175
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729881
Test loss (w/o reg) on all data: 0.012055096
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7639317e-07
Norm of the params: 9.153175
                Loss: fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09398854
Train loss (w/o reg) on all data: 0.08530208
Test loss (w/o reg) on all data: 0.06563645
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.401578e-06
Norm of the params: 13.180636
              Random: fixed  20 labels. Loss 0.06564. Accuracy 0.985.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13544413
Train loss (w/o reg) on all data: 0.12739937
Test loss (w/o reg) on all data: 0.071938746
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6728816e-05
Norm of the params: 12.684448
Flipped loss: 0.07194. Accuracy: 0.969
### Flips: 104, rs: 2, checks: 52
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02691307
Train loss (w/o reg) on all data: 0.017470296
Test loss (w/o reg) on all data: 0.01447067
Train acc on all data:  0.994269340974212
Test acc on all data:   1.0
Norm of the mean of gradients: 1.3510104e-06
Norm of the params: 13.742469
     Influence (LOO): fixed  45 labels. Loss 0.01447. Accuracy 1.000.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015543865
Train loss (w/o reg) on all data: 0.0069531384
Test loss (w/o reg) on all data: 0.026854284
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5611712e-06
Norm of the params: 13.107803
                Loss: fixed  47 labels. Loss 0.02685. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13357383
Train loss (w/o reg) on all data: 0.12572716
Test loss (w/o reg) on all data: 0.07014676
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.8403325e-06
Norm of the params: 12.527303
              Random: fixed   2 labels. Loss 0.07015. Accuracy 0.981.
### Flips: 104, rs: 2, checks: 104
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729765
Test loss (w/o reg) on all data: 0.012054442
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5193498e-06
Norm of the params: 9.153188
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074191447
Train loss (w/o reg) on all data: 0.0026756434
Test loss (w/o reg) on all data: 0.011990918
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7298565e-07
Norm of the params: 9.740124
                Loss: fixed  56 labels. Loss 0.01199. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12707472
Train loss (w/o reg) on all data: 0.1182989
Test loss (w/o reg) on all data: 0.062324967
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.9060153e-05
Norm of the params: 13.248261
              Random: fixed   5 labels. Loss 0.06232. Accuracy 0.981.
### Flips: 104, rs: 2, checks: 156
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729288
Test loss (w/o reg) on all data: 0.012054474
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.931349e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729309
Test loss (w/o reg) on all data: 0.012054344
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6176215e-07
Norm of the params: 9.153237
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12486236
Train loss (w/o reg) on all data: 0.11602752
Test loss (w/o reg) on all data: 0.061495293
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.0620777e-06
Norm of the params: 13.292734
              Random: fixed   6 labels. Loss 0.06150. Accuracy 0.981.
### Flips: 104, rs: 2, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172958
Test loss (w/o reg) on all data: 0.01205425
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.66505e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729574
Test loss (w/o reg) on all data: 0.012054146
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6190222e-07
Norm of the params: 9.153208
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.113942504
Train loss (w/o reg) on all data: 0.10408697
Test loss (w/o reg) on all data: 0.0577908
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.392685e-05
Norm of the params: 14.039608
              Random: fixed  10 labels. Loss 0.05779. Accuracy 0.989.
### Flips: 104, rs: 2, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012054336
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.402535e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729635
Test loss (w/o reg) on all data: 0.012054302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2608768e-07
Norm of the params: 9.153203
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105967395
Train loss (w/o reg) on all data: 0.09532834
Test loss (w/o reg) on all data: 0.05320258
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0085126e-06
Norm of the params: 14.587019
              Random: fixed  12 labels. Loss 0.05320. Accuracy 0.992.
### Flips: 104, rs: 2, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729416
Test loss (w/o reg) on all data: 0.012055374
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0513083e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729402
Test loss (w/o reg) on all data: 0.012055409
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9355218e-07
Norm of the params: 9.153227
                Loss: fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1057087
Train loss (w/o reg) on all data: 0.09523295
Test loss (w/o reg) on all data: 0.051358502
Train acc on all data:  0.9684813753581661
Test acc on all data:   1.0
Norm of the mean of gradients: 4.306611e-06
Norm of the params: 14.474636
              Random: fixed  13 labels. Loss 0.05136. Accuracy 1.000.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11322551
Train loss (w/o reg) on all data: 0.10121698
Test loss (w/o reg) on all data: 0.06553465
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.4653362e-06
Norm of the params: 15.497442
Flipped loss: 0.06553. Accuracy: 0.977
### Flips: 104, rs: 3, checks: 52
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025818996
Train loss (w/o reg) on all data: 0.015986018
Test loss (w/o reg) on all data: 0.028254176
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8172054e-06
Norm of the params: 14.023537
     Influence (LOO): fixed  41 labels. Loss 0.02825. Accuracy 0.989.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020279497
Train loss (w/o reg) on all data: 0.011059566
Test loss (w/o reg) on all data: 0.019644115
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1670148e-06
Norm of the params: 13.579344
                Loss: fixed  43 labels. Loss 0.01964. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11180693
Train loss (w/o reg) on all data: 0.10049887
Test loss (w/o reg) on all data: 0.06007776
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1368014e-05
Norm of the params: 15.038655
              Random: fixed   2 labels. Loss 0.06008. Accuracy 0.985.
### Flips: 104, rs: 3, checks: 104
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00912407
Train loss (w/o reg) on all data: 0.0038195048
Test loss (w/o reg) on all data: 0.010620134
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.572423e-07
Norm of the params: 10.300064
     Influence (LOO): fixed  50 labels. Loss 0.01062. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009856096
Train loss (w/o reg) on all data: 0.004063363
Test loss (w/o reg) on all data: 0.013554848
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3360636e-07
Norm of the params: 10.76358
                Loss: fixed  50 labels. Loss 0.01355. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10490103
Train loss (w/o reg) on all data: 0.093467794
Test loss (w/o reg) on all data: 0.058700487
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1617207e-05
Norm of the params: 15.121664
              Random: fixed   6 labels. Loss 0.05870. Accuracy 0.981.
### Flips: 104, rs: 3, checks: 156
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731013
Test loss (w/o reg) on all data: 0.01205527
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6489658e-07
Norm of the params: 9.15305
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730957
Test loss (w/o reg) on all data: 0.01205514
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7261123e-07
Norm of the params: 9.153055
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100387484
Train loss (w/o reg) on all data: 0.08912941
Test loss (w/o reg) on all data: 0.054009616
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.616447e-05
Norm of the params: 15.005382
              Random: fixed   9 labels. Loss 0.05401. Accuracy 0.985.
### Flips: 104, rs: 3, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.0120552005
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0196804e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729763
Test loss (w/o reg) on all data: 0.01205513
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.8579546e-08
Norm of the params: 9.153189
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09565939
Train loss (w/o reg) on all data: 0.08412141
Test loss (w/o reg) on all data: 0.054509606
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.1548853e-06
Norm of the params: 15.190769
              Random: fixed  11 labels. Loss 0.05451. Accuracy 0.989.
### Flips: 104, rs: 3, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728666
Test loss (w/o reg) on all data: 0.012055272
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8335373e-07
Norm of the params: 9.153307
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728692
Test loss (w/o reg) on all data: 0.012055181
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.447204e-07
Norm of the params: 9.153305
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09565939
Train loss (w/o reg) on all data: 0.08412054
Test loss (w/o reg) on all data: 0.05451401
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.4771745e-06
Norm of the params: 15.191343
              Random: fixed  11 labels. Loss 0.05451. Accuracy 0.989.
### Flips: 104, rs: 3, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012055029
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5710843e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729665
Test loss (w/o reg) on all data: 0.0120551055
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.941192e-08
Norm of the params: 9.153197
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09565938
Train loss (w/o reg) on all data: 0.0841212
Test loss (w/o reg) on all data: 0.054511145
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.262186e-06
Norm of the params: 15.1909075
              Random: fixed  11 labels. Loss 0.05451. Accuracy 0.989.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14131963
Train loss (w/o reg) on all data: 0.13210994
Test loss (w/o reg) on all data: 0.09916946
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.790912e-05
Norm of the params: 13.571809
Flipped loss: 0.09917. Accuracy: 0.962
### Flips: 104, rs: 4, checks: 52
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036730606
Train loss (w/o reg) on all data: 0.025470672
Test loss (w/o reg) on all data: 0.039300997
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.922549e-06
Norm of the params: 15.00662
     Influence (LOO): fixed  43 labels. Loss 0.03930. Accuracy 0.981.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026142249
Train loss (w/o reg) on all data: 0.014692513
Test loss (w/o reg) on all data: 0.033321105
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6194463e-06
Norm of the params: 15.132571
                Loss: fixed  47 labels. Loss 0.03332. Accuracy 0.985.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12958641
Train loss (w/o reg) on all data: 0.119886465
Test loss (w/o reg) on all data: 0.09059876
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.15257435e-05
Norm of the params: 13.928348
              Random: fixed   5 labels. Loss 0.09060. Accuracy 0.958.
### Flips: 104, rs: 4, checks: 104
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015935123
Train loss (w/o reg) on all data: 0.008904421
Test loss (w/o reg) on all data: 0.018363625
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0865107e-06
Norm of the params: 11.858079
     Influence (LOO): fixed  56 labels. Loss 0.01836. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009572963
Train loss (w/o reg) on all data: 0.0037939963
Test loss (w/o reg) on all data: 0.01881053
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.199111e-07
Norm of the params: 10.750783
                Loss: fixed  59 labels. Loss 0.01881. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11983573
Train loss (w/o reg) on all data: 0.11030932
Test loss (w/o reg) on all data: 0.09102157
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3136199e-05
Norm of the params: 13.803194
              Random: fixed   9 labels. Loss 0.09102. Accuracy 0.962.
### Flips: 104, rs: 4, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729812
Test loss (w/o reg) on all data: 0.012055044
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4820114e-08
Norm of the params: 9.153184
     Influence (LOO): fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007175619
Train loss (w/o reg) on all data: 0.0025651339
Test loss (w/o reg) on all data: 0.017068842
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9431158e-07
Norm of the params: 9.602589
                Loss: fixed  61 labels. Loss 0.01707. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118944675
Train loss (w/o reg) on all data: 0.109613694
Test loss (w/o reg) on all data: 0.08553322
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.10858e-06
Norm of the params: 13.660877
              Random: fixed  11 labels. Loss 0.08553. Accuracy 0.966.
### Flips: 104, rs: 4, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.012054662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9176805e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  62 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729602
Test loss (w/o reg) on all data: 0.012054772
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6775212e-07
Norm of the params: 9.153204
                Loss: fixed  62 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111505955
Train loss (w/o reg) on all data: 0.10157943
Test loss (w/o reg) on all data: 0.07902826
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.1195214e-05
Norm of the params: 14.090086
              Random: fixed  15 labels. Loss 0.07903. Accuracy 0.973.
### Flips: 104, rs: 4, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728748
Test loss (w/o reg) on all data: 0.01205468
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.993654e-07
Norm of the params: 9.153299
     Influence (LOO): fixed  62 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172876
Test loss (w/o reg) on all data: 0.012054573
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9361896e-07
Norm of the params: 9.153297
                Loss: fixed  62 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10007298
Train loss (w/o reg) on all data: 0.0893697
Test loss (w/o reg) on all data: 0.07005464
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.6941453e-06
Norm of the params: 14.630982
              Random: fixed  20 labels. Loss 0.07005. Accuracy 0.981.
### Flips: 104, rs: 4, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021730608
Test loss (w/o reg) on all data: 0.01205496
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7573515e-07
Norm of the params: 9.153093
     Influence (LOO): fixed  62 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730599
Test loss (w/o reg) on all data: 0.012055
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.128312e-07
Norm of the params: 9.153095
                Loss: fixed  62 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09472723
Train loss (w/o reg) on all data: 0.08381292
Test loss (w/o reg) on all data: 0.06690819
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.8242578e-06
Norm of the params: 14.77451
              Random: fixed  22 labels. Loss 0.06691. Accuracy 0.977.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1399239
Train loss (w/o reg) on all data: 0.13073777
Test loss (w/o reg) on all data: 0.10591736
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.5893965e-05
Norm of the params: 13.55444
Flipped loss: 0.10592. Accuracy: 0.962
### Flips: 104, rs: 5, checks: 52
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039566573
Train loss (w/o reg) on all data: 0.030233087
Test loss (w/o reg) on all data: 0.033147294
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2541467e-06
Norm of the params: 13.662713
     Influence (LOO): fixed  43 labels. Loss 0.03315. Accuracy 0.992.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021028232
Train loss (w/o reg) on all data: 0.010875724
Test loss (w/o reg) on all data: 0.041607358
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1642394e-06
Norm of the params: 14.249566
                Loss: fixed  47 labels. Loss 0.04161. Accuracy 0.981.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13614516
Train loss (w/o reg) on all data: 0.12687102
Test loss (w/o reg) on all data: 0.10729773
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.2984817e-06
Norm of the params: 13.619208
              Random: fixed   2 labels. Loss 0.10730. Accuracy 0.962.
### Flips: 104, rs: 5, checks: 104
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009183575
Train loss (w/o reg) on all data: 0.003587818
Test loss (w/o reg) on all data: 0.015390877
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.096698e-06
Norm of the params: 10.578996
     Influence (LOO): fixed  57 labels. Loss 0.01539. Accuracy 0.992.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008868916
Train loss (w/o reg) on all data: 0.0035013077
Test loss (w/o reg) on all data: 0.014765152
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.9701616e-08
Norm of the params: 10.36109
                Loss: fixed  58 labels. Loss 0.01477. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12207739
Train loss (w/o reg) on all data: 0.112363674
Test loss (w/o reg) on all data: 0.09972461
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.3625853e-05
Norm of the params: 13.938231
              Random: fixed   9 labels. Loss 0.09972. Accuracy 0.954.
### Flips: 104, rs: 5, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076132957
Train loss (w/o reg) on all data: 0.0028169223
Test loss (w/o reg) on all data: 0.013326265
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1286108e-07
Norm of the params: 9.794257
     Influence (LOO): fixed  59 labels. Loss 0.01333. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076132948
Train loss (w/o reg) on all data: 0.0028169216
Test loss (w/o reg) on all data: 0.013326309
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0559013e-07
Norm of the params: 9.794257
                Loss: fixed  59 labels. Loss 0.01333. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11119094
Train loss (w/o reg) on all data: 0.10130303
Test loss (w/o reg) on all data: 0.10311225
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.148834e-06
Norm of the params: 14.062647
              Random: fixed  13 labels. Loss 0.10311. Accuracy 0.966.
### Flips: 104, rs: 5, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076132948
Train loss (w/o reg) on all data: 0.002817019
Test loss (w/o reg) on all data: 0.013326414
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.792964e-07
Norm of the params: 9.794158
     Influence (LOO): fixed  59 labels. Loss 0.01333. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007613295
Train loss (w/o reg) on all data: 0.0028170156
Test loss (w/o reg) on all data: 0.013326466
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7456388e-07
Norm of the params: 9.794161
                Loss: fixed  59 labels. Loss 0.01333. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10666253
Train loss (w/o reg) on all data: 0.09682849
Test loss (w/o reg) on all data: 0.09887043
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 9.168532e-06
Norm of the params: 14.024292
              Random: fixed  15 labels. Loss 0.09887. Accuracy 0.962.
### Flips: 104, rs: 5, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021728808
Test loss (w/o reg) on all data: 0.012054579
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0039083e-07
Norm of the params: 9.15329
     Influence (LOO): fixed  60 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007613294
Train loss (w/o reg) on all data: 0.0028168745
Test loss (w/o reg) on all data: 0.013326472
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.591965e-07
Norm of the params: 9.794305
                Loss: fixed  59 labels. Loss 0.01333. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10666253
Train loss (w/o reg) on all data: 0.09682886
Test loss (w/o reg) on all data: 0.09887064
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.6566186e-05
Norm of the params: 14.024025
              Random: fixed  15 labels. Loss 0.09887. Accuracy 0.962.
### Flips: 104, rs: 5, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729094
Test loss (w/o reg) on all data: 0.012054671
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7231217e-07
Norm of the params: 9.153261
     Influence (LOO): fixed  60 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007613296
Train loss (w/o reg) on all data: 0.0028170003
Test loss (w/o reg) on all data: 0.013328842
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1357514e-06
Norm of the params: 9.794177
                Loss: fixed  59 labels. Loss 0.01333. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10554069
Train loss (w/o reg) on all data: 0.09604062
Test loss (w/o reg) on all data: 0.09621
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.308633e-06
Norm of the params: 13.784099
              Random: fixed  17 labels. Loss 0.09621. Accuracy 0.966.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1388379
Train loss (w/o reg) on all data: 0.13105316
Test loss (w/o reg) on all data: 0.07181261
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9641995e-05
Norm of the params: 12.4777775
Flipped loss: 0.07181. Accuracy: 0.977
### Flips: 104, rs: 6, checks: 52
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021379618
Train loss (w/o reg) on all data: 0.012876025
Test loss (w/o reg) on all data: 0.017388977
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9614392e-06
Norm of the params: 13.04116
     Influence (LOO): fixed  44 labels. Loss 0.01739. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01343126
Train loss (w/o reg) on all data: 0.0066919983
Test loss (w/o reg) on all data: 0.02625112
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5706457e-06
Norm of the params: 11.609704
                Loss: fixed  48 labels. Loss 0.02625. Accuracy 0.989.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12581135
Train loss (w/o reg) on all data: 0.11740959
Test loss (w/o reg) on all data: 0.067653626
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5483434e-05
Norm of the params: 12.962845
              Random: fixed   6 labels. Loss 0.06765. Accuracy 0.981.
### Flips: 104, rs: 6, checks: 104
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008497652
Train loss (w/o reg) on all data: 0.0037823631
Test loss (w/o reg) on all data: 0.01527202
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.238349e-07
Norm of the params: 9.711117
     Influence (LOO): fixed  52 labels. Loss 0.01527. Accuracy 0.992.
Using normal model
LBFGS training took [35] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.012054813
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.728987e-07
Norm of the params: 9.153204
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12068495
Train loss (w/o reg) on all data: 0.1122414
Test loss (w/o reg) on all data: 0.0621744
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.7821698e-05
Norm of the params: 12.99504
              Random: fixed   8 labels. Loss 0.06217. Accuracy 0.989.
### Flips: 104, rs: 6, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730466
Test loss (w/o reg) on all data: 0.01205524
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5727762e-07
Norm of the params: 9.153111
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730452
Test loss (w/o reg) on all data: 0.012055325
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8643969e-07
Norm of the params: 9.153112
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12068495
Train loss (w/o reg) on all data: 0.11224523
Test loss (w/o reg) on all data: 0.0621643
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.1145015e-06
Norm of the params: 12.992092
              Random: fixed   8 labels. Loss 0.06216. Accuracy 0.989.
### Flips: 104, rs: 6, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729076
Test loss (w/o reg) on all data: 0.012054356
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8482114e-07
Norm of the params: 9.153264
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729092
Test loss (w/o reg) on all data: 0.012054308
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1333975e-07
Norm of the params: 9.153263
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112485625
Train loss (w/o reg) on all data: 0.10461251
Test loss (w/o reg) on all data: 0.06301857
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.036717e-05
Norm of the params: 12.548403
              Random: fixed  13 labels. Loss 0.06302. Accuracy 0.981.
### Flips: 104, rs: 6, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002173033
Test loss (w/o reg) on all data: 0.012054811
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8175043e-07
Norm of the params: 9.153125
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002173026
Test loss (w/o reg) on all data: 0.012054648
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.714233e-07
Norm of the params: 9.153134
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10898768
Train loss (w/o reg) on all data: 0.101158805
Test loss (w/o reg) on all data: 0.060815126
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.7291195e-06
Norm of the params: 12.513097
              Random: fixed  14 labels. Loss 0.06082. Accuracy 0.985.
### Flips: 104, rs: 6, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173013
Test loss (w/o reg) on all data: 0.012054498
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9248368e-07
Norm of the params: 9.153148
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730124
Test loss (w/o reg) on all data: 0.012054547
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7478378e-07
Norm of the params: 9.153148
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09880072
Train loss (w/o reg) on all data: 0.09159628
Test loss (w/o reg) on all data: 0.060775157
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2289753e-05
Norm of the params: 12.003697
              Random: fixed  19 labels. Loss 0.06078. Accuracy 0.985.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13769065
Train loss (w/o reg) on all data: 0.12839013
Test loss (w/o reg) on all data: 0.09823029
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.3978053e-06
Norm of the params: 13.638559
Flipped loss: 0.09823. Accuracy: 0.981
### Flips: 104, rs: 7, checks: 52
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0294457
Train loss (w/o reg) on all data: 0.01987986
Test loss (w/o reg) on all data: 0.035983123
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.036792e-06
Norm of the params: 13.831731
     Influence (LOO): fixed  41 labels. Loss 0.03598. Accuracy 0.985.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012118598
Train loss (w/o reg) on all data: 0.005581888
Test loss (w/o reg) on all data: 0.016561005
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4627359e-06
Norm of the params: 11.433906
                Loss: fixed  49 labels. Loss 0.01656. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13033627
Train loss (w/o reg) on all data: 0.121329
Test loss (w/o reg) on all data: 0.092815846
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0200247e-05
Norm of the params: 13.421825
              Random: fixed   4 labels. Loss 0.09282. Accuracy 0.981.
### Flips: 104, rs: 7, checks: 104
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728943
Test loss (w/o reg) on all data: 0.012054629
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4681873e-07
Norm of the params: 9.153276
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729104
Test loss (w/o reg) on all data: 0.01205498
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.765945e-07
Norm of the params: 9.153258
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12645574
Train loss (w/o reg) on all data: 0.11781938
Test loss (w/o reg) on all data: 0.094432205
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7001441e-05
Norm of the params: 13.142572
              Random: fixed   6 labels. Loss 0.09443. Accuracy 0.981.
### Flips: 104, rs: 7, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729346
Test loss (w/o reg) on all data: 0.012055038
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.714499e-07
Norm of the params: 9.153233
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172935
Test loss (w/o reg) on all data: 0.012054952
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8796214e-07
Norm of the params: 9.153233
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114301965
Train loss (w/o reg) on all data: 0.10503479
Test loss (w/o reg) on all data: 0.094446644
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.3775254e-06
Norm of the params: 13.61409
              Random: fixed  10 labels. Loss 0.09445. Accuracy 0.981.
### Flips: 104, rs: 7, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002173008
Test loss (w/o reg) on all data: 0.012054736
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4486544e-07
Norm of the params: 9.153152
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730077
Test loss (w/o reg) on all data: 0.012054593
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5143968e-07
Norm of the params: 9.153152
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10970989
Train loss (w/o reg) on all data: 0.1009982
Test loss (w/o reg) on all data: 0.09539515
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.964424e-06
Norm of the params: 13.199767
              Random: fixed  12 labels. Loss 0.09540. Accuracy 0.966.
### Flips: 104, rs: 7, checks: 260
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021731125
Test loss (w/o reg) on all data: 0.012056345
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0533816e-07
Norm of the params: 9.15304
     Influence (LOO): fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021731057
Test loss (w/o reg) on all data: 0.012056129
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.302538e-07
Norm of the params: 9.153047
                Loss: fixed  53 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10870214
Train loss (w/o reg) on all data: 0.09972555
Test loss (w/o reg) on all data: 0.09545219
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.0955476e-06
Norm of the params: 13.398943
              Random: fixed  13 labels. Loss 0.09545. Accuracy 0.969.
### Flips: 104, rs: 7, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172955
Test loss (w/o reg) on all data: 0.012054976
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6889638e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729565
Test loss (w/o reg) on all data: 0.012054953
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7420248e-07
Norm of the params: 9.153211
                Loss: fixed  53 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10324683
Train loss (w/o reg) on all data: 0.094313055
Test loss (w/o reg) on all data: 0.09468906
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.952645e-06
Norm of the params: 13.366957
              Random: fixed  15 labels. Loss 0.09469. Accuracy 0.973.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13194537
Train loss (w/o reg) on all data: 0.12175369
Test loss (w/o reg) on all data: 0.08728222
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2837252e-05
Norm of the params: 14.277032
Flipped loss: 0.08728. Accuracy: 0.973
### Flips: 104, rs: 8, checks: 52
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034983065
Train loss (w/o reg) on all data: 0.026273582
Test loss (w/o reg) on all data: 0.032848556
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.73838e-06
Norm of the params: 13.198092
     Influence (LOO): fixed  41 labels. Loss 0.03285. Accuracy 0.985.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01886433
Train loss (w/o reg) on all data: 0.00901728
Test loss (w/o reg) on all data: 0.030395966
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7775019e-06
Norm of the params: 14.033566
                Loss: fixed  47 labels. Loss 0.03040. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13074915
Train loss (w/o reg) on all data: 0.120833844
Test loss (w/o reg) on all data: 0.08265416
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8747678e-05
Norm of the params: 14.082118
              Random: fixed   2 labels. Loss 0.08265. Accuracy 0.973.
### Flips: 104, rs: 8, checks: 104
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008108737
Train loss (w/o reg) on all data: 0.00306431
Test loss (w/o reg) on all data: 0.015014872
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.6433493e-07
Norm of the params: 10.044329
     Influence (LOO): fixed  55 labels. Loss 0.01501. Accuracy 0.996.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0090775825
Train loss (w/o reg) on all data: 0.00337377
Test loss (w/o reg) on all data: 0.012912854
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0245803e-06
Norm of the params: 10.68065
                Loss: fixed  54 labels. Loss 0.01291. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12173789
Train loss (w/o reg) on all data: 0.11104325
Test loss (w/o reg) on all data: 0.07428028
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3388908e-05
Norm of the params: 14.625075
              Random: fixed   7 labels. Loss 0.07428. Accuracy 0.973.
### Flips: 104, rs: 8, checks: 156
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729427
Test loss (w/o reg) on all data: 0.01205448
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1607671e-07
Norm of the params: 9.153226
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172942
Test loss (w/o reg) on all data: 0.012054439
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9867953e-07
Norm of the params: 9.153225
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12046428
Train loss (w/o reg) on all data: 0.10966967
Test loss (w/o reg) on all data: 0.0740712
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.1619231e-05
Norm of the params: 14.693273
              Random: fixed   8 labels. Loss 0.07407. Accuracy 0.973.
### Flips: 104, rs: 8, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729586
Test loss (w/o reg) on all data: 0.012054907
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2779219e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729583
Test loss (w/o reg) on all data: 0.012054862
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.276744e-07
Norm of the params: 9.153208
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11117005
Train loss (w/o reg) on all data: 0.10071751
Test loss (w/o reg) on all data: 0.06624966
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7365726e-05
Norm of the params: 14.458594
              Random: fixed  13 labels. Loss 0.06625. Accuracy 0.981.
### Flips: 104, rs: 8, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172952
Test loss (w/o reg) on all data: 0.01205483
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.716919e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012054871
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4452206e-07
Norm of the params: 9.153204
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10672805
Train loss (w/o reg) on all data: 0.09661969
Test loss (w/o reg) on all data: 0.06510296
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.1427753e-05
Norm of the params: 14.218551
              Random: fixed  15 labels. Loss 0.06510. Accuracy 0.981.
### Flips: 104, rs: 8, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729362
Test loss (w/o reg) on all data: 0.012054933
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2745958e-07
Norm of the params: 9.153234
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172935
Test loss (w/o reg) on all data: 0.012054975
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9094346e-07
Norm of the params: 9.153233
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097739115
Train loss (w/o reg) on all data: 0.087784074
Test loss (w/o reg) on all data: 0.061104752
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0007836e-05
Norm of the params: 14.110307
              Random: fixed  19 labels. Loss 0.06110. Accuracy 0.981.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.112640195
Train loss (w/o reg) on all data: 0.105014354
Test loss (w/o reg) on all data: 0.074226655
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6854605e-05
Norm of the params: 12.34977
Flipped loss: 0.07423. Accuracy: 0.973
### Flips: 104, rs: 9, checks: 52
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009463543
Train loss (w/o reg) on all data: 0.0039931894
Test loss (w/o reg) on all data: 0.0338629
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.6795766e-07
Norm of the params: 10.4597845
     Influence (LOO): fixed  40 labels. Loss 0.03386. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008454241
Train loss (w/o reg) on all data: 0.003151071
Test loss (w/o reg) on all data: 0.02579805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1697004e-07
Norm of the params: 10.298709
                Loss: fixed  40 labels. Loss 0.02580. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11097562
Train loss (w/o reg) on all data: 0.10327925
Test loss (w/o reg) on all data: 0.07459382
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.677103e-06
Norm of the params: 12.40675
              Random: fixed   1 labels. Loss 0.07459. Accuracy 0.973.
### Flips: 104, rs: 9, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172977
Test loss (w/o reg) on all data: 0.012054774
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8147966e-07
Norm of the params: 9.153189
     Influence (LOO): fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172976
Test loss (w/o reg) on all data: 0.012054823
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.270615e-07
Norm of the params: 9.153189
                Loss: fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10152737
Train loss (w/o reg) on all data: 0.09352404
Test loss (w/o reg) on all data: 0.07372564
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.9147766e-05
Norm of the params: 12.651743
              Random: fixed   5 labels. Loss 0.07373. Accuracy 0.989.
### Flips: 104, rs: 9, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729483
Test loss (w/o reg) on all data: 0.012055433
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.389594e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  43 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729474
Test loss (w/o reg) on all data: 0.012055549
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3835695e-07
Norm of the params: 9.153217
                Loss: fixed  43 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101527385
Train loss (w/o reg) on all data: 0.09352088
Test loss (w/o reg) on all data: 0.07372295
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6923143e-05
Norm of the params: 12.654255
              Random: fixed   5 labels. Loss 0.07372. Accuracy 0.989.
### Flips: 104, rs: 9, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730561
Test loss (w/o reg) on all data: 0.012054958
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9437555e-07
Norm of the params: 9.153101
     Influence (LOO): fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173055
Test loss (w/o reg) on all data: 0.012055018
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8414057e-07
Norm of the params: 9.153102
                Loss: fixed  43 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10064904
Train loss (w/o reg) on all data: 0.09284044
Test loss (w/o reg) on all data: 0.071812205
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2469935e-05
Norm of the params: 12.496878
              Random: fixed   6 labels. Loss 0.07181. Accuracy 0.985.
### Flips: 104, rs: 9, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729318
Test loss (w/o reg) on all data: 0.012054877
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.390226e-07
Norm of the params: 9.153236
     Influence (LOO): fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729332
Test loss (w/o reg) on all data: 0.012054861
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.46667e-08
Norm of the params: 9.153235
                Loss: fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09454135
Train loss (w/o reg) on all data: 0.08653738
Test loss (w/o reg) on all data: 0.0670876
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.326007e-06
Norm of the params: 12.652245
              Random: fixed   9 labels. Loss 0.06709. Accuracy 0.989.
### Flips: 104, rs: 9, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173036
Test loss (w/o reg) on all data: 0.012054205
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8701302e-07
Norm of the params: 9.153123
     Influence (LOO): fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730238
Test loss (w/o reg) on all data: 0.012054196
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1832681e-06
Norm of the params: 9.153135
                Loss: fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08484918
Train loss (w/o reg) on all data: 0.07640317
Test loss (w/o reg) on all data: 0.06508456
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.569762e-05
Norm of the params: 12.996931
              Random: fixed  12 labels. Loss 0.06508. Accuracy 0.973.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124576814
Train loss (w/o reg) on all data: 0.11702
Test loss (w/o reg) on all data: 0.077401794
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.57783e-05
Norm of the params: 12.293748
Flipped loss: 0.07740. Accuracy: 0.985
### Flips: 104, rs: 10, checks: 52
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033701323
Train loss (w/o reg) on all data: 0.024406107
Test loss (w/o reg) on all data: 0.03826626
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.808549e-06
Norm of the params: 13.634674
     Influence (LOO): fixed  37 labels. Loss 0.03827. Accuracy 0.992.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0130337365
Train loss (w/o reg) on all data: 0.0058356747
Test loss (w/o reg) on all data: 0.024546882
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8486496e-07
Norm of the params: 11.998384
                Loss: fixed  46 labels. Loss 0.02455. Accuracy 0.989.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119557425
Train loss (w/o reg) on all data: 0.11225481
Test loss (w/o reg) on all data: 0.071851045
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.2431185e-06
Norm of the params: 12.085208
              Random: fixed   4 labels. Loss 0.07185. Accuracy 0.985.
### Flips: 104, rs: 10, checks: 104
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012904959
Train loss (w/o reg) on all data: 0.006039133
Test loss (w/o reg) on all data: 0.023113258
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3092933e-06
Norm of the params: 11.718213
     Influence (LOO): fixed  48 labels. Loss 0.02311. Accuracy 0.989.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00737607
Train loss (w/o reg) on all data: 0.0025708724
Test loss (w/o reg) on all data: 0.012873635
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5475273e-07
Norm of the params: 9.803263
                Loss: fixed  51 labels. Loss 0.01287. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11615825
Train loss (w/o reg) on all data: 0.10886858
Test loss (w/o reg) on all data: 0.07124106
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5320673e-05
Norm of the params: 12.074492
              Random: fixed   6 labels. Loss 0.07124. Accuracy 0.985.
### Flips: 104, rs: 10, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007376071
Train loss (w/o reg) on all data: 0.0025708743
Test loss (w/o reg) on all data: 0.012873171
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.497949e-08
Norm of the params: 9.803262
     Influence (LOO): fixed  51 labels. Loss 0.01287. Accuracy 0.992.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729914
Test loss (w/o reg) on all data: 0.012055624
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3101988e-07
Norm of the params: 9.15317
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10750676
Train loss (w/o reg) on all data: 0.09989781
Test loss (w/o reg) on all data: 0.06863077
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.9094127e-06
Norm of the params: 12.336085
              Random: fixed   9 labels. Loss 0.06863. Accuracy 0.989.
### Flips: 104, rs: 10, checks: 208
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729313
Test loss (w/o reg) on all data: 0.012054743
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6819371e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172931
Test loss (w/o reg) on all data: 0.012054791
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.79036e-07
Norm of the params: 9.153237
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097129
Train loss (w/o reg) on all data: 0.08866734
Test loss (w/o reg) on all data: 0.07026358
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.3889802e-05
Norm of the params: 13.008969
              Random: fixed  11 labels. Loss 0.07026. Accuracy 0.981.
### Flips: 104, rs: 10, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172975
Test loss (w/o reg) on all data: 0.012054979
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4227855e-07
Norm of the params: 9.15319
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729749
Test loss (w/o reg) on all data: 0.012055041
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.161494e-07
Norm of the params: 9.153191
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097129
Train loss (w/o reg) on all data: 0.08866517
Test loss (w/o reg) on all data: 0.07024562
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.2678e-06
Norm of the params: 13.010631
              Random: fixed  11 labels. Loss 0.07025. Accuracy 0.981.
### Flips: 104, rs: 10, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730266
Test loss (w/o reg) on all data: 0.0120541705
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.541424e-07
Norm of the params: 9.153133
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173026
Test loss (w/o reg) on all data: 0.01205436
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5529388e-07
Norm of the params: 9.153133
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09268891
Train loss (w/o reg) on all data: 0.08446428
Test loss (w/o reg) on all data: 0.06461132
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.20492e-06
Norm of the params: 12.825463
              Random: fixed  14 labels. Loss 0.06461. Accuracy 0.981.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14739618
Train loss (w/o reg) on all data: 0.1400541
Test loss (w/o reg) on all data: 0.06728764
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.633537e-06
Norm of the params: 12.117813
Flipped loss: 0.06729. Accuracy: 0.992
### Flips: 104, rs: 11, checks: 52
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030151965
Train loss (w/o reg) on all data: 0.023540206
Test loss (w/o reg) on all data: 0.020369101
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.967204e-07
Norm of the params: 11.499356
     Influence (LOO): fixed  47 labels. Loss 0.02037. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012733882
Train loss (w/o reg) on all data: 0.0053862953
Test loss (w/o reg) on all data: 0.022682413
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1110172e-07
Norm of the params: 12.122366
                Loss: fixed  49 labels. Loss 0.02268. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14739619
Train loss (w/o reg) on all data: 0.14005746
Test loss (w/o reg) on all data: 0.067270875
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4767638e-05
Norm of the params: 12.115054
              Random: fixed   0 labels. Loss 0.06727. Accuracy 0.992.
### Flips: 104, rs: 11, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172916
Test loss (w/o reg) on all data: 0.012054911
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9521364e-07
Norm of the params: 9.153254
     Influence (LOO): fixed  54 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729171
Test loss (w/o reg) on all data: 0.01205486
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.573915e-07
Norm of the params: 9.153253
                Loss: fixed  54 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13656017
Train loss (w/o reg) on all data: 0.12958401
Test loss (w/o reg) on all data: 0.06477701
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.01503165e-05
Norm of the params: 11.811995
              Random: fixed   4 labels. Loss 0.06478. Accuracy 0.992.
### Flips: 104, rs: 11, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730384
Test loss (w/o reg) on all data: 0.012054452
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9044167e-07
Norm of the params: 9.153119
     Influence (LOO): fixed  54 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730398
Test loss (w/o reg) on all data: 0.012054382
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6570134e-07
Norm of the params: 9.153119
                Loss: fixed  54 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12910722
Train loss (w/o reg) on all data: 0.122311264
Test loss (w/o reg) on all data: 0.06192492
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2509874e-05
Norm of the params: 11.658433
              Random: fixed   7 labels. Loss 0.06192. Accuracy 0.989.
### Flips: 104, rs: 11, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729902
Test loss (w/o reg) on all data: 0.01205506
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.557508e-07
Norm of the params: 9.153174
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172989
Test loss (w/o reg) on all data: 0.012055089
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5279456e-07
Norm of the params: 9.153174
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122483484
Train loss (w/o reg) on all data: 0.11554452
Test loss (w/o reg) on all data: 0.06326726
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.310241e-06
Norm of the params: 11.780464
              Random: fixed  10 labels. Loss 0.06327. Accuracy 0.985.
### Flips: 104, rs: 11, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730799
Test loss (w/o reg) on all data: 0.01205492
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.278521e-07
Norm of the params: 9.153072
     Influence (LOO): fixed  54 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002173081
Test loss (w/o reg) on all data: 0.012055024
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.672143e-07
Norm of the params: 9.153074
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11460378
Train loss (w/o reg) on all data: 0.1076993
Test loss (w/o reg) on all data: 0.060078736
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3376473e-05
Norm of the params: 11.751159
              Random: fixed  14 labels. Loss 0.06008. Accuracy 0.989.
### Flips: 104, rs: 11, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173022
Test loss (w/o reg) on all data: 0.012054342
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8156526e-07
Norm of the params: 9.153136
     Influence (LOO): fixed  54 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730224
Test loss (w/o reg) on all data: 0.012054272
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2359801e-07
Norm of the params: 9.153137
                Loss: fixed  54 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09984331
Train loss (w/o reg) on all data: 0.092326656
Test loss (w/o reg) on all data: 0.05802191
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.344412e-06
Norm of the params: 12.261039
              Random: fixed  18 labels. Loss 0.05802. Accuracy 0.989.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11352358
Train loss (w/o reg) on all data: 0.10314909
Test loss (w/o reg) on all data: 0.06167591
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.27606045e-05
Norm of the params: 14.404501
Flipped loss: 0.06168. Accuracy: 0.977
### Flips: 104, rs: 12, checks: 52
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01879841
Train loss (w/o reg) on all data: 0.01070145
Test loss (w/o reg) on all data: 0.021930411
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.921506e-06
Norm of the params: 12.7255335
     Influence (LOO): fixed  39 labels. Loss 0.02193. Accuracy 0.989.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01581017
Train loss (w/o reg) on all data: 0.007480501
Test loss (w/o reg) on all data: 0.018346513
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.8813775e-07
Norm of the params: 12.9071045
                Loss: fixed  39 labels. Loss 0.01835. Accuracy 0.996.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11480623
Train loss (w/o reg) on all data: 0.10530657
Test loss (w/o reg) on all data: 0.053881813
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2239973e-05
Norm of the params: 13.783798
              Random: fixed   2 labels. Loss 0.05388. Accuracy 0.985.
### Flips: 104, rs: 12, checks: 104
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007810209
Train loss (w/o reg) on all data: 0.0029663446
Test loss (w/o reg) on all data: 0.0152704185
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9645288e-06
Norm of the params: 9.842626
     Influence (LOO): fixed  45 labels. Loss 0.01527. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0078102103
Train loss (w/o reg) on all data: 0.0029663355
Test loss (w/o reg) on all data: 0.015270632
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0953043e-07
Norm of the params: 9.842637
                Loss: fixed  45 labels. Loss 0.01527. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11407207
Train loss (w/o reg) on all data: 0.105060905
Test loss (w/o reg) on all data: 0.053706914
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.864107e-06
Norm of the params: 13.424726
              Random: fixed   3 labels. Loss 0.05371. Accuracy 0.985.
### Flips: 104, rs: 12, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007810211
Train loss (w/o reg) on all data: 0.00296606
Test loss (w/o reg) on all data: 0.015269678
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.573977e-07
Norm of the params: 9.8429165
     Influence (LOO): fixed  45 labels. Loss 0.01527. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007810208
Train loss (w/o reg) on all data: 0.0029660584
Test loss (w/o reg) on all data: 0.015269729
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.02884e-07
Norm of the params: 9.842916
                Loss: fixed  45 labels. Loss 0.01527. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10662605
Train loss (w/o reg) on all data: 0.09759011
Test loss (w/o reg) on all data: 0.053028632
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6971255e-05
Norm of the params: 13.443171
              Random: fixed   6 labels. Loss 0.05303. Accuracy 0.989.
### Flips: 104, rs: 12, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728834
Test loss (w/o reg) on all data: 0.0120543
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4390805e-07
Norm of the params: 9.15329
     Influence (LOO): fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0078102094
Train loss (w/o reg) on all data: 0.0029659444
Test loss (w/o reg) on all data: 0.015269968
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6686766e-07
Norm of the params: 9.843033
                Loss: fixed  45 labels. Loss 0.01527. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10158079
Train loss (w/o reg) on all data: 0.09301909
Test loss (w/o reg) on all data: 0.0537631
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.517196e-06
Norm of the params: 13.085644
              Random: fixed   8 labels. Loss 0.05376. Accuracy 0.989.
### Flips: 104, rs: 12, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729376
Test loss (w/o reg) on all data: 0.012054713
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2616748e-07
Norm of the params: 9.153233
     Influence (LOO): fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729355
Test loss (w/o reg) on all data: 0.012054788
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0597317e-07
Norm of the params: 9.153232
                Loss: fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101580806
Train loss (w/o reg) on all data: 0.093019605
Test loss (w/o reg) on all data: 0.053759206
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.988447e-06
Norm of the params: 13.085261
              Random: fixed   8 labels. Loss 0.05376. Accuracy 0.989.
### Flips: 104, rs: 12, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002173034
Test loss (w/o reg) on all data: 0.012053701
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6838995e-07
Norm of the params: 9.153124
     Influence (LOO): fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730335
Test loss (w/o reg) on all data: 0.012053667
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1706349e-07
Norm of the params: 9.153126
                Loss: fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09186895
Train loss (w/o reg) on all data: 0.084237255
Test loss (w/o reg) on all data: 0.04445985
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0612801e-05
Norm of the params: 12.354508
              Random: fixed  13 labels. Loss 0.04446. Accuracy 0.996.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14398119
Train loss (w/o reg) on all data: 0.13381684
Test loss (w/o reg) on all data: 0.06871431
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9471181e-05
Norm of the params: 14.257874
Flipped loss: 0.06871. Accuracy: 0.981
### Flips: 104, rs: 13, checks: 52
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043742325
Train loss (w/o reg) on all data: 0.031559188
Test loss (w/o reg) on all data: 0.026165212
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.7441184e-06
Norm of the params: 15.6097
     Influence (LOO): fixed  42 labels. Loss 0.02617. Accuracy 0.985.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032963753
Train loss (w/o reg) on all data: 0.019918151
Test loss (w/o reg) on all data: 0.020520238
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6069606e-06
Norm of the params: 16.152771
                Loss: fixed  47 labels. Loss 0.02052. Accuracy 0.989.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13693887
Train loss (w/o reg) on all data: 0.12725893
Test loss (w/o reg) on all data: 0.059539415
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.343908e-06
Norm of the params: 13.91398
              Random: fixed   6 labels. Loss 0.05954. Accuracy 0.989.
### Flips: 104, rs: 13, checks: 104
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010901507
Train loss (w/o reg) on all data: 0.0050807837
Test loss (w/o reg) on all data: 0.015447033
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.767187e-07
Norm of the params: 10.789555
     Influence (LOO): fixed  60 labels. Loss 0.01545. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008914974
Train loss (w/o reg) on all data: 0.0034713468
Test loss (w/o reg) on all data: 0.012796882
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9569443e-07
Norm of the params: 10.4342
                Loss: fixed  61 labels. Loss 0.01280. Accuracy 0.996.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13430814
Train loss (w/o reg) on all data: 0.1245387
Test loss (w/o reg) on all data: 0.059084702
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.883416e-06
Norm of the params: 13.978162
              Random: fixed   8 labels. Loss 0.05908. Accuracy 0.989.
### Flips: 104, rs: 13, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728524
Test loss (w/o reg) on all data: 0.012054152
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3891587e-07
Norm of the params: 9.153323
     Influence (LOO): fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076132948
Train loss (w/o reg) on all data: 0.0028168482
Test loss (w/o reg) on all data: 0.013326305
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7232423e-07
Norm of the params: 9.794332
                Loss: fixed  62 labels. Loss 0.01333. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12961379
Train loss (w/o reg) on all data: 0.12020403
Test loss (w/o reg) on all data: 0.060989834
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8939734e-05
Norm of the params: 13.718427
              Random: fixed  12 labels. Loss 0.06099. Accuracy 0.985.
### Flips: 104, rs: 13, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729087
Test loss (w/o reg) on all data: 0.012054968
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5211266e-07
Norm of the params: 9.153261
     Influence (LOO): fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729132
Test loss (w/o reg) on all data: 0.012055017
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.018558e-07
Norm of the params: 9.153257
                Loss: fixed  63 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12698108
Train loss (w/o reg) on all data: 0.11765342
Test loss (w/o reg) on all data: 0.05717302
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7101955e-06
Norm of the params: 13.658444
              Random: fixed  14 labels. Loss 0.05717. Accuracy 0.992.
### Flips: 104, rs: 13, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728927
Test loss (w/o reg) on all data: 0.012055006
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8670547e-07
Norm of the params: 9.15328
     Influence (LOO): fixed  63 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729423
Test loss (w/o reg) on all data: 0.012054002
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9414746e-07
Norm of the params: 9.153225
                Loss: fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118274376
Train loss (w/o reg) on all data: 0.10892321
Test loss (w/o reg) on all data: 0.05387462
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.635112e-06
Norm of the params: 13.675645
              Random: fixed  18 labels. Loss 0.05387. Accuracy 0.996.
### Flips: 104, rs: 13, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729954
Test loss (w/o reg) on all data: 0.012054026
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.797552e-06
Norm of the params: 9.153167
     Influence (LOO): fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729923
Test loss (w/o reg) on all data: 0.012054385
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4508748e-07
Norm of the params: 9.153171
                Loss: fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11740477
Train loss (w/o reg) on all data: 0.10829709
Test loss (w/o reg) on all data: 0.05123255
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.3341869e-06
Norm of the params: 13.496429
              Random: fixed  19 labels. Loss 0.05123. Accuracy 0.996.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12896526
Train loss (w/o reg) on all data: 0.11866343
Test loss (w/o reg) on all data: 0.067999944
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.560468e-06
Norm of the params: 14.35397
Flipped loss: 0.06800. Accuracy: 0.989
### Flips: 104, rs: 14, checks: 52
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023389757
Train loss (w/o reg) on all data: 0.014726041
Test loss (w/o reg) on all data: 0.024657156
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.1385302e-06
Norm of the params: 13.16337
     Influence (LOO): fixed  42 labels. Loss 0.02466. Accuracy 0.992.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014332228
Train loss (w/o reg) on all data: 0.0065166634
Test loss (w/o reg) on all data: 0.016255869
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7454023e-07
Norm of the params: 12.502453
                Loss: fixed  45 labels. Loss 0.01626. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1226719
Train loss (w/o reg) on all data: 0.11210911
Test loss (w/o reg) on all data: 0.07001292
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.2692114e-05
Norm of the params: 14.534645
              Random: fixed   3 labels. Loss 0.07001. Accuracy 0.985.
### Flips: 104, rs: 14, checks: 104
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021732438
Test loss (w/o reg) on all data: 0.0120547395
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.167249e-07
Norm of the params: 9.152894
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173241
Test loss (w/o reg) on all data: 0.012054898
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.33469e-07
Norm of the params: 9.152899
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10991043
Train loss (w/o reg) on all data: 0.09889548
Test loss (w/o reg) on all data: 0.05948313
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.250383e-06
Norm of the params: 14.84247
              Random: fixed   9 labels. Loss 0.05948. Accuracy 0.989.
### Flips: 104, rs: 14, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730578
Test loss (w/o reg) on all data: 0.01205536
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7109235e-07
Norm of the params: 9.153097
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730547
Test loss (w/o reg) on all data: 0.012055423
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3624474e-07
Norm of the params: 9.153101
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10817316
Train loss (w/o reg) on all data: 0.097178936
Test loss (w/o reg) on all data: 0.05933861
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.22828e-06
Norm of the params: 14.8285055
              Random: fixed  10 labels. Loss 0.05934. Accuracy 0.989.
### Flips: 104, rs: 14, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728897
Test loss (w/o reg) on all data: 0.0120552275
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.014294e-07
Norm of the params: 9.153283
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.00217289
Test loss (w/o reg) on all data: 0.01205519
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.385771e-08
Norm of the params: 9.153282
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10314197
Train loss (w/o reg) on all data: 0.09249135
Test loss (w/o reg) on all data: 0.05150943
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.1282653e-06
Norm of the params: 14.594944
              Random: fixed  12 labels. Loss 0.05151. Accuracy 0.989.
### Flips: 104, rs: 14, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729374
Test loss (w/o reg) on all data: 0.012055198
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5996674e-07
Norm of the params: 9.153231
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729383
Test loss (w/o reg) on all data: 0.012055293
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0933889e-07
Norm of the params: 9.153231
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09913776
Train loss (w/o reg) on all data: 0.088005334
Test loss (w/o reg) on all data: 0.055588987
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.400192e-06
Norm of the params: 14.921411
              Random: fixed  14 labels. Loss 0.05559. Accuracy 0.977.
### Flips: 104, rs: 14, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.012055957
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4772603e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172993
Test loss (w/o reg) on all data: 0.0120560145
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9792128e-07
Norm of the params: 9.153171
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07444695
Train loss (w/o reg) on all data: 0.060989976
Test loss (w/o reg) on all data: 0.039977964
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.4895327e-06
Norm of the params: 16.405466
              Random: fixed  21 labels. Loss 0.03998. Accuracy 0.989.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1425479
Train loss (w/o reg) on all data: 0.13425508
Test loss (w/o reg) on all data: 0.07501199
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.080069e-05
Norm of the params: 12.878533
Flipped loss: 0.07501. Accuracy: 0.985
### Flips: 104, rs: 15, checks: 52
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035144113
Train loss (w/o reg) on all data: 0.027232101
Test loss (w/o reg) on all data: 0.0154142305
Train acc on all data:  0.9914040114613181
Test acc on all data:   1.0
Norm of the mean of gradients: 4.0976906e-06
Norm of the params: 12.579358
     Influence (LOO): fixed  45 labels. Loss 0.01541. Accuracy 1.000.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019025102
Train loss (w/o reg) on all data: 0.0092270635
Test loss (w/o reg) on all data: 0.012300876
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.12963e-06
Norm of the params: 13.998599
                Loss: fixed  47 labels. Loss 0.01230. Accuracy 0.996.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13836335
Train loss (w/o reg) on all data: 0.1300155
Test loss (w/o reg) on all data: 0.072642736
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8099543e-05
Norm of the params: 12.92118
              Random: fixed   2 labels. Loss 0.07264. Accuracy 0.989.
### Flips: 104, rs: 15, checks: 104
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729497
Test loss (w/o reg) on all data: 0.012055132
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.027382e-06
Norm of the params: 9.153216
     Influence (LOO): fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0080960095
Train loss (w/o reg) on all data: 0.0029192106
Test loss (w/o reg) on all data: 0.011804504
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.122529e-07
Norm of the params: 10.175262
                Loss: fixed  55 labels. Loss 0.01180. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13498509
Train loss (w/o reg) on all data: 0.12643394
Test loss (w/o reg) on all data: 0.07202623
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.822845e-05
Norm of the params: 13.077579
              Random: fixed   4 labels. Loss 0.07203. Accuracy 0.992.
### Flips: 104, rs: 15, checks: 156
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730056
Test loss (w/o reg) on all data: 0.012055364
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4367304e-07
Norm of the params: 9.153154
     Influence (LOO): fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730051
Test loss (w/o reg) on all data: 0.012055414
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0739892e-07
Norm of the params: 9.153154
                Loss: fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12921984
Train loss (w/o reg) on all data: 0.120579764
Test loss (w/o reg) on all data: 0.06403429
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.854208e-06
Norm of the params: 13.145407
              Random: fixed   7 labels. Loss 0.06403. Accuracy 0.992.
### Flips: 104, rs: 15, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729805
Test loss (w/o reg) on all data: 0.012054485
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.728036e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729807
Test loss (w/o reg) on all data: 0.012054394
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.258046e-07
Norm of the params: 9.153183
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12549745
Train loss (w/o reg) on all data: 0.116801105
Test loss (w/o reg) on all data: 0.060079042
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.7118843e-05
Norm of the params: 13.188133
              Random: fixed   9 labels. Loss 0.06008. Accuracy 0.996.
### Flips: 104, rs: 15, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173138
Test loss (w/o reg) on all data: 0.012055247
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2901342e-07
Norm of the params: 9.153012
     Influence (LOO): fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731325
Test loss (w/o reg) on all data: 0.01205517
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6619097e-07
Norm of the params: 9.153017
                Loss: fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11194434
Train loss (w/o reg) on all data: 0.10222926
Test loss (w/o reg) on all data: 0.063260235
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6429647e-05
Norm of the params: 13.939209
              Random: fixed  13 labels. Loss 0.06326. Accuracy 0.992.
### Flips: 104, rs: 15, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730114
Test loss (w/o reg) on all data: 0.012055325
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4147119e-07
Norm of the params: 9.153148
     Influence (LOO): fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730105
Test loss (w/o reg) on all data: 0.012055253
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8010803e-07
Norm of the params: 9.1531515
                Loss: fixed  57 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.103125304
Train loss (w/o reg) on all data: 0.09343245
Test loss (w/o reg) on all data: 0.05454029
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.8346653e-06
Norm of the params: 13.923257
              Random: fixed  17 labels. Loss 0.05454. Accuracy 0.996.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14444005
Train loss (w/o reg) on all data: 0.13632472
Test loss (w/o reg) on all data: 0.074074835
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.17943e-06
Norm of the params: 12.739963
Flipped loss: 0.07407. Accuracy: 0.992
### Flips: 104, rs: 16, checks: 52
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037820198
Train loss (w/o reg) on all data: 0.026785707
Test loss (w/o reg) on all data: 0.03168646
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0347338e-06
Norm of the params: 14.855633
     Influence (LOO): fixed  43 labels. Loss 0.03169. Accuracy 0.985.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022831313
Train loss (w/o reg) on all data: 0.012514159
Test loss (w/o reg) on all data: 0.014953897
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.316333e-06
Norm of the params: 14.364648
                Loss: fixed  48 labels. Loss 0.01495. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14190102
Train loss (w/o reg) on all data: 0.13422179
Test loss (w/o reg) on all data: 0.070375
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0947913e-05
Norm of the params: 12.392922
              Random: fixed   2 labels. Loss 0.07038. Accuracy 0.992.
### Flips: 104, rs: 16, checks: 104
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00973275
Train loss (w/o reg) on all data: 0.004243756
Test loss (w/o reg) on all data: 0.016372152
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.287622e-06
Norm of the params: 10.47759
     Influence (LOO): fixed  57 labels. Loss 0.01637. Accuracy 0.996.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00973275
Train loss (w/o reg) on all data: 0.0042437566
Test loss (w/o reg) on all data: 0.016371898
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.644927e-07
Norm of the params: 10.47759
                Loss: fixed  57 labels. Loss 0.01637. Accuracy 0.996.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13356228
Train loss (w/o reg) on all data: 0.12621023
Test loss (w/o reg) on all data: 0.062535465
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.7520483e-05
Norm of the params: 12.12605
              Random: fixed   6 labels. Loss 0.06254. Accuracy 0.996.
### Flips: 104, rs: 16, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730375
Test loss (w/o reg) on all data: 0.0120552
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.483511e-07
Norm of the params: 9.153119
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730373
Test loss (w/o reg) on all data: 0.012055124
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.677373e-07
Norm of the params: 9.15312
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1286734
Train loss (w/o reg) on all data: 0.12130227
Test loss (w/o reg) on all data: 0.0600226
Train acc on all data:  0.9551098376313276
Test acc on all data:   1.0
Norm of the mean of gradients: 6.7201745e-06
Norm of the params: 12.141774
              Random: fixed   9 labels. Loss 0.06002. Accuracy 1.000.
### Flips: 104, rs: 16, checks: 208
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730203
Test loss (w/o reg) on all data: 0.012055375
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.569972e-07
Norm of the params: 9.153141
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730189
Test loss (w/o reg) on all data: 0.012055453
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3214562e-07
Norm of the params: 9.153142
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12395919
Train loss (w/o reg) on all data: 0.116199344
Test loss (w/o reg) on all data: 0.05929244
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7525801e-05
Norm of the params: 12.457809
              Random: fixed  11 labels. Loss 0.05929. Accuracy 0.996.
### Flips: 104, rs: 16, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021727332
Test loss (w/o reg) on all data: 0.012054278
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.103649e-06
Norm of the params: 9.153452
     Influence (LOO): fixed  58 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021727371
Test loss (w/o reg) on all data: 0.012054502
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3967567e-07
Norm of the params: 9.153449
                Loss: fixed  58 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12010625
Train loss (w/o reg) on all data: 0.11249027
Test loss (w/o reg) on all data: 0.056066204
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5848923e-05
Norm of the params: 12.341786
              Random: fixed  13 labels. Loss 0.05607. Accuracy 0.996.
### Flips: 104, rs: 16, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730121
Test loss (w/o reg) on all data: 0.01205547
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2291146e-07
Norm of the params: 9.153149
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730103
Test loss (w/o reg) on all data: 0.012055384
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6880434e-07
Norm of the params: 9.153151
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10876004
Train loss (w/o reg) on all data: 0.10081472
Test loss (w/o reg) on all data: 0.057967708
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3116343e-06
Norm of the params: 12.605805
              Random: fixed  18 labels. Loss 0.05797. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13459407
Train loss (w/o reg) on all data: 0.12793145
Test loss (w/o reg) on all data: 0.06606754
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1091561e-05
Norm of the params: 11.543503
Flipped loss: 0.06607. Accuracy: 0.992
### Flips: 104, rs: 17, checks: 52
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008509548
Train loss (w/o reg) on all data: 0.003521788
Test loss (w/o reg) on all data: 0.015631981
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0751994e-06
Norm of the params: 9.987753
     Influence (LOO): fixed  45 labels. Loss 0.01563. Accuracy 0.992.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007864995
Train loss (w/o reg) on all data: 0.0029294332
Test loss (w/o reg) on all data: 0.017243888
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2005576e-07
Norm of the params: 9.935352
                Loss: fixed  44 labels. Loss 0.01724. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1291159
Train loss (w/o reg) on all data: 0.12166203
Test loss (w/o reg) on all data: 0.06391859
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2698415e-05
Norm of the params: 12.209721
              Random: fixed   2 labels. Loss 0.06392. Accuracy 0.992.
### Flips: 104, rs: 17, checks: 104
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731395
Test loss (w/o reg) on all data: 0.012051992
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7291975e-06
Norm of the params: 9.153009
     Influence (LOO): fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [7] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729846
Test loss (w/o reg) on all data: 0.012054917
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.687215e-08
Norm of the params: 9.153179
                Loss: fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119854994
Train loss (w/o reg) on all data: 0.11247769
Test loss (w/o reg) on all data: 0.06010363
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4140627e-05
Norm of the params: 12.1468525
              Random: fixed   6 labels. Loss 0.06010. Accuracy 0.992.
### Flips: 104, rs: 17, checks: 156
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730112
Test loss (w/o reg) on all data: 0.012055256
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9076566e-07
Norm of the params: 9.153151
     Influence (LOO): fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730103
Test loss (w/o reg) on all data: 0.012055118
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2170788e-07
Norm of the params: 9.153151
                Loss: fixed  46 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10840289
Train loss (w/o reg) on all data: 0.10116625
Test loss (w/o reg) on all data: 0.059582766
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8635055e-05
Norm of the params: 12.030497
              Random: fixed  10 labels. Loss 0.05958. Accuracy 0.989.
### Flips: 104, rs: 17, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.012054152
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9305457e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729644
Test loss (w/o reg) on all data: 0.012054254
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.2824877e-07
Norm of the params: 9.153199
                Loss: fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104369774
Train loss (w/o reg) on all data: 0.09674373
Test loss (w/o reg) on all data: 0.054517906
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.089109e-06
Norm of the params: 12.349933
              Random: fixed  11 labels. Loss 0.05452. Accuracy 0.989.
### Flips: 104, rs: 17, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730128
Test loss (w/o reg) on all data: 0.012053889
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2501955e-07
Norm of the params: 9.153148
     Influence (LOO): fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.00217301
Test loss (w/o reg) on all data: 0.012053955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5020352e-07
Norm of the params: 9.153151
                Loss: fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.096352205
Train loss (w/o reg) on all data: 0.089348234
Test loss (w/o reg) on all data: 0.048336696
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.036451e-06
Norm of the params: 11.835518
              Random: fixed  15 labels. Loss 0.04834. Accuracy 0.992.
### Flips: 104, rs: 17, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729781
Test loss (w/o reg) on all data: 0.012054701
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0948306e-07
Norm of the params: 9.153186
     Influence (LOO): fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729763
Test loss (w/o reg) on all data: 0.012054816
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.861283e-07
Norm of the params: 9.153186
                Loss: fixed  46 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082975484
Train loss (w/o reg) on all data: 0.07619339
Test loss (w/o reg) on all data: 0.0453131
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.267724e-06
Norm of the params: 11.646537
              Random: fixed  20 labels. Loss 0.04531. Accuracy 0.989.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1325893
Train loss (w/o reg) on all data: 0.121947706
Test loss (w/o reg) on all data: 0.07084522
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.1764684e-06
Norm of the params: 14.588754
Flipped loss: 0.07085. Accuracy: 0.981
### Flips: 104, rs: 18, checks: 52
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03685314
Train loss (w/o reg) on all data: 0.025681945
Test loss (w/o reg) on all data: 0.047817092
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.766367e-06
Norm of the params: 14.947371
     Influence (LOO): fixed  38 labels. Loss 0.04782. Accuracy 0.989.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025839485
Train loss (w/o reg) on all data: 0.015420952
Test loss (w/o reg) on all data: 0.02720913
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.986649e-07
Norm of the params: 14.43505
                Loss: fixed  46 labels. Loss 0.02721. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12825938
Train loss (w/o reg) on all data: 0.11840732
Test loss (w/o reg) on all data: 0.05723941
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.3522998e-05
Norm of the params: 14.037136
              Random: fixed   3 labels. Loss 0.05724. Accuracy 0.981.
### Flips: 104, rs: 18, checks: 104
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018992258
Train loss (w/o reg) on all data: 0.011096104
Test loss (w/o reg) on all data: 0.031941753
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.135123e-07
Norm of the params: 12.566747
     Influence (LOO): fixed  50 labels. Loss 0.03194. Accuracy 0.985.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010858072
Train loss (w/o reg) on all data: 0.0048023984
Test loss (w/o reg) on all data: 0.020216847
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0713245e-06
Norm of the params: 11.0051565
                Loss: fixed  54 labels. Loss 0.02022. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12677367
Train loss (w/o reg) on all data: 0.11680587
Test loss (w/o reg) on all data: 0.05449475
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.387309e-06
Norm of the params: 14.119347
              Random: fixed   4 labels. Loss 0.05449. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 156
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01255611
Train loss (w/o reg) on all data: 0.0060206573
Test loss (w/o reg) on all data: 0.0208408
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6235184e-06
Norm of the params: 11.432806
     Influence (LOO): fixed  53 labels. Loss 0.02084. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173008
Test loss (w/o reg) on all data: 0.012054042
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6832628e-07
Norm of the params: 9.153153
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12548906
Train loss (w/o reg) on all data: 0.11571673
Test loss (w/o reg) on all data: 0.053590294
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4311557e-05
Norm of the params: 13.980219
              Random: fixed   5 labels. Loss 0.05359. Accuracy 0.989.
### Flips: 104, rs: 18, checks: 208
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008455872
Train loss (w/o reg) on all data: 0.003589539
Test loss (w/o reg) on all data: 0.016756283
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.134657e-07
Norm of the params: 9.865427
     Influence (LOO): fixed  56 labels. Loss 0.01676. Accuracy 0.992.
Using normal model
LBFGS training took [31] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.00217297
Test loss (w/o reg) on all data: 0.012054844
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.9824375e-08
Norm of the params: 9.153196
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109418884
Train loss (w/o reg) on all data: 0.098154016
Test loss (w/o reg) on all data: 0.06024901
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.2644812e-05
Norm of the params: 15.009911
              Random: fixed  10 labels. Loss 0.06025. Accuracy 0.981.
### Flips: 104, rs: 18, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021731136
Test loss (w/o reg) on all data: 0.012054041
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.051593e-07
Norm of the params: 9.153036
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021731113
Test loss (w/o reg) on all data: 0.012054197
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7675568e-07
Norm of the params: 9.153039
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.101091094
Train loss (w/o reg) on all data: 0.09060866
Test loss (w/o reg) on all data: 0.049424328
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.317836e-05
Norm of the params: 14.479251
              Random: fixed  16 labels. Loss 0.04942. Accuracy 0.981.
### Flips: 104, rs: 18, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729544
Test loss (w/o reg) on all data: 0.012054932
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0248162e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729555
Test loss (w/o reg) on all data: 0.012054891
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.006082e-08
Norm of the params: 9.153212
                Loss: fixed  57 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09421477
Train loss (w/o reg) on all data: 0.08340855
Test loss (w/o reg) on all data: 0.04639142
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3698276e-05
Norm of the params: 14.701166
              Random: fixed  18 labels. Loss 0.04639. Accuracy 0.985.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15787183
Train loss (w/o reg) on all data: 0.15164723
Test loss (w/o reg) on all data: 0.09776507
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.7586e-06
Norm of the params: 11.157597
Flipped loss: 0.09777. Accuracy: 0.977
### Flips: 104, rs: 19, checks: 52
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04317086
Train loss (w/o reg) on all data: 0.034257542
Test loss (w/o reg) on all data: 0.057653777
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.7420457e-06
Norm of the params: 13.35164
     Influence (LOO): fixed  47 labels. Loss 0.05765. Accuracy 0.985.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023843322
Train loss (w/o reg) on all data: 0.013233769
Test loss (w/o reg) on all data: 0.044099443
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.3252595e-06
Norm of the params: 14.566779
                Loss: fixed  51 labels. Loss 0.04410. Accuracy 0.977.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15305826
Train loss (w/o reg) on all data: 0.14723757
Test loss (w/o reg) on all data: 0.08964304
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.745499e-05
Norm of the params: 10.78953
              Random: fixed   4 labels. Loss 0.08964. Accuracy 0.973.
### Flips: 104, rs: 19, checks: 104
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015498823
Train loss (w/o reg) on all data: 0.0093106935
Test loss (w/o reg) on all data: 0.018803164
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1406225e-06
Norm of the params: 11.124864
     Influence (LOO): fixed  60 labels. Loss 0.01880. Accuracy 0.992.
Using normal model
LBFGS training took [39] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172983
Test loss (w/o reg) on all data: 0.012055683
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3587565e-07
Norm of the params: 9.153179
                Loss: fixed  64 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14876157
Train loss (w/o reg) on all data: 0.14264846
Test loss (w/o reg) on all data: 0.08900938
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.000102e-05
Norm of the params: 11.05722
              Random: fixed   6 labels. Loss 0.08901. Accuracy 0.973.
### Flips: 104, rs: 19, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729635
Test loss (w/o reg) on all data: 0.012055089
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3835246e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  64 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012055043
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6695785e-07
Norm of the params: 9.153203
                Loss: fixed  64 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14151832
Train loss (w/o reg) on all data: 0.13541695
Test loss (w/o reg) on all data: 0.08528788
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.4629256e-06
Norm of the params: 11.046598
              Random: fixed  10 labels. Loss 0.08529. Accuracy 0.977.
### Flips: 104, rs: 19, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729614
Test loss (w/o reg) on all data: 0.012055004
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2255747e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  64 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172963
Test loss (w/o reg) on all data: 0.012054898
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.707048e-07
Norm of the params: 9.153204
                Loss: fixed  64 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13204941
Train loss (w/o reg) on all data: 0.12554228
Test loss (w/o reg) on all data: 0.079433866
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2511349e-05
Norm of the params: 11.408005
              Random: fixed  15 labels. Loss 0.07943. Accuracy 0.985.
### Flips: 104, rs: 19, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730422
Test loss (w/o reg) on all data: 0.012054371
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.0491895e-07
Norm of the params: 9.153115
     Influence (LOO): fixed  64 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730422
Test loss (w/o reg) on all data: 0.012054217
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9654877e-07
Norm of the params: 9.153116
                Loss: fixed  64 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12740678
Train loss (w/o reg) on all data: 0.12028253
Test loss (w/o reg) on all data: 0.082731746
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5138861e-05
Norm of the params: 11.936705
              Random: fixed  16 labels. Loss 0.08273. Accuracy 0.985.
### Flips: 104, rs: 19, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729274
Test loss (w/o reg) on all data: 0.012055093
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5438103e-07
Norm of the params: 9.153242
     Influence (LOO): fixed  64 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729425
Test loss (w/o reg) on all data: 0.012055215
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9770828e-07
Norm of the params: 9.153224
                Loss: fixed  64 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119915664
Train loss (w/o reg) on all data: 0.112405494
Test loss (w/o reg) on all data: 0.06921911
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.1984064e-06
Norm of the params: 12.255751
              Random: fixed  21 labels. Loss 0.06922. Accuracy 0.989.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13059285
Train loss (w/o reg) on all data: 0.12068675
Test loss (w/o reg) on all data: 0.08883081
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.5619307e-06
Norm of the params: 14.075587
Flipped loss: 0.08883. Accuracy: 0.966
### Flips: 104, rs: 20, checks: 52
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030401707
Train loss (w/o reg) on all data: 0.0220777
Test loss (w/o reg) on all data: 0.023080973
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.08185e-06
Norm of the params: 12.902718
     Influence (LOO): fixed  41 labels. Loss 0.02308. Accuracy 0.989.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010852142
Train loss (w/o reg) on all data: 0.0045025647
Test loss (w/o reg) on all data: 0.013012579
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6287597e-07
Norm of the params: 11.269053
                Loss: fixed  47 labels. Loss 0.01301. Accuracy 0.996.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118640214
Train loss (w/o reg) on all data: 0.109042056
Test loss (w/o reg) on all data: 0.07449165
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7717677e-05
Norm of the params: 13.855077
              Random: fixed   5 labels. Loss 0.07449. Accuracy 0.981.
### Flips: 104, rs: 20, checks: 104
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728256
Test loss (w/o reg) on all data: 0.01205398
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8134957e-07
Norm of the params: 9.153354
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729073
Test loss (w/o reg) on all data: 0.012055178
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1896321e-07
Norm of the params: 9.153264
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10620312
Train loss (w/o reg) on all data: 0.09715431
Test loss (w/o reg) on all data: 0.06702295
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1547411e-05
Norm of the params: 13.452735
              Random: fixed  11 labels. Loss 0.06702. Accuracy 0.981.
### Flips: 104, rs: 20, checks: 156
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729232
Test loss (w/o reg) on all data: 0.012054313
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8489912e-07
Norm of the params: 9.153246
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729239
Test loss (w/o reg) on all data: 0.012054483
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9659344e-07
Norm of the params: 9.153244
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10442625
Train loss (w/o reg) on all data: 0.095435634
Test loss (w/o reg) on all data: 0.065165624
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6661841e-05
Norm of the params: 13.409413
              Random: fixed  12 labels. Loss 0.06517. Accuracy 0.981.
### Flips: 104, rs: 20, checks: 208
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021732394
Test loss (w/o reg) on all data: 0.012056227
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.227087e-07
Norm of the params: 9.152899
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620154
Train loss (w/o reg) on all data: 0.002173207
Test loss (w/o reg) on all data: 0.012056243
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1910413e-06
Norm of the params: 9.152933
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09759641
Train loss (w/o reg) on all data: 0.088185705
Test loss (w/o reg) on all data: 0.07029985
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5067781e-05
Norm of the params: 13.719113
              Random: fixed  14 labels. Loss 0.07030. Accuracy 0.973.
### Flips: 104, rs: 20, checks: 260
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729812
Test loss (w/o reg) on all data: 0.012054823
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2080929e-07
Norm of the params: 9.153182
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729795
Test loss (w/o reg) on all data: 0.012054904
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.85521e-07
Norm of the params: 9.153184
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09644747
Train loss (w/o reg) on all data: 0.08708961
Test loss (w/o reg) on all data: 0.07131972
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3113272e-05
Norm of the params: 13.680536
              Random: fixed  15 labels. Loss 0.07132. Accuracy 0.969.
### Flips: 104, rs: 20, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730317
Test loss (w/o reg) on all data: 0.012056368
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2242546e-06
Norm of the params: 9.153125
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730338
Test loss (w/o reg) on all data: 0.01205611
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1655986e-07
Norm of the params: 9.153127
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0917924
Train loss (w/o reg) on all data: 0.0825702
Test loss (w/o reg) on all data: 0.06585151
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.295414e-06
Norm of the params: 13.581011
              Random: fixed  17 labels. Loss 0.06585. Accuracy 0.977.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13578874
Train loss (w/o reg) on all data: 0.12604356
Test loss (w/o reg) on all data: 0.06253499
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.19430515e-05
Norm of the params: 13.960789
Flipped loss: 0.06253. Accuracy: 0.992
### Flips: 104, rs: 21, checks: 52
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038174637
Train loss (w/o reg) on all data: 0.030212758
Test loss (w/o reg) on all data: 0.02718012
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.270387e-06
Norm of the params: 12.6189375
     Influence (LOO): fixed  42 labels. Loss 0.02718. Accuracy 0.992.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017109923
Train loss (w/o reg) on all data: 0.008378619
Test loss (w/o reg) on all data: 0.013166113
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.1340825e-07
Norm of the params: 13.214615
                Loss: fixed  49 labels. Loss 0.01317. Accuracy 0.996.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13025679
Train loss (w/o reg) on all data: 0.1202206
Test loss (w/o reg) on all data: 0.06202328
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.026439e-05
Norm of the params: 14.167704
              Random: fixed   2 labels. Loss 0.06202. Accuracy 0.992.
### Flips: 104, rs: 21, checks: 104
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009194123
Train loss (w/o reg) on all data: 0.003977382
Test loss (w/o reg) on all data: 0.011498397
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.3428512e-07
Norm of the params: 10.214441
     Influence (LOO): fixed  54 labels. Loss 0.01150. Accuracy 0.996.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007619029
Train loss (w/o reg) on all data: 0.0027654846
Test loss (w/o reg) on all data: 0.010736054
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.931414e-07
Norm of the params: 9.852456
                Loss: fixed  55 labels. Loss 0.01074. Accuracy 0.996.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12826674
Train loss (w/o reg) on all data: 0.118251495
Test loss (w/o reg) on all data: 0.060746163
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2665426e-05
Norm of the params: 14.152911
              Random: fixed   3 labels. Loss 0.06075. Accuracy 0.992.
### Flips: 104, rs: 21, checks: 156
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.012054388
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8231049e-07
Norm of the params: 9.153193
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729711
Test loss (w/o reg) on all data: 0.012054715
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5059188e-07
Norm of the params: 9.153193
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11977955
Train loss (w/o reg) on all data: 0.110104926
Test loss (w/o reg) on all data: 0.054783713
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.3467996e-06
Norm of the params: 13.910159
              Random: fixed   7 labels. Loss 0.05478. Accuracy 0.989.
### Flips: 104, rs: 21, checks: 208
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172936
Test loss (w/o reg) on all data: 0.0120545495
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6929472e-07
Norm of the params: 9.153232
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172936
Test loss (w/o reg) on all data: 0.012054494
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7569614e-07
Norm of the params: 9.153231
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11548141
Train loss (w/o reg) on all data: 0.10594174
Test loss (w/o reg) on all data: 0.054485798
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.382571e-06
Norm of the params: 13.812795
              Random: fixed  10 labels. Loss 0.05449. Accuracy 0.981.
### Flips: 104, rs: 21, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173021
Test loss (w/o reg) on all data: 0.0120544555
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.374735e-07
Norm of the params: 9.153139
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730163
Test loss (w/o reg) on all data: 0.012054299
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.607551e-07
Norm of the params: 9.153145
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.111494064
Train loss (w/o reg) on all data: 0.10191115
Test loss (w/o reg) on all data: 0.05415776
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.3915392e-05
Norm of the params: 13.84407
              Random: fixed  13 labels. Loss 0.05416. Accuracy 0.981.
### Flips: 104, rs: 21, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729937
Test loss (w/o reg) on all data: 0.012054207
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.539552e-07
Norm of the params: 9.153169
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172993
Test loss (w/o reg) on all data: 0.012054443
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.765824e-07
Norm of the params: 9.153172
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104964435
Train loss (w/o reg) on all data: 0.09492979
Test loss (w/o reg) on all data: 0.055382196
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.453739e-05
Norm of the params: 14.166611
              Random: fixed  15 labels. Loss 0.05538. Accuracy 0.981.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12023701
Train loss (w/o reg) on all data: 0.109323986
Test loss (w/o reg) on all data: 0.06556634
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.8680655e-05
Norm of the params: 14.773639
Flipped loss: 0.06557. Accuracy: 0.985
### Flips: 104, rs: 22, checks: 52
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022183515
Train loss (w/o reg) on all data: 0.014333845
Test loss (w/o reg) on all data: 0.022151576
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.612343e-07
Norm of the params: 12.529701
     Influence (LOO): fixed  41 labels. Loss 0.02215. Accuracy 0.992.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010501415
Train loss (w/o reg) on all data: 0.004495198
Test loss (w/o reg) on all data: 0.010141292
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3661752e-07
Norm of the params: 10.960125
                Loss: fixed  44 labels. Loss 0.01014. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12023701
Train loss (w/o reg) on all data: 0.109327614
Test loss (w/o reg) on all data: 0.06556921
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.391134e-05
Norm of the params: 14.771184
              Random: fixed   0 labels. Loss 0.06557. Accuracy 0.985.
### Flips: 104, rs: 22, checks: 104
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009210212
Train loss (w/o reg) on all data: 0.004451307
Test loss (w/o reg) on all data: 0.013357709
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4139076e-07
Norm of the params: 9.755927
     Influence (LOO): fixed  48 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728412
Test loss (w/o reg) on all data: 0.012055188
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4168771e-06
Norm of the params: 9.153336
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106963426
Train loss (w/o reg) on all data: 0.09585857
Test loss (w/o reg) on all data: 0.0687996
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.26656805e-05
Norm of the params: 14.902926
              Random: fixed   6 labels. Loss 0.06880. Accuracy 0.981.
### Flips: 104, rs: 22, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172819
Test loss (w/o reg) on all data: 0.012054266
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8751938e-06
Norm of the params: 9.153359
     Influence (LOO): fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728294
Test loss (w/o reg) on all data: 0.012054579
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5047333e-07
Norm of the params: 9.15335
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10303543
Train loss (w/o reg) on all data: 0.09185259
Test loss (w/o reg) on all data: 0.06733237
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.0415651e-05
Norm of the params: 14.955159
              Random: fixed   9 labels. Loss 0.06733. Accuracy 0.981.
### Flips: 104, rs: 22, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728294
Test loss (w/o reg) on all data: 0.012055332
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.73326e-07
Norm of the params: 9.153348
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728298
Test loss (w/o reg) on all data: 0.01205519
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.776205e-07
Norm of the params: 9.153347
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097203344
Train loss (w/o reg) on all data: 0.08574256
Test loss (w/o reg) on all data: 0.06599945
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8698971e-05
Norm of the params: 15.13987
              Random: fixed  12 labels. Loss 0.06600. Accuracy 0.985.
### Flips: 104, rs: 22, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729935
Test loss (w/o reg) on all data: 0.01205541
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4602664e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172995
Test loss (w/o reg) on all data: 0.012055266
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.946872e-07
Norm of the params: 9.153168
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.089204974
Train loss (w/o reg) on all data: 0.07787746
Test loss (w/o reg) on all data: 0.06470697
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.026693e-06
Norm of the params: 15.051584
              Random: fixed  16 labels. Loss 0.06471. Accuracy 0.985.
### Flips: 104, rs: 22, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729895
Test loss (w/o reg) on all data: 0.012054929
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.676007e-07
Norm of the params: 9.153174
     Influence (LOO): fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729893
Test loss (w/o reg) on all data: 0.012055043
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4357171e-07
Norm of the params: 9.153174
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08920498
Train loss (w/o reg) on all data: 0.077874914
Test loss (w/o reg) on all data: 0.064699076
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0732163e-05
Norm of the params: 15.053287
              Random: fixed  16 labels. Loss 0.06470. Accuracy 0.985.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12415583
Train loss (w/o reg) on all data: 0.11698665
Test loss (w/o reg) on all data: 0.055835962
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.386891e-06
Norm of the params: 11.974291
Flipped loss: 0.05584. Accuracy: 0.989
### Flips: 104, rs: 23, checks: 52
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01275387
Train loss (w/o reg) on all data: 0.00593985
Test loss (w/o reg) on all data: 0.012446775
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.7604854e-07
Norm of the params: 11.67392
     Influence (LOO): fixed  43 labels. Loss 0.01245. Accuracy 0.996.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0106151905
Train loss (w/o reg) on all data: 0.0042658425
Test loss (w/o reg) on all data: 0.008921024
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 7.5651894e-07
Norm of the params: 11.268849
                Loss: fixed  44 labels. Loss 0.00892. Accuracy 1.000.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12148688
Train loss (w/o reg) on all data: 0.114282236
Test loss (w/o reg) on all data: 0.053906687
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9417913e-06
Norm of the params: 12.003869
              Random: fixed   1 labels. Loss 0.05391. Accuracy 0.992.
### Flips: 104, rs: 23, checks: 104
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.01205432
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.054838e-07
Norm of the params: 9.153213
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007806739
Train loss (w/o reg) on all data: 0.0028564262
Test loss (w/o reg) on all data: 0.00958827
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.620117e-07
Norm of the params: 9.95019
                Loss: fixed  47 labels. Loss 0.00959. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11410676
Train loss (w/o reg) on all data: 0.10622474
Test loss (w/o reg) on all data: 0.052901104
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.4936708e-06
Norm of the params: 12.555494
              Random: fixed   3 labels. Loss 0.05290. Accuracy 0.985.
### Flips: 104, rs: 23, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729337
Test loss (w/o reg) on all data: 0.012054876
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4705613e-07
Norm of the params: 9.153235
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172939
Test loss (w/o reg) on all data: 0.012055029
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.215979e-07
Norm of the params: 9.153229
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10981575
Train loss (w/o reg) on all data: 0.10155311
Test loss (w/o reg) on all data: 0.052354597
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6991793e-05
Norm of the params: 12.855062
              Random: fixed   5 labels. Loss 0.05235. Accuracy 0.989.
### Flips: 104, rs: 23, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730661
Test loss (w/o reg) on all data: 0.01205436
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3862324e-07
Norm of the params: 9.153089
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730596
Test loss (w/o reg) on all data: 0.01205439
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.671656e-07
Norm of the params: 9.153097
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10289562
Train loss (w/o reg) on all data: 0.09425599
Test loss (w/o reg) on all data: 0.0464605
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.9712513e-06
Norm of the params: 13.145059
              Random: fixed   8 labels. Loss 0.04646. Accuracy 0.989.
### Flips: 104, rs: 23, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728168
Test loss (w/o reg) on all data: 0.012054764
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4004723e-07
Norm of the params: 9.153362
     Influence (LOO): fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728207
Test loss (w/o reg) on all data: 0.012054838
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5680785e-07
Norm of the params: 9.153358
                Loss: fixed  48 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08863137
Train loss (w/o reg) on all data: 0.07886087
Test loss (w/o reg) on all data: 0.04409717
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.3927823e-06
Norm of the params: 13.978909
              Random: fixed  15 labels. Loss 0.04410. Accuracy 0.992.
### Flips: 104, rs: 23, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730037
Test loss (w/o reg) on all data: 0.012056353
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.187279e-07
Norm of the params: 9.153156
     Influence (LOO): fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729995
Test loss (w/o reg) on all data: 0.012056269
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2396407e-06
Norm of the params: 9.153161
                Loss: fixed  48 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07708559
Train loss (w/o reg) on all data: 0.066585645
Test loss (w/o reg) on all data: 0.04483499
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7998398e-05
Norm of the params: 14.49134
              Random: fixed  19 labels. Loss 0.04483. Accuracy 0.989.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12935884
Train loss (w/o reg) on all data: 0.121528335
Test loss (w/o reg) on all data: 0.067089885
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.8213406e-05
Norm of the params: 12.514402
Flipped loss: 0.06709. Accuracy: 0.985
### Flips: 104, rs: 24, checks: 52
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014988329
Train loss (w/o reg) on all data: 0.008143247
Test loss (w/o reg) on all data: 0.018492112
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.194868e-06
Norm of the params: 11.700498
     Influence (LOO): fixed  45 labels. Loss 0.01849. Accuracy 0.989.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009991067
Train loss (w/o reg) on all data: 0.003949636
Test loss (w/o reg) on all data: 0.014593441
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.590131e-07
Norm of the params: 10.992208
                Loss: fixed  45 labels. Loss 0.01459. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12271685
Train loss (w/o reg) on all data: 0.11471343
Test loss (w/o reg) on all data: 0.063260645
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.7404195e-06
Norm of the params: 12.651814
              Random: fixed   3 labels. Loss 0.06326. Accuracy 0.985.
### Flips: 104, rs: 24, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006934945
Train loss (w/o reg) on all data: 0.0024074381
Test loss (w/o reg) on all data: 0.011969398
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1309965e-06
Norm of the params: 9.515783
     Influence (LOO): fixed  49 labels. Loss 0.01197. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008525481
Train loss (w/o reg) on all data: 0.0032029331
Test loss (w/o reg) on all data: 0.013458567
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8453705e-07
Norm of the params: 10.317509
                Loss: fixed  47 labels. Loss 0.01346. Accuracy 0.992.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11948749
Train loss (w/o reg) on all data: 0.11170928
Test loss (w/o reg) on all data: 0.058002256
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.273399e-05
Norm of the params: 12.472532
              Random: fixed   5 labels. Loss 0.05800. Accuracy 0.992.
### Flips: 104, rs: 24, checks: 156
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728994
Test loss (w/o reg) on all data: 0.012056097
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.930243e-07
Norm of the params: 9.153273
     Influence (LOO): fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006934945
Train loss (w/o reg) on all data: 0.0024074325
Test loss (w/o reg) on all data: 0.011969945
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.22786e-07
Norm of the params: 9.51579
                Loss: fixed  49 labels. Loss 0.01197. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11153271
Train loss (w/o reg) on all data: 0.10378067
Test loss (w/o reg) on all data: 0.054477226
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9664083e-05
Norm of the params: 12.451534
              Random: fixed   9 labels. Loss 0.05448. Accuracy 0.992.
### Flips: 104, rs: 24, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012054565
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6873326e-08
Norm of the params: 9.153184
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729812
Test loss (w/o reg) on all data: 0.0120546
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4066036e-07
Norm of the params: 9.153184
                Loss: fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10592753
Train loss (w/o reg) on all data: 0.09906858
Test loss (w/o reg) on all data: 0.046088945
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.491877e-06
Norm of the params: 11.712339
              Random: fixed  12 labels. Loss 0.04609. Accuracy 0.996.
### Flips: 104, rs: 24, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173127
Test loss (w/o reg) on all data: 0.01205432
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9874857e-07
Norm of the params: 9.153023
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730273
Test loss (w/o reg) on all data: 0.012055154
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6047215e-07
Norm of the params: 9.153132
                Loss: fixed  50 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09753132
Train loss (w/o reg) on all data: 0.09010864
Test loss (w/o reg) on all data: 0.040268034
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.77808e-06
Norm of the params: 12.184152
              Random: fixed  16 labels. Loss 0.04027. Accuracy 0.996.
### Flips: 104, rs: 24, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729572
Test loss (w/o reg) on all data: 0.012053446
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.906727e-07
Norm of the params: 9.153209
     Influence (LOO): fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012053587
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3646222e-07
Norm of the params: 9.153209
                Loss: fixed  50 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093174264
Train loss (w/o reg) on all data: 0.08625645
Test loss (w/o reg) on all data: 0.038073543
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1739904e-05
Norm of the params: 11.762492
              Random: fixed  18 labels. Loss 0.03807. Accuracy 0.996.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11994036
Train loss (w/o reg) on all data: 0.11121917
Test loss (w/o reg) on all data: 0.09217763
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0176083e-05
Norm of the params: 13.206964
Flipped loss: 0.09218. Accuracy: 0.973
### Flips: 104, rs: 25, checks: 52
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016863173
Train loss (w/o reg) on all data: 0.008902531
Test loss (w/o reg) on all data: 0.028138634
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.317584e-07
Norm of the params: 12.617958
     Influence (LOO): fixed  42 labels. Loss 0.02814. Accuracy 0.985.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013278944
Train loss (w/o reg) on all data: 0.0060284943
Test loss (w/o reg) on all data: 0.0140455095
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5934403e-06
Norm of the params: 12.041969
                Loss: fixed  44 labels. Loss 0.01405. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11763303
Train loss (w/o reg) on all data: 0.10887588
Test loss (w/o reg) on all data: 0.09077978
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.262385e-06
Norm of the params: 13.234163
              Random: fixed   1 labels. Loss 0.09078. Accuracy 0.973.
### Flips: 104, rs: 25, checks: 104
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075251944
Train loss (w/o reg) on all data: 0.0028544914
Test loss (w/o reg) on all data: 0.012152552
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0277574e-06
Norm of the params: 9.665095
     Influence (LOO): fixed  48 labels. Loss 0.01215. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007824826
Train loss (w/o reg) on all data: 0.002921134
Test loss (w/o reg) on all data: 0.00897508
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1300646e-07
Norm of the params: 9.903224
                Loss: fixed  48 labels. Loss 0.00898. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11262474
Train loss (w/o reg) on all data: 0.10343399
Test loss (w/o reg) on all data: 0.08966172
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.922477e-06
Norm of the params: 13.557842
              Random: fixed   3 labels. Loss 0.08966. Accuracy 0.977.
### Flips: 104, rs: 25, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729257
Test loss (w/o reg) on all data: 0.012055033
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.581219e-08
Norm of the params: 9.153245
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729618
Test loss (w/o reg) on all data: 0.01205482
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2228388e-07
Norm of the params: 9.153204
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10286058
Train loss (w/o reg) on all data: 0.093657516
Test loss (w/o reg) on all data: 0.074882664
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6904978e-05
Norm of the params: 13.566919
              Random: fixed   6 labels. Loss 0.07488. Accuracy 0.977.
### Flips: 104, rs: 25, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729784
Test loss (w/o reg) on all data: 0.012054998
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1995513e-07
Norm of the params: 9.153187
     Influence (LOO): fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729777
Test loss (w/o reg) on all data: 0.01205494
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7945644e-07
Norm of the params: 9.153188
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09619847
Train loss (w/o reg) on all data: 0.08650168
Test loss (w/o reg) on all data: 0.06705343
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.435757e-06
Norm of the params: 13.926085
              Random: fixed  10 labels. Loss 0.06705. Accuracy 0.973.
### Flips: 104, rs: 25, checks: 260
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730224
Test loss (w/o reg) on all data: 0.01205521
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7764568e-07
Norm of the params: 9.153137
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173021
Test loss (w/o reg) on all data: 0.012055094
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5788588e-07
Norm of the params: 9.153139
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08929829
Train loss (w/o reg) on all data: 0.07926961
Test loss (w/o reg) on all data: 0.0684102
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7093167e-05
Norm of the params: 14.1624
              Random: fixed  12 labels. Loss 0.06841. Accuracy 0.977.
### Flips: 104, rs: 25, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172963
Test loss (w/o reg) on all data: 0.012055178
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.2476e-08
Norm of the params: 9.153202
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729618
Test loss (w/o reg) on all data: 0.01205521
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6148132e-07
Norm of the params: 9.153203
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08215545
Train loss (w/o reg) on all data: 0.071977414
Test loss (w/o reg) on all data: 0.06331824
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.3147738e-06
Norm of the params: 14.267472
              Random: fixed  16 labels. Loss 0.06332. Accuracy 0.981.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1180031
Train loss (w/o reg) on all data: 0.10927259
Test loss (w/o reg) on all data: 0.051384024
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.1143637e-06
Norm of the params: 13.214015
Flipped loss: 0.05138. Accuracy: 0.992
### Flips: 104, rs: 26, checks: 52
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01843755
Train loss (w/o reg) on all data: 0.010124041
Test loss (w/o reg) on all data: 0.021097785
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.995853e-07
Norm of the params: 12.894579
     Influence (LOO): fixed  40 labels. Loss 0.02110. Accuracy 0.992.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01338743
Train loss (w/o reg) on all data: 0.0057722037
Test loss (w/o reg) on all data: 0.014203823
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.938183e-07
Norm of the params: 12.341172
                Loss: fixed  42 labels. Loss 0.01420. Accuracy 0.996.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11722495
Train loss (w/o reg) on all data: 0.10850389
Test loss (w/o reg) on all data: 0.04945491
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.4432258e-05
Norm of the params: 13.206855
              Random: fixed   1 labels. Loss 0.04945. Accuracy 0.989.
### Flips: 104, rs: 26, checks: 104
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.0120549975
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0419335e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729546
Test loss (w/o reg) on all data: 0.012054959
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.13638585e-07
Norm of the params: 9.153212
                Loss: fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11250602
Train loss (w/o reg) on all data: 0.10348228
Test loss (w/o reg) on all data: 0.047943413
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6365593e-05
Norm of the params: 13.434089
              Random: fixed   3 labels. Loss 0.04794. Accuracy 0.996.
### Flips: 104, rs: 26, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729427
Test loss (w/o reg) on all data: 0.012055082
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5377853e-07
Norm of the params: 9.153224
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729441
Test loss (w/o reg) on all data: 0.01205514
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6692763e-07
Norm of the params: 9.153222
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10273504
Train loss (w/o reg) on all data: 0.09408521
Test loss (w/o reg) on all data: 0.046737533
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.928684e-06
Norm of the params: 13.15282
              Random: fixed   7 labels. Loss 0.04674. Accuracy 0.996.
### Flips: 104, rs: 26, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729995
Test loss (w/o reg) on all data: 0.012055383
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3952505e-07
Norm of the params: 9.153164
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172999
Test loss (w/o reg) on all data: 0.012055495
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.800322e-07
Norm of the params: 9.153165
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09316917
Train loss (w/o reg) on all data: 0.08433617
Test loss (w/o reg) on all data: 0.035968665
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.1394883e-06
Norm of the params: 13.2913475
              Random: fixed  10 labels. Loss 0.03597. Accuracy 0.996.
### Flips: 104, rs: 26, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021731446
Test loss (w/o reg) on all data: 0.012056136
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1013735e-07
Norm of the params: 9.153003
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730142
Test loss (w/o reg) on all data: 0.012054682
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1183174e-07
Norm of the params: 9.153147
                Loss: fixed  47 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08564642
Train loss (w/o reg) on all data: 0.07565291
Test loss (w/o reg) on all data: 0.03818385
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.978522e-06
Norm of the params: 14.137545
              Random: fixed  13 labels. Loss 0.03818. Accuracy 0.992.
### Flips: 104, rs: 26, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730373
Test loss (w/o reg) on all data: 0.012055609
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6497426e-07
Norm of the params: 9.153121
     Influence (LOO): fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002173035
Test loss (w/o reg) on all data: 0.012055671
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.659478e-07
Norm of the params: 9.153122
                Loss: fixed  47 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08276768
Train loss (w/o reg) on all data: 0.072970636
Test loss (w/o reg) on all data: 0.03706668
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.2013395e-06
Norm of the params: 13.997889
              Random: fixed  15 labels. Loss 0.03707. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14852053
Train loss (w/o reg) on all data: 0.14052995
Test loss (w/o reg) on all data: 0.113360055
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.860095e-05
Norm of the params: 12.6416645
Flipped loss: 0.11336. Accuracy: 0.950
### Flips: 104, rs: 27, checks: 52
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03180817
Train loss (w/o reg) on all data: 0.022775155
Test loss (w/o reg) on all data: 0.06406746
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.8269533e-06
Norm of the params: 13.440994
     Influence (LOO): fixed  44 labels. Loss 0.06407. Accuracy 0.973.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02086958
Train loss (w/o reg) on all data: 0.010437859
Test loss (w/o reg) on all data: 0.0417418
Train acc on all data:  1.0
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.5947526e-06
Norm of the params: 14.444182
                Loss: fixed  49 labels. Loss 0.04174. Accuracy 0.973.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14021805
Train loss (w/o reg) on all data: 0.13221477
Test loss (w/o reg) on all data: 0.112611435
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.824461e-06
Norm of the params: 12.651698
              Random: fixed   3 labels. Loss 0.11261. Accuracy 0.947.
### Flips: 104, rs: 27, checks: 104
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009560412
Train loss (w/o reg) on all data: 0.004448628
Test loss (w/o reg) on all data: 0.0121561065
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0714015e-06
Norm of the params: 10.111166
     Influence (LOO): fixed  57 labels. Loss 0.01216. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00876481
Train loss (w/o reg) on all data: 0.0035086586
Test loss (w/o reg) on all data: 0.012249401
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.609837e-07
Norm of the params: 10.252953
                Loss: fixed  58 labels. Loss 0.01225. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14021805
Train loss (w/o reg) on all data: 0.1322115
Test loss (w/o reg) on all data: 0.112622745
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 9.377382e-06
Norm of the params: 12.654285
              Random: fixed   3 labels. Loss 0.11262. Accuracy 0.947.
### Flips: 104, rs: 27, checks: 156
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.012054271
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.732387e-07
Norm of the params: 9.153184
     Influence (LOO): fixed  59 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729793
Test loss (w/o reg) on all data: 0.0120543325
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8998188e-07
Norm of the params: 9.153184
                Loss: fixed  59 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13028908
Train loss (w/o reg) on all data: 0.12174584
Test loss (w/o reg) on all data: 0.106363885
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.8589398e-05
Norm of the params: 13.07152
              Random: fixed   9 labels. Loss 0.10636. Accuracy 0.954.
### Flips: 104, rs: 27, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730235
Test loss (w/o reg) on all data: 0.0120540345
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.9246007e-07
Norm of the params: 9.153137
     Influence (LOO): fixed  59 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730207
Test loss (w/o reg) on all data: 0.012054217
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2108156e-07
Norm of the params: 9.153138
                Loss: fixed  59 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12887527
Train loss (w/o reg) on all data: 0.12040629
Test loss (w/o reg) on all data: 0.10595275
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.003757e-06
Norm of the params: 13.014588
              Random: fixed  10 labels. Loss 0.10595. Accuracy 0.954.
### Flips: 104, rs: 27, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.002173067
Test loss (w/o reg) on all data: 0.012054431
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.091734e-07
Norm of the params: 9.153087
     Influence (LOO): fixed  59 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730664
Test loss (w/o reg) on all data: 0.012054519
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0034848e-07
Norm of the params: 9.153088
                Loss: fixed  59 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12520981
Train loss (w/o reg) on all data: 0.11672682
Test loss (w/o reg) on all data: 0.09193287
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.850129e-06
Norm of the params: 13.025354
              Random: fixed  12 labels. Loss 0.09193. Accuracy 0.973.
### Flips: 104, rs: 27, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729213
Test loss (w/o reg) on all data: 0.012054126
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3655936e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  59 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729225
Test loss (w/o reg) on all data: 0.01205418
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5906888e-07
Norm of the params: 9.153245
                Loss: fixed  59 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119658574
Train loss (w/o reg) on all data: 0.11104688
Test loss (w/o reg) on all data: 0.0874634
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1408821e-05
Norm of the params: 13.123791
              Random: fixed  15 labels. Loss 0.08746. Accuracy 0.969.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1248838
Train loss (w/o reg) on all data: 0.11718437
Test loss (w/o reg) on all data: 0.07275128
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8790455e-05
Norm of the params: 12.409213
Flipped loss: 0.07275. Accuracy: 0.985
### Flips: 104, rs: 28, checks: 52
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013069691
Train loss (w/o reg) on all data: 0.0065321373
Test loss (w/o reg) on all data: 0.012993816
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4198594e-06
Norm of the params: 11.434644
     Influence (LOO): fixed  46 labels. Loss 0.01299. Accuracy 0.992.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009779332
Train loss (w/o reg) on all data: 0.004135771
Test loss (w/o reg) on all data: 0.012481139
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3960231e-06
Norm of the params: 10.624087
                Loss: fixed  46 labels. Loss 0.01248. Accuracy 0.996.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12277633
Train loss (w/o reg) on all data: 0.11488642
Test loss (w/o reg) on all data: 0.07568678
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.95368e-06
Norm of the params: 12.561778
              Random: fixed   1 labels. Loss 0.07569. Accuracy 0.977.
### Flips: 104, rs: 28, checks: 104
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729025
Test loss (w/o reg) on all data: 0.012055244
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6871003e-07
Norm of the params: 9.153268
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172903
Test loss (w/o reg) on all data: 0.012055277
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6899848e-07
Norm of the params: 9.153267
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11596124
Train loss (w/o reg) on all data: 0.1075202
Test loss (w/o reg) on all data: 0.07184785
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.4117371e-05
Norm of the params: 12.993104
              Random: fixed   4 labels. Loss 0.07185. Accuracy 0.977.
### Flips: 104, rs: 28, checks: 156
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729053
Test loss (w/o reg) on all data: 0.012055385
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7877167e-07
Norm of the params: 9.153267
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172911
Test loss (w/o reg) on all data: 0.012054964
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.326422e-06
Norm of the params: 9.153259
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10951497
Train loss (w/o reg) on all data: 0.10041847
Test loss (w/o reg) on all data: 0.058636446
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0227073e-05
Norm of the params: 13.48814
              Random: fixed   7 labels. Loss 0.05864. Accuracy 0.985.
### Flips: 104, rs: 28, checks: 208
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730151
Test loss (w/o reg) on all data: 0.012055242
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.071098e-07
Norm of the params: 9.153146
     Influence (LOO): fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730128
Test loss (w/o reg) on all data: 0.012055281
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.063221e-07
Norm of the params: 9.153147
                Loss: fixed  49 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.107204884
Train loss (w/o reg) on all data: 0.098009855
Test loss (w/o reg) on all data: 0.056017414
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.642865e-06
Norm of the params: 13.560996
              Random: fixed   8 labels. Loss 0.05602. Accuracy 0.985.
### Flips: 104, rs: 28, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.012054777
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.749117e-07
Norm of the params: 9.153213
     Influence (LOO): fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172954
Test loss (w/o reg) on all data: 0.012054824
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6673903e-07
Norm of the params: 9.153212
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09773335
Train loss (w/o reg) on all data: 0.08860593
Test loss (w/o reg) on all data: 0.053508982
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.111587e-06
Norm of the params: 13.511043
              Random: fixed  13 labels. Loss 0.05351. Accuracy 0.985.
### Flips: 104, rs: 28, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730834
Test loss (w/o reg) on all data: 0.012054161
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.36777e-06
Norm of the params: 9.1530695
     Influence (LOO): fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.002173077
Test loss (w/o reg) on all data: 0.012054487
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0319538e-07
Norm of the params: 9.153075
                Loss: fixed  49 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08730029
Train loss (w/o reg) on all data: 0.077784486
Test loss (w/o reg) on all data: 0.049386345
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5498377e-05
Norm of the params: 13.795511
              Random: fixed  17 labels. Loss 0.04939. Accuracy 0.985.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13950337
Train loss (w/o reg) on all data: 0.13256197
Test loss (w/o reg) on all data: 0.07930438
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8687617e-05
Norm of the params: 11.782532
Flipped loss: 0.07930. Accuracy: 0.981
### Flips: 104, rs: 29, checks: 52
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027163878
Train loss (w/o reg) on all data: 0.019242352
Test loss (w/o reg) on all data: 0.035642546
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.7402166e-06
Norm of the params: 12.58692
     Influence (LOO): fixed  44 labels. Loss 0.03564. Accuracy 0.989.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014181649
Train loss (w/o reg) on all data: 0.006220599
Test loss (w/o reg) on all data: 0.026219161
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4011357e-06
Norm of the params: 12.61828
                Loss: fixed  47 labels. Loss 0.02622. Accuracy 0.989.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1330678
Train loss (w/o reg) on all data: 0.12584828
Test loss (w/o reg) on all data: 0.0776055
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.947248e-06
Norm of the params: 12.016258
              Random: fixed   3 labels. Loss 0.07761. Accuracy 0.981.
### Flips: 104, rs: 29, checks: 104
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007445342
Train loss (w/o reg) on all data: 0.002908787
Test loss (w/o reg) on all data: 0.014290814
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5904958e-07
Norm of the params: 9.525289
     Influence (LOO): fixed  53 labels. Loss 0.01429. Accuracy 0.992.
Using normal model
LBFGS training took [32] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729597
Test loss (w/o reg) on all data: 0.012055263
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.778936e-08
Norm of the params: 9.153208
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1288735
Train loss (w/o reg) on all data: 0.12183242
Test loss (w/o reg) on all data: 0.07439775
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7121618e-05
Norm of the params: 11.866823
              Random: fixed   6 labels. Loss 0.07440. Accuracy 0.985.
### Flips: 104, rs: 29, checks: 156
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729611
Test loss (w/o reg) on all data: 0.012055248
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3985894e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729667
Test loss (w/o reg) on all data: 0.012055024
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.892218e-07
Norm of the params: 9.153197
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119672336
Train loss (w/o reg) on all data: 0.112160265
Test loss (w/o reg) on all data: 0.084352024
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.1080516e-05
Norm of the params: 12.257298
              Random: fixed   8 labels. Loss 0.08435. Accuracy 0.969.
### Flips: 104, rs: 29, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730023
Test loss (w/o reg) on all data: 0.012056637
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.837421e-07
Norm of the params: 9.153158
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729998
Test loss (w/o reg) on all data: 0.012056388
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0256634e-06
Norm of the params: 9.153162
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11604505
Train loss (w/o reg) on all data: 0.10890026
Test loss (w/o reg) on all data: 0.082944945
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.876748e-06
Norm of the params: 11.953909
              Random: fixed  10 labels. Loss 0.08294. Accuracy 0.962.
### Flips: 104, rs: 29, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729695
Test loss (w/o reg) on all data: 0.012055192
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1949678e-07
Norm of the params: 9.153194
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729707
Test loss (w/o reg) on all data: 0.01205524
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.910113e-07
Norm of the params: 9.153194
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11604505
Train loss (w/o reg) on all data: 0.10889525
Test loss (w/o reg) on all data: 0.0829647
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.889792e-06
Norm of the params: 11.958095
              Random: fixed  10 labels. Loss 0.08296. Accuracy 0.962.
### Flips: 104, rs: 29, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729814
Test loss (w/o reg) on all data: 0.0120550515
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.300181e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729805
Test loss (w/o reg) on all data: 0.012055192
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1918425e-07
Norm of the params: 9.153185
                Loss: fixed  54 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10714642
Train loss (w/o reg) on all data: 0.09954637
Test loss (w/o reg) on all data: 0.079740815
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1021395e-05
Norm of the params: 12.328868
              Random: fixed  13 labels. Loss 0.07974. Accuracy 0.966.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13249843
Train loss (w/o reg) on all data: 0.123484276
Test loss (w/o reg) on all data: 0.07557766
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.7030585e-06
Norm of the params: 13.426954
Flipped loss: 0.07558. Accuracy: 0.981
### Flips: 104, rs: 30, checks: 52
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025545854
Train loss (w/o reg) on all data: 0.017020686
Test loss (w/o reg) on all data: 0.03460282
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4746046e-06
Norm of the params: 13.0576935
     Influence (LOO): fixed  42 labels. Loss 0.03460. Accuracy 0.985.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013715789
Train loss (w/o reg) on all data: 0.00617854
Test loss (w/o reg) on all data: 0.019445146
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0271468e-06
Norm of the params: 12.277824
                Loss: fixed  47 labels. Loss 0.01945. Accuracy 0.989.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13079494
Train loss (w/o reg) on all data: 0.12194126
Test loss (w/o reg) on all data: 0.070222884
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.335999e-06
Norm of the params: 13.306901
              Random: fixed   1 labels. Loss 0.07022. Accuracy 0.985.
### Flips: 104, rs: 30, checks: 104
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074688075
Train loss (w/o reg) on all data: 0.002753705
Test loss (w/o reg) on all data: 0.015202532
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.807494e-07
Norm of the params: 9.710924
     Influence (LOO): fixed  51 labels. Loss 0.01520. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172914
Test loss (w/o reg) on all data: 0.012054641
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.488624e-07
Norm of the params: 9.153257
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13079496
Train loss (w/o reg) on all data: 0.12194301
Test loss (w/o reg) on all data: 0.07021571
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.042508e-05
Norm of the params: 13.305597
              Random: fixed   1 labels. Loss 0.07022. Accuracy 0.985.
### Flips: 104, rs: 30, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730275
Test loss (w/o reg) on all data: 0.012054979
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9839544e-07
Norm of the params: 9.1531315
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730238
Test loss (w/o reg) on all data: 0.012054905
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3135954e-07
Norm of the params: 9.153134
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12431223
Train loss (w/o reg) on all data: 0.115230925
Test loss (w/o reg) on all data: 0.06651542
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.808526e-06
Norm of the params: 13.476873
              Random: fixed   4 labels. Loss 0.06652. Accuracy 0.981.
### Flips: 104, rs: 30, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730138
Test loss (w/o reg) on all data: 0.012054805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.321961e-07
Norm of the params: 9.153147
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730114
Test loss (w/o reg) on all data: 0.012054853
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3951792e-07
Norm of the params: 9.15315
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11390711
Train loss (w/o reg) on all data: 0.10450595
Test loss (w/o reg) on all data: 0.059790783
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.632059e-06
Norm of the params: 13.712155
              Random: fixed   8 labels. Loss 0.05979. Accuracy 0.977.
### Flips: 104, rs: 30, checks: 260
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729104
Test loss (w/o reg) on all data: 0.012054389
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8431494e-07
Norm of the params: 9.15326
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729157
Test loss (w/o reg) on all data: 0.0120544
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.69039e-07
Norm of the params: 9.1532545
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10603622
Train loss (w/o reg) on all data: 0.096302375
Test loss (w/o reg) on all data: 0.05473345
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.7924113e-06
Norm of the params: 13.952668
              Random: fixed  11 labels. Loss 0.05473. Accuracy 0.981.
### Flips: 104, rs: 30, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729942
Test loss (w/o reg) on all data: 0.012055454
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9169541e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [2] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729711
Test loss (w/o reg) on all data: 0.0120540215
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9797366e-07
Norm of the params: 9.153192
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09971859
Train loss (w/o reg) on all data: 0.08955427
Test loss (w/o reg) on all data: 0.045408677
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1091467e-05
Norm of the params: 14.2578535
              Random: fixed  14 labels. Loss 0.04541. Accuracy 0.989.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129739
Train loss (w/o reg) on all data: 0.120294005
Test loss (w/o reg) on all data: 0.07745514
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.133446e-06
Norm of the params: 13.744085
Flipped loss: 0.07746. Accuracy: 0.981
### Flips: 104, rs: 31, checks: 52
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035878185
Train loss (w/o reg) on all data: 0.026124505
Test loss (w/o reg) on all data: 0.025141
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2789181e-06
Norm of the params: 13.966874
     Influence (LOO): fixed  40 labels. Loss 0.02514. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014880411
Train loss (w/o reg) on all data: 0.00712015
Test loss (w/o reg) on all data: 0.029141674
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5099797e-06
Norm of the params: 12.458139
                Loss: fixed  47 labels. Loss 0.02914. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12607917
Train loss (w/o reg) on all data: 0.116335295
Test loss (w/o reg) on all data: 0.072191544
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.5623262e-06
Norm of the params: 13.959858
              Random: fixed   2 labels. Loss 0.07219. Accuracy 0.981.
### Flips: 104, rs: 31, checks: 104
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010800714
Train loss (w/o reg) on all data: 0.004803362
Test loss (w/o reg) on all data: 0.01679067
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2426757e-06
Norm of the params: 10.952033
     Influence (LOO): fixed  53 labels. Loss 0.01679. Accuracy 0.992.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010508681
Train loss (w/o reg) on all data: 0.004450921
Test loss (w/o reg) on all data: 0.017697051
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.986837e-07
Norm of the params: 11.007051
                Loss: fixed  52 labels. Loss 0.01770. Accuracy 0.992.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122865565
Train loss (w/o reg) on all data: 0.11283479
Test loss (w/o reg) on all data: 0.069935866
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5684269e-05
Norm of the params: 14.163879
              Random: fixed   3 labels. Loss 0.06994. Accuracy 0.981.
### Flips: 104, rs: 31, checks: 156
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0094060665
Train loss (w/o reg) on all data: 0.0038843069
Test loss (w/o reg) on all data: 0.017830309
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0209366e-06
Norm of the params: 10.508816
     Influence (LOO): fixed  54 labels. Loss 0.01783. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008674882
Train loss (w/o reg) on all data: 0.0033812174
Test loss (w/o reg) on all data: 0.014997551
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3401687e-07
Norm of the params: 10.289475
                Loss: fixed  54 labels. Loss 0.01500. Accuracy 0.992.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121154994
Train loss (w/o reg) on all data: 0.11102619
Test loss (w/o reg) on all data: 0.064054005
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0398673e-05
Norm of the params: 14.2329235
              Random: fixed   5 labels. Loss 0.06405. Accuracy 0.989.
### Flips: 104, rs: 31, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729143
Test loss (w/o reg) on all data: 0.012055616
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4174528e-07
Norm of the params: 9.153255
     Influence (LOO): fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729148
Test loss (w/o reg) on all data: 0.012055517
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.963574e-07
Norm of the params: 9.1532545
                Loss: fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116472706
Train loss (w/o reg) on all data: 0.106710404
Test loss (w/o reg) on all data: 0.054846805
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2153743e-05
Norm of the params: 13.973047
              Random: fixed   8 labels. Loss 0.05485. Accuracy 0.992.
### Flips: 104, rs: 31, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729788
Test loss (w/o reg) on all data: 0.012054558
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.089609e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729802
Test loss (w/o reg) on all data: 0.012054691
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7835222e-07
Norm of the params: 9.153184
                Loss: fixed  56 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10012723
Train loss (w/o reg) on all data: 0.09026423
Test loss (w/o reg) on all data: 0.060182534
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.696361e-06
Norm of the params: 14.044928
              Random: fixed  14 labels. Loss 0.06018. Accuracy 0.985.
### Flips: 104, rs: 31, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729139
Test loss (w/o reg) on all data: 0.012055041
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.618226e-07
Norm of the params: 9.153255
     Influence (LOO): fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729157
Test loss (w/o reg) on all data: 0.0120551195
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8892581e-07
Norm of the params: 9.1532545
                Loss: fixed  56 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08867469
Train loss (w/o reg) on all data: 0.07799628
Test loss (w/o reg) on all data: 0.054848116
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2242214e-06
Norm of the params: 14.613974
              Random: fixed  18 labels. Loss 0.05485. Accuracy 0.985.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10977456
Train loss (w/o reg) on all data: 0.1013227
Test loss (w/o reg) on all data: 0.07869001
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.4330434e-06
Norm of the params: 13.001429
Flipped loss: 0.07869. Accuracy: 0.973
### Flips: 104, rs: 32, checks: 52
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0153288
Train loss (w/o reg) on all data: 0.008988055
Test loss (w/o reg) on all data: 0.02331717
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.614973e-07
Norm of the params: 11.261212
     Influence (LOO): fixed  39 labels. Loss 0.02332. Accuracy 0.992.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009607948
Train loss (w/o reg) on all data: 0.0038329666
Test loss (w/o reg) on all data: 0.017880429
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7361747e-07
Norm of the params: 10.747075
                Loss: fixed  39 labels. Loss 0.01788. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10701904
Train loss (w/o reg) on all data: 0.09917559
Test loss (w/o reg) on all data: 0.0661004
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.859261e-06
Norm of the params: 12.5247345
              Random: fixed   4 labels. Loss 0.06610. Accuracy 0.985.
### Flips: 104, rs: 32, checks: 104
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729995
Test loss (w/o reg) on all data: 0.0120551875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.181347e-07
Norm of the params: 9.153161
     Influence (LOO): fixed  43 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077479435
Train loss (w/o reg) on all data: 0.0027885565
Test loss (w/o reg) on all data: 0.011812037
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6542853e-07
Norm of the params: 9.959305
                Loss: fixed  42 labels. Loss 0.01181. Accuracy 0.992.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.094692655
Train loss (w/o reg) on all data: 0.085737124
Test loss (w/o reg) on all data: 0.06018645
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.430262e-06
Norm of the params: 13.383223
              Random: fixed   8 labels. Loss 0.06019. Accuracy 0.981.
### Flips: 104, rs: 32, checks: 156
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729958
Test loss (w/o reg) on all data: 0.012054703
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5608309e-07
Norm of the params: 9.153167
     Influence (LOO): fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172996
Test loss (w/o reg) on all data: 0.012054751
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2886177e-07
Norm of the params: 9.153168
                Loss: fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07886967
Train loss (w/o reg) on all data: 0.06964056
Test loss (w/o reg) on all data: 0.052010328
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.059628e-06
Norm of the params: 13.5861
              Random: fixed  14 labels. Loss 0.05201. Accuracy 0.981.
### Flips: 104, rs: 32, checks: 208
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620214
Train loss (w/o reg) on all data: 0.0021730028
Test loss (w/o reg) on all data: 0.012054277
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5457495e-07
Norm of the params: 9.153162
     Influence (LOO): fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729933
Test loss (w/o reg) on all data: 0.012054554
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9329955e-07
Norm of the params: 9.153168
                Loss: fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07292993
Train loss (w/o reg) on all data: 0.06343988
Test loss (w/o reg) on all data: 0.056078102
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.4838034e-06
Norm of the params: 13.776826
              Random: fixed  16 labels. Loss 0.05608. Accuracy 0.977.
### Flips: 104, rs: 32, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730093
Test loss (w/o reg) on all data: 0.012055235
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9488663e-07
Norm of the params: 9.153152
     Influence (LOO): fixed  43 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173007
Test loss (w/o reg) on all data: 0.012055321
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2600187e-07
Norm of the params: 9.153154
                Loss: fixed  43 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0692337
Train loss (w/o reg) on all data: 0.05879462
Test loss (w/o reg) on all data: 0.053873822
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.1617225e-06
Norm of the params: 14.449278
              Random: fixed  17 labels. Loss 0.05387. Accuracy 0.977.
### Flips: 104, rs: 32, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729846
Test loss (w/o reg) on all data: 0.01205346
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8158495e-07
Norm of the params: 9.153181
     Influence (LOO): fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172983
Test loss (w/o reg) on all data: 0.01205343
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.525955e-07
Norm of the params: 9.153181
                Loss: fixed  43 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066086315
Train loss (w/o reg) on all data: 0.055644996
Test loss (w/o reg) on all data: 0.05228631
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4989894e-05
Norm of the params: 14.450828
              Random: fixed  18 labels. Loss 0.05229. Accuracy 0.977.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15727998
Train loss (w/o reg) on all data: 0.15011333
Test loss (w/o reg) on all data: 0.10154722
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.995999e-06
Norm of the params: 11.972176
Flipped loss: 0.10155. Accuracy: 0.969
### Flips: 104, rs: 33, checks: 52
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037897438
Train loss (w/o reg) on all data: 0.02707376
Test loss (w/o reg) on all data: 0.07337702
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4841402e-06
Norm of the params: 14.713039
     Influence (LOO): fixed  43 labels. Loss 0.07338. Accuracy 0.969.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018959077
Train loss (w/o reg) on all data: 0.009310492
Test loss (w/o reg) on all data: 0.0395308
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2472595e-06
Norm of the params: 13.891425
                Loss: fixed  52 labels. Loss 0.03953. Accuracy 0.985.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15446886
Train loss (w/o reg) on all data: 0.14760841
Test loss (w/o reg) on all data: 0.09307967
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1477978e-05
Norm of the params: 11.713625
              Random: fixed   2 labels. Loss 0.09308. Accuracy 0.969.
### Flips: 104, rs: 33, checks: 104
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01139543
Train loss (w/o reg) on all data: 0.005302618
Test loss (w/o reg) on all data: 0.024530211
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.834135e-07
Norm of the params: 11.038852
     Influence (LOO): fixed  59 labels. Loss 0.02453. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008264304
Train loss (w/o reg) on all data: 0.0032539554
Test loss (w/o reg) on all data: 0.014793541
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5840455e-07
Norm of the params: 10.010344
                Loss: fixed  62 labels. Loss 0.01479. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15071136
Train loss (w/o reg) on all data: 0.14322948
Test loss (w/o reg) on all data: 0.089751795
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5084322e-05
Norm of the params: 12.232636
              Random: fixed   3 labels. Loss 0.08975. Accuracy 0.969.
### Flips: 104, rs: 33, checks: 156
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008264306
Train loss (w/o reg) on all data: 0.0032536043
Test loss (w/o reg) on all data: 0.014793195
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3027079e-06
Norm of the params: 10.010696
     Influence (LOO): fixed  62 labels. Loss 0.01479. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008264305
Train loss (w/o reg) on all data: 0.003253607
Test loss (w/o reg) on all data: 0.014792899
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.454557e-07
Norm of the params: 10.010693
                Loss: fixed  62 labels. Loss 0.01479. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14672916
Train loss (w/o reg) on all data: 0.13900824
Test loss (w/o reg) on all data: 0.08975483
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.8673e-06
Norm of the params: 12.4265175
              Random: fixed   5 labels. Loss 0.08975. Accuracy 0.962.
### Flips: 104, rs: 33, checks: 208
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008264305
Train loss (w/o reg) on all data: 0.003253879
Test loss (w/o reg) on all data: 0.014794572
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1073755e-06
Norm of the params: 10.010422
     Influence (LOO): fixed  62 labels. Loss 0.01479. Accuracy 0.992.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729728
Test loss (w/o reg) on all data: 0.012054887
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.8979044e-08
Norm of the params: 9.153193
                Loss: fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13879624
Train loss (w/o reg) on all data: 0.13150582
Test loss (w/o reg) on all data: 0.08418495
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.7672577e-06
Norm of the params: 12.075114
              Random: fixed  10 labels. Loss 0.08418. Accuracy 0.973.
### Flips: 104, rs: 33, checks: 260
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008264306
Train loss (w/o reg) on all data: 0.0032540287
Test loss (w/o reg) on all data: 0.014793932
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.529306e-07
Norm of the params: 10.010272
     Influence (LOO): fixed  62 labels. Loss 0.01479. Accuracy 0.992.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729718
Test loss (w/o reg) on all data: 0.012054888
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.062616e-08
Norm of the params: 9.153193
                Loss: fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13220763
Train loss (w/o reg) on all data: 0.12473429
Test loss (w/o reg) on all data: 0.079604104
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6606551e-05
Norm of the params: 12.225666
              Random: fixed  14 labels. Loss 0.07960. Accuracy 0.977.
### Flips: 104, rs: 33, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730666
Test loss (w/o reg) on all data: 0.012054202
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6091359e-07
Norm of the params: 9.1530905
     Influence (LOO): fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730491
Test loss (w/o reg) on all data: 0.012054102
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.451002e-07
Norm of the params: 9.153108
                Loss: fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12199418
Train loss (w/o reg) on all data: 0.11473063
Test loss (w/o reg) on all data: 0.06637077
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6100802e-05
Norm of the params: 12.052844
              Random: fixed  20 labels. Loss 0.06637. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13907634
Train loss (w/o reg) on all data: 0.12846798
Test loss (w/o reg) on all data: 0.075336255
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.28443735e-05
Norm of the params: 14.56596
Flipped loss: 0.07534. Accuracy: 0.985
### Flips: 104, rs: 34, checks: 52
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031012926
Train loss (w/o reg) on all data: 0.021453349
Test loss (w/o reg) on all data: 0.019908527
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7915117e-06
Norm of the params: 13.827204
     Influence (LOO): fixed  44 labels. Loss 0.01991. Accuracy 0.992.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018359525
Train loss (w/o reg) on all data: 0.0088917725
Test loss (w/o reg) on all data: 0.014277496
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.665598e-07
Norm of the params: 13.760634
                Loss: fixed  48 labels. Loss 0.01428. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1339759
Train loss (w/o reg) on all data: 0.12382064
Test loss (w/o reg) on all data: 0.07768544
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.24006065e-05
Norm of the params: 14.251494
              Random: fixed   4 labels. Loss 0.07769. Accuracy 0.985.
### Flips: 104, rs: 34, checks: 104
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007545241
Train loss (w/o reg) on all data: 0.0028551486
Test loss (w/o reg) on all data: 0.012165289
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9617354e-07
Norm of the params: 9.685136
     Influence (LOO): fixed  57 labels. Loss 0.01217. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073764366
Train loss (w/o reg) on all data: 0.0026223036
Test loss (w/o reg) on all data: 0.013233407
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.933355e-08
Norm of the params: 9.751034
                Loss: fixed  57 labels. Loss 0.01323. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12271572
Train loss (w/o reg) on all data: 0.112469405
Test loss (w/o reg) on all data: 0.07464642
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.903174e-06
Norm of the params: 14.315248
              Random: fixed   9 labels. Loss 0.07465. Accuracy 0.985.
### Flips: 104, rs: 34, checks: 156
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172901
Test loss (w/o reg) on all data: 0.01205525
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2808214e-07
Norm of the params: 9.153271
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007376435
Train loss (w/o reg) on all data: 0.0026223178
Test loss (w/o reg) on all data: 0.013233155
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9400241e-07
Norm of the params: 9.751019
                Loss: fixed  57 labels. Loss 0.01323. Accuracy 0.992.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11888214
Train loss (w/o reg) on all data: 0.108526975
Test loss (w/o reg) on all data: 0.07407755
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7647717e-06
Norm of the params: 14.391084
              Random: fixed  11 labels. Loss 0.07408. Accuracy 0.985.
### Flips: 104, rs: 34, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729548
Test loss (w/o reg) on all data: 0.012056032
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4631737e-07
Norm of the params: 9.153211
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729537
Test loss (w/o reg) on all data: 0.012055878
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.755887e-07
Norm of the params: 9.153212
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11200047
Train loss (w/o reg) on all data: 0.101862
Test loss (w/o reg) on all data: 0.07105621
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0441163e-05
Norm of the params: 14.239717
              Random: fixed  14 labels. Loss 0.07106. Accuracy 0.981.
### Flips: 104, rs: 34, checks: 260
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728983
Test loss (w/o reg) on all data: 0.012054726
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2599261e-07
Norm of the params: 9.153274
     Influence (LOO): fixed  58 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729039
Test loss (w/o reg) on all data: 0.012054792
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9802487e-07
Norm of the params: 9.153267
                Loss: fixed  58 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10033806
Train loss (w/o reg) on all data: 0.08922374
Test loss (w/o reg) on all data: 0.06955962
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.5706325e-06
Norm of the params: 14.909266
              Random: fixed  17 labels. Loss 0.06956. Accuracy 0.985.
### Flips: 104, rs: 34, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728072
Test loss (w/o reg) on all data: 0.012055277
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0483858e-06
Norm of the params: 9.153373
     Influence (LOO): fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728077
Test loss (w/o reg) on all data: 0.012055097
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.404624e-07
Norm of the params: 9.153371
                Loss: fixed  58 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.097203314
Train loss (w/o reg) on all data: 0.08632078
Test loss (w/o reg) on all data: 0.068483144
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.889132e-06
Norm of the params: 14.752992
              Random: fixed  20 labels. Loss 0.06848. Accuracy 0.985.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124123566
Train loss (w/o reg) on all data: 0.11328346
Test loss (w/o reg) on all data: 0.066668935
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.1805357e-05
Norm of the params: 14.724201
Flipped loss: 0.06667. Accuracy: 0.973
### Flips: 104, rs: 35, checks: 52
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03351959
Train loss (w/o reg) on all data: 0.023616819
Test loss (w/o reg) on all data: 0.025573343
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0634993e-06
Norm of the params: 14.073217
     Influence (LOO): fixed  40 labels. Loss 0.02557. Accuracy 0.992.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019268814
Train loss (w/o reg) on all data: 0.010571389
Test loss (w/o reg) on all data: 0.023965811
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9541294e-06
Norm of the params: 13.188954
                Loss: fixed  43 labels. Loss 0.02397. Accuracy 0.989.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11850692
Train loss (w/o reg) on all data: 0.10772832
Test loss (w/o reg) on all data: 0.062022567
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.239783e-06
Norm of the params: 14.682375
              Random: fixed   4 labels. Loss 0.06202. Accuracy 0.977.
### Flips: 104, rs: 35, checks: 104
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008712474
Train loss (w/o reg) on all data: 0.0034624478
Test loss (w/o reg) on all data: 0.011827353
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1037757e-07
Norm of the params: 10.246978
     Influence (LOO): fixed  49 labels. Loss 0.01183. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729711
Test loss (w/o reg) on all data: 0.012054446
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0397994e-07
Norm of the params: 9.153193
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11689571
Train loss (w/o reg) on all data: 0.106032915
Test loss (w/o reg) on all data: 0.062154837
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0523432e-05
Norm of the params: 14.739603
              Random: fixed   5 labels. Loss 0.06215. Accuracy 0.973.
### Flips: 104, rs: 35, checks: 156
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729122
Test loss (w/o reg) on all data: 0.012055734
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9199654e-07
Norm of the params: 9.153256
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729185
Test loss (w/o reg) on all data: 0.0120554725
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.011778e-07
Norm of the params: 9.153253
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11225563
Train loss (w/o reg) on all data: 0.10142663
Test loss (w/o reg) on all data: 0.061000638
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.720277e-06
Norm of the params: 14.716655
              Random: fixed   7 labels. Loss 0.06100. Accuracy 0.973.
### Flips: 104, rs: 35, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729677
Test loss (w/o reg) on all data: 0.012055061
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7550639e-07
Norm of the params: 9.153197
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729663
Test loss (w/o reg) on all data: 0.012055126
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2859724e-07
Norm of the params: 9.153198
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10931045
Train loss (w/o reg) on all data: 0.098150454
Test loss (w/o reg) on all data: 0.06042439
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.4375817e-05
Norm of the params: 14.9398775
              Random: fixed   8 labels. Loss 0.06042. Accuracy 0.973.
### Flips: 104, rs: 35, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730564
Test loss (w/o reg) on all data: 0.012055654
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.768718e-07
Norm of the params: 9.153101
     Influence (LOO): fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730524
Test loss (w/o reg) on all data: 0.012055553
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5784916e-07
Norm of the params: 9.153105
                Loss: fixed  51 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10857462
Train loss (w/o reg) on all data: 0.09827156
Test loss (w/o reg) on all data: 0.053190723
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4412823e-05
Norm of the params: 14.354833
              Random: fixed  11 labels. Loss 0.05319. Accuracy 0.985.
### Flips: 104, rs: 35, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729637
Test loss (w/o reg) on all data: 0.012054935
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0531507e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729635
Test loss (w/o reg) on all data: 0.012054892
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2358345e-07
Norm of the params: 9.153201
                Loss: fixed  51 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10303958
Train loss (w/o reg) on all data: 0.093112834
Test loss (w/o reg) on all data: 0.051161654
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3724334e-05
Norm of the params: 14.0902405
              Random: fixed  13 labels. Loss 0.05116. Accuracy 0.985.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13825664
Train loss (w/o reg) on all data: 0.12868172
Test loss (w/o reg) on all data: 0.06954872
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.37135285e-05
Norm of the params: 13.83829
Flipped loss: 0.06955. Accuracy: 0.992
### Flips: 104, rs: 36, checks: 52
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025681159
Train loss (w/o reg) on all data: 0.019054832
Test loss (w/o reg) on all data: 0.022913197
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0202499e-06
Norm of the params: 11.512017
     Influence (LOO): fixed  44 labels. Loss 0.02291. Accuracy 0.992.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012529547
Train loss (w/o reg) on all data: 0.005693343
Test loss (w/o reg) on all data: 0.016097806
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0764626e-06
Norm of the params: 11.692907
                Loss: fixed  48 labels. Loss 0.01610. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13576637
Train loss (w/o reg) on all data: 0.12614416
Test loss (w/o reg) on all data: 0.06821248
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.86801e-06
Norm of the params: 13.872429
              Random: fixed   1 labels. Loss 0.06821. Accuracy 0.996.
### Flips: 104, rs: 36, checks: 104
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009354463
Train loss (w/o reg) on all data: 0.00438291
Test loss (w/o reg) on all data: 0.014621969
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0230197e-07
Norm of the params: 9.971513
     Influence (LOO): fixed  51 labels. Loss 0.01462. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021730661
Test loss (w/o reg) on all data: 0.012055765
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.945134e-07
Norm of the params: 9.153089
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1284996
Train loss (w/o reg) on all data: 0.11881245
Test loss (w/o reg) on all data: 0.062164247
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1202337e-05
Norm of the params: 13.919161
              Random: fixed   4 labels. Loss 0.06216. Accuracy 0.992.
### Flips: 104, rs: 36, checks: 156
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729672
Test loss (w/o reg) on all data: 0.01205479
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5582165e-07
Norm of the params: 9.153198
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729658
Test loss (w/o reg) on all data: 0.012054745
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.388041e-08
Norm of the params: 9.153199
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12067936
Train loss (w/o reg) on all data: 0.111096874
Test loss (w/o reg) on all data: 0.05754657
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.524785e-06
Norm of the params: 13.843762
              Random: fixed   7 labels. Loss 0.05755. Accuracy 0.992.
### Flips: 104, rs: 36, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728931
Test loss (w/o reg) on all data: 0.012054097
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.944e-07
Norm of the params: 9.153278
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172897
Test loss (w/o reg) on all data: 0.012054196
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0878095e-07
Norm of the params: 9.1532755
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11547379
Train loss (w/o reg) on all data: 0.10637258
Test loss (w/o reg) on all data: 0.057416487
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0932375e-05
Norm of the params: 13.491638
              Random: fixed  10 labels. Loss 0.05742. Accuracy 0.989.
### Flips: 104, rs: 36, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.002172954
Test loss (w/o reg) on all data: 0.012055226
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.1360825e-08
Norm of the params: 9.153214
     Influence (LOO): fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172953
Test loss (w/o reg) on all data: 0.012055218
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1532733e-07
Norm of the params: 9.153214
                Loss: fixed  52 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1085995
Train loss (w/o reg) on all data: 0.09989624
Test loss (w/o reg) on all data: 0.05831783
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.588597e-06
Norm of the params: 13.193377
              Random: fixed  14 labels. Loss 0.05832. Accuracy 0.989.
### Flips: 104, rs: 36, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.002172959
Test loss (w/o reg) on all data: 0.012054536
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2915548e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729574
Test loss (w/o reg) on all data: 0.012054573
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6752835e-07
Norm of the params: 9.153209
                Loss: fixed  52 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1063827
Train loss (w/o reg) on all data: 0.097669356
Test loss (w/o reg) on all data: 0.056010604
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.1324596e-06
Norm of the params: 13.201015
              Random: fixed  15 labels. Loss 0.05601. Accuracy 0.989.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15669513
Train loss (w/o reg) on all data: 0.14879437
Test loss (w/o reg) on all data: 0.077772066
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3106147e-05
Norm of the params: 12.570415
Flipped loss: 0.07777. Accuracy: 0.996
### Flips: 104, rs: 37, checks: 52
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057156876
Train loss (w/o reg) on all data: 0.046839587
Test loss (w/o reg) on all data: 0.04787506
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.525007e-06
Norm of the params: 14.364741
     Influence (LOO): fixed  43 labels. Loss 0.04788. Accuracy 0.985.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033156157
Train loss (w/o reg) on all data: 0.020222427
Test loss (w/o reg) on all data: 0.032188434
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.5955993e-06
Norm of the params: 16.083364
                Loss: fixed  49 labels. Loss 0.03219. Accuracy 0.989.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14955343
Train loss (w/o reg) on all data: 0.14162117
Test loss (w/o reg) on all data: 0.074475095
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.252373e-06
Norm of the params: 12.595448
              Random: fixed   4 labels. Loss 0.07448. Accuracy 0.992.
### Flips: 104, rs: 37, checks: 104
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014304009
Train loss (w/o reg) on all data: 0.008013684
Test loss (w/o reg) on all data: 0.019461064
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8331319e-06
Norm of the params: 11.21635
     Influence (LOO): fixed  63 labels. Loss 0.01946. Accuracy 0.989.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011383962
Train loss (w/o reg) on all data: 0.0047083977
Test loss (w/o reg) on all data: 0.015913885
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5192317e-06
Norm of the params: 11.554708
                Loss: fixed  63 labels. Loss 0.01591. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1461684
Train loss (w/o reg) on all data: 0.13836284
Test loss (w/o reg) on all data: 0.06682001
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3219529e-05
Norm of the params: 12.494444
              Random: fixed   7 labels. Loss 0.06682. Accuracy 0.992.
### Flips: 104, rs: 37, checks: 156
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729537
Test loss (w/o reg) on all data: 0.012054161
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.327851e-07
Norm of the params: 9.153213
     Influence (LOO): fixed  67 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077131046
Train loss (w/o reg) on all data: 0.0028698891
Test loss (w/o reg) on all data: 0.015329057
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.532715e-07
Norm of the params: 9.841968
                Loss: fixed  66 labels. Loss 0.01533. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13791937
Train loss (w/o reg) on all data: 0.12963207
Test loss (w/o reg) on all data: 0.0618264
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.0218154e-06
Norm of the params: 12.874229
              Random: fixed  11 labels. Loss 0.06183. Accuracy 0.996.
### Flips: 104, rs: 37, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730147
Test loss (w/o reg) on all data: 0.012055116
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0912537e-07
Norm of the params: 9.153146
     Influence (LOO): fixed  67 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730128
Test loss (w/o reg) on all data: 0.012055171
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1635137e-07
Norm of the params: 9.153148
                Loss: fixed  67 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13163511
Train loss (w/o reg) on all data: 0.122787185
Test loss (w/o reg) on all data: 0.058946177
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4192828e-05
Norm of the params: 13.302583
              Random: fixed  13 labels. Loss 0.05895. Accuracy 0.996.
### Flips: 104, rs: 37, checks: 260
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729618
Test loss (w/o reg) on all data: 0.0120549835
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7014513e-07
Norm of the params: 9.153205
     Influence (LOO): fixed  67 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012054828
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.42753e-07
Norm of the params: 9.153201
                Loss: fixed  67 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12888575
Train loss (w/o reg) on all data: 0.12025628
Test loss (w/o reg) on all data: 0.06070999
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8523988e-05
Norm of the params: 13.137327
              Random: fixed  16 labels. Loss 0.06071. Accuracy 0.996.
### Flips: 104, rs: 37, checks: 312
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729479
Test loss (w/o reg) on all data: 0.012055562
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.52827e-07
Norm of the params: 9.153219
     Influence (LOO): fixed  67 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729516
Test loss (w/o reg) on all data: 0.012055302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.673029e-07
Norm of the params: 9.153216
                Loss: fixed  67 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11716089
Train loss (w/o reg) on all data: 0.107852645
Test loss (w/o reg) on all data: 0.05004572
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6017299e-05
Norm of the params: 13.644221
              Random: fixed  22 labels. Loss 0.05005. Accuracy 0.996.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1442609
Train loss (w/o reg) on all data: 0.13752647
Test loss (w/o reg) on all data: 0.097550824
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0699657e-05
Norm of the params: 11.605537
Flipped loss: 0.09755. Accuracy: 0.981
### Flips: 104, rs: 38, checks: 52
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028953664
Train loss (w/o reg) on all data: 0.01932942
Test loss (w/o reg) on all data: 0.03519512
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6581106e-06
Norm of the params: 13.873893
     Influence (LOO): fixed  46 labels. Loss 0.03520. Accuracy 0.989.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010529647
Train loss (w/o reg) on all data: 0.004519108
Test loss (w/o reg) on all data: 0.01802312
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.2632165e-07
Norm of the params: 10.9640665
                Loss: fixed  51 labels. Loss 0.01802. Accuracy 0.989.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13799416
Train loss (w/o reg) on all data: 0.13104102
Test loss (w/o reg) on all data: 0.092710145
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6376189e-05
Norm of the params: 11.792485
              Random: fixed   3 labels. Loss 0.09271. Accuracy 0.973.
### Flips: 104, rs: 38, checks: 104
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729565
Test loss (w/o reg) on all data: 0.012054856
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3053923e-07
Norm of the params: 9.153213
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172956
Test loss (w/o reg) on all data: 0.012054905
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3682514e-07
Norm of the params: 9.153211
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13002288
Train loss (w/o reg) on all data: 0.12307009
Test loss (w/o reg) on all data: 0.08407135
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2570687e-05
Norm of the params: 11.79219
              Random: fixed   7 labels. Loss 0.08407. Accuracy 0.981.
### Flips: 104, rs: 38, checks: 156
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729886
Test loss (w/o reg) on all data: 0.01205521
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.537617e-07
Norm of the params: 9.153174
     Influence (LOO): fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729872
Test loss (w/o reg) on all data: 0.012055244
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.07472474e-07
Norm of the params: 9.153174
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123194486
Train loss (w/o reg) on all data: 0.11576158
Test loss (w/o reg) on all data: 0.0821048
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.0612796e-06
Norm of the params: 12.192549
              Random: fixed  10 labels. Loss 0.08210. Accuracy 0.977.
### Flips: 104, rs: 38, checks: 208
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729798
Test loss (w/o reg) on all data: 0.01205479
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1155903e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172978
Test loss (w/o reg) on all data: 0.012054842
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.6836443e-07
Norm of the params: 9.153185
                Loss: fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11255398
Train loss (w/o reg) on all data: 0.104933344
Test loss (w/o reg) on all data: 0.0790372
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.6839234e-06
Norm of the params: 12.345554
              Random: fixed  14 labels. Loss 0.07904. Accuracy 0.977.
### Flips: 104, rs: 38, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173024
Test loss (w/o reg) on all data: 0.012054921
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3717847e-07
Norm of the params: 9.153136
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729986
Test loss (w/o reg) on all data: 0.012055096
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.272234e-07
Norm of the params: 9.153162
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10574457
Train loss (w/o reg) on all data: 0.09779797
Test loss (w/o reg) on all data: 0.07987778
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.7839065e-06
Norm of the params: 12.606827
              Random: fixed  17 labels. Loss 0.07988. Accuracy 0.977.
### Flips: 104, rs: 38, checks: 312
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172986
Test loss (w/o reg) on all data: 0.012054813
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7149904e-07
Norm of the params: 9.153177
     Influence (LOO): fixed  55 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729784
Test loss (w/o reg) on all data: 0.012055162
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.277893e-08
Norm of the params: 9.153185
                Loss: fixed  55 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10096543
Train loss (w/o reg) on all data: 0.093487896
Test loss (w/o reg) on all data: 0.078781635
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.03443e-06
Norm of the params: 12.229092
              Random: fixed  19 labels. Loss 0.07878. Accuracy 0.977.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15981781
Train loss (w/o reg) on all data: 0.15225102
Test loss (w/o reg) on all data: 0.08641345
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9631477e-05
Norm of the params: 12.301866
Flipped loss: 0.08641. Accuracy: 0.992
### Flips: 104, rs: 39, checks: 52
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04054827
Train loss (w/o reg) on all data: 0.030386994
Test loss (w/o reg) on all data: 0.028984755
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.8200225e-06
Norm of the params: 14.255718
     Influence (LOO): fixed  45 labels. Loss 0.02898. Accuracy 0.989.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01949847
Train loss (w/o reg) on all data: 0.009701651
Test loss (w/o reg) on all data: 0.018193748
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0037896e-06
Norm of the params: 13.997727
                Loss: fixed  51 labels. Loss 0.01819. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15697053
Train loss (w/o reg) on all data: 0.14889492
Test loss (w/o reg) on all data: 0.084635675
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1177601e-05
Norm of the params: 12.708751
              Random: fixed   2 labels. Loss 0.08464. Accuracy 0.989.
### Flips: 104, rs: 39, checks: 104
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011335151
Train loss (w/o reg) on all data: 0.005875362
Test loss (w/o reg) on all data: 0.015265874
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1258082e-07
Norm of the params: 10.449678
     Influence (LOO): fixed  61 labels. Loss 0.01527. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007950282
Train loss (w/o reg) on all data: 0.0029494513
Test loss (w/o reg) on all data: 0.014971188
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7327624e-07
Norm of the params: 10.00083
                Loss: fixed  62 labels. Loss 0.01497. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14784957
Train loss (w/o reg) on all data: 0.13985091
Test loss (w/o reg) on all data: 0.073604465
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.9291026e-05
Norm of the params: 12.648052
              Random: fixed   7 labels. Loss 0.07360. Accuracy 0.977.
### Flips: 104, rs: 39, checks: 156
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00795028
Train loss (w/o reg) on all data: 0.002949368
Test loss (w/o reg) on all data: 0.014970331
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9264663e-07
Norm of the params: 10.000912
     Influence (LOO): fixed  62 labels. Loss 0.01497. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007950282
Train loss (w/o reg) on all data: 0.0029493708
Test loss (w/o reg) on all data: 0.014970413
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7587097e-07
Norm of the params: 10.000912
                Loss: fixed  62 labels. Loss 0.01497. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14092466
Train loss (w/o reg) on all data: 0.13277408
Test loss (w/o reg) on all data: 0.07062944
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1463758e-05
Norm of the params: 12.767593
              Random: fixed   9 labels. Loss 0.07063. Accuracy 0.981.
### Flips: 104, rs: 39, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729597
Test loss (w/o reg) on all data: 0.012053491
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8397213e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729583
Test loss (w/o reg) on all data: 0.012053583
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2882283e-07
Norm of the params: 9.153208
                Loss: fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14092466
Train loss (w/o reg) on all data: 0.13277557
Test loss (w/o reg) on all data: 0.07062669
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.873536e-06
Norm of the params: 12.766424
              Random: fixed   9 labels. Loss 0.07063. Accuracy 0.981.
### Flips: 104, rs: 39, checks: 260
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021727933
Test loss (w/o reg) on all data: 0.01205432
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0327602e-07
Norm of the params: 9.153389
     Influence (LOO): fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728177
Test loss (w/o reg) on all data: 0.012054226
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.224962e-07
Norm of the params: 9.153362
                Loss: fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13301183
Train loss (w/o reg) on all data: 0.124399096
Test loss (w/o reg) on all data: 0.066187926
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.2106527e-05
Norm of the params: 13.124588
              Random: fixed  13 labels. Loss 0.06619. Accuracy 0.985.
### Flips: 104, rs: 39, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173024
Test loss (w/o reg) on all data: 0.012053205
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7907196e-06
Norm of the params: 9.153136
     Influence (LOO): fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730193
Test loss (w/o reg) on all data: 0.012053608
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5953656e-07
Norm of the params: 9.153141
                Loss: fixed  63 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12606923
Train loss (w/o reg) on all data: 0.11681965
Test loss (w/o reg) on all data: 0.06360599
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.854753e-06
Norm of the params: 13.60117
              Random: fixed  16 labels. Loss 0.06361. Accuracy 0.981.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17582993
Train loss (w/o reg) on all data: 0.16646259
Test loss (w/o reg) on all data: 0.094342455
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.2397802e-05
Norm of the params: 13.687477
Flipped loss: 0.09434. Accuracy: 0.985
### Flips: 156, rs: 0, checks: 52
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080038905
Train loss (w/o reg) on all data: 0.06810415
Test loss (w/o reg) on all data: 0.053678434
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.3837507e-06
Norm of the params: 15.44976
     Influence (LOO): fixed  41 labels. Loss 0.05368. Accuracy 0.981.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049185023
Train loss (w/o reg) on all data: 0.0340042
Test loss (w/o reg) on all data: 0.048372936
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.322622e-06
Norm of the params: 17.424595
                Loss: fixed  51 labels. Loss 0.04837. Accuracy 0.977.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17186984
Train loss (w/o reg) on all data: 0.16220522
Test loss (w/o reg) on all data: 0.09386171
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.761656e-06
Norm of the params: 13.902968
              Random: fixed   2 labels. Loss 0.09386. Accuracy 0.985.
### Flips: 156, rs: 0, checks: 104
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030641016
Train loss (w/o reg) on all data: 0.022157105
Test loss (w/o reg) on all data: 0.032800186
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.344348e-06
Norm of the params: 13.026059
     Influence (LOO): fixed  68 labels. Loss 0.03280. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010005575
Train loss (w/o reg) on all data: 0.003939018
Test loss (w/o reg) on all data: 0.016509572
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5214282e-06
Norm of the params: 11.015041
                Loss: fixed  78 labels. Loss 0.01651. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16324782
Train loss (w/o reg) on all data: 0.15332562
Test loss (w/o reg) on all data: 0.09178486
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.9241787e-06
Norm of the params: 14.087021
              Random: fixed   6 labels. Loss 0.09178. Accuracy 0.977.
### Flips: 156, rs: 0, checks: 156
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014839724
Train loss (w/o reg) on all data: 0.008692536
Test loss (w/o reg) on all data: 0.020684985
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.4545693e-07
Norm of the params: 11.088001
     Influence (LOO): fixed  77 labels. Loss 0.02068. Accuracy 0.989.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010005573
Train loss (w/o reg) on all data: 0.0039388626
Test loss (w/o reg) on all data: 0.016509548
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2292266e-07
Norm of the params: 11.015182
                Loss: fixed  78 labels. Loss 0.01651. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15807213
Train loss (w/o reg) on all data: 0.14810473
Test loss (w/o reg) on all data: 0.094490275
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.388416e-06
Norm of the params: 14.119061
              Random: fixed   9 labels. Loss 0.09449. Accuracy 0.969.
### Flips: 156, rs: 0, checks: 208
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009553244
Train loss (w/o reg) on all data: 0.0041079503
Test loss (w/o reg) on all data: 0.019601205
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.4156704e-07
Norm of the params: 10.435798
     Influence (LOO): fixed  79 labels. Loss 0.01960. Accuracy 0.989.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010005573
Train loss (w/o reg) on all data: 0.0039388705
Test loss (w/o reg) on all data: 0.016510444
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2785213e-06
Norm of the params: 11.015174
                Loss: fixed  78 labels. Loss 0.01651. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15446508
Train loss (w/o reg) on all data: 0.14508687
Test loss (w/o reg) on all data: 0.09124502
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.1605592e-05
Norm of the params: 13.695413
              Random: fixed  12 labels. Loss 0.09125. Accuracy 0.981.
### Flips: 156, rs: 0, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729132
Test loss (w/o reg) on all data: 0.012055005
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.472616e-07
Norm of the params: 9.153257
     Influence (LOO): fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00740896
Train loss (w/o reg) on all data: 0.0027598636
Test loss (w/o reg) on all data: 0.012726167
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5921027e-07
Norm of the params: 9.642714
                Loss: fixed  80 labels. Loss 0.01273. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15043426
Train loss (w/o reg) on all data: 0.14103371
Test loss (w/o reg) on all data: 0.085601315
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.13356255e-05
Norm of the params: 13.71171
              Random: fixed  15 labels. Loss 0.08560. Accuracy 0.985.
### Flips: 156, rs: 0, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021730238
Test loss (w/o reg) on all data: 0.0120546
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.31772e-07
Norm of the params: 9.153137
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730196
Test loss (w/o reg) on all data: 0.012054687
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.367808e-07
Norm of the params: 9.15314
                Loss: fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14303406
Train loss (w/o reg) on all data: 0.13294005
Test loss (w/o reg) on all data: 0.08108356
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.35519795e-05
Norm of the params: 14.208452
              Random: fixed  18 labels. Loss 0.08108. Accuracy 0.985.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18662985
Train loss (w/o reg) on all data: 0.17901784
Test loss (w/o reg) on all data: 0.12289545
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.554814e-06
Norm of the params: 12.338563
Flipped loss: 0.12290. Accuracy: 0.962
### Flips: 156, rs: 1, checks: 52
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082356825
Train loss (w/o reg) on all data: 0.07176415
Test loss (w/o reg) on all data: 0.07598877
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.455817e-06
Norm of the params: 14.55519
     Influence (LOO): fixed  45 labels. Loss 0.07599. Accuracy 0.973.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054831438
Train loss (w/o reg) on all data: 0.03995681
Test loss (w/o reg) on all data: 0.07789588
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.99122e-06
Norm of the params: 17.247972
                Loss: fixed  51 labels. Loss 0.07790. Accuracy 0.966.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1859533
Train loss (w/o reg) on all data: 0.17843068
Test loss (w/o reg) on all data: 0.12099042
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.6354312e-05
Norm of the params: 12.265905
              Random: fixed   1 labels. Loss 0.12099. Accuracy 0.954.
### Flips: 156, rs: 1, checks: 104
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032604426
Train loss (w/o reg) on all data: 0.024876846
Test loss (w/o reg) on all data: 0.037956327
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5360711e-06
Norm of the params: 12.43188
     Influence (LOO): fixed  69 labels. Loss 0.03796. Accuracy 0.985.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012718252
Train loss (w/o reg) on all data: 0.0054849763
Test loss (w/o reg) on all data: 0.026217286
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9405576e-07
Norm of the params: 12.027698
                Loss: fixed  77 labels. Loss 0.02622. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17872544
Train loss (w/o reg) on all data: 0.17129575
Test loss (w/o reg) on all data: 0.109087534
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.130078e-06
Norm of the params: 12.189903
              Random: fixed   7 labels. Loss 0.10909. Accuracy 0.973.
### Flips: 156, rs: 1, checks: 156
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0156053305
Train loss (w/o reg) on all data: 0.009400941
Test loss (w/o reg) on all data: 0.030561
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6239513e-06
Norm of the params: 11.139469
     Influence (LOO): fixed  77 labels. Loss 0.03056. Accuracy 0.985.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076928935
Train loss (w/o reg) on all data: 0.002884522
Test loss (w/o reg) on all data: 0.014483813
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4923502e-07
Norm of the params: 9.806499
                Loss: fixed  82 labels. Loss 0.01448. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17189415
Train loss (w/o reg) on all data: 0.16432455
Test loss (w/o reg) on all data: 0.10490005
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.2215226e-05
Norm of the params: 12.304137
              Random: fixed  10 labels. Loss 0.10490. Accuracy 0.962.
### Flips: 156, rs: 1, checks: 208
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010076605
Train loss (w/o reg) on all data: 0.0047586793
Test loss (w/o reg) on all data: 0.017589292
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.309247e-07
Norm of the params: 10.313026
     Influence (LOO): fixed  81 labels. Loss 0.01759. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076928944
Train loss (w/o reg) on all data: 0.0028846227
Test loss (w/o reg) on all data: 0.01448341
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2716382e-07
Norm of the params: 9.806398
                Loss: fixed  82 labels. Loss 0.01448. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16830228
Train loss (w/o reg) on all data: 0.16043898
Test loss (w/o reg) on all data: 0.09245486
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1205513e-05
Norm of the params: 12.540571
              Random: fixed  13 labels. Loss 0.09245. Accuracy 0.977.
### Flips: 156, rs: 1, checks: 260
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010076603
Train loss (w/o reg) on all data: 0.004758501
Test loss (w/o reg) on all data: 0.017589694
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8701249e-07
Norm of the params: 10.313197
     Influence (LOO): fixed  81 labels. Loss 0.01759. Accuracy 0.992.
Using normal model
LBFGS training took [33] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729069
Test loss (w/o reg) on all data: 0.012055161
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4070349e-07
Norm of the params: 9.153264
                Loss: fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15443785
Train loss (w/o reg) on all data: 0.14549965
Test loss (w/o reg) on all data: 0.07841989
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.653232e-05
Norm of the params: 13.370269
              Random: fixed  19 labels. Loss 0.07842. Accuracy 0.985.
### Flips: 156, rs: 1, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730836
Test loss (w/o reg) on all data: 0.012055025
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.5277505e-07
Norm of the params: 9.15307
     Influence (LOO): fixed  83 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.00217308
Test loss (w/o reg) on all data: 0.012054857
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2870662e-07
Norm of the params: 9.153071
                Loss: fixed  83 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14738509
Train loss (w/o reg) on all data: 0.13876916
Test loss (w/o reg) on all data: 0.081553735
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.425805e-06
Norm of the params: 13.127017
              Random: fixed  23 labels. Loss 0.08155. Accuracy 0.973.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18274851
Train loss (w/o reg) on all data: 0.17386556
Test loss (w/o reg) on all data: 0.13078804
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2851974e-05
Norm of the params: 13.328881
Flipped loss: 0.13079. Accuracy: 0.966
### Flips: 156, rs: 2, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08422437
Train loss (w/o reg) on all data: 0.072445296
Test loss (w/o reg) on all data: 0.08238844
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.9119503e-06
Norm of the params: 15.348668
     Influence (LOO): fixed  44 labels. Loss 0.08239. Accuracy 0.969.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05598191
Train loss (w/o reg) on all data: 0.038747184
Test loss (w/o reg) on all data: 0.09118543
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.8652494e-06
Norm of the params: 18.565952
                Loss: fixed  51 labels. Loss 0.09119. Accuracy 0.973.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17685969
Train loss (w/o reg) on all data: 0.16817646
Test loss (w/o reg) on all data: 0.117880434
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.909859e-06
Norm of the params: 13.178195
              Random: fixed   5 labels. Loss 0.11788. Accuracy 0.969.
### Flips: 156, rs: 2, checks: 104
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036955226
Train loss (w/o reg) on all data: 0.026138667
Test loss (w/o reg) on all data: 0.045934517
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.038022e-06
Norm of the params: 14.7082
     Influence (LOO): fixed  69 labels. Loss 0.04593. Accuracy 0.985.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016226934
Train loss (w/o reg) on all data: 0.0076537966
Test loss (w/o reg) on all data: 0.024862157
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.317186e-06
Norm of the params: 13.094379
                Loss: fixed  77 labels. Loss 0.02486. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17117366
Train loss (w/o reg) on all data: 0.16240665
Test loss (w/o reg) on all data: 0.11335396
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.719185e-05
Norm of the params: 13.2416115
              Random: fixed   9 labels. Loss 0.11335. Accuracy 0.969.
### Flips: 156, rs: 2, checks: 156
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02194845
Train loss (w/o reg) on all data: 0.013160358
Test loss (w/o reg) on all data: 0.020130789
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.890513e-06
Norm of the params: 13.25752
     Influence (LOO): fixed  77 labels. Loss 0.02013. Accuracy 0.992.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012948377
Train loss (w/o reg) on all data: 0.005752796
Test loss (w/o reg) on all data: 0.018944753
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.984982e-07
Norm of the params: 11.996318
                Loss: fixed  79 labels. Loss 0.01894. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1637967
Train loss (w/o reg) on all data: 0.1544482
Test loss (w/o reg) on all data: 0.10991868
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5544643e-05
Norm of the params: 13.673693
              Random: fixed  12 labels. Loss 0.10992. Accuracy 0.973.
### Flips: 156, rs: 2, checks: 208
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011343397
Train loss (w/o reg) on all data: 0.0049706744
Test loss (w/o reg) on all data: 0.015399964
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.906847e-07
Norm of the params: 11.289574
     Influence (LOO): fixed  82 labels. Loss 0.01540. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010841074
Train loss (w/o reg) on all data: 0.004590797
Test loss (w/o reg) on all data: 0.015687268
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6803382e-07
Norm of the params: 11.180589
                Loss: fixed  81 labels. Loss 0.01569. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15716493
Train loss (w/o reg) on all data: 0.1481847
Test loss (w/o reg) on all data: 0.10268967
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0468972e-05
Norm of the params: 13.401667
              Random: fixed  17 labels. Loss 0.10269. Accuracy 0.973.
### Flips: 156, rs: 2, checks: 260
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010671189
Train loss (w/o reg) on all data: 0.004468375
Test loss (w/o reg) on all data: 0.01552359
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5866142e-06
Norm of the params: 11.138056
     Influence (LOO): fixed  83 labels. Loss 0.01552. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0086588785
Train loss (w/o reg) on all data: 0.0033788872
Test loss (w/o reg) on all data: 0.015381254
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4741295e-07
Norm of the params: 10.276177
                Loss: fixed  83 labels. Loss 0.01538. Accuracy 0.992.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15281004
Train loss (w/o reg) on all data: 0.14391479
Test loss (w/o reg) on all data: 0.095893554
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.055459e-05
Norm of the params: 13.338106
              Random: fixed  22 labels. Loss 0.09589. Accuracy 0.973.
### Flips: 156, rs: 2, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728966
Test loss (w/o reg) on all data: 0.012055216
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6102808e-07
Norm of the params: 9.153274
     Influence (LOO): fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008258713
Train loss (w/o reg) on all data: 0.0031594378
Test loss (w/o reg) on all data: 0.012062132
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.964095e-07
Norm of the params: 10.098787
                Loss: fixed  84 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14518853
Train loss (w/o reg) on all data: 0.13624729
Test loss (w/o reg) on all data: 0.096488394
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0376208e-05
Norm of the params: 13.372536
              Random: fixed  25 labels. Loss 0.09649. Accuracy 0.973.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16847612
Train loss (w/o reg) on all data: 0.16060041
Test loss (w/o reg) on all data: 0.10456234
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.9157746e-05
Norm of the params: 12.550467
Flipped loss: 0.10456. Accuracy: 0.969
### Flips: 156, rs: 3, checks: 52
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063883655
Train loss (w/o reg) on all data: 0.054070175
Test loss (w/o reg) on all data: 0.055948995
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.26918285e-05
Norm of the params: 14.009626
     Influence (LOO): fixed  46 labels. Loss 0.05595. Accuracy 0.977.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032725785
Train loss (w/o reg) on all data: 0.02058952
Test loss (w/o reg) on all data: 0.048206743
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.2963037e-06
Norm of the params: 15.579644
                Loss: fixed  52 labels. Loss 0.04821. Accuracy 0.977.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16390157
Train loss (w/o reg) on all data: 0.15625969
Test loss (w/o reg) on all data: 0.098074116
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.3088121e-05
Norm of the params: 12.36275
              Random: fixed   3 labels. Loss 0.09807. Accuracy 0.966.
### Flips: 156, rs: 3, checks: 104
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023416482
Train loss (w/o reg) on all data: 0.014918943
Test loss (w/o reg) on all data: 0.033960268
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.479525e-06
Norm of the params: 13.036517
     Influence (LOO): fixed  64 labels. Loss 0.03396. Accuracy 0.981.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009894561
Train loss (w/o reg) on all data: 0.0040776446
Test loss (w/o reg) on all data: 0.018085375
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.8445115e-07
Norm of the params: 10.786025
                Loss: fixed  70 labels. Loss 0.01809. Accuracy 0.985.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14879388
Train loss (w/o reg) on all data: 0.14040619
Test loss (w/o reg) on all data: 0.09727624
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.6240556e-05
Norm of the params: 12.951974
              Random: fixed   8 labels. Loss 0.09728. Accuracy 0.966.
### Flips: 156, rs: 3, checks: 156
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075961007
Train loss (w/o reg) on all data: 0.0028793397
Test loss (w/o reg) on all data: 0.013192563
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1217353e-07
Norm of the params: 9.712632
     Influence (LOO): fixed  72 labels. Loss 0.01319. Accuracy 0.992.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009894565
Train loss (w/o reg) on all data: 0.0040776734
Test loss (w/o reg) on all data: 0.018084642
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1903716e-07
Norm of the params: 10.786001
                Loss: fixed  70 labels. Loss 0.01808. Accuracy 0.985.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14730167
Train loss (w/o reg) on all data: 0.13904792
Test loss (w/o reg) on all data: 0.08490145
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5243693e-05
Norm of the params: 12.848155
              Random: fixed  10 labels. Loss 0.08490. Accuracy 0.969.
### Flips: 156, rs: 3, checks: 208
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729926
Test loss (w/o reg) on all data: 0.012054178
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.927497e-07
Norm of the params: 9.153171
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009894563
Train loss (w/o reg) on all data: 0.0040777754
Test loss (w/o reg) on all data: 0.018085433
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.7799566e-07
Norm of the params: 10.785905
                Loss: fixed  70 labels. Loss 0.01809. Accuracy 0.985.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14003302
Train loss (w/o reg) on all data: 0.13213219
Test loss (w/o reg) on all data: 0.07487626
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.769468e-05
Norm of the params: 12.570473
              Random: fixed  15 labels. Loss 0.07488. Accuracy 0.981.
### Flips: 156, rs: 3, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729816
Test loss (w/o reg) on all data: 0.0120546995
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8100307e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075961007
Train loss (w/o reg) on all data: 0.002879171
Test loss (w/o reg) on all data: 0.013193779
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.1641207e-07
Norm of the params: 9.712806
                Loss: fixed  72 labels. Loss 0.01319. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1320626
Train loss (w/o reg) on all data: 0.12389213
Test loss (w/o reg) on all data: 0.07190377
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0857469e-05
Norm of the params: 12.783174
              Random: fixed  19 labels. Loss 0.07190. Accuracy 0.985.
### Flips: 156, rs: 3, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728768
Test loss (w/o reg) on all data: 0.012054174
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7505063e-07
Norm of the params: 9.153297
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728785
Test loss (w/o reg) on all data: 0.012054272
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5670246e-07
Norm of the params: 9.153296
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1259815
Train loss (w/o reg) on all data: 0.11758141
Test loss (w/o reg) on all data: 0.068259455
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3223571e-05
Norm of the params: 12.961547
              Random: fixed  24 labels. Loss 0.06826. Accuracy 0.989.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20025773
Train loss (w/o reg) on all data: 0.19430797
Test loss (w/o reg) on all data: 0.10441582
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8141736e-05
Norm of the params: 10.908495
Flipped loss: 0.10442. Accuracy: 0.981
### Flips: 156, rs: 4, checks: 52
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11006046
Train loss (w/o reg) on all data: 0.09949974
Test loss (w/o reg) on all data: 0.068696186
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.794413e-06
Norm of the params: 14.533218
     Influence (LOO): fixed  43 labels. Loss 0.06870. Accuracy 0.981.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054311953
Train loss (w/o reg) on all data: 0.038484026
Test loss (w/o reg) on all data: 0.042842932
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.6666205e-06
Norm of the params: 17.792093
                Loss: fixed  51 labels. Loss 0.04284. Accuracy 0.989.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19198507
Train loss (w/o reg) on all data: 0.18616773
Test loss (w/o reg) on all data: 0.095526636
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.397147e-05
Norm of the params: 10.786413
              Random: fixed   5 labels. Loss 0.09553. Accuracy 0.985.
### Flips: 156, rs: 4, checks: 104
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05313762
Train loss (w/o reg) on all data: 0.04282614
Test loss (w/o reg) on all data: 0.04942955
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.1280764e-06
Norm of the params: 14.360698
     Influence (LOO): fixed  72 labels. Loss 0.04943. Accuracy 0.981.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023496728
Train loss (w/o reg) on all data: 0.012232935
Test loss (w/o reg) on all data: 0.02897494
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.669261e-07
Norm of the params: 15.0091915
                Loss: fixed  78 labels. Loss 0.02897. Accuracy 0.985.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17606868
Train loss (w/o reg) on all data: 0.16935445
Test loss (w/o reg) on all data: 0.0888501
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.613986e-06
Norm of the params: 11.588118
              Random: fixed  13 labels. Loss 0.08885. Accuracy 0.985.
### Flips: 156, rs: 4, checks: 156
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018543791
Train loss (w/o reg) on all data: 0.011489162
Test loss (w/o reg) on all data: 0.028288322
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.095413e-07
Norm of the params: 11.87824
     Influence (LOO): fixed  87 labels. Loss 0.02829. Accuracy 0.989.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013432363
Train loss (w/o reg) on all data: 0.0059378366
Test loss (w/o reg) on all data: 0.020351557
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.2779853e-07
Norm of the params: 12.242979
                Loss: fixed  88 labels. Loss 0.02035. Accuracy 0.989.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16642982
Train loss (w/o reg) on all data: 0.15973504
Test loss (w/o reg) on all data: 0.07936001
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3877416e-05
Norm of the params: 11.571332
              Random: fixed  18 labels. Loss 0.07936. Accuracy 0.985.
### Flips: 156, rs: 4, checks: 208
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010670413
Train loss (w/o reg) on all data: 0.005482548
Test loss (w/o reg) on all data: 0.017901935
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5884585e-07
Norm of the params: 10.186133
     Influence (LOO): fixed  92 labels. Loss 0.01790. Accuracy 0.992.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008071647
Train loss (w/o reg) on all data: 0.0029261352
Test loss (w/o reg) on all data: 0.014634636
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1909634e-07
Norm of the params: 10.144468
                Loss: fixed  92 labels. Loss 0.01463. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16402651
Train loss (w/o reg) on all data: 0.1570983
Test loss (w/o reg) on all data: 0.079297505
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7125833e-05
Norm of the params: 11.771333
              Random: fixed  20 labels. Loss 0.07930. Accuracy 0.985.
### Flips: 156, rs: 4, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362016
Train loss (w/o reg) on all data: 0.0021730429
Test loss (w/o reg) on all data: 0.01205515
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5703364e-07
Norm of the params: 9.153112
     Influence (LOO): fixed  94 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008071646
Train loss (w/o reg) on all data: 0.002926241
Test loss (w/o reg) on all data: 0.014635275
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.917677e-07
Norm of the params: 10.144363
                Loss: fixed  92 labels. Loss 0.01464. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1633677
Train loss (w/o reg) on all data: 0.15664709
Test loss (w/o reg) on all data: 0.078252465
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0862513e-05
Norm of the params: 11.593639
              Random: fixed  23 labels. Loss 0.07825. Accuracy 0.985.
### Flips: 156, rs: 4, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729958
Test loss (w/o reg) on all data: 0.012053945
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.248137e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  94 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0073426887
Train loss (w/o reg) on all data: 0.0026147992
Test loss (w/o reg) on all data: 0.014110396
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4280178e-07
Norm of the params: 9.724083
                Loss: fixed  93 labels. Loss 0.01411. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16336775
Train loss (w/o reg) on all data: 0.15664566
Test loss (w/o reg) on all data: 0.0782419
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8205494e-05
Norm of the params: 11.594909
              Random: fixed  23 labels. Loss 0.07824. Accuracy 0.985.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17502083
Train loss (w/o reg) on all data: 0.16793288
Test loss (w/o reg) on all data: 0.11174752
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.3226701e-05
Norm of the params: 11.90625
Flipped loss: 0.11175. Accuracy: 0.969
### Flips: 156, rs: 5, checks: 52
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07717785
Train loss (w/o reg) on all data: 0.066628546
Test loss (w/o reg) on all data: 0.080723494
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3380044e-05
Norm of the params: 14.525362
     Influence (LOO): fixed  42 labels. Loss 0.08072. Accuracy 0.977.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035439264
Train loss (w/o reg) on all data: 0.020920187
Test loss (w/o reg) on all data: 0.06132176
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.2119848e-06
Norm of the params: 17.040586
                Loss: fixed  52 labels. Loss 0.06132. Accuracy 0.981.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16848868
Train loss (w/o reg) on all data: 0.16175567
Test loss (w/o reg) on all data: 0.109865725
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.2147617e-05
Norm of the params: 11.604324
              Random: fixed   5 labels. Loss 0.10987. Accuracy 0.969.
### Flips: 156, rs: 5, checks: 104
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019335834
Train loss (w/o reg) on all data: 0.010505835
Test loss (w/o reg) on all data: 0.034184854
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.077286e-07
Norm of the params: 13.289093
     Influence (LOO): fixed  67 labels. Loss 0.03418. Accuracy 0.989.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011245389
Train loss (w/o reg) on all data: 0.0047464855
Test loss (w/o reg) on all data: 0.020617748
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.842564e-06
Norm of the params: 11.400792
                Loss: fixed  71 labels. Loss 0.02062. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16476478
Train loss (w/o reg) on all data: 0.15785724
Test loss (w/o reg) on all data: 0.10232829
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2610217e-05
Norm of the params: 11.753757
              Random: fixed   9 labels. Loss 0.10233. Accuracy 0.966.
### Flips: 156, rs: 5, checks: 156
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077947928
Train loss (w/o reg) on all data: 0.0029181151
Test loss (w/o reg) on all data: 0.016802885
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2757078e-07
Norm of the params: 9.875908
     Influence (LOO): fixed  74 labels. Loss 0.01680. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008880904
Train loss (w/o reg) on all data: 0.0035754011
Test loss (w/o reg) on all data: 0.019304331
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4120141e-06
Norm of the params: 10.300974
                Loss: fixed  73 labels. Loss 0.01930. Accuracy 0.992.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15632665
Train loss (w/o reg) on all data: 0.14948499
Test loss (w/o reg) on all data: 0.09221994
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.8526216e-05
Norm of the params: 11.697577
              Random: fixed  15 labels. Loss 0.09222. Accuracy 0.977.
### Flips: 156, rs: 5, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173015
Test loss (w/o reg) on all data: 0.01205504
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8055864e-07
Norm of the params: 9.153147
     Influence (LOO): fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730135
Test loss (w/o reg) on all data: 0.012055084
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.895272e-08
Norm of the params: 9.153148
                Loss: fixed  75 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15265942
Train loss (w/o reg) on all data: 0.14607213
Test loss (w/o reg) on all data: 0.08031618
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8625214e-06
Norm of the params: 11.478054
              Random: fixed  18 labels. Loss 0.08032. Accuracy 0.989.
### Flips: 156, rs: 5, checks: 260
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172972
Test loss (w/o reg) on all data: 0.0120544685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.657313e-07
Norm of the params: 9.1531925
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729728
Test loss (w/o reg) on all data: 0.012054537
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8903643e-07
Norm of the params: 9.1531925
                Loss: fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15103045
Train loss (w/o reg) on all data: 0.1445719
Test loss (w/o reg) on all data: 0.07913014
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5318128e-05
Norm of the params: 11.365338
              Random: fixed  20 labels. Loss 0.07913. Accuracy 0.989.
### Flips: 156, rs: 5, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729232
Test loss (w/o reg) on all data: 0.012054754
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1636177e-07
Norm of the params: 9.153247
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729218
Test loss (w/o reg) on all data: 0.012054799
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6550977e-07
Norm of the params: 9.153247
                Loss: fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1468895
Train loss (w/o reg) on all data: 0.14003648
Test loss (w/o reg) on all data: 0.07811646
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2499877e-05
Norm of the params: 11.707277
              Random: fixed  22 labels. Loss 0.07812. Accuracy 0.981.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17735904
Train loss (w/o reg) on all data: 0.1690879
Test loss (w/o reg) on all data: 0.094011664
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.681025e-06
Norm of the params: 12.861686
Flipped loss: 0.09401. Accuracy: 0.977
### Flips: 156, rs: 6, checks: 52
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07131478
Train loss (w/o reg) on all data: 0.061320607
Test loss (w/o reg) on all data: 0.051839374
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3696498e-06
Norm of the params: 14.138014
     Influence (LOO): fixed  45 labels. Loss 0.05184. Accuracy 0.985.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049104042
Train loss (w/o reg) on all data: 0.0337
Test loss (w/o reg) on all data: 0.04029441
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0103513e-05
Norm of the params: 17.55223
                Loss: fixed  49 labels. Loss 0.04029. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16845304
Train loss (w/o reg) on all data: 0.15965289
Test loss (w/o reg) on all data: 0.086916
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.1252875e-05
Norm of the params: 13.266612
              Random: fixed   5 labels. Loss 0.08692. Accuracy 0.977.
### Flips: 156, rs: 6, checks: 104
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029055763
Train loss (w/o reg) on all data: 0.020943504
Test loss (w/o reg) on all data: 0.018261582
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.0457892e-06
Norm of the params: 12.73755
     Influence (LOO): fixed  68 labels. Loss 0.01826. Accuracy 0.996.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012410595
Train loss (w/o reg) on all data: 0.005389752
Test loss (w/o reg) on all data: 0.015028506
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.0352043e-07
Norm of the params: 11.849763
                Loss: fixed  75 labels. Loss 0.01503. Accuracy 0.996.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15962534
Train loss (w/o reg) on all data: 0.15109007
Test loss (w/o reg) on all data: 0.0893399
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.5515918e-05
Norm of the params: 13.065424
              Random: fixed  10 labels. Loss 0.08934. Accuracy 0.969.
### Flips: 156, rs: 6, checks: 156
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012560533
Train loss (w/o reg) on all data: 0.007053141
Test loss (w/o reg) on all data: 0.011939383
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.295786e-07
Norm of the params: 10.495134
     Influence (LOO): fixed  76 labels. Loss 0.01194. Accuracy 0.992.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007519581
Train loss (w/o reg) on all data: 0.0027001635
Test loss (w/o reg) on all data: 0.013839076
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9377914e-07
Norm of the params: 9.817758
                Loss: fixed  78 labels. Loss 0.01384. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14736125
Train loss (w/o reg) on all data: 0.1387559
Test loss (w/o reg) on all data: 0.084759764
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.2530645e-05
Norm of the params: 13.11895
              Random: fixed  16 labels. Loss 0.08476. Accuracy 0.969.
### Flips: 156, rs: 6, checks: 208
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009210212
Train loss (w/o reg) on all data: 0.004451315
Test loss (w/o reg) on all data: 0.0133584915
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.744028e-07
Norm of the params: 9.7559185
     Influence (LOO): fixed  78 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007519583
Train loss (w/o reg) on all data: 0.0027000625
Test loss (w/o reg) on all data: 0.013839014
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0357201e-06
Norm of the params: 9.8178625
                Loss: fixed  78 labels. Loss 0.01384. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14130305
Train loss (w/o reg) on all data: 0.1326597
Test loss (w/o reg) on all data: 0.07871921
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.641388e-06
Norm of the params: 13.147885
              Random: fixed  20 labels. Loss 0.07872. Accuracy 0.981.
### Flips: 156, rs: 6, checks: 260
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009210212
Train loss (w/o reg) on all data: 0.0044515524
Test loss (w/o reg) on all data: 0.013357863
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.3903783e-07
Norm of the params: 9.755675
     Influence (LOO): fixed  78 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075195823
Train loss (w/o reg) on all data: 0.0027002068
Test loss (w/o reg) on all data: 0.013838062
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.631271e-07
Norm of the params: 9.817714
                Loss: fixed  78 labels. Loss 0.01384. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13887534
Train loss (w/o reg) on all data: 0.13168451
Test loss (w/o reg) on all data: 0.079578385
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8506434e-05
Norm of the params: 11.992348
              Random: fixed  24 labels. Loss 0.07958. Accuracy 0.981.
### Flips: 156, rs: 6, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172885
Test loss (w/o reg) on all data: 0.012055181
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0282849e-06
Norm of the params: 9.153287
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728866
Test loss (w/o reg) on all data: 0.0120554
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3329456e-07
Norm of the params: 9.153285
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1305082
Train loss (w/o reg) on all data: 0.12371477
Test loss (w/o reg) on all data: 0.081020445
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7626207e-05
Norm of the params: 11.656275
              Random: fixed  28 labels. Loss 0.08102. Accuracy 0.981.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17605974
Train loss (w/o reg) on all data: 0.16957758
Test loss (w/o reg) on all data: 0.08964144
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.112447e-05
Norm of the params: 11.386096
Flipped loss: 0.08964. Accuracy: 0.985
### Flips: 156, rs: 7, checks: 52
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06980087
Train loss (w/o reg) on all data: 0.0566659
Test loss (w/o reg) on all data: 0.05680471
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3562078e-05
Norm of the params: 16.208006
     Influence (LOO): fixed  44 labels. Loss 0.05680. Accuracy 0.973.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045873016
Train loss (w/o reg) on all data: 0.031165844
Test loss (w/o reg) on all data: 0.04632073
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.371484e-06
Norm of the params: 17.150612
                Loss: fixed  49 labels. Loss 0.04632. Accuracy 0.985.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1708845
Train loss (w/o reg) on all data: 0.16323371
Test loss (w/o reg) on all data: 0.090823255
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.387851e-05
Norm of the params: 12.369964
              Random: fixed   1 labels. Loss 0.09082. Accuracy 0.981.
### Flips: 156, rs: 7, checks: 104
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024846546
Train loss (w/o reg) on all data: 0.016502839
Test loss (w/o reg) on all data: 0.03529966
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.457029e-06
Norm of the params: 12.917978
     Influence (LOO): fixed  67 labels. Loss 0.03530. Accuracy 0.989.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0116726635
Train loss (w/o reg) on all data: 0.0051389183
Test loss (w/o reg) on all data: 0.016457891
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.5995478e-07
Norm of the params: 11.431313
                Loss: fixed  74 labels. Loss 0.01646. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16774122
Train loss (w/o reg) on all data: 0.15990418
Test loss (w/o reg) on all data: 0.090339325
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.046579e-06
Norm of the params: 12.51962
              Random: fixed   3 labels. Loss 0.09034. Accuracy 0.981.
### Flips: 156, rs: 7, checks: 156
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013381211
Train loss (w/o reg) on all data: 0.0074023525
Test loss (w/o reg) on all data: 0.018023083
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.741948e-07
Norm of the params: 10.935134
     Influence (LOO): fixed  75 labels. Loss 0.01802. Accuracy 0.992.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008765902
Train loss (w/o reg) on all data: 0.0034101533
Test loss (w/o reg) on all data: 0.012942602
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.695652e-07
Norm of the params: 10.349637
                Loss: fixed  76 labels. Loss 0.01294. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15513508
Train loss (w/o reg) on all data: 0.14833575
Test loss (w/o reg) on all data: 0.0874771
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.2746917e-05
Norm of the params: 11.661328
              Random: fixed  11 labels. Loss 0.08748. Accuracy 0.977.
### Flips: 156, rs: 7, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00945244
Train loss (w/o reg) on all data: 0.0039494066
Test loss (w/o reg) on all data: 0.016863465
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.69384e-07
Norm of the params: 10.49098
     Influence (LOO): fixed  76 labels. Loss 0.01686. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076518687
Train loss (w/o reg) on all data: 0.0027776868
Test loss (w/o reg) on all data: 0.014519419
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8422654e-07
Norm of the params: 9.873381
                Loss: fixed  77 labels. Loss 0.01452. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.153053
Train loss (w/o reg) on all data: 0.14617012
Test loss (w/o reg) on all data: 0.087586805
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2402326e-05
Norm of the params: 11.732759
              Random: fixed  12 labels. Loss 0.08759. Accuracy 0.977.
### Flips: 156, rs: 7, checks: 260
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729928
Test loss (w/o reg) on all data: 0.012054807
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0679616e-07
Norm of the params: 9.153168
     Influence (LOO): fixed  78 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172993
Test loss (w/o reg) on all data: 0.012054784
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4772e-08
Norm of the params: 9.153169
                Loss: fixed  78 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14365394
Train loss (w/o reg) on all data: 0.13632277
Test loss (w/o reg) on all data: 0.085485406
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.97495e-06
Norm of the params: 12.108824
              Random: fixed  16 labels. Loss 0.08549. Accuracy 0.981.
### Flips: 156, rs: 7, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729975
Test loss (w/o reg) on all data: 0.01205444
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.419039e-07
Norm of the params: 9.153165
     Influence (LOO): fixed  78 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729972
Test loss (w/o reg) on all data: 0.012054502
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9076938e-07
Norm of the params: 9.153165
                Loss: fixed  78 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13865075
Train loss (w/o reg) on all data: 0.1313377
Test loss (w/o reg) on all data: 0.08115039
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.5033152e-05
Norm of the params: 12.093842
              Random: fixed  19 labels. Loss 0.08115. Accuracy 0.981.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17783347
Train loss (w/o reg) on all data: 0.16944326
Test loss (w/o reg) on all data: 0.109014034
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.990565e-05
Norm of the params: 12.953925
Flipped loss: 0.10901. Accuracy: 0.973
### Flips: 156, rs: 8, checks: 52
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06909807
Train loss (w/o reg) on all data: 0.058099516
Test loss (w/o reg) on all data: 0.059131134
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0279851e-05
Norm of the params: 14.831422
     Influence (LOO): fixed  44 labels. Loss 0.05913. Accuracy 0.981.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05109044
Train loss (w/o reg) on all data: 0.036958154
Test loss (w/o reg) on all data: 0.058652923
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.3316405e-06
Norm of the params: 16.81207
                Loss: fixed  49 labels. Loss 0.05865. Accuracy 0.977.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17196581
Train loss (w/o reg) on all data: 0.16346554
Test loss (w/o reg) on all data: 0.103311986
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6981936e-05
Norm of the params: 13.0386095
              Random: fixed   4 labels. Loss 0.10331. Accuracy 0.977.
### Flips: 156, rs: 8, checks: 104
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024677724
Train loss (w/o reg) on all data: 0.01562065
Test loss (w/o reg) on all data: 0.028752355
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2220477e-06
Norm of the params: 13.45888
     Influence (LOO): fixed  66 labels. Loss 0.02875. Accuracy 0.985.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012121554
Train loss (w/o reg) on all data: 0.005607749
Test loss (w/o reg) on all data: 0.012326879
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.306059e-06
Norm of the params: 11.413856
                Loss: fixed  73 labels. Loss 0.01233. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1719658
Train loss (w/o reg) on all data: 0.16346952
Test loss (w/o reg) on all data: 0.103288755
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.5762023e-05
Norm of the params: 13.03554
              Random: fixed   4 labels. Loss 0.10329. Accuracy 0.977.
### Flips: 156, rs: 8, checks: 156
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011023849
Train loss (w/o reg) on all data: 0.0052047335
Test loss (w/o reg) on all data: 0.022364577
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.096214e-07
Norm of the params: 10.788064
     Influence (LOO): fixed  74 labels. Loss 0.02236. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620154
Train loss (w/o reg) on all data: 0.0021728547
Test loss (w/o reg) on all data: 0.012055527
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.517718e-07
Norm of the params: 9.153317
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16700915
Train loss (w/o reg) on all data: 0.15812585
Test loss (w/o reg) on all data: 0.10473266
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.8907234e-05
Norm of the params: 13.3291445
              Random: fixed   6 labels. Loss 0.10473. Accuracy 0.966.
### Flips: 156, rs: 8, checks: 208
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0085133985
Train loss (w/o reg) on all data: 0.0037311462
Test loss (w/o reg) on all data: 0.017686853
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5453496e-07
Norm of the params: 9.779829
     Influence (LOO): fixed  76 labels. Loss 0.01769. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729798
Test loss (w/o reg) on all data: 0.012054431
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1917949e-07
Norm of the params: 9.153184
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15813869
Train loss (w/o reg) on all data: 0.14912443
Test loss (w/o reg) on all data: 0.1004199
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.5778892e-05
Norm of the params: 13.427039
              Random: fixed  13 labels. Loss 0.10042. Accuracy 0.969.
### Flips: 156, rs: 8, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008513399
Train loss (w/o reg) on all data: 0.0037310899
Test loss (w/o reg) on all data: 0.017686477
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8639664e-07
Norm of the params: 9.779887
     Influence (LOO): fixed  76 labels. Loss 0.01769. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729076
Test loss (w/o reg) on all data: 0.0120547125
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.222783e-07
Norm of the params: 9.153265
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14415066
Train loss (w/o reg) on all data: 0.13534397
Test loss (w/o reg) on all data: 0.08705469
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9316109e-05
Norm of the params: 13.271543
              Random: fixed  20 labels. Loss 0.08705. Accuracy 0.985.
### Flips: 156, rs: 8, checks: 312
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172986
Test loss (w/o reg) on all data: 0.012054801
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4959743e-07
Norm of the params: 9.153176
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729846
Test loss (w/o reg) on all data: 0.012054779
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9834317e-07
Norm of the params: 9.153177
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1306635
Train loss (w/o reg) on all data: 0.12152851
Test loss (w/o reg) on all data: 0.08417156
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.932902e-06
Norm of the params: 13.516657
              Random: fixed  25 labels. Loss 0.08417. Accuracy 0.973.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18412739
Train loss (w/o reg) on all data: 0.17662773
Test loss (w/o reg) on all data: 0.11424583
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.477228e-05
Norm of the params: 12.247182
Flipped loss: 0.11425. Accuracy: 0.962
### Flips: 156, rs: 9, checks: 52
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0767217
Train loss (w/o reg) on all data: 0.066172436
Test loss (w/o reg) on all data: 0.0459294
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.6174843e-06
Norm of the params: 14.525332
     Influence (LOO): fixed  46 labels. Loss 0.04593. Accuracy 0.981.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05196949
Train loss (w/o reg) on all data: 0.03657995
Test loss (w/o reg) on all data: 0.07424177
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7531147e-06
Norm of the params: 17.543968
                Loss: fixed  52 labels. Loss 0.07424. Accuracy 0.981.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17311081
Train loss (w/o reg) on all data: 0.16566752
Test loss (w/o reg) on all data: 0.10186678
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.0793e-05
Norm of the params: 12.201055
              Random: fixed   7 labels. Loss 0.10187. Accuracy 0.969.
### Flips: 156, rs: 9, checks: 104
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03616965
Train loss (w/o reg) on all data: 0.028627517
Test loss (w/o reg) on all data: 0.025563892
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6298769e-06
Norm of the params: 12.281801
     Influence (LOO): fixed  70 labels. Loss 0.02556. Accuracy 0.989.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01587696
Train loss (w/o reg) on all data: 0.0073311715
Test loss (w/o reg) on all data: 0.015313627
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.8579594e-07
Norm of the params: 13.073475
                Loss: fixed  78 labels. Loss 0.01531. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17242345
Train loss (w/o reg) on all data: 0.16491425
Test loss (w/o reg) on all data: 0.102492794
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.0588892e-05
Norm of the params: 12.254957
              Random: fixed   8 labels. Loss 0.10249. Accuracy 0.969.
### Flips: 156, rs: 9, checks: 156
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008433315
Train loss (w/o reg) on all data: 0.0033560432
Test loss (w/o reg) on all data: 0.015631683
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8691163e-07
Norm of the params: 10.076976
     Influence (LOO): fixed  82 labels. Loss 0.01563. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010877894
Train loss (w/o reg) on all data: 0.0045626187
Test loss (w/o reg) on all data: 0.009983753
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.4524975e-07
Norm of the params: 11.238573
                Loss: fixed  81 labels. Loss 0.00998. Accuracy 0.996.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16841131
Train loss (w/o reg) on all data: 0.16071013
Test loss (w/o reg) on all data: 0.10153984
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.7166098e-05
Norm of the params: 12.41063
              Random: fixed  11 labels. Loss 0.10154. Accuracy 0.969.
### Flips: 156, rs: 9, checks: 208
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008013477
Train loss (w/o reg) on all data: 0.0032719793
Test loss (w/o reg) on all data: 0.015295016
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.893752e-07
Norm of the params: 9.738067
     Influence (LOO): fixed  84 labels. Loss 0.01530. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008306536
Train loss (w/o reg) on all data: 0.0030123426
Test loss (w/o reg) on all data: 0.009268969
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.5822888e-07
Norm of the params: 10.2899885
                Loss: fixed  83 labels. Loss 0.00927. Accuracy 0.996.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16015588
Train loss (w/o reg) on all data: 0.15261945
Test loss (w/o reg) on all data: 0.093866274
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0289775e-05
Norm of the params: 12.277153
              Random: fixed  16 labels. Loss 0.09387. Accuracy 0.969.
### Flips: 156, rs: 9, checks: 260
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729283
Test loss (w/o reg) on all data: 0.012055183
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2918971e-07
Norm of the params: 9.153239
     Influence (LOO): fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007376436
Train loss (w/o reg) on all data: 0.002622274
Test loss (w/o reg) on all data: 0.013233487
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6359559e-07
Norm of the params: 9.751063
                Loss: fixed  84 labels. Loss 0.01323. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15284026
Train loss (w/o reg) on all data: 0.14520322
Test loss (w/o reg) on all data: 0.09460592
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.9348303e-05
Norm of the params: 12.358836
              Random: fixed  19 labels. Loss 0.09461. Accuracy 0.962.
### Flips: 156, rs: 9, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620214
Train loss (w/o reg) on all data: 0.0021730266
Test loss (w/o reg) on all data: 0.012054637
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0815298e-07
Norm of the params: 9.153135
     Influence (LOO): fixed  85 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007376437
Train loss (w/o reg) on all data: 0.0026223015
Test loss (w/o reg) on all data: 0.013233224
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4971701e-07
Norm of the params: 9.751037
                Loss: fixed  84 labels. Loss 0.01323. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14881837
Train loss (w/o reg) on all data: 0.1417061
Test loss (w/o reg) on all data: 0.08183304
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.845531e-05
Norm of the params: 11.926672
              Random: fixed  25 labels. Loss 0.08183. Accuracy 0.977.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1892376
Train loss (w/o reg) on all data: 0.18036982
Test loss (w/o reg) on all data: 0.10845314
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.3332434e-05
Norm of the params: 13.317484
Flipped loss: 0.10845. Accuracy: 0.973
### Flips: 156, rs: 10, checks: 52
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08872151
Train loss (w/o reg) on all data: 0.07618392
Test loss (w/o reg) on all data: 0.06407872
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9300658e-05
Norm of the params: 15.835141
     Influence (LOO): fixed  45 labels. Loss 0.06408. Accuracy 0.985.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055103783
Train loss (w/o reg) on all data: 0.03903636
Test loss (w/o reg) on all data: 0.06684023
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8109597e-05
Norm of the params: 17.926195
                Loss: fixed  52 labels. Loss 0.06684. Accuracy 0.981.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18358782
Train loss (w/o reg) on all data: 0.17529407
Test loss (w/o reg) on all data: 0.10462866
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.7969836e-05
Norm of the params: 12.879251
              Random: fixed   5 labels. Loss 0.10463. Accuracy 0.969.
### Flips: 156, rs: 10, checks: 104
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038278382
Train loss (w/o reg) on all data: 0.026496941
Test loss (w/o reg) on all data: 0.058597293
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.001545e-06
Norm of the params: 15.350206
     Influence (LOO): fixed  67 labels. Loss 0.05860. Accuracy 0.981.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015083965
Train loss (w/o reg) on all data: 0.006964592
Test loss (w/o reg) on all data: 0.0463613
Train acc on all data:  1.0
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.0376624e-07
Norm of the params: 12.7431345
                Loss: fixed  75 labels. Loss 0.04636. Accuracy 0.977.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18202738
Train loss (w/o reg) on all data: 0.17386808
Test loss (w/o reg) on all data: 0.10332839
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0370116e-05
Norm of the params: 12.774434
              Random: fixed   6 labels. Loss 0.10333. Accuracy 0.969.
### Flips: 156, rs: 10, checks: 156
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018230692
Train loss (w/o reg) on all data: 0.011411474
Test loss (w/o reg) on all data: 0.032689214
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.779017e-06
Norm of the params: 11.678371
     Influence (LOO): fixed  79 labels. Loss 0.03269. Accuracy 0.985.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014630721
Train loss (w/o reg) on all data: 0.007084005
Test loss (w/o reg) on all data: 0.040957328
Train acc on all data:  1.0
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.282271e-07
Norm of the params: 12.285533
                Loss: fixed  78 labels. Loss 0.04096. Accuracy 0.977.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17200379
Train loss (w/o reg) on all data: 0.16375867
Test loss (w/o reg) on all data: 0.096914776
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.262358e-06
Norm of the params: 12.841433
              Random: fixed  11 labels. Loss 0.09691. Accuracy 0.966.
### Flips: 156, rs: 10, checks: 208
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011487806
Train loss (w/o reg) on all data: 0.0060562883
Test loss (w/o reg) on all data: 0.017004197
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1396928e-06
Norm of the params: 10.42259
     Influence (LOO): fixed  83 labels. Loss 0.01700. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011287864
Train loss (w/o reg) on all data: 0.005143711
Test loss (w/o reg) on all data: 0.02750099
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.4584337e-07
Norm of the params: 11.085263
                Loss: fixed  82 labels. Loss 0.02750. Accuracy 0.989.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15971185
Train loss (w/o reg) on all data: 0.15211807
Test loss (w/o reg) on all data: 0.081295066
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.337737e-05
Norm of the params: 12.323789
              Random: fixed  21 labels. Loss 0.08130. Accuracy 0.973.
### Flips: 156, rs: 10, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008606703
Train loss (w/o reg) on all data: 0.0034947835
Test loss (w/o reg) on all data: 0.016748246
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9219665e-07
Norm of the params: 10.1112995
     Influence (LOO): fixed  84 labels. Loss 0.01675. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007071388
Train loss (w/o reg) on all data: 0.0025224157
Test loss (w/o reg) on all data: 0.015385304
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.5459195e-07
Norm of the params: 9.538315
                Loss: fixed  85 labels. Loss 0.01539. Accuracy 0.989.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14777827
Train loss (w/o reg) on all data: 0.1401989
Test loss (w/o reg) on all data: 0.07047801
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5355334e-05
Norm of the params: 12.312087
              Random: fixed  28 labels. Loss 0.07048. Accuracy 0.977.
### Flips: 156, rs: 10, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008606704
Train loss (w/o reg) on all data: 0.0034948601
Test loss (w/o reg) on all data: 0.016747674
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.7379026e-07
Norm of the params: 10.111225
     Influence (LOO): fixed  84 labels. Loss 0.01675. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007071388
Train loss (w/o reg) on all data: 0.0025224139
Test loss (w/o reg) on all data: 0.015385065
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.058767e-07
Norm of the params: 9.538316
                Loss: fixed  85 labels. Loss 0.01539. Accuracy 0.989.
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14525414
Train loss (w/o reg) on all data: 0.13779217
Test loss (w/o reg) on all data: 0.067100905
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.678775e-05
Norm of the params: 12.216359
              Random: fixed  30 labels. Loss 0.06710. Accuracy 0.985.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19197993
Train loss (w/o reg) on all data: 0.1830539
Test loss (w/o reg) on all data: 0.10946692
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.020569e-06
Norm of the params: 13.361159
Flipped loss: 0.10947. Accuracy: 0.977
### Flips: 156, rs: 11, checks: 52
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095856145
Train loss (w/o reg) on all data: 0.08195688
Test loss (w/o reg) on all data: 0.05144994
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.917093e-06
Norm of the params: 16.672892
     Influence (LOO): fixed  43 labels. Loss 0.05145. Accuracy 0.989.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060956843
Train loss (w/o reg) on all data: 0.042878225
Test loss (w/o reg) on all data: 0.07113609
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.9921217e-06
Norm of the params: 19.015057
                Loss: fixed  51 labels. Loss 0.07114. Accuracy 0.969.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18419331
Train loss (w/o reg) on all data: 0.17507032
Test loss (w/o reg) on all data: 0.10419529
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.807353e-05
Norm of the params: 13.507777
              Random: fixed   4 labels. Loss 0.10420. Accuracy 0.969.
### Flips: 156, rs: 11, checks: 104
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04402493
Train loss (w/o reg) on all data: 0.033238064
Test loss (w/o reg) on all data: 0.03276559
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8223815e-06
Norm of the params: 14.687999
     Influence (LOO): fixed  70 labels. Loss 0.03277. Accuracy 0.985.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020564638
Train loss (w/o reg) on all data: 0.010093682
Test loss (w/o reg) on all data: 0.011222299
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.739674e-07
Norm of the params: 14.471321
                Loss: fixed  77 labels. Loss 0.01122. Accuracy 0.996.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18407634
Train loss (w/o reg) on all data: 0.17508706
Test loss (w/o reg) on all data: 0.10304833
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.809878e-05
Norm of the params: 13.408408
              Random: fixed   5 labels. Loss 0.10305. Accuracy 0.973.
### Flips: 156, rs: 11, checks: 156
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012874333
Train loss (w/o reg) on all data: 0.0065732715
Test loss (w/o reg) on all data: 0.014068209
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.7142098e-07
Norm of the params: 11.225918
     Influence (LOO): fixed  84 labels. Loss 0.01407. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010362335
Train loss (w/o reg) on all data: 0.004618248
Test loss (w/o reg) on all data: 0.010558691
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2788682e-06
Norm of the params: 10.71829
                Loss: fixed  84 labels. Loss 0.01056. Accuracy 0.996.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17918289
Train loss (w/o reg) on all data: 0.17035979
Test loss (w/o reg) on all data: 0.09941009
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.0074183e-05
Norm of the params: 13.283899
              Random: fixed  10 labels. Loss 0.09941. Accuracy 0.981.
### Flips: 156, rs: 11, checks: 208
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009018102
Train loss (w/o reg) on all data: 0.0037671453
Test loss (w/o reg) on all data: 0.010902682
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6963283e-07
Norm of the params: 10.247885
     Influence (LOO): fixed  85 labels. Loss 0.01090. Accuracy 0.996.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009018102
Train loss (w/o reg) on all data: 0.0037671325
Test loss (w/o reg) on all data: 0.0109025845
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9999673e-07
Norm of the params: 10.247897
                Loss: fixed  85 labels. Loss 0.01090. Accuracy 0.996.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17370619
Train loss (w/o reg) on all data: 0.16490224
Test loss (w/o reg) on all data: 0.09354159
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.856794e-06
Norm of the params: 13.269474
              Random: fixed  13 labels. Loss 0.09354. Accuracy 0.973.
### Flips: 156, rs: 11, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002173061
Test loss (w/o reg) on all data: 0.012055205
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7521198e-07
Norm of the params: 9.153094
     Influence (LOO): fixed  87 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009018101
Train loss (w/o reg) on all data: 0.0037671067
Test loss (w/o reg) on all data: 0.0109026255
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.471424e-07
Norm of the params: 10.247921
                Loss: fixed  85 labels. Loss 0.01090. Accuracy 0.996.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16963892
Train loss (w/o reg) on all data: 0.1605638
Test loss (w/o reg) on all data: 0.0915842
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.539474e-06
Norm of the params: 13.472284
              Random: fixed  15 labels. Loss 0.09158. Accuracy 0.973.
### Flips: 156, rs: 11, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729579
Test loss (w/o reg) on all data: 0.012055798
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.772752e-07
Norm of the params: 9.153207
     Influence (LOO): fixed  87 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008522692
Train loss (w/o reg) on all data: 0.003430873
Test loss (w/o reg) on all data: 0.010343677
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.2973308e-07
Norm of the params: 10.091401
                Loss: fixed  86 labels. Loss 0.01034. Accuracy 0.996.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16061166
Train loss (w/o reg) on all data: 0.15169187
Test loss (w/o reg) on all data: 0.089118704
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5810043e-05
Norm of the params: 13.356491
              Random: fixed  20 labels. Loss 0.08912. Accuracy 0.981.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1806358
Train loss (w/o reg) on all data: 0.17202848
Test loss (w/o reg) on all data: 0.09658927
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.506969e-05
Norm of the params: 13.120455
Flipped loss: 0.09659. Accuracy: 0.973
### Flips: 156, rs: 12, checks: 52
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084185444
Train loss (w/o reg) on all data: 0.073278904
Test loss (w/o reg) on all data: 0.03810344
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5249251e-05
Norm of the params: 14.769252
     Influence (LOO): fixed  42 labels. Loss 0.03810. Accuracy 0.989.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0484242
Train loss (w/o reg) on all data: 0.031899046
Test loss (w/o reg) on all data: 0.034358673
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0000377e-05
Norm of the params: 18.179743
                Loss: fixed  52 labels. Loss 0.03436. Accuracy 0.989.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17700349
Train loss (w/o reg) on all data: 0.16855745
Test loss (w/o reg) on all data: 0.08867242
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2923544e-05
Norm of the params: 12.996955
              Random: fixed   2 labels. Loss 0.08867. Accuracy 0.977.
### Flips: 156, rs: 12, checks: 104
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0299304
Train loss (w/o reg) on all data: 0.020083804
Test loss (w/o reg) on all data: 0.034703
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.5692806e-06
Norm of the params: 14.033244
     Influence (LOO): fixed  68 labels. Loss 0.03470. Accuracy 0.989.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014170764
Train loss (w/o reg) on all data: 0.0068629766
Test loss (w/o reg) on all data: 0.021305984
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.727933e-07
Norm of the params: 12.089488
                Loss: fixed  76 labels. Loss 0.02131. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17055373
Train loss (w/o reg) on all data: 0.16240029
Test loss (w/o reg) on all data: 0.081790306
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.1422056e-05
Norm of the params: 12.76984
              Random: fixed   6 labels. Loss 0.08179. Accuracy 0.981.
### Flips: 156, rs: 12, checks: 156
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010842272
Train loss (w/o reg) on all data: 0.0056589586
Test loss (w/o reg) on all data: 0.018641988
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5196244e-07
Norm of the params: 10.1816635
     Influence (LOO): fixed  79 labels. Loss 0.01864. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00795028
Train loss (w/o reg) on all data: 0.0029495028
Test loss (w/o reg) on all data: 0.014970602
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7712348e-07
Norm of the params: 10.000776
                Loss: fixed  80 labels. Loss 0.01497. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16265777
Train loss (w/o reg) on all data: 0.15371531
Test loss (w/o reg) on all data: 0.08042701
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.875948e-05
Norm of the params: 13.373447
              Random: fixed  10 labels. Loss 0.08043. Accuracy 0.981.
### Flips: 156, rs: 12, checks: 208
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009210212
Train loss (w/o reg) on all data: 0.004451289
Test loss (w/o reg) on all data: 0.013358078
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0018651e-06
Norm of the params: 9.755944
     Influence (LOO): fixed  80 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730005
Test loss (w/o reg) on all data: 0.01205529
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.15479594e-07
Norm of the params: 9.153162
                Loss: fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15608487
Train loss (w/o reg) on all data: 0.1475076
Test loss (w/o reg) on all data: 0.072875135
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.273535e-05
Norm of the params: 13.097532
              Random: fixed  15 labels. Loss 0.07288. Accuracy 0.985.
### Flips: 156, rs: 12, checks: 260
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00921021
Train loss (w/o reg) on all data: 0.0044512125
Test loss (w/o reg) on all data: 0.013358508
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2359247e-07
Norm of the params: 9.7560215
     Influence (LOO): fixed  80 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730098
Test loss (w/o reg) on all data: 0.012055309
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.8960185e-08
Norm of the params: 9.1531515
                Loss: fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14759007
Train loss (w/o reg) on all data: 0.13899428
Test loss (w/o reg) on all data: 0.06746126
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0535303e-05
Norm of the params: 13.111675
              Random: fixed  20 labels. Loss 0.06746. Accuracy 0.985.
### Flips: 156, rs: 12, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728675
Test loss (w/o reg) on all data: 0.012055155
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5550605e-07
Norm of the params: 9.153307
     Influence (LOO): fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728678
Test loss (w/o reg) on all data: 0.0120552275
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2752238e-07
Norm of the params: 9.153306
                Loss: fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14106894
Train loss (w/o reg) on all data: 0.13291621
Test loss (w/o reg) on all data: 0.068743296
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.1638137e-06
Norm of the params: 12.7692795
              Random: fixed  24 labels. Loss 0.06874. Accuracy 0.985.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19501513
Train loss (w/o reg) on all data: 0.18875961
Test loss (w/o reg) on all data: 0.103538945
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.6833294e-05
Norm of the params: 11.185272
Flipped loss: 0.10354. Accuracy: 0.981
### Flips: 156, rs: 13, checks: 52
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08611315
Train loss (w/o reg) on all data: 0.0763408
Test loss (w/o reg) on all data: 0.06592142
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.855718e-05
Norm of the params: 13.980232
     Influence (LOO): fixed  43 labels. Loss 0.06592. Accuracy 0.977.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055584915
Train loss (w/o reg) on all data: 0.041319124
Test loss (w/o reg) on all data: 0.057202794
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6819525e-06
Norm of the params: 16.891293
                Loss: fixed  52 labels. Loss 0.05720. Accuracy 0.985.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18620159
Train loss (w/o reg) on all data: 0.17914617
Test loss (w/o reg) on all data: 0.10221859
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.067278e-05
Norm of the params: 11.878901
              Random: fixed   4 labels. Loss 0.10222. Accuracy 0.981.
### Flips: 156, rs: 13, checks: 104
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03955746
Train loss (w/o reg) on all data: 0.03168702
Test loss (w/o reg) on all data: 0.019174108
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.4789326e-06
Norm of the params: 12.546265
     Influence (LOO): fixed  68 labels. Loss 0.01917. Accuracy 0.996.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01325373
Train loss (w/o reg) on all data: 0.0064271893
Test loss (w/o reg) on all data: 0.022198347
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.243573e-07
Norm of the params: 11.684639
                Loss: fixed  81 labels. Loss 0.02220. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18092963
Train loss (w/o reg) on all data: 0.17392637
Test loss (w/o reg) on all data: 0.0958937
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.7198788e-05
Norm of the params: 11.83491
              Random: fixed   8 labels. Loss 0.09589. Accuracy 0.985.
### Flips: 156, rs: 13, checks: 156
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013920279
Train loss (w/o reg) on all data: 0.0076191034
Test loss (w/o reg) on all data: 0.012087283
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1220719e-06
Norm of the params: 11.22602
     Influence (LOO): fixed  81 labels. Loss 0.01209. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008296253
Train loss (w/o reg) on all data: 0.0031558354
Test loss (w/o reg) on all data: 0.014560728
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1964302e-06
Norm of the params: 10.139446
                Loss: fixed  84 labels. Loss 0.01456. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1721578
Train loss (w/o reg) on all data: 0.16521756
Test loss (w/o reg) on all data: 0.08955224
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.978451e-05
Norm of the params: 11.781531
              Random: fixed  14 labels. Loss 0.08955. Accuracy 0.977.
### Flips: 156, rs: 13, checks: 208
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730135
Test loss (w/o reg) on all data: 0.01205424
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.368532e-07
Norm of the params: 9.153148
     Influence (LOO): fixed  86 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007406675
Train loss (w/o reg) on all data: 0.00265809
Test loss (w/o reg) on all data: 0.013139119
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8290071e-07
Norm of the params: 9.745342
                Loss: fixed  85 labels. Loss 0.01314. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15617785
Train loss (w/o reg) on all data: 0.14881511
Test loss (w/o reg) on all data: 0.091143265
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.868846e-05
Norm of the params: 12.134862
              Random: fixed  20 labels. Loss 0.09114. Accuracy 0.977.
### Flips: 156, rs: 13, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730955
Test loss (w/o reg) on all data: 0.012054701
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.412789e-07
Norm of the params: 9.153058
     Influence (LOO): fixed  86 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074066734
Train loss (w/o reg) on all data: 0.002658097
Test loss (w/o reg) on all data: 0.013138721
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0963499e-07
Norm of the params: 9.745334
                Loss: fixed  85 labels. Loss 0.01314. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1464019
Train loss (w/o reg) on all data: 0.1395359
Test loss (w/o reg) on all data: 0.088651806
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7955414e-05
Norm of the params: 11.718355
              Random: fixed  24 labels. Loss 0.08865. Accuracy 0.981.
### Flips: 156, rs: 13, checks: 312
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730198
Test loss (w/o reg) on all data: 0.012054333
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4547535e-07
Norm of the params: 9.153142
     Influence (LOO): fixed  86 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730165
Test loss (w/o reg) on all data: 0.012054393
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.838903e-07
Norm of the params: 9.153144
                Loss: fixed  86 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13559105
Train loss (w/o reg) on all data: 0.12829639
Test loss (w/o reg) on all data: 0.080762066
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0173642e-05
Norm of the params: 12.078621
              Random: fixed  30 labels. Loss 0.08076. Accuracy 0.977.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18596072
Train loss (w/o reg) on all data: 0.17828254
Test loss (w/o reg) on all data: 0.105167694
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.787495e-05
Norm of the params: 12.3920765
Flipped loss: 0.10517. Accuracy: 0.969
### Flips: 156, rs: 14, checks: 52
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0835761
Train loss (w/o reg) on all data: 0.073758714
Test loss (w/o reg) on all data: 0.042801887
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7335238e-05
Norm of the params: 14.012411
     Influence (LOO): fixed  45 labels. Loss 0.04280. Accuracy 0.989.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048545774
Train loss (w/o reg) on all data: 0.034067515
Test loss (w/o reg) on all data: 0.046700954
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.993717e-06
Norm of the params: 17.016615
                Loss: fixed  51 labels. Loss 0.04670. Accuracy 0.981.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17711507
Train loss (w/o reg) on all data: 0.16945705
Test loss (w/o reg) on all data: 0.094663344
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.339276e-05
Norm of the params: 12.375793
              Random: fixed   5 labels. Loss 0.09466. Accuracy 0.981.
### Flips: 156, rs: 14, checks: 104
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01829308
Train loss (w/o reg) on all data: 0.010867249
Test loss (w/o reg) on all data: 0.0217202
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.273763e-06
Norm of the params: 12.186739
     Influence (LOO): fixed  71 labels. Loss 0.02172. Accuracy 0.989.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009502067
Train loss (w/o reg) on all data: 0.003939315
Test loss (w/o reg) on all data: 0.014908621
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4774055e-06
Norm of the params: 10.54775
                Loss: fixed  75 labels. Loss 0.01491. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17485635
Train loss (w/o reg) on all data: 0.16724367
Test loss (w/o reg) on all data: 0.09133969
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.4223184e-05
Norm of the params: 12.339107
              Random: fixed   6 labels. Loss 0.09134. Accuracy 0.985.
### Flips: 156, rs: 14, checks: 156
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011735776
Train loss (w/o reg) on all data: 0.005984882
Test loss (w/o reg) on all data: 0.018466681
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.4330635e-07
Norm of the params: 10.72464
     Influence (LOO): fixed  74 labels. Loss 0.01847. Accuracy 0.989.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729805
Test loss (w/o reg) on all data: 0.0120546855
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.416826e-07
Norm of the params: 9.153184
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16758929
Train loss (w/o reg) on all data: 0.15973888
Test loss (w/o reg) on all data: 0.09027958
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.423179e-06
Norm of the params: 12.530291
              Random: fixed  10 labels. Loss 0.09028. Accuracy 0.985.
### Flips: 156, rs: 14, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729858
Test loss (w/o reg) on all data: 0.012054982
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.267869e-08
Norm of the params: 9.153176
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729849
Test loss (w/o reg) on all data: 0.012055001
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5358631e-07
Norm of the params: 9.153177
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16236115
Train loss (w/o reg) on all data: 0.15412094
Test loss (w/o reg) on all data: 0.088059835
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4055833e-05
Norm of the params: 12.837604
              Random: fixed  12 labels. Loss 0.08806. Accuracy 0.989.
### Flips: 156, rs: 14, checks: 260
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172957
Test loss (w/o reg) on all data: 0.012054274
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8037172e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729553
Test loss (w/o reg) on all data: 0.012054216
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.526705e-07
Norm of the params: 9.153209
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14678694
Train loss (w/o reg) on all data: 0.1366257
Test loss (w/o reg) on all data: 0.07893875
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5115894e-05
Norm of the params: 14.2556925
              Random: fixed  19 labels. Loss 0.07894. Accuracy 0.985.
### Flips: 156, rs: 14, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729455
Test loss (w/o reg) on all data: 0.01205498
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5705453e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729462
Test loss (w/o reg) on all data: 0.012054877
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3678872e-07
Norm of the params: 9.153222
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14225559
Train loss (w/o reg) on all data: 0.1322785
Test loss (w/o reg) on all data: 0.069383964
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3562563e-05
Norm of the params: 14.125922
              Random: fixed  22 labels. Loss 0.06938. Accuracy 0.989.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1744894
Train loss (w/o reg) on all data: 0.16603293
Test loss (w/o reg) on all data: 0.11104757
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.998036e-06
Norm of the params: 13.004977
Flipped loss: 0.11105. Accuracy: 0.973
### Flips: 156, rs: 15, checks: 52
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07716442
Train loss (w/o reg) on all data: 0.06674094
Test loss (w/o reg) on all data: 0.054887734
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.4840417e-06
Norm of the params: 14.438479
     Influence (LOO): fixed  44 labels. Loss 0.05489. Accuracy 0.981.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046775416
Train loss (w/o reg) on all data: 0.031955305
Test loss (w/o reg) on all data: 0.05312023
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.408021e-06
Norm of the params: 17.216335
                Loss: fixed  52 labels. Loss 0.05312. Accuracy 0.977.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16769287
Train loss (w/o reg) on all data: 0.15921263
Test loss (w/o reg) on all data: 0.10655431
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.766461e-06
Norm of the params: 13.023236
              Random: fixed   4 labels. Loss 0.10655. Accuracy 0.962.
### Flips: 156, rs: 15, checks: 104
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024608579
Train loss (w/o reg) on all data: 0.015326772
Test loss (w/o reg) on all data: 0.02631638
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.739085e-07
Norm of the params: 13.624835
     Influence (LOO): fixed  67 labels. Loss 0.02632. Accuracy 0.989.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009751552
Train loss (w/o reg) on all data: 0.0041186293
Test loss (w/o reg) on all data: 0.012094303
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9970662e-06
Norm of the params: 10.614068
                Loss: fixed  75 labels. Loss 0.01209. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15997854
Train loss (w/o reg) on all data: 0.15155225
Test loss (w/o reg) on all data: 0.10585544
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.1691917e-05
Norm of the params: 12.981746
              Random: fixed   8 labels. Loss 0.10586. Accuracy 0.958.
### Flips: 156, rs: 15, checks: 156
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0093708765
Train loss (w/o reg) on all data: 0.0040852856
Test loss (w/o reg) on all data: 0.012748292
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0047131e-06
Norm of the params: 10.281626
     Influence (LOO): fixed  77 labels. Loss 0.01275. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00872382
Train loss (w/o reg) on all data: 0.0034938366
Test loss (w/o reg) on all data: 0.011610576
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.62885e-07
Norm of the params: 10.227397
                Loss: fixed  77 labels. Loss 0.01161. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15253992
Train loss (w/o reg) on all data: 0.14392221
Test loss (w/o reg) on all data: 0.104214504
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.0093108e-05
Norm of the params: 13.128377
              Random: fixed  11 labels. Loss 0.10421. Accuracy 0.950.
### Flips: 156, rs: 15, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730012
Test loss (w/o reg) on all data: 0.012054251
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7684842e-07
Norm of the params: 9.153161
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075635524
Train loss (w/o reg) on all data: 0.002864317
Test loss (w/o reg) on all data: 0.010624768
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0488524e-07
Norm of the params: 9.6945715
                Loss: fixed  78 labels. Loss 0.01062. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14439242
Train loss (w/o reg) on all data: 0.13587846
Test loss (w/o reg) on all data: 0.10133902
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.322892e-06
Norm of the params: 13.0491085
              Random: fixed  18 labels. Loss 0.10134. Accuracy 0.969.
### Flips: 156, rs: 15, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729493
Test loss (w/o reg) on all data: 0.012055039
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.16722894e-07
Norm of the params: 9.153219
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007563553
Train loss (w/o reg) on all data: 0.0028642465
Test loss (w/o reg) on all data: 0.010624836
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4774487e-07
Norm of the params: 9.694644
                Loss: fixed  78 labels. Loss 0.01062. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14054884
Train loss (w/o reg) on all data: 0.131952
Test loss (w/o reg) on all data: 0.100238696
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.338246e-06
Norm of the params: 13.112469
              Random: fixed  20 labels. Loss 0.10024. Accuracy 0.966.
### Flips: 156, rs: 15, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729546
Test loss (w/o reg) on all data: 0.012055018
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6700574e-07
Norm of the params: 9.153212
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729574
Test loss (w/o reg) on all data: 0.012054981
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2018513e-07
Norm of the params: 9.153209
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121727735
Train loss (w/o reg) on all data: 0.112722725
Test loss (w/o reg) on all data: 0.0735509
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3128469e-05
Norm of the params: 13.420143
              Random: fixed  29 labels. Loss 0.07355. Accuracy 0.985.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17448384
Train loss (w/o reg) on all data: 0.16709563
Test loss (w/o reg) on all data: 0.12194929
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2103827e-05
Norm of the params: 12.155827
Flipped loss: 0.12195. Accuracy: 0.954
### Flips: 156, rs: 16, checks: 52
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06458433
Train loss (w/o reg) on all data: 0.052187603
Test loss (w/o reg) on all data: 0.07546989
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.643051e-06
Norm of the params: 15.745935
     Influence (LOO): fixed  44 labels. Loss 0.07547. Accuracy 0.969.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043715294
Train loss (w/o reg) on all data: 0.028295957
Test loss (w/o reg) on all data: 0.09145381
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.8563065e-06
Norm of the params: 17.560946
                Loss: fixed  50 labels. Loss 0.09145. Accuracy 0.966.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1683583
Train loss (w/o reg) on all data: 0.16129419
Test loss (w/o reg) on all data: 0.11675115
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.4787134e-05
Norm of the params: 11.886212
              Random: fixed   5 labels. Loss 0.11675. Accuracy 0.954.
### Flips: 156, rs: 16, checks: 104
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019131608
Train loss (w/o reg) on all data: 0.010831818
Test loss (w/o reg) on all data: 0.027503563
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.7081065e-07
Norm of the params: 12.883935
     Influence (LOO): fixed  68 labels. Loss 0.02750. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014476209
Train loss (w/o reg) on all data: 0.0068233344
Test loss (w/o reg) on all data: 0.01964151
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.054216e-07
Norm of the params: 12.371641
                Loss: fixed  71 labels. Loss 0.01964. Accuracy 0.992.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16248755
Train loss (w/o reg) on all data: 0.15494478
Test loss (w/o reg) on all data: 0.11637311
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.9725363e-05
Norm of the params: 12.282327
              Random: fixed   8 labels. Loss 0.11637. Accuracy 0.954.
### Flips: 156, rs: 16, checks: 156
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010540188
Train loss (w/o reg) on all data: 0.0044215336
Test loss (w/o reg) on all data: 0.013806955
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.732645e-07
Norm of the params: 11.062238
     Influence (LOO): fixed  75 labels. Loss 0.01381. Accuracy 0.996.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011730218
Train loss (w/o reg) on all data: 0.0052041323
Test loss (w/o reg) on all data: 0.020259487
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.2189877e-07
Norm of the params: 11.424611
                Loss: fixed  74 labels. Loss 0.02026. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16112462
Train loss (w/o reg) on all data: 0.15358862
Test loss (w/o reg) on all data: 0.11217616
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.1036216e-05
Norm of the params: 12.276797
              Random: fixed  10 labels. Loss 0.11218. Accuracy 0.950.
### Flips: 156, rs: 16, checks: 208
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009494475
Train loss (w/o reg) on all data: 0.0038225634
Test loss (w/o reg) on all data: 0.01126256
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4695202e-07
Norm of the params: 10.650739
     Influence (LOO): fixed  76 labels. Loss 0.01126. Accuracy 0.996.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007910372
Train loss (w/o reg) on all data: 0.0029112257
Test loss (w/o reg) on all data: 0.015208801
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7861494e-07
Norm of the params: 9.999147
                Loss: fixed  76 labels. Loss 0.01521. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1562897
Train loss (w/o reg) on all data: 0.14876546
Test loss (w/o reg) on all data: 0.10831972
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.0992625e-05
Norm of the params: 12.2672205
              Random: fixed  13 labels. Loss 0.10832. Accuracy 0.962.
### Flips: 156, rs: 16, checks: 260
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008311654
Train loss (w/o reg) on all data: 0.003282095
Test loss (w/o reg) on all data: 0.01257084
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5368463e-07
Norm of the params: 10.029516
     Influence (LOO): fixed  77 labels. Loss 0.01257. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729015
Test loss (w/o reg) on all data: 0.012054496
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3874471e-07
Norm of the params: 9.153267
                Loss: fixed  78 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14545086
Train loss (w/o reg) on all data: 0.13707626
Test loss (w/o reg) on all data: 0.09777477
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.839927e-05
Norm of the params: 12.941874
              Random: fixed  19 labels. Loss 0.09777. Accuracy 0.966.
### Flips: 156, rs: 16, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729984
Test loss (w/o reg) on all data: 0.01205488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.387003e-07
Norm of the params: 9.153166
     Influence (LOO): fixed  78 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729812
Test loss (w/o reg) on all data: 0.012054386
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2814973e-07
Norm of the params: 9.153184
                Loss: fixed  78 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13433899
Train loss (w/o reg) on all data: 0.12518543
Test loss (w/o reg) on all data: 0.09153473
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.639826e-06
Norm of the params: 13.530384
              Random: fixed  23 labels. Loss 0.09153. Accuracy 0.969.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17383555
Train loss (w/o reg) on all data: 0.16480424
Test loss (w/o reg) on all data: 0.094265684
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3292786e-05
Norm of the params: 13.439725
Flipped loss: 0.09427. Accuracy: 0.981
### Flips: 156, rs: 17, checks: 52
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07656274
Train loss (w/o reg) on all data: 0.06672174
Test loss (w/o reg) on all data: 0.044939958
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.814869e-05
Norm of the params: 14.029258
     Influence (LOO): fixed  44 labels. Loss 0.04494. Accuracy 0.992.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047579084
Train loss (w/o reg) on all data: 0.03150033
Test loss (w/o reg) on all data: 0.048712622
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.6169042e-06
Norm of the params: 17.932516
                Loss: fixed  51 labels. Loss 0.04871. Accuracy 0.973.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16491252
Train loss (w/o reg) on all data: 0.15519491
Test loss (w/o reg) on all data: 0.09245287
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.3023229e-05
Norm of the params: 13.941026
              Random: fixed   5 labels. Loss 0.09245. Accuracy 0.973.
### Flips: 156, rs: 17, checks: 104
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024261605
Train loss (w/o reg) on all data: 0.017500214
Test loss (w/o reg) on all data: 0.036879573
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.772791e-07
Norm of the params: 11.628749
     Influence (LOO): fixed  66 labels. Loss 0.03688. Accuracy 0.985.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069710165
Train loss (w/o reg) on all data: 0.0026113132
Test loss (w/o reg) on all data: 0.011676851
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.418331e-07
Norm of the params: 9.337776
                Loss: fixed  73 labels. Loss 0.01168. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1489734
Train loss (w/o reg) on all data: 0.1390936
Test loss (w/o reg) on all data: 0.082286745
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.333672e-06
Norm of the params: 14.056888
              Random: fixed  14 labels. Loss 0.08229. Accuracy 0.977.
### Flips: 156, rs: 17, checks: 156
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011496244
Train loss (w/o reg) on all data: 0.0060475105
Test loss (w/o reg) on all data: 0.018419703
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.763067e-07
Norm of the params: 10.439094
     Influence (LOO): fixed  71 labels. Loss 0.01842. Accuracy 0.992.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069710165
Train loss (w/o reg) on all data: 0.0026112136
Test loss (w/o reg) on all data: 0.011676756
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1922158e-07
Norm of the params: 9.337882
                Loss: fixed  73 labels. Loss 0.01168. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14023201
Train loss (w/o reg) on all data: 0.1303014
Test loss (w/o reg) on all data: 0.07303773
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.3368171e-05
Norm of the params: 14.092985
              Random: fixed  18 labels. Loss 0.07304. Accuracy 0.977.
### Flips: 156, rs: 17, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729756
Test loss (w/o reg) on all data: 0.012054743
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8312404e-07
Norm of the params: 9.153188
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069710147
Train loss (w/o reg) on all data: 0.0026114064
Test loss (w/o reg) on all data: 0.01167684
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2383581e-07
Norm of the params: 9.337675
                Loss: fixed  73 labels. Loss 0.01168. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12112042
Train loss (w/o reg) on all data: 0.11054382
Test loss (w/o reg) on all data: 0.05978858
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.274068e-06
Norm of the params: 14.544144
              Random: fixed  26 labels. Loss 0.05979. Accuracy 0.985.
### Flips: 156, rs: 17, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729707
Test loss (w/o reg) on all data: 0.012055308
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5204004e-07
Norm of the params: 9.1531925
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729702
Test loss (w/o reg) on all data: 0.01205504
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.441174e-07
Norm of the params: 9.153194
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11777368
Train loss (w/o reg) on all data: 0.106863216
Test loss (w/o reg) on all data: 0.056782223
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.07279e-05
Norm of the params: 14.77191
              Random: fixed  29 labels. Loss 0.05678. Accuracy 0.992.
### Flips: 156, rs: 17, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729523
Test loss (w/o reg) on all data: 0.012054226
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6806828e-07
Norm of the params: 9.153214
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729528
Test loss (w/o reg) on all data: 0.012054215
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7900782e-07
Norm of the params: 9.1532135
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1097721
Train loss (w/o reg) on all data: 0.098549515
Test loss (w/o reg) on all data: 0.053053867
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.715944e-06
Norm of the params: 14.981715
              Random: fixed  33 labels. Loss 0.05305. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1851858
Train loss (w/o reg) on all data: 0.17951824
Test loss (w/o reg) on all data: 0.12988059
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.050071e-05
Norm of the params: 10.646656
Flipped loss: 0.12988. Accuracy: 0.954
### Flips: 156, rs: 18, checks: 52
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07609853
Train loss (w/o reg) on all data: 0.065690935
Test loss (w/o reg) on all data: 0.070575655
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.9538125e-06
Norm of the params: 14.427472
     Influence (LOO): fixed  47 labels. Loss 0.07058. Accuracy 0.973.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046313956
Train loss (w/o reg) on all data: 0.031016294
Test loss (w/o reg) on all data: 0.10682416
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.885719e-06
Norm of the params: 17.49152
                Loss: fixed  52 labels. Loss 0.10682. Accuracy 0.954.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17848523
Train loss (w/o reg) on all data: 0.17299978
Test loss (w/o reg) on all data: 0.11922779
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.75141e-05
Norm of the params: 10.474209
              Random: fixed   4 labels. Loss 0.11923. Accuracy 0.962.
### Flips: 156, rs: 18, checks: 104
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034753997
Train loss (w/o reg) on all data: 0.026818767
Test loss (w/o reg) on all data: 0.030093005
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2542763e-06
Norm of the params: 12.597802
     Influence (LOO): fixed  69 labels. Loss 0.03009. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007616435
Train loss (w/o reg) on all data: 0.0027159897
Test loss (w/o reg) on all data: 0.0129078645
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4698772e-07
Norm of the params: 9.899945
                Loss: fixed  82 labels. Loss 0.01291. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16592883
Train loss (w/o reg) on all data: 0.15983906
Test loss (w/o reg) on all data: 0.119089015
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.4245946e-05
Norm of the params: 11.036087
              Random: fixed  10 labels. Loss 0.11909. Accuracy 0.962.
### Flips: 156, rs: 18, checks: 156
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0134576745
Train loss (w/o reg) on all data: 0.0072663217
Test loss (w/o reg) on all data: 0.018639186
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0474123e-07
Norm of the params: 11.127761
     Influence (LOO): fixed  80 labels. Loss 0.01864. Accuracy 0.992.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006934946
Train loss (w/o reg) on all data: 0.0024075196
Test loss (w/o reg) on all data: 0.011970529
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.762807e-07
Norm of the params: 9.515699
                Loss: fixed  83 labels. Loss 0.01197. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1587418
Train loss (w/o reg) on all data: 0.15226792
Test loss (w/o reg) on all data: 0.11395254
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.838016e-05
Norm of the params: 11.378819
              Random: fixed  15 labels. Loss 0.11395. Accuracy 0.966.
### Flips: 156, rs: 18, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009772562
Train loss (w/o reg) on all data: 0.0046906327
Test loss (w/o reg) on all data: 0.012590945
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.9411466e-07
Norm of the params: 10.081596
     Influence (LOO): fixed  82 labels. Loss 0.01259. Accuracy 0.992.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069349427
Train loss (w/o reg) on all data: 0.0024075157
Test loss (w/o reg) on all data: 0.01197036
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.21422e-07
Norm of the params: 9.5157
                Loss: fixed  83 labels. Loss 0.01197. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15349048
Train loss (w/o reg) on all data: 0.14705084
Test loss (w/o reg) on all data: 0.1067945
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.2193388e-05
Norm of the params: 11.348692
              Random: fixed  18 labels. Loss 0.10679. Accuracy 0.962.
### Flips: 156, rs: 18, checks: 260
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069349455
Train loss (w/o reg) on all data: 0.0024071557
Test loss (w/o reg) on all data: 0.011970824
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.085038e-07
Norm of the params: 9.516081
     Influence (LOO): fixed  83 labels. Loss 0.01197. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069349473
Train loss (w/o reg) on all data: 0.0024071608
Test loss (w/o reg) on all data: 0.01197099
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.734196e-07
Norm of the params: 9.516077
                Loss: fixed  83 labels. Loss 0.01197. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15033884
Train loss (w/o reg) on all data: 0.14404039
Test loss (w/o reg) on all data: 0.094726816
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.921052e-06
Norm of the params: 11.223598
              Random: fixed  22 labels. Loss 0.09473. Accuracy 0.969.
### Flips: 156, rs: 18, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006934945
Train loss (w/o reg) on all data: 0.0024074432
Test loss (w/o reg) on all data: 0.011969998
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6019828e-07
Norm of the params: 9.515779
     Influence (LOO): fixed  83 labels. Loss 0.01197. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006934945
Train loss (w/o reg) on all data: 0.0024074474
Test loss (w/o reg) on all data: 0.011969917
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3955023e-07
Norm of the params: 9.515774
                Loss: fixed  83 labels. Loss 0.01197. Accuracy 0.992.
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13636151
Train loss (w/o reg) on all data: 0.1290127
Test loss (w/o reg) on all data: 0.10241058
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2746434e-05
Norm of the params: 12.123372
              Random: fixed  27 labels. Loss 0.10241. Accuracy 0.962.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17529498
Train loss (w/o reg) on all data: 0.16866845
Test loss (w/o reg) on all data: 0.11521507
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5009131e-05
Norm of the params: 11.5122
Flipped loss: 0.11522. Accuracy: 0.981
### Flips: 156, rs: 19, checks: 52
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0597423
Train loss (w/o reg) on all data: 0.049443834
Test loss (w/o reg) on all data: 0.05146953
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.759539e-05
Norm of the params: 14.351632
     Influence (LOO): fixed  45 labels. Loss 0.05147. Accuracy 0.981.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03427796
Train loss (w/o reg) on all data: 0.022297451
Test loss (w/o reg) on all data: 0.07344646
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.8146137e-06
Norm of the params: 15.479348
                Loss: fixed  52 labels. Loss 0.07345. Accuracy 0.969.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16426992
Train loss (w/o reg) on all data: 0.15743943
Test loss (w/o reg) on all data: 0.11234491
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.890954e-05
Norm of the params: 11.688023
              Random: fixed   7 labels. Loss 0.11234. Accuracy 0.981.
### Flips: 156, rs: 19, checks: 104
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019695157
Train loss (w/o reg) on all data: 0.011957303
Test loss (w/o reg) on all data: 0.028142167
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.89536e-07
Norm of the params: 12.440139
     Influence (LOO): fixed  66 labels. Loss 0.02814. Accuracy 0.989.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006816094
Train loss (w/o reg) on all data: 0.0024067196
Test loss (w/o reg) on all data: 0.013059788
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9570349e-07
Norm of the params: 9.39082
                Loss: fixed  73 labels. Loss 0.01306. Accuracy 0.996.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15958858
Train loss (w/o reg) on all data: 0.15293917
Test loss (w/o reg) on all data: 0.103993945
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4587996e-05
Norm of the params: 11.532042
              Random: fixed  10 labels. Loss 0.10399. Accuracy 0.977.
### Flips: 156, rs: 19, checks: 156
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0089714965
Train loss (w/o reg) on all data: 0.0041994248
Test loss (w/o reg) on all data: 0.010504974
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.674274e-07
Norm of the params: 9.769413
     Influence (LOO): fixed  73 labels. Loss 0.01050. Accuracy 0.996.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0068160947
Train loss (w/o reg) on all data: 0.0024066477
Test loss (w/o reg) on all data: 0.013059654
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.32038e-07
Norm of the params: 9.390897
                Loss: fixed  73 labels. Loss 0.01306. Accuracy 0.996.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14694956
Train loss (w/o reg) on all data: 0.14051293
Test loss (w/o reg) on all data: 0.07996652
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.7054e-06
Norm of the params: 11.346037
              Random: fixed  16 labels. Loss 0.07997. Accuracy 0.977.
### Flips: 156, rs: 19, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729795
Test loss (w/o reg) on all data: 0.012055025
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5064363e-07
Norm of the params: 9.153183
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0068160933
Train loss (w/o reg) on all data: 0.0024065594
Test loss (w/o reg) on all data: 0.013059504
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3010165e-07
Norm of the params: 9.39099
                Loss: fixed  73 labels. Loss 0.01306. Accuracy 0.996.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14211126
Train loss (w/o reg) on all data: 0.13561858
Test loss (w/o reg) on all data: 0.075663544
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.776841e-05
Norm of the params: 11.39533
              Random: fixed  19 labels. Loss 0.07566. Accuracy 0.981.
### Flips: 156, rs: 19, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620163
Train loss (w/o reg) on all data: 0.0021729877
Test loss (w/o reg) on all data: 0.012054943
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3762022e-07
Norm of the params: 9.153173
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006816094
Train loss (w/o reg) on all data: 0.002406589
Test loss (w/o reg) on all data: 0.013059619
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.1412102e-07
Norm of the params: 9.390958
                Loss: fixed  73 labels. Loss 0.01306. Accuracy 0.996.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1387145
Train loss (w/o reg) on all data: 0.1323231
Test loss (w/o reg) on all data: 0.07690668
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1991364e-05
Norm of the params: 11.306114
              Random: fixed  21 labels. Loss 0.07691. Accuracy 0.981.
### Flips: 156, rs: 19, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730114
Test loss (w/o reg) on all data: 0.012054685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3093604e-07
Norm of the params: 9.1531515
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730107
Test loss (w/o reg) on all data: 0.012054596
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7037112e-07
Norm of the params: 9.1531515
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12523142
Train loss (w/o reg) on all data: 0.11797749
Test loss (w/o reg) on all data: 0.07352474
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.741737e-05
Norm of the params: 12.044857
              Random: fixed  26 labels. Loss 0.07352. Accuracy 0.981.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1951583
Train loss (w/o reg) on all data: 0.18814172
Test loss (w/o reg) on all data: 0.12322183
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.651678e-05
Norm of the params: 11.846169
Flipped loss: 0.12322. Accuracy: 0.969
### Flips: 156, rs: 20, checks: 52
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09725727
Train loss (w/o reg) on all data: 0.08667526
Test loss (w/o reg) on all data: 0.07346972
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.8000027e-05
Norm of the params: 14.547861
     Influence (LOO): fixed  45 labels. Loss 0.07347. Accuracy 0.981.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06730459
Train loss (w/o reg) on all data: 0.053260323
Test loss (w/o reg) on all data: 0.09762211
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.3078925e-06
Norm of the params: 16.759634
                Loss: fixed  50 labels. Loss 0.09762. Accuracy 0.969.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18966866
Train loss (w/o reg) on all data: 0.18238813
Test loss (w/o reg) on all data: 0.119775474
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.7425946e-05
Norm of the params: 12.066918
              Random: fixed   3 labels. Loss 0.11978. Accuracy 0.969.
### Flips: 156, rs: 20, checks: 104
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047270045
Train loss (w/o reg) on all data: 0.03929263
Test loss (w/o reg) on all data: 0.053420145
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.601154e-06
Norm of the params: 12.631244
     Influence (LOO): fixed  68 labels. Loss 0.05342. Accuracy 0.977.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00796219
Train loss (w/o reg) on all data: 0.002912494
Test loss (w/o reg) on all data: 0.017243998
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.829904e-07
Norm of the params: 10.049573
                Loss: fixed  83 labels. Loss 0.01724. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18487005
Train loss (w/o reg) on all data: 0.17763337
Test loss (w/o reg) on all data: 0.11837013
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6967404e-05
Norm of the params: 12.030528
              Random: fixed   6 labels. Loss 0.11837. Accuracy 0.969.
### Flips: 156, rs: 20, checks: 156
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010352798
Train loss (w/o reg) on all data: 0.0051584537
Test loss (w/o reg) on all data: 0.016800206
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9375911e-07
Norm of the params: 10.192492
     Influence (LOO): fixed  83 labels. Loss 0.01680. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007929785
Train loss (w/o reg) on all data: 0.003019898
Test loss (w/o reg) on all data: 0.019055486
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.1215905e-07
Norm of the params: 9.909477
                Loss: fixed  84 labels. Loss 0.01906. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1777955
Train loss (w/o reg) on all data: 0.1707898
Test loss (w/o reg) on all data: 0.11992235
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.472419e-05
Norm of the params: 11.83698
              Random: fixed  11 labels. Loss 0.11992. Accuracy 0.969.
### Flips: 156, rs: 20, checks: 208
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008013477
Train loss (w/o reg) on all data: 0.0032719974
Test loss (w/o reg) on all data: 0.01529528
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0087728e-07
Norm of the params: 9.738049
     Influence (LOO): fixed  84 labels. Loss 0.01530. Accuracy 0.992.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730186
Test loss (w/o reg) on all data: 0.012054889
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2407409e-07
Norm of the params: 9.153143
                Loss: fixed  85 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1704967
Train loss (w/o reg) on all data: 0.16347387
Test loss (w/o reg) on all data: 0.10745222
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2072628e-05
Norm of the params: 11.851431
              Random: fixed  17 labels. Loss 0.10745. Accuracy 0.981.
### Flips: 156, rs: 20, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728924
Test loss (w/o reg) on all data: 0.0120543875
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3153264e-07
Norm of the params: 9.15328
     Influence (LOO): fixed  85 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728927
Test loss (w/o reg) on all data: 0.012054463
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.539261e-07
Norm of the params: 9.153277
                Loss: fixed  85 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16128632
Train loss (w/o reg) on all data: 0.15369517
Test loss (w/o reg) on all data: 0.10538098
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.6321359e-05
Norm of the params: 12.321661
              Random: fixed  21 labels. Loss 0.10538. Accuracy 0.973.
### Flips: 156, rs: 20, checks: 312
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729167
Test loss (w/o reg) on all data: 0.012055276
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.893965e-07
Norm of the params: 9.153254
     Influence (LOO): fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729188
Test loss (w/o reg) on all data: 0.012055159
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1840944e-07
Norm of the params: 9.153254
                Loss: fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1612863
Train loss (w/o reg) on all data: 0.15369771
Test loss (w/o reg) on all data: 0.10538167
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.706331e-06
Norm of the params: 12.319565
              Random: fixed  21 labels. Loss 0.10538. Accuracy 0.973.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16589636
Train loss (w/o reg) on all data: 0.1547127
Test loss (w/o reg) on all data: 0.106051475
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.537966e-05
Norm of the params: 14.955695
Flipped loss: 0.10605. Accuracy: 0.973
### Flips: 156, rs: 21, checks: 52
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08041441
Train loss (w/o reg) on all data: 0.069167584
Test loss (w/o reg) on all data: 0.048864532
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.9649702e-06
Norm of the params: 14.997881
     Influence (LOO): fixed  42 labels. Loss 0.04886. Accuracy 0.989.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0480325
Train loss (w/o reg) on all data: 0.034423865
Test loss (w/o reg) on all data: 0.043423656
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.1394103e-06
Norm of the params: 16.497656
                Loss: fixed  51 labels. Loss 0.04342. Accuracy 0.989.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16262285
Train loss (w/o reg) on all data: 0.15127712
Test loss (w/o reg) on all data: 0.09759881
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1241955e-05
Norm of the params: 15.063688
              Random: fixed   4 labels. Loss 0.09760. Accuracy 0.985.
### Flips: 156, rs: 21, checks: 104
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038537506
Train loss (w/o reg) on all data: 0.030228268
Test loss (w/o reg) on all data: 0.026588446
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0135636e-06
Norm of the params: 12.891266
     Influence (LOO): fixed  65 labels. Loss 0.02659. Accuracy 0.996.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011453744
Train loss (w/o reg) on all data: 0.0047608586
Test loss (w/o reg) on all data: 0.016518682
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1857575e-07
Norm of the params: 11.56969
                Loss: fixed  72 labels. Loss 0.01652. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15770146
Train loss (w/o reg) on all data: 0.146341
Test loss (w/o reg) on all data: 0.09441987
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9901259e-05
Norm of the params: 15.073464
              Random: fixed   7 labels. Loss 0.09442. Accuracy 0.985.
### Flips: 156, rs: 21, checks: 156
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009354463
Train loss (w/o reg) on all data: 0.004382762
Test loss (w/o reg) on all data: 0.014623824
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.061969e-07
Norm of the params: 9.97166
     Influence (LOO): fixed  76 labels. Loss 0.01462. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008326443
Train loss (w/o reg) on all data: 0.0030542344
Test loss (w/o reg) on all data: 0.014529528
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1427367e-07
Norm of the params: 10.2686
                Loss: fixed  75 labels. Loss 0.01453. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1534053
Train loss (w/o reg) on all data: 0.14197853
Test loss (w/o reg) on all data: 0.089663826
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4682199e-05
Norm of the params: 15.117383
              Random: fixed  11 labels. Loss 0.08966. Accuracy 0.981.
### Flips: 156, rs: 21, checks: 208
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729602
Test loss (w/o reg) on all data: 0.012055056
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4200386e-07
Norm of the params: 9.153206
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729597
Test loss (w/o reg) on all data: 0.012054924
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6633194e-07
Norm of the params: 9.153206
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14876351
Train loss (w/o reg) on all data: 0.13753164
Test loss (w/o reg) on all data: 0.086540736
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9859897e-05
Norm of the params: 14.987907
              Random: fixed  14 labels. Loss 0.08654. Accuracy 0.977.
### Flips: 156, rs: 21, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172972
Test loss (w/o reg) on all data: 0.012054894
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3657465e-07
Norm of the params: 9.1531925
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172972
Test loss (w/o reg) on all data: 0.012055007
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9040424e-07
Norm of the params: 9.1531925
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1462094
Train loss (w/o reg) on all data: 0.13518715
Test loss (w/o reg) on all data: 0.08284095
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.7806176e-05
Norm of the params: 14.847399
              Random: fixed  16 labels. Loss 0.08284. Accuracy 0.981.
### Flips: 156, rs: 21, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729097
Test loss (w/o reg) on all data: 0.012054514
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5203811e-07
Norm of the params: 9.15326
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729153
Test loss (w/o reg) on all data: 0.012054646
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0441795e-07
Norm of the params: 9.153255
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14250703
Train loss (w/o reg) on all data: 0.13135858
Test loss (w/o reg) on all data: 0.081007674
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.685063e-06
Norm of the params: 14.932144
              Random: fixed  18 labels. Loss 0.08101. Accuracy 0.981.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18985617
Train loss (w/o reg) on all data: 0.18203364
Test loss (w/o reg) on all data: 0.11296991
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4871201e-05
Norm of the params: 12.508019
Flipped loss: 0.11297. Accuracy: 0.966
### Flips: 156, rs: 22, checks: 52
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082818516
Train loss (w/o reg) on all data: 0.07293774
Test loss (w/o reg) on all data: 0.06752587
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5092166e-06
Norm of the params: 14.057577
     Influence (LOO): fixed  44 labels. Loss 0.06753. Accuracy 0.981.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057132415
Train loss (w/o reg) on all data: 0.042060908
Test loss (w/o reg) on all data: 0.08835706
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.853158e-06
Norm of the params: 17.361742
                Loss: fixed  50 labels. Loss 0.08836. Accuracy 0.958.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17809427
Train loss (w/o reg) on all data: 0.16999157
Test loss (w/o reg) on all data: 0.09928698
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0721415e-05
Norm of the params: 12.730039
              Random: fixed   8 labels. Loss 0.09929. Accuracy 0.973.
### Flips: 156, rs: 22, checks: 104
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027435157
Train loss (w/o reg) on all data: 0.01751058
Test loss (w/o reg) on all data: 0.03369036
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4067883e-06
Norm of the params: 14.088704
     Influence (LOO): fixed  69 labels. Loss 0.03369. Accuracy 0.992.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014180368
Train loss (w/o reg) on all data: 0.006736577
Test loss (w/o reg) on all data: 0.018144304
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0458779e-06
Norm of the params: 12.2014675
                Loss: fixed  76 labels. Loss 0.01814. Accuracy 0.996.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17534836
Train loss (w/o reg) on all data: 0.16784434
Test loss (w/o reg) on all data: 0.09865901
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7429824e-05
Norm of the params: 12.250723
              Random: fixed  10 labels. Loss 0.09866. Accuracy 0.977.
### Flips: 156, rs: 22, checks: 156
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010295989
Train loss (w/o reg) on all data: 0.0048677465
Test loss (w/o reg) on all data: 0.016825058
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3206105e-07
Norm of the params: 10.419446
     Influence (LOO): fixed  80 labels. Loss 0.01683. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074989237
Train loss (w/o reg) on all data: 0.0026459594
Test loss (w/o reg) on all data: 0.009037538
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.2603835e-07
Norm of the params: 9.851868
                Loss: fixed  81 labels. Loss 0.00904. Accuracy 0.996.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16596454
Train loss (w/o reg) on all data: 0.15812036
Test loss (w/o reg) on all data: 0.0947897
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.8033332e-06
Norm of the params: 12.525315
              Random: fixed  15 labels. Loss 0.09479. Accuracy 0.981.
### Flips: 156, rs: 22, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729758
Test loss (w/o reg) on all data: 0.012055106
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2700684e-06
Norm of the params: 9.15319
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729732
Test loss (w/o reg) on all data: 0.012055285
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1246143e-07
Norm of the params: 9.153191
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14396158
Train loss (w/o reg) on all data: 0.13533401
Test loss (w/o reg) on all data: 0.09189529
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.6869645e-05
Norm of the params: 13.13588
              Random: fixed  24 labels. Loss 0.09190. Accuracy 0.973.
### Flips: 156, rs: 22, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730608
Test loss (w/o reg) on all data: 0.012054791
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.363755e-07
Norm of the params: 9.153097
     Influence (LOO): fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730578
Test loss (w/o reg) on all data: 0.012054678
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.415825e-07
Norm of the params: 9.153098
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13990615
Train loss (w/o reg) on all data: 0.13100252
Test loss (w/o reg) on all data: 0.08802211
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1716746e-05
Norm of the params: 13.344393
              Random: fixed  26 labels. Loss 0.08802. Accuracy 0.977.
### Flips: 156, rs: 22, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729337
Test loss (w/o reg) on all data: 0.012054844
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0964874e-07
Norm of the params: 9.153237
     Influence (LOO): fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729334
Test loss (w/o reg) on all data: 0.012054887
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7845805e-07
Norm of the params: 9.153236
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12703383
Train loss (w/o reg) on all data: 0.11733067
Test loss (w/o reg) on all data: 0.07902491
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.34116735e-05
Norm of the params: 13.930655
              Random: fixed  31 labels. Loss 0.07902. Accuracy 0.977.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19227749
Train loss (w/o reg) on all data: 0.1852892
Test loss (w/o reg) on all data: 0.13168177
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.95024e-05
Norm of the params: 11.822263
Flipped loss: 0.13168. Accuracy: 0.962
### Flips: 156, rs: 23, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09167691
Train loss (w/o reg) on all data: 0.08214915
Test loss (w/o reg) on all data: 0.086836174
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.7923709e-05
Norm of the params: 13.804177
     Influence (LOO): fixed  44 labels. Loss 0.08684. Accuracy 0.958.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050140858
Train loss (w/o reg) on all data: 0.035072275
Test loss (w/o reg) on all data: 0.11382294
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 7.3551087e-06
Norm of the params: 17.36006
                Loss: fixed  52 labels. Loss 0.11382. Accuracy 0.943.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18791926
Train loss (w/o reg) on all data: 0.18126586
Test loss (w/o reg) on all data: 0.12214923
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.0484602e-05
Norm of the params: 11.535515
              Random: fixed   4 labels. Loss 0.12215. Accuracy 0.966.
### Flips: 156, rs: 23, checks: 104
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028531116
Train loss (w/o reg) on all data: 0.02159503
Test loss (w/o reg) on all data: 0.025237603
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4267266e-06
Norm of the params: 11.778018
     Influence (LOO): fixed  71 labels. Loss 0.02524. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007485821
Train loss (w/o reg) on all data: 0.002749123
Test loss (w/o reg) on all data: 0.012725361
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.932757e-07
Norm of the params: 9.733137
                Loss: fixed  78 labels. Loss 0.01273. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1782485
Train loss (w/o reg) on all data: 0.1706978
Test loss (w/o reg) on all data: 0.11651677
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.0558413e-05
Norm of the params: 12.288782
              Random: fixed   8 labels. Loss 0.11652. Accuracy 0.962.
### Flips: 156, rs: 23, checks: 156
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729504
Test loss (w/o reg) on all data: 0.012055081
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3545375e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729493
Test loss (w/o reg) on all data: 0.012055215
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.447329e-07
Norm of the params: 9.153218
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16950303
Train loss (w/o reg) on all data: 0.16231549
Test loss (w/o reg) on all data: 0.10997667
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.486662e-05
Norm of the params: 11.989615
              Random: fixed  13 labels. Loss 0.10998. Accuracy 0.966.
### Flips: 156, rs: 23, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729502
Test loss (w/o reg) on all data: 0.012054476
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5203133e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729511
Test loss (w/o reg) on all data: 0.012054574
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7208966e-07
Norm of the params: 9.153216
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16950302
Train loss (w/o reg) on all data: 0.1623164
Test loss (w/o reg) on all data: 0.10998043
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.9202114e-05
Norm of the params: 11.988848
              Random: fixed  13 labels. Loss 0.10998. Accuracy 0.966.
### Flips: 156, rs: 23, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729097
Test loss (w/o reg) on all data: 0.012054677
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0459407e-07
Norm of the params: 9.15326
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729125
Test loss (w/o reg) on all data: 0.01205476
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8866443e-07
Norm of the params: 9.15326
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16313824
Train loss (w/o reg) on all data: 0.15549545
Test loss (w/o reg) on all data: 0.1132476
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.799891e-05
Norm of the params: 12.363483
              Random: fixed  15 labels. Loss 0.11325. Accuracy 0.962.
### Flips: 156, rs: 23, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728922
Test loss (w/o reg) on all data: 0.012054657
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.393699e-07
Norm of the params: 9.15328
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172894
Test loss (w/o reg) on all data: 0.012054736
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1975181e-07
Norm of the params: 9.153278
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15834111
Train loss (w/o reg) on all data: 0.15015782
Test loss (w/o reg) on all data: 0.11269149
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0373488e-05
Norm of the params: 12.793186
              Random: fixed  18 labels. Loss 0.11269. Accuracy 0.966.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16954486
Train loss (w/o reg) on all data: 0.16263624
Test loss (w/o reg) on all data: 0.11687118
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.34292e-06
Norm of the params: 11.754685
Flipped loss: 0.11687. Accuracy: 0.969
### Flips: 156, rs: 24, checks: 52
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07479211
Train loss (w/o reg) on all data: 0.06276242
Test loss (w/o reg) on all data: 0.07440767
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.790119e-06
Norm of the params: 15.511086
     Influence (LOO): fixed  42 labels. Loss 0.07441. Accuracy 0.981.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043434113
Train loss (w/o reg) on all data: 0.03009394
Test loss (w/o reg) on all data: 0.057216886
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.108927e-06
Norm of the params: 16.33412
                Loss: fixed  51 labels. Loss 0.05722. Accuracy 0.977.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1626545
Train loss (w/o reg) on all data: 0.1550754
Test loss (w/o reg) on all data: 0.11924028
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.9525183e-06
Norm of the params: 12.311863
              Random: fixed   3 labels. Loss 0.11924. Accuracy 0.966.
### Flips: 156, rs: 24, checks: 104
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025511304
Train loss (w/o reg) on all data: 0.0186741
Test loss (w/o reg) on all data: 0.027231729
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4417027e-06
Norm of the params: 11.693763
     Influence (LOO): fixed  67 labels. Loss 0.02723. Accuracy 0.996.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729716
Test loss (w/o reg) on all data: 0.012055114
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0117328e-07
Norm of the params: 9.1531925
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15875283
Train loss (w/o reg) on all data: 0.15107071
Test loss (w/o reg) on all data: 0.11670381
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.5838327e-06
Norm of the params: 12.395258
              Random: fixed   5 labels. Loss 0.11670. Accuracy 0.969.
### Flips: 156, rs: 24, checks: 156
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011392977
Train loss (w/o reg) on all data: 0.005841755
Test loss (w/o reg) on all data: 0.015293167
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5028123e-07
Norm of the params: 10.536815
     Influence (LOO): fixed  72 labels. Loss 0.01529. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729222
Test loss (w/o reg) on all data: 0.012055591
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.02139516e-07
Norm of the params: 9.153248
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15134765
Train loss (w/o reg) on all data: 0.14355032
Test loss (w/o reg) on all data: 0.11177742
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.1261043e-05
Norm of the params: 12.487861
              Random: fixed   9 labels. Loss 0.11178. Accuracy 0.962.
### Flips: 156, rs: 24, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729232
Test loss (w/o reg) on all data: 0.01205537
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6326285e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729227
Test loss (w/o reg) on all data: 0.012055401
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6434684e-07
Norm of the params: 9.153247
                Loss: fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14424884
Train loss (w/o reg) on all data: 0.13617262
Test loss (w/o reg) on all data: 0.0973499
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.2213804e-05
Norm of the params: 12.709222
              Random: fixed  13 labels. Loss 0.09735. Accuracy 0.973.
### Flips: 156, rs: 24, checks: 260
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729565
Test loss (w/o reg) on all data: 0.012054654
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.034493e-07
Norm of the params: 9.153209
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729558
Test loss (w/o reg) on all data: 0.012054742
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1930789e-07
Norm of the params: 9.153209
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12596622
Train loss (w/o reg) on all data: 0.11742023
Test loss (w/o reg) on all data: 0.09591852
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.9436846e-05
Norm of the params: 13.073633
              Random: fixed  21 labels. Loss 0.09592. Accuracy 0.969.
### Flips: 156, rs: 24, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729225
Test loss (w/o reg) on all data: 0.012055078
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.018711e-07
Norm of the params: 9.153248
     Influence (LOO): fixed  74 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729213
Test loss (w/o reg) on all data: 0.0120549435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.446636e-07
Norm of the params: 9.153247
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11135066
Train loss (w/o reg) on all data: 0.10295071
Test loss (w/o reg) on all data: 0.079642914
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.17418995e-05
Norm of the params: 12.96145
              Random: fixed  28 labels. Loss 0.07964. Accuracy 0.977.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18832646
Train loss (w/o reg) on all data: 0.18112864
Test loss (w/o reg) on all data: 0.09972712
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.7605547e-05
Norm of the params: 11.998187
Flipped loss: 0.09973. Accuracy: 0.973
### Flips: 156, rs: 25, checks: 52
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08244691
Train loss (w/o reg) on all data: 0.0707005
Test loss (w/o reg) on all data: 0.06803997
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.7548955e-06
Norm of the params: 15.32737
     Influence (LOO): fixed  45 labels. Loss 0.06804. Accuracy 0.981.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055635013
Train loss (w/o reg) on all data: 0.039658163
Test loss (w/o reg) on all data: 0.064439274
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.5703932e-06
Norm of the params: 17.875597
                Loss: fixed  52 labels. Loss 0.06444. Accuracy 0.977.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17622992
Train loss (w/o reg) on all data: 0.16850315
Test loss (w/o reg) on all data: 0.09978168
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.6882743e-05
Norm of the params: 12.431226
              Random: fixed   7 labels. Loss 0.09978. Accuracy 0.962.
### Flips: 156, rs: 25, checks: 104
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0360201
Train loss (w/o reg) on all data: 0.02637247
Test loss (w/o reg) on all data: 0.0425074
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.9371403e-06
Norm of the params: 13.8907385
     Influence (LOO): fixed  68 labels. Loss 0.04251. Accuracy 0.989.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013543344
Train loss (w/o reg) on all data: 0.00577768
Test loss (w/o reg) on all data: 0.034196123
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5754754e-06
Norm of the params: 12.462476
                Loss: fixed  78 labels. Loss 0.03420. Accuracy 0.985.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17133527
Train loss (w/o reg) on all data: 0.16356954
Test loss (w/o reg) on all data: 0.09244456
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.999658e-05
Norm of the params: 12.46253
              Random: fixed  11 labels. Loss 0.09244. Accuracy 0.969.
### Flips: 156, rs: 25, checks: 156
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016746644
Train loss (w/o reg) on all data: 0.009462272
Test loss (w/o reg) on all data: 0.039439756
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.6904754e-07
Norm of the params: 12.070106
     Influence (LOO): fixed  78 labels. Loss 0.03944. Accuracy 0.981.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010998886
Train loss (w/o reg) on all data: 0.004408166
Test loss (w/o reg) on all data: 0.021899668
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2300997e-07
Norm of the params: 11.481047
                Loss: fixed  81 labels. Loss 0.02190. Accuracy 0.989.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16688533
Train loss (w/o reg) on all data: 0.15906678
Test loss (w/o reg) on all data: 0.08459659
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.4470351e-05
Norm of the params: 12.504833
              Random: fixed  14 labels. Loss 0.08460. Accuracy 0.977.
### Flips: 156, rs: 25, checks: 208
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011639979
Train loss (w/o reg) on all data: 0.0060164654
Test loss (w/o reg) on all data: 0.01815263
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.161034e-07
Norm of the params: 10.6052
     Influence (LOO): fixed  83 labels. Loss 0.01815. Accuracy 0.992.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008291248
Train loss (w/o reg) on all data: 0.003132456
Test loss (w/o reg) on all data: 0.017233036
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.082436e-07
Norm of the params: 10.157552
                Loss: fixed  84 labels. Loss 0.01723. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15578479
Train loss (w/o reg) on all data: 0.14765319
Test loss (w/o reg) on all data: 0.078709885
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.981341e-05
Norm of the params: 12.752719
              Random: fixed  19 labels. Loss 0.07871. Accuracy 0.985.
### Flips: 156, rs: 25, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077947937
Train loss (w/o reg) on all data: 0.0029181566
Test loss (w/o reg) on all data: 0.016803864
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.483271e-07
Norm of the params: 9.875867
     Influence (LOO): fixed  85 labels. Loss 0.01680. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00829125
Train loss (w/o reg) on all data: 0.0031327403
Test loss (w/o reg) on all data: 0.017231489
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.391445e-07
Norm of the params: 10.157273
                Loss: fixed  84 labels. Loss 0.01723. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14321053
Train loss (w/o reg) on all data: 0.13427605
Test loss (w/o reg) on all data: 0.07838362
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.709597e-06
Norm of the params: 13.36749
              Random: fixed  24 labels. Loss 0.07838. Accuracy 0.977.
### Flips: 156, rs: 25, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.012054507
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3398766e-07
Norm of the params: 9.153226
     Influence (LOO): fixed  86 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0070390534
Train loss (w/o reg) on all data: 0.0024784976
Test loss (w/o reg) on all data: 0.012760287
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3393485e-07
Norm of the params: 9.550451
                Loss: fixed  85 labels. Loss 0.01276. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12321674
Train loss (w/o reg) on all data: 0.113622665
Test loss (w/o reg) on all data: 0.07839059
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.8392939e-05
Norm of the params: 13.852133
              Random: fixed  33 labels. Loss 0.07839. Accuracy 0.969.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19138555
Train loss (w/o reg) on all data: 0.18434998
Test loss (w/o reg) on all data: 0.09659248
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1923467e-05
Norm of the params: 11.862178
Flipped loss: 0.09659. Accuracy: 0.981
### Flips: 156, rs: 26, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106739
Train loss (w/o reg) on all data: 0.096666254
Test loss (w/o reg) on all data: 0.06640194
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.2450968e-06
Norm of the params: 14.193481
     Influence (LOO): fixed  40 labels. Loss 0.06640. Accuracy 0.981.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06394494
Train loss (w/o reg) on all data: 0.05050354
Test loss (w/o reg) on all data: 0.062248617
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.295433e-06
Norm of the params: 16.395977
                Loss: fixed  52 labels. Loss 0.06225. Accuracy 0.981.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18610822
Train loss (w/o reg) on all data: 0.17902142
Test loss (w/o reg) on all data: 0.093413055
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0347122e-05
Norm of the params: 11.90529
              Random: fixed   3 labels. Loss 0.09341. Accuracy 0.977.
### Flips: 156, rs: 26, checks: 104
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048498735
Train loss (w/o reg) on all data: 0.03804879
Test loss (w/o reg) on all data: 0.03599826
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.946079e-06
Norm of the params: 14.456795
     Influence (LOO): fixed  66 labels. Loss 0.03600. Accuracy 0.989.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012192862
Train loss (w/o reg) on all data: 0.0054947957
Test loss (w/o reg) on all data: 0.016064322
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1828859e-06
Norm of the params: 11.574166
                Loss: fixed  81 labels. Loss 0.01606. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17594677
Train loss (w/o reg) on all data: 0.1688511
Test loss (w/o reg) on all data: 0.087713346
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1683288e-05
Norm of the params: 11.912735
              Random: fixed   9 labels. Loss 0.08771. Accuracy 0.981.
### Flips: 156, rs: 26, checks: 156
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014529016
Train loss (w/o reg) on all data: 0.007492799
Test loss (w/o reg) on all data: 0.013986607
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4561173e-06
Norm of the params: 11.862729
     Influence (LOO): fixed  80 labels. Loss 0.01399. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172971
Test loss (w/o reg) on all data: 0.012054726
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.939792e-08
Norm of the params: 9.153195
                Loss: fixed  85 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17315182
Train loss (w/o reg) on all data: 0.16589566
Test loss (w/o reg) on all data: 0.08645225
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7049471e-05
Norm of the params: 12.046718
              Random: fixed  10 labels. Loss 0.08645. Accuracy 0.985.
### Flips: 156, rs: 26, checks: 208
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729404
Test loss (w/o reg) on all data: 0.0120552275
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5034439e-07
Norm of the params: 9.153228
     Influence (LOO): fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729413
Test loss (w/o reg) on all data: 0.012055273
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4099025e-07
Norm of the params: 9.153228
                Loss: fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1713768
Train loss (w/o reg) on all data: 0.1642778
Test loss (w/o reg) on all data: 0.08398613
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.3716075e-05
Norm of the params: 11.915523
              Random: fixed  13 labels. Loss 0.08399. Accuracy 0.985.
### Flips: 156, rs: 26, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729865
Test loss (w/o reg) on all data: 0.012055296
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0831558e-07
Norm of the params: 9.153178
     Influence (LOO): fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172981
Test loss (w/o reg) on all data: 0.012055457
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.173789e-07
Norm of the params: 9.153184
                Loss: fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16714312
Train loss (w/o reg) on all data: 0.15992378
Test loss (w/o reg) on all data: 0.08530658
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9360108e-05
Norm of the params: 12.016105
              Random: fixed  15 labels. Loss 0.08531. Accuracy 0.981.
### Flips: 156, rs: 26, checks: 312
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172968
Test loss (w/o reg) on all data: 0.012055361
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1296352e-06
Norm of the params: 9.153196
     Influence (LOO): fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729681
Test loss (w/o reg) on all data: 0.0120555675
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3030058e-07
Norm of the params: 9.153196
                Loss: fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15440793
Train loss (w/o reg) on all data: 0.14641818
Test loss (w/o reg) on all data: 0.084882595
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3596315e-05
Norm of the params: 12.641001
              Random: fixed  20 labels. Loss 0.08488. Accuracy 0.977.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18195662
Train loss (w/o reg) on all data: 0.174421
Test loss (w/o reg) on all data: 0.101685196
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3718163e-05
Norm of the params: 12.276502
Flipped loss: 0.10169. Accuracy: 0.985
### Flips: 156, rs: 27, checks: 52
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07918815
Train loss (w/o reg) on all data: 0.070303194
Test loss (w/o reg) on all data: 0.04893058
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.4903614e-06
Norm of the params: 13.330389
     Influence (LOO): fixed  45 labels. Loss 0.04893. Accuracy 0.989.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051793866
Train loss (w/o reg) on all data: 0.037203044
Test loss (w/o reg) on all data: 0.04359865
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.6124295e-06
Norm of the params: 17.082636
                Loss: fixed  50 labels. Loss 0.04360. Accuracy 0.989.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17602845
Train loss (w/o reg) on all data: 0.16829406
Test loss (w/o reg) on all data: 0.09807531
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.052157e-06
Norm of the params: 12.437351
              Random: fixed   4 labels. Loss 0.09808. Accuracy 0.981.
### Flips: 156, rs: 27, checks: 104
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033779293
Train loss (w/o reg) on all data: 0.025588693
Test loss (w/o reg) on all data: 0.021311577
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.168518e-06
Norm of the params: 12.798905
     Influence (LOO): fixed  65 labels. Loss 0.02131. Accuracy 0.992.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008135273
Train loss (w/o reg) on all data: 0.0032026886
Test loss (w/o reg) on all data: 0.012199009
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3655979e-07
Norm of the params: 9.932356
                Loss: fixed  76 labels. Loss 0.01220. Accuracy 0.996.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17349257
Train loss (w/o reg) on all data: 0.16582622
Test loss (w/o reg) on all data: 0.09490332
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6274884e-05
Norm of the params: 12.382529
              Random: fixed   6 labels. Loss 0.09490. Accuracy 0.985.
### Flips: 156, rs: 27, checks: 156
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730016
Test loss (w/o reg) on all data: 0.012054855
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3653747e-07
Norm of the params: 9.153161
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008135272
Train loss (w/o reg) on all data: 0.0032027245
Test loss (w/o reg) on all data: 0.01219873
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.2399884e-07
Norm of the params: 9.932319
                Loss: fixed  76 labels. Loss 0.01220. Accuracy 0.996.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17011286
Train loss (w/o reg) on all data: 0.16224548
Test loss (w/o reg) on all data: 0.09215435
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5828315e-05
Norm of the params: 12.543831
              Random: fixed   9 labels. Loss 0.09215. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 208
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021728752
Test loss (w/o reg) on all data: 0.012054662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0094844e-07
Norm of the params: 9.153298
     Influence (LOO): fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728755
Test loss (w/o reg) on all data: 0.012054619
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1632681e-07
Norm of the params: 9.153296
                Loss: fixed  77 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15621345
Train loss (w/o reg) on all data: 0.1480957
Test loss (w/o reg) on all data: 0.0789934
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.6209971e-05
Norm of the params: 12.741861
              Random: fixed  14 labels. Loss 0.07899. Accuracy 0.985.
### Flips: 156, rs: 27, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730459
Test loss (w/o reg) on all data: 0.012055103
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.551226e-07
Norm of the params: 9.1531105
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173046
Test loss (w/o reg) on all data: 0.012055262
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4004956e-07
Norm of the params: 9.153111
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14496852
Train loss (w/o reg) on all data: 0.13589919
Test loss (w/o reg) on all data: 0.07570649
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.118179e-06
Norm of the params: 13.467993
              Random: fixed  19 labels. Loss 0.07571. Accuracy 0.989.
### Flips: 156, rs: 27, checks: 312
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729497
Test loss (w/o reg) on all data: 0.012055205
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5376294e-07
Norm of the params: 9.153218
     Influence (LOO): fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729493
Test loss (w/o reg) on all data: 0.012055107
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6964707e-07
Norm of the params: 9.153217
                Loss: fixed  77 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13688073
Train loss (w/o reg) on all data: 0.12721269
Test loss (w/o reg) on all data: 0.07590111
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.7396604e-06
Norm of the params: 13.905419
              Random: fixed  23 labels. Loss 0.07590. Accuracy 0.989.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18409508
Train loss (w/o reg) on all data: 0.17793778
Test loss (w/o reg) on all data: 0.09719833
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5295871e-05
Norm of the params: 11.09712
Flipped loss: 0.09720. Accuracy: 0.977
### Flips: 156, rs: 28, checks: 52
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.069420666
Train loss (w/o reg) on all data: 0.059860516
Test loss (w/o reg) on all data: 0.05502651
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.03778275e-05
Norm of the params: 13.827617
     Influence (LOO): fixed  43 labels. Loss 0.05503. Accuracy 0.981.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03842128
Train loss (w/o reg) on all data: 0.025031507
Test loss (w/o reg) on all data: 0.051472884
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.157233e-06
Norm of the params: 16.364458
                Loss: fixed  51 labels. Loss 0.05147. Accuracy 0.981.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17600884
Train loss (w/o reg) on all data: 0.16975221
Test loss (w/o reg) on all data: 0.0932449
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5581976e-05
Norm of the params: 11.186268
              Random: fixed   4 labels. Loss 0.09324. Accuracy 0.977.
### Flips: 156, rs: 28, checks: 104
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01983741
Train loss (w/o reg) on all data: 0.011917574
Test loss (w/o reg) on all data: 0.02636564
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6456186e-06
Norm of the params: 12.585576
     Influence (LOO): fixed  66 labels. Loss 0.02637. Accuracy 0.989.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007863149
Train loss (w/o reg) on all data: 0.0029497503
Test loss (w/o reg) on all data: 0.013346931
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0523089e-06
Norm of the params: 9.91302
                Loss: fixed  73 labels. Loss 0.01335. Accuracy 0.992.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16708755
Train loss (w/o reg) on all data: 0.16036905
Test loss (w/o reg) on all data: 0.080028065
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.185589e-06
Norm of the params: 11.591804
              Random: fixed   9 labels. Loss 0.08003. Accuracy 0.985.
### Flips: 156, rs: 28, checks: 156
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007680513
Train loss (w/o reg) on all data: 0.003056856
Test loss (w/o reg) on all data: 0.014216035
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8909277e-07
Norm of the params: 9.616296
     Influence (LOO): fixed  73 labels. Loss 0.01422. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007863149
Train loss (w/o reg) on all data: 0.002949641
Test loss (w/o reg) on all data: 0.013348329
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.111794e-07
Norm of the params: 9.913132
                Loss: fixed  73 labels. Loss 0.01335. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16368754
Train loss (w/o reg) on all data: 0.15705009
Test loss (w/o reg) on all data: 0.077250645
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.5513927e-05
Norm of the params: 11.521684
              Random: fixed  11 labels. Loss 0.07725. Accuracy 0.985.
### Flips: 156, rs: 28, checks: 208
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012054569
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.506915e-07
Norm of the params: 9.153204
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729642
Test loss (w/o reg) on all data: 0.012054522
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.20079e-07
Norm of the params: 9.153204
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16084127
Train loss (w/o reg) on all data: 0.15418614
Test loss (w/o reg) on all data: 0.07447959
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.2030173e-05
Norm of the params: 11.537003
              Random: fixed  13 labels. Loss 0.07448. Accuracy 0.985.
### Flips: 156, rs: 28, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729479
Test loss (w/o reg) on all data: 0.012054947
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5777427e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172947
Test loss (w/o reg) on all data: 0.012054982
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2299601e-07
Norm of the params: 9.153221
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1477847
Train loss (w/o reg) on all data: 0.13905133
Test loss (w/o reg) on all data: 0.07926829
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4283854e-05
Norm of the params: 13.216173
              Random: fixed  16 labels. Loss 0.07927. Accuracy 0.981.
### Flips: 156, rs: 28, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728761
Test loss (w/o reg) on all data: 0.012054586
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.976208e-07
Norm of the params: 9.153298
     Influence (LOO): fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021728803
Test loss (w/o reg) on all data: 0.012054691
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.064847e-07
Norm of the params: 9.153294
                Loss: fixed  74 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14295894
Train loss (w/o reg) on all data: 0.13405494
Test loss (w/o reg) on all data: 0.07650055
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0827274e-05
Norm of the params: 13.344656
              Random: fixed  18 labels. Loss 0.07650. Accuracy 0.981.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18994065
Train loss (w/o reg) on all data: 0.1836291
Test loss (w/o reg) on all data: 0.09395562
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.535777e-05
Norm of the params: 11.235253
Flipped loss: 0.09396. Accuracy: 0.981
### Flips: 156, rs: 29, checks: 52
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08417963
Train loss (w/o reg) on all data: 0.07408128
Test loss (w/o reg) on all data: 0.06372987
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.774462e-06
Norm of the params: 14.211511
     Influence (LOO): fixed  46 labels. Loss 0.06373. Accuracy 0.977.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046843037
Train loss (w/o reg) on all data: 0.032343395
Test loss (w/o reg) on all data: 0.05227781
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.7990047e-06
Norm of the params: 17.029177
                Loss: fixed  51 labels. Loss 0.05228. Accuracy 0.969.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18645464
Train loss (w/o reg) on all data: 0.18034565
Test loss (w/o reg) on all data: 0.085283145
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9614452e-05
Norm of the params: 11.053495
              Random: fixed   4 labels. Loss 0.08528. Accuracy 0.989.
### Flips: 156, rs: 29, checks: 104
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027321361
Train loss (w/o reg) on all data: 0.020079423
Test loss (w/o reg) on all data: 0.03764453
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.64956e-07
Norm of the params: 12.034898
     Influence (LOO): fixed  70 labels. Loss 0.03764. Accuracy 0.985.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009918953
Train loss (w/o reg) on all data: 0.0039513046
Test loss (w/o reg) on all data: 0.015237276
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2785235e-07
Norm of the params: 10.924879
                Loss: fixed  76 labels. Loss 0.01524. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16957575
Train loss (w/o reg) on all data: 0.163281
Test loss (w/o reg) on all data: 0.08491681
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.3097037e-06
Norm of the params: 11.220305
              Random: fixed  13 labels. Loss 0.08492. Accuracy 0.981.
### Flips: 156, rs: 29, checks: 156
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010979189
Train loss (w/o reg) on all data: 0.0054254406
Test loss (w/o reg) on all data: 0.010200726
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.8978367e-07
Norm of the params: 10.539211
     Influence (LOO): fixed  76 labels. Loss 0.01020. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730163
Test loss (w/o reg) on all data: 0.012055059
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.499006e-07
Norm of the params: 9.153146
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16413686
Train loss (w/o reg) on all data: 0.15716903
Test loss (w/o reg) on all data: 0.08659394
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2092656e-05
Norm of the params: 11.804933
              Random: fixed  14 labels. Loss 0.08659. Accuracy 0.981.
### Flips: 156, rs: 29, checks: 208
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007525196
Train loss (w/o reg) on all data: 0.0028545647
Test loss (w/o reg) on all data: 0.0121523915
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.035014e-07
Norm of the params: 9.665021
     Influence (LOO): fixed  78 labels. Loss 0.01215. Accuracy 0.992.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172967
Test loss (w/o reg) on all data: 0.012055169
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.584016e-07
Norm of the params: 9.153199
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15777801
Train loss (w/o reg) on all data: 0.15062144
Test loss (w/o reg) on all data: 0.08320797
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5000849e-05
Norm of the params: 11.963748
              Random: fixed  17 labels. Loss 0.08321. Accuracy 0.985.
### Flips: 156, rs: 29, checks: 260
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729516
Test loss (w/o reg) on all data: 0.012054734
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.6567566e-07
Norm of the params: 9.153215
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.012054869
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3472715e-07
Norm of the params: 9.153214
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15531601
Train loss (w/o reg) on all data: 0.14816041
Test loss (w/o reg) on all data: 0.08218123
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.189873e-06
Norm of the params: 11.96294
              Random: fixed  19 labels. Loss 0.08218. Accuracy 0.985.
### Flips: 156, rs: 29, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729635
Test loss (w/o reg) on all data: 0.012054669
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4164172e-07
Norm of the params: 9.153202
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729646
Test loss (w/o reg) on all data: 0.012054755
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.055948e-07
Norm of the params: 9.153201
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14765964
Train loss (w/o reg) on all data: 0.14054519
Test loss (w/o reg) on all data: 0.0755565
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0615088e-05
Norm of the params: 11.9285
              Random: fixed  23 labels. Loss 0.07556. Accuracy 0.989.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17031491
Train loss (w/o reg) on all data: 0.16207732
Test loss (w/o reg) on all data: 0.09818497
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1955476e-05
Norm of the params: 12.835558
Flipped loss: 0.09818. Accuracy: 0.966
### Flips: 156, rs: 30, checks: 52
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062473506
Train loss (w/o reg) on all data: 0.0501933
Test loss (w/o reg) on all data: 0.08713077
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.2951624e-06
Norm of the params: 15.671764
     Influence (LOO): fixed  44 labels. Loss 0.08713. Accuracy 0.966.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03870931
Train loss (w/o reg) on all data: 0.025528619
Test loss (w/o reg) on all data: 0.07278169
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.4145554e-06
Norm of the params: 16.236189
                Loss: fixed  51 labels. Loss 0.07278. Accuracy 0.969.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16931963
Train loss (w/o reg) on all data: 0.16123869
Test loss (w/o reg) on all data: 0.097543284
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.6119207e-05
Norm of the params: 12.712939
              Random: fixed   1 labels. Loss 0.09754. Accuracy 0.966.
### Flips: 156, rs: 30, checks: 104
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026551742
Train loss (w/o reg) on all data: 0.017803565
Test loss (w/o reg) on all data: 0.02999078
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.21749e-07
Norm of the params: 13.227379
     Influence (LOO): fixed  64 labels. Loss 0.02999. Accuracy 0.992.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010361824
Train loss (w/o reg) on all data: 0.0043877508
Test loss (w/o reg) on all data: 0.020510692
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.943394e-07
Norm of the params: 10.9307575
                Loss: fixed  72 labels. Loss 0.02051. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16315696
Train loss (w/o reg) on all data: 0.15504545
Test loss (w/o reg) on all data: 0.090857916
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0205546e-05
Norm of the params: 12.736966
              Random: fixed   5 labels. Loss 0.09086. Accuracy 0.977.
### Flips: 156, rs: 30, checks: 156
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010091348
Train loss (w/o reg) on all data: 0.0049317647
Test loss (w/o reg) on all data: 0.011900645
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.2053656e-07
Norm of the params: 10.15833
     Influence (LOO): fixed  72 labels. Loss 0.01190. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0069710156
Train loss (w/o reg) on all data: 0.002611277
Test loss (w/o reg) on all data: 0.0116768535
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.35086e-08
Norm of the params: 9.337814
                Loss: fixed  74 labels. Loss 0.01168. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1528146
Train loss (w/o reg) on all data: 0.14478403
Test loss (w/o reg) on all data: 0.089678094
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.1825175e-05
Norm of the params: 12.67325
              Random: fixed  11 labels. Loss 0.08968. Accuracy 0.969.
### Flips: 156, rs: 30, checks: 208
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729933
Test loss (w/o reg) on all data: 0.012054401
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7739306e-07
Norm of the params: 9.153171
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729902
Test loss (w/o reg) on all data: 0.0120544005
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.731208e-07
Norm of the params: 9.153172
                Loss: fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15071508
Train loss (w/o reg) on all data: 0.1433955
Test loss (w/o reg) on all data: 0.088167466
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.91195e-06
Norm of the params: 12.099239
              Random: fixed  14 labels. Loss 0.08817. Accuracy 0.969.
### Flips: 156, rs: 30, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729001
Test loss (w/o reg) on all data: 0.012054543
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.208801e-07
Norm of the params: 9.153272
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729025
Test loss (w/o reg) on all data: 0.012054621
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2992695e-07
Norm of the params: 9.153271
                Loss: fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14724946
Train loss (w/o reg) on all data: 0.13980353
Test loss (w/o reg) on all data: 0.08359488
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.2743534e-05
Norm of the params: 12.203226
              Random: fixed  16 labels. Loss 0.08359. Accuracy 0.981.
### Flips: 156, rs: 30, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729462
Test loss (w/o reg) on all data: 0.012054504
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.33382e-07
Norm of the params: 9.153221
     Influence (LOO): fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729479
Test loss (w/o reg) on all data: 0.012054475
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0694446e-07
Norm of the params: 9.153218
                Loss: fixed  75 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14447735
Train loss (w/o reg) on all data: 0.13698713
Test loss (w/o reg) on all data: 0.07820987
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0299542e-05
Norm of the params: 12.239457
              Random: fixed  19 labels. Loss 0.07821. Accuracy 0.981.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18656744
Train loss (w/o reg) on all data: 0.17982626
Test loss (w/o reg) on all data: 0.11838377
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.236959e-05
Norm of the params: 11.611355
Flipped loss: 0.11838. Accuracy: 0.962
### Flips: 156, rs: 31, checks: 52
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07738951
Train loss (w/o reg) on all data: 0.06579791
Test loss (w/o reg) on all data: 0.07864795
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.5402273e-06
Norm of the params: 15.22603
     Influence (LOO): fixed  45 labels. Loss 0.07865. Accuracy 0.969.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04649019
Train loss (w/o reg) on all data: 0.032351945
Test loss (w/o reg) on all data: 0.08179048
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1004332e-05
Norm of the params: 16.815615
                Loss: fixed  52 labels. Loss 0.08179. Accuracy 0.969.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17188317
Train loss (w/o reg) on all data: 0.16467279
Test loss (w/o reg) on all data: 0.1098694
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.0685988e-05
Norm of the params: 12.008639
              Random: fixed   6 labels. Loss 0.10987. Accuracy 0.966.
### Flips: 156, rs: 31, checks: 104
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031845495
Train loss (w/o reg) on all data: 0.022175439
Test loss (w/o reg) on all data: 0.034224693
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3595233e-06
Norm of the params: 13.906874
     Influence (LOO): fixed  69 labels. Loss 0.03422. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011378145
Train loss (w/o reg) on all data: 0.0053015687
Test loss (w/o reg) on all data: 0.016291237
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.353419e-07
Norm of the params: 11.024134
                Loss: fixed  77 labels. Loss 0.01629. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16589296
Train loss (w/o reg) on all data: 0.15878594
Test loss (w/o reg) on all data: 0.09977447
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.977983e-06
Norm of the params: 11.922262
              Random: fixed  12 labels. Loss 0.09977. Accuracy 0.969.
### Flips: 156, rs: 31, checks: 156
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012415407
Train loss (w/o reg) on all data: 0.0062918966
Test loss (w/o reg) on all data: 0.014231344
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1492209e-06
Norm of the params: 11.066627
     Influence (LOO): fixed  79 labels. Loss 0.01423. Accuracy 0.996.
Using normal model
LBFGS training took [44] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007596102
Train loss (w/o reg) on all data: 0.0028792461
Test loss (w/o reg) on all data: 0.013193449
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.948733e-08
Norm of the params: 9.712729
                Loss: fixed  80 labels. Loss 0.01319. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15660232
Train loss (w/o reg) on all data: 0.14923486
Test loss (w/o reg) on all data: 0.087518156
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3775365e-05
Norm of the params: 12.138748
              Random: fixed  17 labels. Loss 0.08752. Accuracy 0.969.
### Flips: 156, rs: 31, checks: 208
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.012054388
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.941411e-07
Norm of the params: 9.153224
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007596101
Train loss (w/o reg) on all data: 0.0028792773
Test loss (w/o reg) on all data: 0.013195362
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.67045e-07
Norm of the params: 9.712696
                Loss: fixed  80 labels. Loss 0.01320. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15027337
Train loss (w/o reg) on all data: 0.14287664
Test loss (w/o reg) on all data: 0.081261754
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.728586e-06
Norm of the params: 12.162834
              Random: fixed  21 labels. Loss 0.08126. Accuracy 0.981.
### Flips: 156, rs: 31, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172884
Test loss (w/o reg) on all data: 0.012054085
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6669183e-07
Norm of the params: 9.153291
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075961007
Train loss (w/o reg) on all data: 0.0028791428
Test loss (w/o reg) on all data: 0.01319447
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9655218e-07
Norm of the params: 9.712835
                Loss: fixed  80 labels. Loss 0.01319. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13962679
Train loss (w/o reg) on all data: 0.13173138
Test loss (w/o reg) on all data: 0.07760595
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.721436e-06
Norm of the params: 12.566158
              Random: fixed  26 labels. Loss 0.07761. Accuracy 0.977.
### Flips: 156, rs: 31, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729583
Test loss (w/o reg) on all data: 0.01205432
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8516996e-07
Norm of the params: 9.153207
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172958
Test loss (w/o reg) on all data: 0.012054276
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3141911e-07
Norm of the params: 9.153207
                Loss: fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13585363
Train loss (w/o reg) on all data: 0.12774648
Test loss (w/o reg) on all data: 0.07378377
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.271093e-06
Norm of the params: 12.7335415
              Random: fixed  28 labels. Loss 0.07378. Accuracy 0.977.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1911694
Train loss (w/o reg) on all data: 0.18312724
Test loss (w/o reg) on all data: 0.103243046
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.816509e-05
Norm of the params: 12.682391
Flipped loss: 0.10324. Accuracy: 0.989
### Flips: 156, rs: 32, checks: 52
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080496795
Train loss (w/o reg) on all data: 0.070142195
Test loss (w/o reg) on all data: 0.05662394
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.9752437e-06
Norm of the params: 14.390694
     Influence (LOO): fixed  46 labels. Loss 0.05662. Accuracy 0.981.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05463996
Train loss (w/o reg) on all data: 0.04118511
Test loss (w/o reg) on all data: 0.0812346
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.615605e-06
Norm of the params: 16.404179
                Loss: fixed  52 labels. Loss 0.08123. Accuracy 0.966.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19095074
Train loss (w/o reg) on all data: 0.18312718
Test loss (w/o reg) on all data: 0.10198047
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0721101e-05
Norm of the params: 12.508842
              Random: fixed   1 labels. Loss 0.10198. Accuracy 0.985.
### Flips: 156, rs: 32, checks: 104
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031026078
Train loss (w/o reg) on all data: 0.023467384
Test loss (w/o reg) on all data: 0.041712217
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.409252e-06
Norm of the params: 12.2952795
     Influence (LOO): fixed  70 labels. Loss 0.04171. Accuracy 0.977.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009896103
Train loss (w/o reg) on all data: 0.0040546665
Test loss (w/o reg) on all data: 0.0192
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.61198e-07
Norm of the params: 10.808734
                Loss: fixed  78 labels. Loss 0.01920. Accuracy 0.989.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18164451
Train loss (w/o reg) on all data: 0.17397232
Test loss (w/o reg) on all data: 0.09772251
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8386329e-05
Norm of the params: 12.387249
              Random: fixed   5 labels. Loss 0.09772. Accuracy 0.981.
### Flips: 156, rs: 32, checks: 156
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010038184
Train loss (w/o reg) on all data: 0.004576522
Test loss (w/o reg) on all data: 0.0143431015
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4397017e-07
Norm of the params: 10.451471
     Influence (LOO): fixed  79 labels. Loss 0.01434. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008696733
Train loss (w/o reg) on all data: 0.0033814525
Test loss (w/o reg) on all data: 0.017603554
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.866254e-07
Norm of the params: 10.310462
                Loss: fixed  79 labels. Loss 0.01760. Accuracy 0.989.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17075014
Train loss (w/o reg) on all data: 0.16258325
Test loss (w/o reg) on all data: 0.08939914
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9021158e-05
Norm of the params: 12.780375
              Random: fixed  10 labels. Loss 0.08940. Accuracy 0.985.
### Flips: 156, rs: 32, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729735
Test loss (w/o reg) on all data: 0.012054991
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.266427e-07
Norm of the params: 9.15319
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00765187
Train loss (w/o reg) on all data: 0.0027776936
Test loss (w/o reg) on all data: 0.014519362
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1754472e-07
Norm of the params: 9.873375
                Loss: fixed  80 labels. Loss 0.01452. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16089699
Train loss (w/o reg) on all data: 0.15273595
Test loss (w/o reg) on all data: 0.081150174
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9332323e-05
Norm of the params: 12.77579
              Random: fixed  16 labels. Loss 0.08115. Accuracy 0.985.
### Flips: 156, rs: 32, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172965
Test loss (w/o reg) on all data: 0.012054887
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6363946e-07
Norm of the params: 9.153201
     Influence (LOO): fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729656
Test loss (w/o reg) on all data: 0.012054849
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1684281e-07
Norm of the params: 9.153201
                Loss: fixed  81 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14940841
Train loss (w/o reg) on all data: 0.14034484
Test loss (w/o reg) on all data: 0.080242865
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7783499e-05
Norm of the params: 13.463714
              Random: fixed  20 labels. Loss 0.08024. Accuracy 0.985.
### Flips: 156, rs: 32, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729504
Test loss (w/o reg) on all data: 0.012055082
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.696532e-08
Norm of the params: 9.153217
     Influence (LOO): fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729507
Test loss (w/o reg) on all data: 0.012055108
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2603788e-07
Norm of the params: 9.153217
                Loss: fixed  81 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14588247
Train loss (w/o reg) on all data: 0.1368046
Test loss (w/o reg) on all data: 0.08005178
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4696082e-05
Norm of the params: 13.474331
              Random: fixed  22 labels. Loss 0.08005. Accuracy 0.981.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16910188
Train loss (w/o reg) on all data: 0.15920419
Test loss (w/o reg) on all data: 0.13465437
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.1297723e-05
Norm of the params: 14.069613
Flipped loss: 0.13465. Accuracy: 0.962
### Flips: 156, rs: 33, checks: 52
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06707215
Train loss (w/o reg) on all data: 0.05504287
Test loss (w/o reg) on all data: 0.090797655
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.1080663e-06
Norm of the params: 15.510824
     Influence (LOO): fixed  44 labels. Loss 0.09080. Accuracy 0.969.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046829075
Train loss (w/o reg) on all data: 0.031535167
Test loss (w/o reg) on all data: 0.07707638
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.3550648e-06
Norm of the params: 17.489372
                Loss: fixed  51 labels. Loss 0.07708. Accuracy 0.969.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15726502
Train loss (w/o reg) on all data: 0.14661644
Test loss (w/o reg) on all data: 0.1256592
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.5086953e-05
Norm of the params: 14.593545
              Random: fixed   6 labels. Loss 0.12566. Accuracy 0.966.
### Flips: 156, rs: 33, checks: 104
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029501738
Train loss (w/o reg) on all data: 0.020853458
Test loss (w/o reg) on all data: 0.030351901
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.436492e-06
Norm of the params: 13.151638
     Influence (LOO): fixed  64 labels. Loss 0.03035. Accuracy 0.992.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012891633
Train loss (w/o reg) on all data: 0.0059434515
Test loss (w/o reg) on all data: 0.02379009
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9596112e-06
Norm of the params: 11.788284
                Loss: fixed  72 labels. Loss 0.02379. Accuracy 0.985.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14931816
Train loss (w/o reg) on all data: 0.13800819
Test loss (w/o reg) on all data: 0.11983699
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.0071868e-05
Norm of the params: 15.0399275
              Random: fixed  11 labels. Loss 0.11984. Accuracy 0.969.
### Flips: 156, rs: 33, checks: 156
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009703865
Train loss (w/o reg) on all data: 0.0041495985
Test loss (w/o reg) on all data: 0.015745234
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.013529e-07
Norm of the params: 10.539702
     Influence (LOO): fixed  74 labels. Loss 0.01575. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009502715
Train loss (w/o reg) on all data: 0.0038705044
Test loss (w/o reg) on all data: 0.014819682
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0251936e-07
Norm of the params: 10.613398
                Loss: fixed  74 labels. Loss 0.01482. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14742497
Train loss (w/o reg) on all data: 0.1358137
Test loss (w/o reg) on all data: 0.12077183
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.207924e-05
Norm of the params: 15.238937
              Random: fixed  12 labels. Loss 0.12077. Accuracy 0.973.
### Flips: 156, rs: 33, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730403
Test loss (w/o reg) on all data: 0.012054793
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5531824e-07
Norm of the params: 9.153118
     Influence (LOO): fixed  76 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007258295
Train loss (w/o reg) on all data: 0.0025843864
Test loss (w/o reg) on all data: 0.013022718
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.136369e-07
Norm of the params: 9.668411
                Loss: fixed  75 labels. Loss 0.01302. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13967547
Train loss (w/o reg) on all data: 0.12779246
Test loss (w/o reg) on all data: 0.11785686
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.3082765e-06
Norm of the params: 15.416226
              Random: fixed  16 labels. Loss 0.11786. Accuracy 0.969.
### Flips: 156, rs: 33, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362017
Train loss (w/o reg) on all data: 0.0021729912
Test loss (w/o reg) on all data: 0.012054958
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5086862e-07
Norm of the params: 9.15317
     Influence (LOO): fixed  76 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0072582965
Train loss (w/o reg) on all data: 0.0025844635
Test loss (w/o reg) on all data: 0.013023258
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8846289e-07
Norm of the params: 9.668333
                Loss: fixed  75 labels. Loss 0.01302. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13148308
Train loss (w/o reg) on all data: 0.11906163
Test loss (w/o reg) on all data: 0.11222519
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.757235e-06
Norm of the params: 15.761636
              Random: fixed  19 labels. Loss 0.11223. Accuracy 0.966.
### Flips: 156, rs: 33, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729644
Test loss (w/o reg) on all data: 0.0120550115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.170313e-08
Norm of the params: 9.1532
     Influence (LOO): fixed  76 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729665
Test loss (w/o reg) on all data: 0.012054966
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.613154e-08
Norm of the params: 9.1532
                Loss: fixed  76 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12294656
Train loss (w/o reg) on all data: 0.110604376
Test loss (w/o reg) on all data: 0.10878406
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.3096355e-05
Norm of the params: 15.711262
              Random: fixed  23 labels. Loss 0.10878. Accuracy 0.962.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18653679
Train loss (w/o reg) on all data: 0.17777546
Test loss (w/o reg) on all data: 0.15589541
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 9.267633e-06
Norm of the params: 13.237317
Flipped loss: 0.15590. Accuracy: 0.943
### Flips: 156, rs: 34, checks: 52
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09303146
Train loss (w/o reg) on all data: 0.08091935
Test loss (w/o reg) on all data: 0.10981911
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.2113616e-05
Norm of the params: 15.564134
     Influence (LOO): fixed  40 labels. Loss 0.10982. Accuracy 0.954.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06528924
Train loss (w/o reg) on all data: 0.05159457
Test loss (w/o reg) on all data: 0.110663004
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.1938894e-06
Norm of the params: 16.549725
                Loss: fixed  50 labels. Loss 0.11066. Accuracy 0.966.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17636755
Train loss (w/o reg) on all data: 0.16726814
Test loss (w/o reg) on all data: 0.13657574
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.054834e-06
Norm of the params: 13.490301
              Random: fixed   7 labels. Loss 0.13658. Accuracy 0.966.
### Flips: 156, rs: 34, checks: 104
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046452537
Train loss (w/o reg) on all data: 0.035375264
Test loss (w/o reg) on all data: 0.068914734
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3790037e-06
Norm of the params: 14.884404
     Influence (LOO): fixed  63 labels. Loss 0.06891. Accuracy 0.969.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014579985
Train loss (w/o reg) on all data: 0.0066795032
Test loss (w/o reg) on all data: 0.026394
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.681815e-06
Norm of the params: 12.5701885
                Loss: fixed  79 labels. Loss 0.02639. Accuracy 0.992.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1711016
Train loss (w/o reg) on all data: 0.16201933
Test loss (w/o reg) on all data: 0.13260965
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5205757e-05
Norm of the params: 13.4775915
              Random: fixed   9 labels. Loss 0.13261. Accuracy 0.969.
### Flips: 156, rs: 34, checks: 156
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016433284
Train loss (w/o reg) on all data: 0.008906512
Test loss (w/o reg) on all data: 0.0253176
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.877021e-07
Norm of the params: 12.269289
     Influence (LOO): fixed  79 labels. Loss 0.02532. Accuracy 0.989.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0082290135
Train loss (w/o reg) on all data: 0.0030722595
Test loss (w/o reg) on all data: 0.016711075
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1518491e-06
Norm of the params: 10.155543
                Loss: fixed  84 labels. Loss 0.01671. Accuracy 0.989.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17063247
Train loss (w/o reg) on all data: 0.16166271
Test loss (w/o reg) on all data: 0.12954585
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.39357e-05
Norm of the params: 13.393849
              Random: fixed  10 labels. Loss 0.12955. Accuracy 0.966.
### Flips: 156, rs: 34, checks: 208
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075251954
Train loss (w/o reg) on all data: 0.0028546741
Test loss (w/o reg) on all data: 0.01215276
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8886967e-07
Norm of the params: 9.664907
     Influence (LOO): fixed  84 labels. Loss 0.01215. Accuracy 0.992.
Using normal model
LBFGS training took [28] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729895
Test loss (w/o reg) on all data: 0.012055046
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1168515e-07
Norm of the params: 9.153174
                Loss: fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16177793
Train loss (w/o reg) on all data: 0.15322262
Test loss (w/o reg) on all data: 0.111675195
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1139412e-05
Norm of the params: 13.080757
              Random: fixed  18 labels. Loss 0.11168. Accuracy 0.973.
### Flips: 156, rs: 34, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730375
Test loss (w/o reg) on all data: 0.012055391
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.4608207e-07
Norm of the params: 9.153122
     Influence (LOO): fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.002173036
Test loss (w/o reg) on all data: 0.012055268
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3900328e-07
Norm of the params: 9.153122
                Loss: fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15925148
Train loss (w/o reg) on all data: 0.15061843
Test loss (w/o reg) on all data: 0.10274704
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.4553592e-05
Norm of the params: 13.140053
              Random: fixed  21 labels. Loss 0.10275. Accuracy 0.977.
### Flips: 156, rs: 34, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729567
Test loss (w/o reg) on all data: 0.012055363
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0291624e-07
Norm of the params: 9.15321
     Influence (LOO): fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729565
Test loss (w/o reg) on all data: 0.0120553225
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.14484e-08
Norm of the params: 9.15321
                Loss: fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15445738
Train loss (w/o reg) on all data: 0.14567426
Test loss (w/o reg) on all data: 0.096230865
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.7953895e-05
Norm of the params: 13.253766
              Random: fixed  23 labels. Loss 0.09623. Accuracy 0.969.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17426942
Train loss (w/o reg) on all data: 0.16537587
Test loss (w/o reg) on all data: 0.11398554
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.05199415e-05
Norm of the params: 13.336825
Flipped loss: 0.11399. Accuracy: 0.962
### Flips: 156, rs: 35, checks: 52
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083904184
Train loss (w/o reg) on all data: 0.073916346
Test loss (w/o reg) on all data: 0.056203555
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.8406855e-06
Norm of the params: 14.1335335
     Influence (LOO): fixed  44 labels. Loss 0.05620. Accuracy 0.985.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.049609218
Train loss (w/o reg) on all data: 0.03411312
Test loss (w/o reg) on all data: 0.042298153
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1038751e-05
Norm of the params: 17.6046
                Loss: fixed  51 labels. Loss 0.04230. Accuracy 0.981.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16357951
Train loss (w/o reg) on all data: 0.15419813
Test loss (w/o reg) on all data: 0.1074901
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.002935e-06
Norm of the params: 13.697724
              Random: fixed   7 labels. Loss 0.10749. Accuracy 0.973.
### Flips: 156, rs: 35, checks: 104
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022740666
Train loss (w/o reg) on all data: 0.015012969
Test loss (w/o reg) on all data: 0.024514522
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7728994e-06
Norm of the params: 12.431972
     Influence (LOO): fixed  71 labels. Loss 0.02451. Accuracy 0.992.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013843616
Train loss (w/o reg) on all data: 0.0060653533
Test loss (w/o reg) on all data: 0.015003531
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3994864e-06
Norm of the params: 12.47258
                Loss: fixed  72 labels. Loss 0.01500. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16206454
Train loss (w/o reg) on all data: 0.15264262
Test loss (w/o reg) on all data: 0.1083358
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.4334138e-05
Norm of the params: 13.727287
              Random: fixed   8 labels. Loss 0.10834. Accuracy 0.969.
### Flips: 156, rs: 35, checks: 156
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012032212
Train loss (w/o reg) on all data: 0.0065780873
Test loss (w/o reg) on all data: 0.015697278
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.383006e-07
Norm of the params: 10.444256
     Influence (LOO): fixed  76 labels. Loss 0.01570. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010223924
Train loss (w/o reg) on all data: 0.004208615
Test loss (w/o reg) on all data: 0.016681759
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8657116e-07
Norm of the params: 10.968417
                Loss: fixed  75 labels. Loss 0.01668. Accuracy 0.989.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15341601
Train loss (w/o reg) on all data: 0.14391509
Test loss (w/o reg) on all data: 0.11057377
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.4679997e-05
Norm of the params: 13.784716
              Random: fixed  12 labels. Loss 0.11057. Accuracy 0.969.
### Flips: 156, rs: 35, checks: 208
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011336297
Train loss (w/o reg) on all data: 0.0062861834
Test loss (w/o reg) on all data: 0.014850773
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5634815e-07
Norm of the params: 10.049989
     Influence (LOO): fixed  77 labels. Loss 0.01485. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008592669
Train loss (w/o reg) on all data: 0.0034231676
Test loss (w/o reg) on all data: 0.01278528
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.742167e-07
Norm of the params: 10.168088
                Loss: fixed  77 labels. Loss 0.01279. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14875834
Train loss (w/o reg) on all data: 0.13955669
Test loss (w/o reg) on all data: 0.10494409
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5876425e-05
Norm of the params: 13.565874
              Random: fixed  15 labels. Loss 0.10494. Accuracy 0.969.
### Flips: 156, rs: 35, checks: 260
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.002172951
Test loss (w/o reg) on all data: 0.01205466
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7224501e-07
Norm of the params: 9.153217
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007376435
Train loss (w/o reg) on all data: 0.0026222777
Test loss (w/o reg) on all data: 0.013233342
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5174183e-07
Norm of the params: 9.751059
                Loss: fixed  78 labels. Loss 0.01323. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13681197
Train loss (w/o reg) on all data: 0.12765048
Test loss (w/o reg) on all data: 0.099597365
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0162756e-05
Norm of the params: 13.536234
              Random: fixed  21 labels. Loss 0.09960. Accuracy 0.973.
### Flips: 156, rs: 35, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.0120544955
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9394206e-07
Norm of the params: 9.153227
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729418
Test loss (w/o reg) on all data: 0.01205443
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.00593894e-07
Norm of the params: 9.153226
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1321602
Train loss (w/o reg) on all data: 0.122948475
Test loss (w/o reg) on all data: 0.09409537
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.1148513e-05
Norm of the params: 13.573307
              Random: fixed  24 labels. Loss 0.09410. Accuracy 0.981.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19083166
Train loss (w/o reg) on all data: 0.18274632
Test loss (w/o reg) on all data: 0.11085634
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0386789e-05
Norm of the params: 12.716401
Flipped loss: 0.11086. Accuracy: 0.985
### Flips: 156, rs: 36, checks: 52
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08824535
Train loss (w/o reg) on all data: 0.07718599
Test loss (w/o reg) on all data: 0.07569233
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.9810912e-05
Norm of the params: 14.872364
     Influence (LOO): fixed  45 labels. Loss 0.07569. Accuracy 0.985.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05964477
Train loss (w/o reg) on all data: 0.044080697
Test loss (w/o reg) on all data: 0.07766171
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1803523e-05
Norm of the params: 17.64317
                Loss: fixed  51 labels. Loss 0.07766. Accuracy 0.966.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17710327
Train loss (w/o reg) on all data: 0.16886176
Test loss (w/o reg) on all data: 0.09984232
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1392683e-05
Norm of the params: 12.838616
              Random: fixed   8 labels. Loss 0.09984. Accuracy 0.985.
### Flips: 156, rs: 36, checks: 104
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03287964
Train loss (w/o reg) on all data: 0.023328383
Test loss (w/o reg) on all data: 0.04081267
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.9663965e-06
Norm of the params: 13.821183
     Influence (LOO): fixed  69 labels. Loss 0.04081. Accuracy 0.992.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011888904
Train loss (w/o reg) on all data: 0.005311327
Test loss (w/o reg) on all data: 0.015268465
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5240702e-06
Norm of the params: 11.469591
                Loss: fixed  81 labels. Loss 0.01527. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17312947
Train loss (w/o reg) on all data: 0.16496691
Test loss (w/o reg) on all data: 0.09279232
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.5529365e-06
Norm of the params: 12.77698
              Random: fixed  12 labels. Loss 0.09279. Accuracy 0.992.
### Flips: 156, rs: 36, checks: 156
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016424268
Train loss (w/o reg) on all data: 0.0092933215
Test loss (w/o reg) on all data: 0.02272652
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2702513e-06
Norm of the params: 11.942317
     Influence (LOO): fixed  80 labels. Loss 0.02273. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071353735
Train loss (w/o reg) on all data: 0.002526487
Test loss (w/o reg) on all data: 0.011917069
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8559554e-07
Norm of the params: 9.6009245
                Loss: fixed  84 labels. Loss 0.01192. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15976565
Train loss (w/o reg) on all data: 0.15105365
Test loss (w/o reg) on all data: 0.09073491
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.045941e-06
Norm of the params: 13.199997
              Random: fixed  18 labels. Loss 0.09073. Accuracy 0.985.
### Flips: 156, rs: 36, checks: 208
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008013478
Train loss (w/o reg) on all data: 0.0032720151
Test loss (w/o reg) on all data: 0.015295559
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9160696e-07
Norm of the params: 9.73803
     Influence (LOO): fixed  84 labels. Loss 0.01530. Accuracy 0.992.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362021
Train loss (w/o reg) on all data: 0.0021729637
Test loss (w/o reg) on all data: 0.012055237
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.5019836e-08
Norm of the params: 9.153204
                Loss: fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14475822
Train loss (w/o reg) on all data: 0.1348074
Test loss (w/o reg) on all data: 0.084544465
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.458336e-06
Norm of the params: 14.107323
              Random: fixed  24 labels. Loss 0.08454. Accuracy 0.985.
### Flips: 156, rs: 36, checks: 260
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729579
Test loss (w/o reg) on all data: 0.012055157
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0491005e-07
Norm of the params: 9.153208
     Influence (LOO): fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729562
Test loss (w/o reg) on all data: 0.012055189
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.536735e-08
Norm of the params: 9.153209
                Loss: fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13192785
Train loss (w/o reg) on all data: 0.12224859
Test loss (w/o reg) on all data: 0.07776472
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7119406e-05
Norm of the params: 13.913493
              Random: fixed  31 labels. Loss 0.07776. Accuracy 0.981.
### Flips: 156, rs: 36, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012055165
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.709942e-07
Norm of the params: 9.153203
     Influence (LOO): fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729623
Test loss (w/o reg) on all data: 0.012055107
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.09588704e-07
Norm of the params: 9.153202
                Loss: fixed  85 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12535588
Train loss (w/o reg) on all data: 0.11603299
Test loss (w/o reg) on all data: 0.070365205
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.406498e-06
Norm of the params: 13.654963
              Random: fixed  35 labels. Loss 0.07037. Accuracy 0.985.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17781249
Train loss (w/o reg) on all data: 0.16884749
Test loss (w/o reg) on all data: 0.12342998
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.738081e-06
Norm of the params: 13.3903
Flipped loss: 0.12343. Accuracy: 0.966
### Flips: 156, rs: 37, checks: 52
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08140843
Train loss (w/o reg) on all data: 0.07018491
Test loss (w/o reg) on all data: 0.071927875
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.154224e-06
Norm of the params: 14.982339
     Influence (LOO): fixed  41 labels. Loss 0.07193. Accuracy 0.973.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047249574
Train loss (w/o reg) on all data: 0.033091728
Test loss (w/o reg) on all data: 0.072292924
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.0048468e-06
Norm of the params: 16.827269
                Loss: fixed  52 labels. Loss 0.07229. Accuracy 0.977.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17185013
Train loss (w/o reg) on all data: 0.16345519
Test loss (w/o reg) on all data: 0.11691423
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4128221e-05
Norm of the params: 12.957583
              Random: fixed   3 labels. Loss 0.11691. Accuracy 0.973.
### Flips: 156, rs: 37, checks: 104
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029320281
Train loss (w/o reg) on all data: 0.02090733
Test loss (w/o reg) on all data: 0.033893753
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1049897e-06
Norm of the params: 12.971471
     Influence (LOO): fixed  69 labels. Loss 0.03389. Accuracy 0.989.
Using normal model
LBFGS training took [54] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074646436
Train loss (w/o reg) on all data: 0.0028154096
Test loss (w/o reg) on all data: 0.015516831
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5166506e-07
Norm of the params: 9.642857
                Loss: fixed  78 labels. Loss 0.01552. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16225384
Train loss (w/o reg) on all data: 0.1536483
Test loss (w/o reg) on all data: 0.111639194
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1799319e-05
Norm of the params: 13.1191025
              Random: fixed   9 labels. Loss 0.11164. Accuracy 0.977.
### Flips: 156, rs: 37, checks: 156
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011437273
Train loss (w/o reg) on all data: 0.00625685
Test loss (w/o reg) on all data: 0.016273107
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.97607e-07
Norm of the params: 10.178824
     Influence (LOO): fixed  77 labels. Loss 0.01627. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729693
Test loss (w/o reg) on all data: 0.012055132
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1761355e-07
Norm of the params: 9.153195
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1621572
Train loss (w/o reg) on all data: 0.15391083
Test loss (w/o reg) on all data: 0.10861549
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.224148e-05
Norm of the params: 12.842399
              Random: fixed  10 labels. Loss 0.10862. Accuracy 0.977.
### Flips: 156, rs: 37, checks: 208
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00921021
Train loss (w/o reg) on all data: 0.0044514006
Test loss (w/o reg) on all data: 0.013357779
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1615325e-07
Norm of the params: 9.755829
     Influence (LOO): fixed  78 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.00217283
Test loss (w/o reg) on all data: 0.012055319
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1322128e-06
Norm of the params: 9.153349
                Loss: fixed  79 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15234403
Train loss (w/o reg) on all data: 0.14389771
Test loss (w/o reg) on all data: 0.0974709
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.1399032e-06
Norm of the params: 12.997172
              Random: fixed  15 labels. Loss 0.09747. Accuracy 0.981.
### Flips: 156, rs: 37, checks: 260
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002173061
Test loss (w/o reg) on all data: 0.012053981
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.178453e-07
Norm of the params: 9.153095
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730603
Test loss (w/o reg) on all data: 0.012054112
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.358477e-07
Norm of the params: 9.153095
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14856853
Train loss (w/o reg) on all data: 0.14064223
Test loss (w/o reg) on all data: 0.08944939
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.0433177e-05
Norm of the params: 12.590714
              Random: fixed  18 labels. Loss 0.08945. Accuracy 0.985.
### Flips: 156, rs: 37, checks: 312
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729795
Test loss (w/o reg) on all data: 0.012054825
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9734009e-07
Norm of the params: 9.153185
     Influence (LOO): fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729777
Test loss (w/o reg) on all data: 0.0120548615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3012563e-07
Norm of the params: 9.153185
                Loss: fixed  79 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14127183
Train loss (w/o reg) on all data: 0.1330942
Test loss (w/o reg) on all data: 0.08472811
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4149905e-05
Norm of the params: 12.788758
              Random: fixed  21 labels. Loss 0.08473. Accuracy 0.981.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18022054
Train loss (w/o reg) on all data: 0.17436118
Test loss (w/o reg) on all data: 0.097113706
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8329569e-05
Norm of the params: 10.825305
Flipped loss: 0.09711. Accuracy: 0.989
### Flips: 156, rs: 38, checks: 52
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06356637
Train loss (w/o reg) on all data: 0.05437243
Test loss (w/o reg) on all data: 0.043524303
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.721793e-05
Norm of the params: 13.560193
     Influence (LOO): fixed  46 labels. Loss 0.04352. Accuracy 0.981.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03733752
Train loss (w/o reg) on all data: 0.025514076
Test loss (w/o reg) on all data: 0.060127564
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.545061e-06
Norm of the params: 15.377543
                Loss: fixed  52 labels. Loss 0.06013. Accuracy 0.973.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17658207
Train loss (w/o reg) on all data: 0.17035422
Test loss (w/o reg) on all data: 0.09361184
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.9830622e-05
Norm of the params: 11.160509
              Random: fixed   2 labels. Loss 0.09361. Accuracy 0.989.
### Flips: 156, rs: 38, checks: 104
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01812957
Train loss (w/o reg) on all data: 0.010916271
Test loss (w/o reg) on all data: 0.025147058
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.821778e-07
Norm of the params: 12.011078
     Influence (LOO): fixed  67 labels. Loss 0.02515. Accuracy 0.985.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729402
Test loss (w/o reg) on all data: 0.012054758
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7934984e-08
Norm of the params: 9.153227
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17252687
Train loss (w/o reg) on all data: 0.16615619
Test loss (w/o reg) on all data: 0.09212722
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.351256e-05
Norm of the params: 11.287763
              Random: fixed   4 labels. Loss 0.09213. Accuracy 0.985.
### Flips: 156, rs: 38, checks: 156
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0107171815
Train loss (w/o reg) on all data: 0.005500946
Test loss (w/o reg) on all data: 0.018973144
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0783025e-06
Norm of the params: 10.213947
     Influence (LOO): fixed  71 labels. Loss 0.01897. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729728
Test loss (w/o reg) on all data: 0.012054691
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.069753e-07
Norm of the params: 9.153193
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1609215
Train loss (w/o reg) on all data: 0.15383865
Test loss (w/o reg) on all data: 0.09327973
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.5930733e-05
Norm of the params: 11.901976
              Random: fixed   9 labels. Loss 0.09328. Accuracy 0.981.
### Flips: 156, rs: 38, checks: 208
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021730047
Test loss (w/o reg) on all data: 0.012054638
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7264986e-07
Norm of the params: 9.153156
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730044
Test loss (w/o reg) on all data: 0.012054682
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.79502e-07
Norm of the params: 9.153157
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1507231
Train loss (w/o reg) on all data: 0.14355455
Test loss (w/o reg) on all data: 0.080048494
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.5699316e-05
Norm of the params: 11.973761
              Random: fixed  14 labels. Loss 0.08005. Accuracy 0.981.
### Flips: 156, rs: 38, checks: 260
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729083
Test loss (w/o reg) on all data: 0.012054408
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.319104e-07
Norm of the params: 9.153262
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729097
Test loss (w/o reg) on all data: 0.012054305
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4468837e-07
Norm of the params: 9.153262
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12952764
Train loss (w/o reg) on all data: 0.12119045
Test loss (w/o reg) on all data: 0.080870576
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.371079e-05
Norm of the params: 12.91293
              Random: fixed  23 labels. Loss 0.08087. Accuracy 0.969.
### Flips: 156, rs: 38, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730098
Test loss (w/o reg) on all data: 0.01205488
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4987548e-07
Norm of the params: 9.153151
     Influence (LOO): fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730084
Test loss (w/o reg) on all data: 0.012054942
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7462556e-07
Norm of the params: 9.1531515
                Loss: fixed  73 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1277145
Train loss (w/o reg) on all data: 0.119661234
Test loss (w/o reg) on all data: 0.07958177
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.0363003e-06
Norm of the params: 12.691151
              Random: fixed  25 labels. Loss 0.07958. Accuracy 0.973.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1759776
Train loss (w/o reg) on all data: 0.1664974
Test loss (w/o reg) on all data: 0.10180117
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.99839e-06
Norm of the params: 13.769687
Flipped loss: 0.10180. Accuracy: 0.962
### Flips: 156, rs: 39, checks: 52
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08298234
Train loss (w/o reg) on all data: 0.071912415
Test loss (w/o reg) on all data: 0.068887666
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.851309e-06
Norm of the params: 14.879465
     Influence (LOO): fixed  43 labels. Loss 0.06889. Accuracy 0.985.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059255935
Train loss (w/o reg) on all data: 0.044054374
Test loss (w/o reg) on all data: 0.04872905
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.610519e-06
Norm of the params: 17.436491
                Loss: fixed  48 labels. Loss 0.04873. Accuracy 0.981.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17226799
Train loss (w/o reg) on all data: 0.16243592
Test loss (w/o reg) on all data: 0.10303529
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.850375e-06
Norm of the params: 14.022891
              Random: fixed   3 labels. Loss 0.10304. Accuracy 0.969.
### Flips: 156, rs: 39, checks: 104
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030964278
Train loss (w/o reg) on all data: 0.021853285
Test loss (w/o reg) on all data: 0.029809305
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.1259445e-06
Norm of the params: 13.498885
     Influence (LOO): fixed  70 labels. Loss 0.02981. Accuracy 0.989.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024463223
Train loss (w/o reg) on all data: 0.01425163
Test loss (w/o reg) on all data: 0.028961549
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.546809e-06
Norm of the params: 14.290972
                Loss: fixed  73 labels. Loss 0.02896. Accuracy 0.989.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16583729
Train loss (w/o reg) on all data: 0.15561876
Test loss (w/o reg) on all data: 0.103098914
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.342874e-05
Norm of the params: 14.295829
              Random: fixed   7 labels. Loss 0.10310. Accuracy 0.969.
### Flips: 156, rs: 39, checks: 156
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018862814
Train loss (w/o reg) on all data: 0.011395112
Test loss (w/o reg) on all data: 0.022599366
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1907401e-06
Norm of the params: 12.22105
     Influence (LOO): fixed  76 labels. Loss 0.02260. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729455
Test loss (w/o reg) on all data: 0.012054861
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.5929855e-08
Norm of the params: 9.153221
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1623735
Train loss (w/o reg) on all data: 0.15259804
Test loss (w/o reg) on all data: 0.09523609
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.82047e-05
Norm of the params: 13.982464
              Random: fixed   9 labels. Loss 0.09524. Accuracy 0.977.
### Flips: 156, rs: 39, checks: 208
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009503983
Train loss (w/o reg) on all data: 0.0041299835
Test loss (w/o reg) on all data: 0.012596925
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5058517e-07
Norm of the params: 10.367255
     Influence (LOO): fixed  81 labels. Loss 0.01260. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730696
Test loss (w/o reg) on all data: 0.012054248
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1893294e-07
Norm of the params: 9.153087
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15862781
Train loss (w/o reg) on all data: 0.14890853
Test loss (w/o reg) on all data: 0.096897565
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.3076705e-05
Norm of the params: 13.942224
              Random: fixed  13 labels. Loss 0.09690. Accuracy 0.973.
### Flips: 156, rs: 39, checks: 260
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172978
Test loss (w/o reg) on all data: 0.012055198
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.6021946e-07
Norm of the params: 9.153187
     Influence (LOO): fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729772
Test loss (w/o reg) on all data: 0.012055104
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2014095e-07
Norm of the params: 9.153187
                Loss: fixed  82 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15019755
Train loss (w/o reg) on all data: 0.14001903
Test loss (w/o reg) on all data: 0.097144164
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7112654e-05
Norm of the params: 14.267816
              Random: fixed  17 labels. Loss 0.09714. Accuracy 0.977.
### Flips: 156, rs: 39, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729085
Test loss (w/o reg) on all data: 0.012054455
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4515667e-07
Norm of the params: 9.153263
     Influence (LOO): fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729073
Test loss (w/o reg) on all data: 0.012054562
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7283076e-07
Norm of the params: 9.153263
                Loss: fixed  82 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14081334
Train loss (w/o reg) on all data: 0.13077876
Test loss (w/o reg) on all data: 0.084126174
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.31767e-06
Norm of the params: 14.166563
              Random: fixed  23 labels. Loss 0.08413. Accuracy 0.981.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2147813
Train loss (w/o reg) on all data: 0.20649615
Test loss (w/o reg) on all data: 0.11669783
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.871716e-05
Norm of the params: 12.87257
Flipped loss: 0.11670. Accuracy: 0.973
### Flips: 208, rs: 0, checks: 52
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11945453
Train loss (w/o reg) on all data: 0.10785665
Test loss (w/o reg) on all data: 0.0829814
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.5892283e-06
Norm of the params: 15.230153
     Influence (LOO): fixed  44 labels. Loss 0.08298. Accuracy 0.966.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08266155
Train loss (w/o reg) on all data: 0.06491309
Test loss (w/o reg) on all data: 0.06633307
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.8157864e-06
Norm of the params: 18.840626
                Loss: fixed  51 labels. Loss 0.06633. Accuracy 0.977.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20773685
Train loss (w/o reg) on all data: 0.19944508
Test loss (w/o reg) on all data: 0.10278612
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0941137e-05
Norm of the params: 12.8777075
              Random: fixed   9 labels. Loss 0.10279. Accuracy 0.981.
### Flips: 208, rs: 0, checks: 104
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07203288
Train loss (w/o reg) on all data: 0.060867947
Test loss (w/o reg) on all data: 0.05695843
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.0001835e-06
Norm of the params: 14.943177
     Influence (LOO): fixed  67 labels. Loss 0.05696. Accuracy 0.985.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024231706
Train loss (w/o reg) on all data: 0.013129829
Test loss (w/o reg) on all data: 0.021284163
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3245566e-06
Norm of the params: 14.900923
                Loss: fixed  88 labels. Loss 0.02128. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20038469
Train loss (w/o reg) on all data: 0.19148235
Test loss (w/o reg) on all data: 0.09719689
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0501747e-05
Norm of the params: 13.343422
              Random: fixed  12 labels. Loss 0.09720. Accuracy 0.981.
### Flips: 208, rs: 0, checks: 156
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030872848
Train loss (w/o reg) on all data: 0.020729998
Test loss (w/o reg) on all data: 0.01753408
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3115283e-06
Norm of the params: 14.242788
     Influence (LOO): fixed  88 labels. Loss 0.01753. Accuracy 0.992.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014159331
Train loss (w/o reg) on all data: 0.006674434
Test loss (w/o reg) on all data: 0.01446996
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6091957e-06
Norm of the params: 12.235111
                Loss: fixed  96 labels. Loss 0.01447. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18887071
Train loss (w/o reg) on all data: 0.17983222
Test loss (w/o reg) on all data: 0.09185155
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0654536e-05
Norm of the params: 13.445064
              Random: fixed  19 labels. Loss 0.09185. Accuracy 0.981.
### Flips: 208, rs: 0, checks: 208
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009337548
Train loss (w/o reg) on all data: 0.0041989316
Test loss (w/o reg) on all data: 0.012079305
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.647201e-07
Norm of the params: 10.137669
     Influence (LOO): fixed  99 labels. Loss 0.01208. Accuracy 0.992.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010687232
Train loss (w/o reg) on all data: 0.0045377514
Test loss (w/o reg) on all data: 0.013656984
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3070979e-06
Norm of the params: 11.090069
                Loss: fixed  99 labels. Loss 0.01366. Accuracy 0.992.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17970581
Train loss (w/o reg) on all data: 0.17091212
Test loss (w/o reg) on all data: 0.084552065
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.2871372e-05
Norm of the params: 13.261746
              Random: fixed  24 labels. Loss 0.08455. Accuracy 0.981.
### Flips: 208, rs: 0, checks: 260
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730436
Test loss (w/o reg) on all data: 0.012054973
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0157388e-07
Norm of the params: 9.153113
     Influence (LOO): fixed 101 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0088424375
Train loss (w/o reg) on all data: 0.0035699531
Test loss (w/o reg) on all data: 0.012751858
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6080724e-07
Norm of the params: 10.268869
                Loss: fixed 100 labels. Loss 0.01275. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17585424
Train loss (w/o reg) on all data: 0.16677606
Test loss (w/o reg) on all data: 0.082962506
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.45194e-05
Norm of the params: 13.474548
              Random: fixed  25 labels. Loss 0.08296. Accuracy 0.981.
### Flips: 208, rs: 0, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729758
Test loss (w/o reg) on all data: 0.012054932
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.876443e-07
Norm of the params: 9.153191
     Influence (LOO): fixed 101 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729737
Test loss (w/o reg) on all data: 0.012055091
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0918552e-07
Norm of the params: 9.153191
                Loss: fixed 101 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16799338
Train loss (w/o reg) on all data: 0.15917897
Test loss (w/o reg) on all data: 0.08353218
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.16008e-06
Norm of the params: 13.277354
              Random: fixed  29 labels. Loss 0.08353. Accuracy 0.981.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22280574
Train loss (w/o reg) on all data: 0.21697114
Test loss (w/o reg) on all data: 0.13948013
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3292486e-05
Norm of the params: 10.802399
Flipped loss: 0.13948. Accuracy: 0.977
### Flips: 208, rs: 1, checks: 52
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1379573
Train loss (w/o reg) on all data: 0.12820733
Test loss (w/o reg) on all data: 0.10769498
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.705704e-05
Norm of the params: 13.964223
     Influence (LOO): fixed  41 labels. Loss 0.10769. Accuracy 0.966.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10276723
Train loss (w/o reg) on all data: 0.09030061
Test loss (w/o reg) on all data: 0.10579433
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.858364e-05
Norm of the params: 15.790263
                Loss: fixed  49 labels. Loss 0.10579. Accuracy 0.966.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21135092
Train loss (w/o reg) on all data: 0.20488583
Test loss (w/o reg) on all data: 0.1336395
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0950414e-05
Norm of the params: 11.371101
              Random: fixed   7 labels. Loss 0.13364. Accuracy 0.977.
### Flips: 208, rs: 1, checks: 104
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08325575
Train loss (w/o reg) on all data: 0.07286651
Test loss (w/o reg) on all data: 0.08550135
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.1278123e-05
Norm of the params: 14.414748
     Influence (LOO): fixed  73 labels. Loss 0.08550. Accuracy 0.977.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027886342
Train loss (w/o reg) on all data: 0.016360683
Test loss (w/o reg) on all data: 0.07211844
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2616603e-06
Norm of the params: 15.18266
                Loss: fixed  90 labels. Loss 0.07212. Accuracy 0.977.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19896993
Train loss (w/o reg) on all data: 0.19216698
Test loss (w/o reg) on all data: 0.12501897
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.490778e-05
Norm of the params: 11.664426
              Random: fixed  14 labels. Loss 0.12502. Accuracy 0.969.
### Flips: 208, rs: 1, checks: 156
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036600977
Train loss (w/o reg) on all data: 0.026995601
Test loss (w/o reg) on all data: 0.03959703
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.856125e-06
Norm of the params: 13.860287
     Influence (LOO): fixed  94 labels. Loss 0.03960. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01437668
Train loss (w/o reg) on all data: 0.0071115447
Test loss (w/o reg) on all data: 0.034261215
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.9393006e-07
Norm of the params: 12.054158
                Loss: fixed 104 labels. Loss 0.03426. Accuracy 0.985.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19769049
Train loss (w/o reg) on all data: 0.1908153
Test loss (w/o reg) on all data: 0.11918933
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.9049654e-05
Norm of the params: 11.726198
              Random: fixed  17 labels. Loss 0.11919. Accuracy 0.973.
### Flips: 208, rs: 1, checks: 208
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017842662
Train loss (w/o reg) on all data: 0.010929403
Test loss (w/o reg) on all data: 0.026196543
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.385927e-07
Norm of the params: 11.758622
     Influence (LOO): fixed 106 labels. Loss 0.02620. Accuracy 0.985.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01163687
Train loss (w/o reg) on all data: 0.004936877
Test loss (w/o reg) on all data: 0.024347749
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.5212035e-07
Norm of the params: 11.57583
                Loss: fixed 107 labels. Loss 0.02435. Accuracy 0.989.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19503623
Train loss (w/o reg) on all data: 0.18830761
Test loss (w/o reg) on all data: 0.12343199
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0221822e-05
Norm of the params: 11.600538
              Random: fixed  19 labels. Loss 0.12343. Accuracy 0.969.
### Flips: 208, rs: 1, checks: 260
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00927802
Train loss (w/o reg) on all data: 0.003932197
Test loss (w/o reg) on all data: 0.016020888
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9712957e-07
Norm of the params: 10.340041
     Influence (LOO): fixed 110 labels. Loss 0.01602. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009884154
Train loss (w/o reg) on all data: 0.0039755944
Test loss (w/o reg) on all data: 0.019968672
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2554157e-07
Norm of the params: 10.870658
                Loss: fixed 108 labels. Loss 0.01997. Accuracy 0.992.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19213055
Train loss (w/o reg) on all data: 0.18562944
Test loss (w/o reg) on all data: 0.11848882
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.24666e-05
Norm of the params: 11.402722
              Random: fixed  22 labels. Loss 0.11849. Accuracy 0.977.
### Flips: 208, rs: 1, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173043
Test loss (w/o reg) on all data: 0.012056075
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1117282e-07
Norm of the params: 9.153115
     Influence (LOO): fixed 112 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009221943
Train loss (w/o reg) on all data: 0.0035888273
Test loss (w/o reg) on all data: 0.01808724
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6293457e-07
Norm of the params: 10.614251
                Loss: fixed 109 labels. Loss 0.01809. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1823238
Train loss (w/o reg) on all data: 0.1759941
Test loss (w/o reg) on all data: 0.107704
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.091208e-05
Norm of the params: 11.251399
              Random: fixed  29 labels. Loss 0.10770. Accuracy 0.981.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22711614
Train loss (w/o reg) on all data: 0.22074993
Test loss (w/o reg) on all data: 0.1539473
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.619887e-05
Norm of the params: 11.283797
Flipped loss: 0.15395. Accuracy: 0.920
### Flips: 208, rs: 2, checks: 52
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12973124
Train loss (w/o reg) on all data: 0.11873472
Test loss (w/o reg) on all data: 0.13208756
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 6.412592e-06
Norm of the params: 14.830051
     Influence (LOO): fixed  46 labels. Loss 0.13209. Accuracy 0.920.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09240952
Train loss (w/o reg) on all data: 0.07595558
Test loss (w/o reg) on all data: 0.13565741
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.079761e-05
Norm of the params: 18.140533
                Loss: fixed  51 labels. Loss 0.13566. Accuracy 0.935.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2184509
Train loss (w/o reg) on all data: 0.21155712
Test loss (w/o reg) on all data: 0.15060882
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 4.2741434e-05
Norm of the params: 11.7420435
              Random: fixed   5 labels. Loss 0.15061. Accuracy 0.931.
### Flips: 208, rs: 2, checks: 104
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07352529
Train loss (w/o reg) on all data: 0.06224567
Test loss (w/o reg) on all data: 0.082410224
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.465373e-06
Norm of the params: 15.0197315
     Influence (LOO): fixed  71 labels. Loss 0.08241. Accuracy 0.958.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03288789
Train loss (w/o reg) on all data: 0.021452816
Test loss (w/o reg) on all data: 0.08161178
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.3175962e-06
Norm of the params: 15.122881
                Loss: fixed  90 labels. Loss 0.08161. Accuracy 0.958.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2120042
Train loss (w/o reg) on all data: 0.20524213
Test loss (w/o reg) on all data: 0.14234403
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.5413365e-05
Norm of the params: 11.629335
              Random: fixed  12 labels. Loss 0.14234. Accuracy 0.943.
### Flips: 208, rs: 2, checks: 156
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042610936
Train loss (w/o reg) on all data: 0.032517835
Test loss (w/o reg) on all data: 0.055123925
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.397433e-06
Norm of the params: 14.207817
     Influence (LOO): fixed  88 labels. Loss 0.05512. Accuracy 0.973.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018868387
Train loss (w/o reg) on all data: 0.009913274
Test loss (w/o reg) on all data: 0.019050615
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6047363e-06
Norm of the params: 13.38291
                Loss: fixed 101 labels. Loss 0.01905. Accuracy 0.996.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1978514
Train loss (w/o reg) on all data: 0.19047007
Test loss (w/o reg) on all data: 0.14246073
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.1197343e-05
Norm of the params: 12.150174
              Random: fixed  18 labels. Loss 0.14246. Accuracy 0.935.
### Flips: 208, rs: 2, checks: 208
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031335406
Train loss (w/o reg) on all data: 0.02294019
Test loss (w/o reg) on all data: 0.05322828
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.488137e-06
Norm of the params: 12.957791
     Influence (LOO): fixed  95 labels. Loss 0.05323. Accuracy 0.962.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011814799
Train loss (w/o reg) on all data: 0.0050243195
Test loss (w/o reg) on all data: 0.0155298235
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.2198254e-07
Norm of the params: 11.653738
                Loss: fixed 106 labels. Loss 0.01553. Accuracy 0.996.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19005361
Train loss (w/o reg) on all data: 0.18247606
Test loss (w/o reg) on all data: 0.12153147
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.5467136e-05
Norm of the params: 12.310615
              Random: fixed  24 labels. Loss 0.12153. Accuracy 0.950.
### Flips: 208, rs: 2, checks: 260
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01996722
Train loss (w/o reg) on all data: 0.01332395
Test loss (w/o reg) on all data: 0.03598311
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3028117e-06
Norm of the params: 11.526726
     Influence (LOO): fixed 103 labels. Loss 0.03598. Accuracy 0.977.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01027222
Train loss (w/o reg) on all data: 0.0043303943
Test loss (w/o reg) on all data: 0.014751748
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0487675e-07
Norm of the params: 10.9012165
                Loss: fixed 109 labels. Loss 0.01475. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18828684
Train loss (w/o reg) on all data: 0.18075854
Test loss (w/o reg) on all data: 0.12047967
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.9418912e-05
Norm of the params: 12.27054
              Random: fixed  26 labels. Loss 0.12048. Accuracy 0.954.
### Flips: 208, rs: 2, checks: 312
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010215491
Train loss (w/o reg) on all data: 0.00461663
Test loss (w/o reg) on all data: 0.015989682
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.819627e-07
Norm of the params: 10.581929
     Influence (LOO): fixed 108 labels. Loss 0.01599. Accuracy 0.989.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077574784
Train loss (w/o reg) on all data: 0.0028861836
Test loss (w/o reg) on all data: 0.011173532
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.7224887e-07
Norm of the params: 9.870456
                Loss: fixed 110 labels. Loss 0.01117. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17957033
Train loss (w/o reg) on all data: 0.17184472
Test loss (w/o reg) on all data: 0.11500656
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3470457e-05
Norm of the params: 12.430291
              Random: fixed  32 labels. Loss 0.11501. Accuracy 0.962.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20308025
Train loss (w/o reg) on all data: 0.19628201
Test loss (w/o reg) on all data: 0.1042302
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0574276e-05
Norm of the params: 11.660391
Flipped loss: 0.10423. Accuracy: 0.977
### Flips: 208, rs: 3, checks: 52
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116888374
Train loss (w/o reg) on all data: 0.10597526
Test loss (w/o reg) on all data: 0.08263332
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3786307e-05
Norm of the params: 14.773698
     Influence (LOO): fixed  40 labels. Loss 0.08263. Accuracy 0.969.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08022763
Train loss (w/o reg) on all data: 0.0632983
Test loss (w/o reg) on all data: 0.06559547
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.444833e-06
Norm of the params: 18.400724
                Loss: fixed  51 labels. Loss 0.06560. Accuracy 0.969.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19145177
Train loss (w/o reg) on all data: 0.18410298
Test loss (w/o reg) on all data: 0.0885423
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.100884e-05
Norm of the params: 12.123358
              Random: fixed   7 labels. Loss 0.08854. Accuracy 0.977.
### Flips: 208, rs: 3, checks: 104
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062357854
Train loss (w/o reg) on all data: 0.052028496
Test loss (w/o reg) on all data: 0.05278992
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.469922e-06
Norm of the params: 14.373139
     Influence (LOO): fixed  70 labels. Loss 0.05279. Accuracy 0.981.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034006804
Train loss (w/o reg) on all data: 0.02115606
Test loss (w/o reg) on all data: 0.053871017
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.1846126e-06
Norm of the params: 16.031683
                Loss: fixed  83 labels. Loss 0.05387. Accuracy 0.981.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1829473
Train loss (w/o reg) on all data: 0.17555411
Test loss (w/o reg) on all data: 0.08549427
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.217251e-06
Norm of the params: 12.1599245
              Random: fixed  11 labels. Loss 0.08549. Accuracy 0.981.
### Flips: 208, rs: 3, checks: 156
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029669683
Train loss (w/o reg) on all data: 0.019487156
Test loss (w/o reg) on all data: 0.027440127
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2082525e-06
Norm of the params: 14.270618
     Influence (LOO): fixed  87 labels. Loss 0.02744. Accuracy 0.996.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015327987
Train loss (w/o reg) on all data: 0.0071455343
Test loss (w/o reg) on all data: 0.021619473
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0589467e-06
Norm of the params: 12.79254
                Loss: fixed  94 labels. Loss 0.02162. Accuracy 0.989.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17032625
Train loss (w/o reg) on all data: 0.16318144
Test loss (w/o reg) on all data: 0.07137591
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1323476e-05
Norm of the params: 11.953925
              Random: fixed  21 labels. Loss 0.07138. Accuracy 0.985.
### Flips: 208, rs: 3, checks: 208
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024470486
Train loss (w/o reg) on all data: 0.015324588
Test loss (w/o reg) on all data: 0.026318977
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5892912e-06
Norm of the params: 13.524715
     Influence (LOO): fixed  90 labels. Loss 0.02632. Accuracy 0.992.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012452968
Train loss (w/o reg) on all data: 0.0056600915
Test loss (w/o reg) on all data: 0.024503887
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5601253e-06
Norm of the params: 11.655794
                Loss: fixed  97 labels. Loss 0.02450. Accuracy 0.989.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1622883
Train loss (w/o reg) on all data: 0.15513822
Test loss (w/o reg) on all data: 0.07012802
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5072222e-05
Norm of the params: 11.958313
              Random: fixed  26 labels. Loss 0.07013. Accuracy 0.981.
### Flips: 208, rs: 3, checks: 260
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011075951
Train loss (w/o reg) on all data: 0.004785134
Test loss (w/o reg) on all data: 0.015871167
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0725569e-06
Norm of the params: 11.216789
     Influence (LOO): fixed  98 labels. Loss 0.01587. Accuracy 0.996.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010263804
Train loss (w/o reg) on all data: 0.0043981434
Test loss (w/o reg) on all data: 0.015019059
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.326441e-07
Norm of the params: 10.831123
                Loss: fixed  99 labels. Loss 0.01502. Accuracy 0.989.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15871814
Train loss (w/o reg) on all data: 0.15150511
Test loss (w/o reg) on all data: 0.06944015
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7149417e-05
Norm of the params: 12.010857
              Random: fixed  29 labels. Loss 0.06944. Accuracy 0.985.
### Flips: 208, rs: 3, checks: 312
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00890265
Train loss (w/o reg) on all data: 0.003637497
Test loss (w/o reg) on all data: 0.01350086
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0804103e-07
Norm of the params: 10.261728
     Influence (LOO): fixed 100 labels. Loss 0.01350. Accuracy 0.992.
Using normal model
LBFGS training took [30] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.002172961
Test loss (w/o reg) on all data: 0.012054984
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5742307e-07
Norm of the params: 9.153204
                Loss: fixed 101 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15364294
Train loss (w/o reg) on all data: 0.1461024
Test loss (w/o reg) on all data: 0.06635685
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.7001366e-05
Norm of the params: 12.280502
              Random: fixed  31 labels. Loss 0.06636. Accuracy 0.985.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21658598
Train loss (w/o reg) on all data: 0.21020988
Test loss (w/o reg) on all data: 0.17167827
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.951313e-05
Norm of the params: 11.29257
Flipped loss: 0.17168. Accuracy: 0.939
### Flips: 208, rs: 4, checks: 52
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12860575
Train loss (w/o reg) on all data: 0.1181853
Test loss (w/o reg) on all data: 0.12603748
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1179365e-05
Norm of the params: 14.436381
     Influence (LOO): fixed  43 labels. Loss 0.12604. Accuracy 0.962.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08135436
Train loss (w/o reg) on all data: 0.06479085
Test loss (w/o reg) on all data: 0.14658614
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 6.3903835e-06
Norm of the params: 18.200827
                Loss: fixed  52 labels. Loss 0.14659. Accuracy 0.931.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21089868
Train loss (w/o reg) on all data: 0.20415477
Test loss (w/o reg) on all data: 0.16965698
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.624086e-05
Norm of the params: 11.61371
              Random: fixed   4 labels. Loss 0.16966. Accuracy 0.939.
### Flips: 208, rs: 4, checks: 104
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06487914
Train loss (w/o reg) on all data: 0.05407448
Test loss (w/o reg) on all data: 0.043392077
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.7920382e-06
Norm of the params: 14.700109
     Influence (LOO): fixed  74 labels. Loss 0.04339. Accuracy 0.989.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02700204
Train loss (w/o reg) on all data: 0.015381373
Test loss (w/o reg) on all data: 0.05537706
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6160074e-06
Norm of the params: 15.245109
                Loss: fixed  89 labels. Loss 0.05538. Accuracy 0.969.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20560001
Train loss (w/o reg) on all data: 0.198794
Test loss (w/o reg) on all data: 0.16884702
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.502342e-05
Norm of the params: 11.667054
              Random: fixed   8 labels. Loss 0.16885. Accuracy 0.943.
### Flips: 208, rs: 4, checks: 156
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028440218
Train loss (w/o reg) on all data: 0.0201532
Test loss (w/o reg) on all data: 0.029760543
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2415708e-06
Norm of the params: 12.874018
     Influence (LOO): fixed  94 labels. Loss 0.02976. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0132622635
Train loss (w/o reg) on all data: 0.006015249
Test loss (w/o reg) on all data: 0.039441727
Train acc on all data:  1.0
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2003354e-06
Norm of the params: 12.039115
                Loss: fixed  99 labels. Loss 0.03944. Accuracy 0.981.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19814916
Train loss (w/o reg) on all data: 0.19148125
Test loss (w/o reg) on all data: 0.1596921
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.3493998e-05
Norm of the params: 11.548082
              Random: fixed  14 labels. Loss 0.15969. Accuracy 0.950.
### Flips: 208, rs: 4, checks: 208
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015682366
Train loss (w/o reg) on all data: 0.009099851
Test loss (w/o reg) on all data: 0.014859498
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2648168e-07
Norm of the params: 11.473896
     Influence (LOO): fixed 101 labels. Loss 0.01486. Accuracy 0.992.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011481652
Train loss (w/o reg) on all data: 0.004919917
Test loss (w/o reg) on all data: 0.026514662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.611925e-07
Norm of the params: 11.455771
                Loss: fixed 101 labels. Loss 0.02651. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18496874
Train loss (w/o reg) on all data: 0.177528
Test loss (w/o reg) on all data: 0.13853543
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 8.924429e-06
Norm of the params: 12.198969
              Random: fixed  22 labels. Loss 0.13854. Accuracy 0.954.
### Flips: 208, rs: 4, checks: 260
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010139594
Train loss (w/o reg) on all data: 0.0050226995
Test loss (w/o reg) on all data: 0.011304426
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.8227865e-07
Norm of the params: 10.116219
     Influence (LOO): fixed 103 labels. Loss 0.01130. Accuracy 0.996.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009628632
Train loss (w/o reg) on all data: 0.0037090867
Test loss (w/o reg) on all data: 0.019893091
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.8364956e-07
Norm of the params: 10.880758
                Loss: fixed 102 labels. Loss 0.01989. Accuracy 0.989.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1785227
Train loss (w/o reg) on all data: 0.1707618
Test loss (w/o reg) on all data: 0.13619915
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.7161985e-05
Norm of the params: 12.458656
              Random: fixed  27 labels. Loss 0.13620. Accuracy 0.954.
### Flips: 208, rs: 4, checks: 312
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729646
Test loss (w/o reg) on all data: 0.012054554
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3900896e-07
Norm of the params: 9.153201
     Influence (LOO): fixed 105 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008169742
Train loss (w/o reg) on all data: 0.0030077286
Test loss (w/o reg) on all data: 0.014295119
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2539893e-07
Norm of the params: 10.160722
                Loss: fixed 103 labels. Loss 0.01430. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17003597
Train loss (w/o reg) on all data: 0.16148224
Test loss (w/o reg) on all data: 0.13700508
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.900456e-05
Norm of the params: 13.079545
              Random: fixed  32 labels. Loss 0.13701. Accuracy 0.958.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23063889
Train loss (w/o reg) on all data: 0.2237968
Test loss (w/o reg) on all data: 0.15346262
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.02644e-05
Norm of the params: 11.6979475
Flipped loss: 0.15346. Accuracy: 0.958
### Flips: 208, rs: 5, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11437982
Train loss (w/o reg) on all data: 0.10180703
Test loss (w/o reg) on all data: 0.116303705
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.6645765e-05
Norm of the params: 15.857361
     Influence (LOO): fixed  48 labels. Loss 0.11630. Accuracy 0.954.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093202785
Train loss (w/o reg) on all data: 0.07504588
Test loss (w/o reg) on all data: 0.11593896
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.26275e-05
Norm of the params: 19.056181
                Loss: fixed  52 labels. Loss 0.11594. Accuracy 0.954.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21794222
Train loss (w/o reg) on all data: 0.2105564
Test loss (w/o reg) on all data: 0.14188148
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 9.001967e-06
Norm of the params: 12.153868
              Random: fixed   8 labels. Loss 0.14188. Accuracy 0.947.
### Flips: 208, rs: 5, checks: 104
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076725364
Train loss (w/o reg) on all data: 0.06493793
Test loss (w/o reg) on all data: 0.11126183
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.0493924e-05
Norm of the params: 15.35411
     Influence (LOO): fixed  67 labels. Loss 0.11126. Accuracy 0.954.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03747201
Train loss (w/o reg) on all data: 0.02318727
Test loss (w/o reg) on all data: 0.054226995
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.329314e-06
Norm of the params: 16.902508
                Loss: fixed  90 labels. Loss 0.05423. Accuracy 0.973.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20595534
Train loss (w/o reg) on all data: 0.19774826
Test loss (w/o reg) on all data: 0.13183309
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3925006e-05
Norm of the params: 12.811783
              Random: fixed  15 labels. Loss 0.13183. Accuracy 0.969.
### Flips: 208, rs: 5, checks: 156
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052282438
Train loss (w/o reg) on all data: 0.0412994
Test loss (w/o reg) on all data: 0.08765747
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3198977e-06
Norm of the params: 14.820956
     Influence (LOO): fixed  84 labels. Loss 0.08766. Accuracy 0.962.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018992437
Train loss (w/o reg) on all data: 0.009424795
Test loss (w/o reg) on all data: 0.022199256
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3540877e-06
Norm of the params: 13.833034
                Loss: fixed 106 labels. Loss 0.02220. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19430296
Train loss (w/o reg) on all data: 0.18548079
Test loss (w/o reg) on all data: 0.1279309
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.3302084e-05
Norm of the params: 13.283205
              Random: fixed  21 labels. Loss 0.12793. Accuracy 0.966.
### Flips: 208, rs: 5, checks: 208
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030410416
Train loss (w/o reg) on all data: 0.021596242
Test loss (w/o reg) on all data: 0.034661707
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.488609e-06
Norm of the params: 13.27718
     Influence (LOO): fixed  98 labels. Loss 0.03466. Accuracy 0.985.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012076251
Train loss (w/o reg) on all data: 0.0051012356
Test loss (w/o reg) on all data: 0.016298078
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9060742e-06
Norm of the params: 11.811024
                Loss: fixed 110 labels. Loss 0.01630. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1887134
Train loss (w/o reg) on all data: 0.17971149
Test loss (w/o reg) on all data: 0.12819661
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.233597e-05
Norm of the params: 13.417837
              Random: fixed  24 labels. Loss 0.12820. Accuracy 0.962.
### Flips: 208, rs: 5, checks: 260
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01202096
Train loss (w/o reg) on all data: 0.005702243
Test loss (w/o reg) on all data: 0.014549768
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.377264e-07
Norm of the params: 11.241635
     Influence (LOO): fixed 110 labels. Loss 0.01455. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008275315
Train loss (w/o reg) on all data: 0.0031797397
Test loss (w/o reg) on all data: 0.011147053
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.68391e-07
Norm of the params: 10.095123
                Loss: fixed 113 labels. Loss 0.01115. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18296637
Train loss (w/o reg) on all data: 0.17459682
Test loss (w/o reg) on all data: 0.12276775
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.984524e-05
Norm of the params: 12.937969
              Random: fixed  30 labels. Loss 0.12277. Accuracy 0.962.
### Flips: 208, rs: 5, checks: 312
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007851765
Train loss (w/o reg) on all data: 0.0030549248
Test loss (w/o reg) on all data: 0.011449439
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0199696e-07
Norm of the params: 9.794733
     Influence (LOO): fixed 113 labels. Loss 0.01145. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008275317
Train loss (w/o reg) on all data: 0.0031799844
Test loss (w/o reg) on all data: 0.011147147
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.795206e-07
Norm of the params: 10.094882
                Loss: fixed 113 labels. Loss 0.01115. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18390991
Train loss (w/o reg) on all data: 0.17684291
Test loss (w/o reg) on all data: 0.1226545
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2120601e-05
Norm of the params: 11.888646
              Random: fixed  34 labels. Loss 0.12265. Accuracy 0.966.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22799262
Train loss (w/o reg) on all data: 0.22193971
Test loss (w/o reg) on all data: 0.16213135
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.4320775e-05
Norm of the params: 11.002648
Flipped loss: 0.16213. Accuracy: 0.935
### Flips: 208, rs: 6, checks: 52
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13197434
Train loss (w/o reg) on all data: 0.12133155
Test loss (w/o reg) on all data: 0.12845498
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 5.4079715e-06
Norm of the params: 14.589581
     Influence (LOO): fixed  44 labels. Loss 0.12845. Accuracy 0.943.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09771492
Train loss (w/o reg) on all data: 0.08186921
Test loss (w/o reg) on all data: 0.1503627
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.095022e-05
Norm of the params: 17.80209
                Loss: fixed  51 labels. Loss 0.15036. Accuracy 0.924.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2239013
Train loss (w/o reg) on all data: 0.21813706
Test loss (w/o reg) on all data: 0.14500067
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.27334115e-05
Norm of the params: 10.737077
              Random: fixed   6 labels. Loss 0.14500. Accuracy 0.958.
### Flips: 208, rs: 6, checks: 104
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07929346
Train loss (w/o reg) on all data: 0.0680957
Test loss (w/o reg) on all data: 0.08430549
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.6561804e-06
Norm of the params: 14.965133
     Influence (LOO): fixed  73 labels. Loss 0.08431. Accuracy 0.962.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031654365
Train loss (w/o reg) on all data: 0.017428398
Test loss (w/o reg) on all data: 0.06663116
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.883059e-06
Norm of the params: 16.8677
                Loss: fixed  89 labels. Loss 0.06663. Accuracy 0.973.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21720241
Train loss (w/o reg) on all data: 0.21147771
Test loss (w/o reg) on all data: 0.14006901
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.9456118e-05
Norm of the params: 10.70019
              Random: fixed  10 labels. Loss 0.14007. Accuracy 0.962.
### Flips: 208, rs: 6, checks: 156
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036611512
Train loss (w/o reg) on all data: 0.026305165
Test loss (w/o reg) on all data: 0.051828157
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.7319825e-06
Norm of the params: 14.357123
     Influence (LOO): fixed  94 labels. Loss 0.05183. Accuracy 0.977.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016404718
Train loss (w/o reg) on all data: 0.0076344972
Test loss (w/o reg) on all data: 0.034476113
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1126511e-06
Norm of the params: 13.244033
                Loss: fixed 103 labels. Loss 0.03448. Accuracy 0.989.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2105612
Train loss (w/o reg) on all data: 0.2045682
Test loss (w/o reg) on all data: 0.13658433
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.5079007e-05
Norm of the params: 10.948051
              Random: fixed  15 labels. Loss 0.13658. Accuracy 0.958.
### Flips: 208, rs: 6, checks: 208
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014585808
Train loss (w/o reg) on all data: 0.008226614
Test loss (w/o reg) on all data: 0.020880956
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.661266e-07
Norm of the params: 11.277582
     Influence (LOO): fixed 106 labels. Loss 0.02088. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011480164
Train loss (w/o reg) on all data: 0.0047557405
Test loss (w/o reg) on all data: 0.02905603
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.724865e-07
Norm of the params: 11.596916
                Loss: fixed 105 labels. Loss 0.02906. Accuracy 0.989.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20563908
Train loss (w/o reg) on all data: 0.19948332
Test loss (w/o reg) on all data: 0.13208522
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.6070044e-05
Norm of the params: 11.095729
              Random: fixed  17 labels. Loss 0.13209. Accuracy 0.958.
### Flips: 208, rs: 6, checks: 260
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021728592
Test loss (w/o reg) on all data: 0.012054765
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.6897111e-07
Norm of the params: 9.153316
     Influence (LOO): fixed 110 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172861
Test loss (w/o reg) on all data: 0.012054699
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3534857e-07
Norm of the params: 9.153315
                Loss: fixed 110 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2017481
Train loss (w/o reg) on all data: 0.19568987
Test loss (w/o reg) on all data: 0.124649964
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 9.2212185e-06
Norm of the params: 11.007476
              Random: fixed  21 labels. Loss 0.12465. Accuracy 0.962.
### Flips: 208, rs: 6, checks: 312
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002173033
Test loss (w/o reg) on all data: 0.012054971
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8965714e-07
Norm of the params: 9.153125
     Influence (LOO): fixed 110 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730016
Test loss (w/o reg) on all data: 0.012054408
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.325412e-07
Norm of the params: 9.15316
                Loss: fixed 110 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19511671
Train loss (w/o reg) on all data: 0.18923981
Test loss (w/o reg) on all data: 0.12384511
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.68262e-05
Norm of the params: 10.84149
              Random: fixed  26 labels. Loss 0.12385. Accuracy 0.966.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22276646
Train loss (w/o reg) on all data: 0.21554215
Test loss (w/o reg) on all data: 0.1715937
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.6448644e-05
Norm of the params: 12.020234
Flipped loss: 0.17159. Accuracy: 0.943
### Flips: 208, rs: 7, checks: 52
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14226726
Train loss (w/o reg) on all data: 0.13225357
Test loss (w/o reg) on all data: 0.11576414
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.1709853e-05
Norm of the params: 14.1518135
     Influence (LOO): fixed  42 labels. Loss 0.11576. Accuracy 0.947.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093226336
Train loss (w/o reg) on all data: 0.07686308
Test loss (w/o reg) on all data: 0.16740401
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.0019603e-05
Norm of the params: 18.09047
                Loss: fixed  51 labels. Loss 0.16740. Accuracy 0.943.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21013467
Train loss (w/o reg) on all data: 0.20272824
Test loss (w/o reg) on all data: 0.1689699
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.9119644e-05
Norm of the params: 12.17081
              Random: fixed   7 labels. Loss 0.16897. Accuracy 0.939.
### Flips: 208, rs: 7, checks: 104
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07809324
Train loss (w/o reg) on all data: 0.066897675
Test loss (w/o reg) on all data: 0.086726494
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.884881e-06
Norm of the params: 14.963663
     Influence (LOO): fixed  73 labels. Loss 0.08673. Accuracy 0.973.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042084325
Train loss (w/o reg) on all data: 0.027148364
Test loss (w/o reg) on all data: 0.08475608
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1059673e-05
Norm of the params: 17.283495
                Loss: fixed  87 labels. Loss 0.08476. Accuracy 0.973.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20761019
Train loss (w/o reg) on all data: 0.20013137
Test loss (w/o reg) on all data: 0.16407284
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.2936441e-05
Norm of the params: 12.230145
              Random: fixed  10 labels. Loss 0.16407. Accuracy 0.939.
### Flips: 208, rs: 7, checks: 156
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047347974
Train loss (w/o reg) on all data: 0.03731497
Test loss (w/o reg) on all data: 0.05161831
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.7920144e-06
Norm of the params: 14.165453
     Influence (LOO): fixed  92 labels. Loss 0.05162. Accuracy 0.985.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021503858
Train loss (w/o reg) on all data: 0.012351748
Test loss (w/o reg) on all data: 0.060094483
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.550994e-06
Norm of the params: 13.529309
                Loss: fixed 104 labels. Loss 0.06009. Accuracy 0.977.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18885922
Train loss (w/o reg) on all data: 0.18056512
Test loss (w/o reg) on all data: 0.16913302
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.6785536e-05
Norm of the params: 12.879527
              Random: fixed  18 labels. Loss 0.16913. Accuracy 0.935.
### Flips: 208, rs: 7, checks: 208
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025030684
Train loss (w/o reg) on all data: 0.015818194
Test loss (w/o reg) on all data: 0.038455825
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.028465e-06
Norm of the params: 13.573865
     Influence (LOO): fixed 103 labels. Loss 0.03846. Accuracy 0.985.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010576716
Train loss (w/o reg) on all data: 0.004572609
Test loss (w/o reg) on all data: 0.019493526
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8985394e-07
Norm of the params: 10.9581995
                Loss: fixed 112 labels. Loss 0.01949. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18415384
Train loss (w/o reg) on all data: 0.1761616
Test loss (w/o reg) on all data: 0.15554138
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.4734501e-05
Norm of the params: 12.642968
              Random: fixed  26 labels. Loss 0.15554. Accuracy 0.943.
### Flips: 208, rs: 7, checks: 260
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010413077
Train loss (w/o reg) on all data: 0.0046276506
Test loss (w/o reg) on all data: 0.015654732
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4527374e-07
Norm of the params: 10.756789
     Influence (LOO): fixed 112 labels. Loss 0.01565. Accuracy 0.992.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730035
Test loss (w/o reg) on all data: 0.012055188
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.570357e-07
Norm of the params: 9.153158
                Loss: fixed 114 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18067193
Train loss (w/o reg) on all data: 0.17241278
Test loss (w/o reg) on all data: 0.15558784
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.9338305e-05
Norm of the params: 12.852355
              Random: fixed  28 labels. Loss 0.15559. Accuracy 0.950.
### Flips: 208, rs: 7, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002173036
Test loss (w/o reg) on all data: 0.012054636
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4198243e-07
Norm of the params: 9.153124
     Influence (LOO): fixed 114 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730326
Test loss (w/o reg) on all data: 0.012054699
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0926264e-07
Norm of the params: 9.153125
                Loss: fixed 114 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16541632
Train loss (w/o reg) on all data: 0.1561163
Test loss (w/o reg) on all data: 0.14765269
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.4374325e-05
Norm of the params: 13.638185
              Random: fixed  35 labels. Loss 0.14765. Accuracy 0.947.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20570508
Train loss (w/o reg) on all data: 0.19851339
Test loss (w/o reg) on all data: 0.15069588
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.8420774e-05
Norm of the params: 11.993075
Flipped loss: 0.15070. Accuracy: 0.950
### Flips: 208, rs: 8, checks: 52
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10252859
Train loss (w/o reg) on all data: 0.09108384
Test loss (w/o reg) on all data: 0.11187206
Train acc on all data:  0.956064947468959
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.7201499e-05
Norm of the params: 15.129272
     Influence (LOO): fixed  42 labels. Loss 0.11187. Accuracy 0.950.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08139434
Train loss (w/o reg) on all data: 0.065288916
Test loss (w/o reg) on all data: 0.105846494
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.5776224e-05
Norm of the params: 17.94738
                Loss: fixed  50 labels. Loss 0.10585. Accuracy 0.962.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20348588
Train loss (w/o reg) on all data: 0.19634794
Test loss (w/o reg) on all data: 0.14506118
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.586699e-05
Norm of the params: 11.948173
              Random: fixed   2 labels. Loss 0.14506. Accuracy 0.950.
### Flips: 208, rs: 8, checks: 104
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067567416
Train loss (w/o reg) on all data: 0.058267273
Test loss (w/o reg) on all data: 0.08389157
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.2044485e-06
Norm of the params: 13.638288
     Influence (LOO): fixed  67 labels. Loss 0.08389. Accuracy 0.966.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03202937
Train loss (w/o reg) on all data: 0.020067777
Test loss (w/o reg) on all data: 0.035133366
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.9769125e-06
Norm of the params: 15.467122
                Loss: fixed  83 labels. Loss 0.03513. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19935504
Train loss (w/o reg) on all data: 0.19200853
Test loss (w/o reg) on all data: 0.14095013
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.156006e-05
Norm of the params: 12.12148
              Random: fixed   5 labels. Loss 0.14095. Accuracy 0.958.
### Flips: 208, rs: 8, checks: 156
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035261907
Train loss (w/o reg) on all data: 0.0266627
Test loss (w/o reg) on all data: 0.043974712
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1330764e-06
Norm of the params: 13.114271
     Influence (LOO): fixed  85 labels. Loss 0.04397. Accuracy 0.985.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01194223
Train loss (w/o reg) on all data: 0.005460143
Test loss (w/o reg) on all data: 0.020846285
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.9924763e-07
Norm of the params: 11.386033
                Loss: fixed  97 labels. Loss 0.02085. Accuracy 0.992.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1901715
Train loss (w/o reg) on all data: 0.18281884
Test loss (w/o reg) on all data: 0.12545833
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.97999e-05
Norm of the params: 12.126542
              Random: fixed  12 labels. Loss 0.12546. Accuracy 0.958.
### Flips: 208, rs: 8, checks: 208
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01806577
Train loss (w/o reg) on all data: 0.0104411375
Test loss (w/o reg) on all data: 0.024628447
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3191908e-06
Norm of the params: 12.348791
     Influence (LOO): fixed  94 labels. Loss 0.02463. Accuracy 0.989.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00824312
Train loss (w/o reg) on all data: 0.0031556652
Test loss (w/o reg) on all data: 0.017285822
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1068212e-07
Norm of the params: 10.087076
                Loss: fixed 100 labels. Loss 0.01729. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18022685
Train loss (w/o reg) on all data: 0.17227907
Test loss (w/o reg) on all data: 0.11919742
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 9.718096e-06
Norm of the params: 12.607752
              Random: fixed  18 labels. Loss 0.11920. Accuracy 0.958.
### Flips: 208, rs: 8, checks: 260
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01156541
Train loss (w/o reg) on all data: 0.005493782
Test loss (w/o reg) on all data: 0.020691605
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.7748847e-06
Norm of the params: 11.019644
     Influence (LOO): fixed  98 labels. Loss 0.02069. Accuracy 0.996.
Using normal model
LBFGS training took [42] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008243119
Train loss (w/o reg) on all data: 0.003155832
Test loss (w/o reg) on all data: 0.01728638
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.466293e-07
Norm of the params: 10.08691
                Loss: fixed 100 labels. Loss 0.01729. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17121853
Train loss (w/o reg) on all data: 0.16364822
Test loss (w/o reg) on all data: 0.110206224
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.7761998e-05
Norm of the params: 12.304722
              Random: fixed  26 labels. Loss 0.11021. Accuracy 0.962.
### Flips: 208, rs: 8, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729844
Test loss (w/o reg) on all data: 0.012054302
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.877166e-07
Norm of the params: 9.153178
     Influence (LOO): fixed 101 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [37] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008243119
Train loss (w/o reg) on all data: 0.0031558035
Test loss (w/o reg) on all data: 0.017286135
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.790289e-08
Norm of the params: 10.086938
                Loss: fixed 100 labels. Loss 0.01729. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15792617
Train loss (w/o reg) on all data: 0.15012455
Test loss (w/o reg) on all data: 0.10396386
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.043446e-05
Norm of the params: 12.491297
              Random: fixed  34 labels. Loss 0.10396. Accuracy 0.969.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23357546
Train loss (w/o reg) on all data: 0.22771797
Test loss (w/o reg) on all data: 0.14515756
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.395004e-05
Norm of the params: 10.823581
Flipped loss: 0.14516. Accuracy: 0.962
### Flips: 208, rs: 9, checks: 52
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15039246
Train loss (w/o reg) on all data: 0.14149377
Test loss (w/o reg) on all data: 0.093409866
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.243864e-05
Norm of the params: 13.340683
     Influence (LOO): fixed  42 labels. Loss 0.09341. Accuracy 0.977.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10681912
Train loss (w/o reg) on all data: 0.09348132
Test loss (w/o reg) on all data: 0.09789051
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.4175227e-06
Norm of the params: 16.332672
                Loss: fixed  52 labels. Loss 0.09789. Accuracy 0.958.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22752641
Train loss (w/o reg) on all data: 0.22153056
Test loss (w/o reg) on all data: 0.14241068
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.9199717e-05
Norm of the params: 10.950661
              Random: fixed   4 labels. Loss 0.14241. Accuracy 0.966.
### Flips: 208, rs: 9, checks: 104
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082253635
Train loss (w/o reg) on all data: 0.07277862
Test loss (w/o reg) on all data: 0.056422662
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.702873e-06
Norm of the params: 13.765913
     Influence (LOO): fixed  74 labels. Loss 0.05642. Accuracy 0.981.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025052216
Train loss (w/o reg) on all data: 0.014062061
Test loss (w/o reg) on all data: 0.04425527
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.4695137e-07
Norm of the params: 14.825759
                Loss: fixed  92 labels. Loss 0.04426. Accuracy 0.977.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22354753
Train loss (w/o reg) on all data: 0.21771759
Test loss (w/o reg) on all data: 0.1396602
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.830326e-05
Norm of the params: 10.7981
              Random: fixed   8 labels. Loss 0.13966. Accuracy 0.966.
### Flips: 208, rs: 9, checks: 156
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041969664
Train loss (w/o reg) on all data: 0.033588894
Test loss (w/o reg) on all data: 0.036251225
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.84312e-06
Norm of the params: 12.946635
     Influence (LOO): fixed  94 labels. Loss 0.03625. Accuracy 0.989.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014031136
Train loss (w/o reg) on all data: 0.0068344497
Test loss (w/o reg) on all data: 0.028567445
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.372111e-06
Norm of the params: 11.997239
                Loss: fixed 103 labels. Loss 0.02857. Accuracy 0.989.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22158231
Train loss (w/o reg) on all data: 0.21548638
Test loss (w/o reg) on all data: 0.14051026
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.8196371e-05
Norm of the params: 11.041682
              Random: fixed   9 labels. Loss 0.14051. Accuracy 0.969.
### Flips: 208, rs: 9, checks: 208
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012856885
Train loss (w/o reg) on all data: 0.00616293
Test loss (w/o reg) on all data: 0.021265142
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.04237e-07
Norm of the params: 11.570614
     Influence (LOO): fixed 105 labels. Loss 0.02127. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011119353
Train loss (w/o reg) on all data: 0.004856243
Test loss (w/o reg) on all data: 0.020443574
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.2996232e-07
Norm of the params: 11.1920595
                Loss: fixed 105 labels. Loss 0.02044. Accuracy 0.989.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21450071
Train loss (w/o reg) on all data: 0.20848612
Test loss (w/o reg) on all data: 0.13491724
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.6062782e-05
Norm of the params: 10.96776
              Random: fixed  16 labels. Loss 0.13492. Accuracy 0.969.
### Flips: 208, rs: 9, checks: 260
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006805393
Train loss (w/o reg) on all data: 0.0024350926
Test loss (w/o reg) on all data: 0.010978604
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.982436e-07
Norm of the params: 9.349118
     Influence (LOO): fixed 109 labels. Loss 0.01098. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010626057
Train loss (w/o reg) on all data: 0.0044497526
Test loss (w/o reg) on all data: 0.01899343
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5557525e-07
Norm of the params: 11.114229
                Loss: fixed 106 labels. Loss 0.01899. Accuracy 0.992.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19806106
Train loss (w/o reg) on all data: 0.19125257
Test loss (w/o reg) on all data: 0.12617004
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.0107283e-05
Norm of the params: 11.669176
              Random: fixed  25 labels. Loss 0.12617. Accuracy 0.969.
### Flips: 208, rs: 9, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729849
Test loss (w/o reg) on all data: 0.012054571
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.492796e-07
Norm of the params: 9.153177
     Influence (LOO): fixed 110 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008441605
Train loss (w/o reg) on all data: 0.0033448413
Test loss (w/o reg) on all data: 0.013186199
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6566433e-07
Norm of the params: 10.0963
                Loss: fixed 108 labels. Loss 0.01319. Accuracy 0.992.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19039755
Train loss (w/o reg) on all data: 0.183627
Test loss (w/o reg) on all data: 0.12118999
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.7043206e-05
Norm of the params: 11.6366205
              Random: fixed  30 labels. Loss 0.12119. Accuracy 0.977.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23819026
Train loss (w/o reg) on all data: 0.23263024
Test loss (w/o reg) on all data: 0.15661728
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.2547126e-05
Norm of the params: 10.54517
Flipped loss: 0.15662. Accuracy: 0.958
### Flips: 208, rs: 10, checks: 52
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1614138
Train loss (w/o reg) on all data: 0.15208021
Test loss (w/o reg) on all data: 0.13639285
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.7900029e-05
Norm of the params: 13.662794
     Influence (LOO): fixed  36 labels. Loss 0.13639. Accuracy 0.947.
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123222426
Train loss (w/o reg) on all data: 0.111171566
Test loss (w/o reg) on all data: 0.13339743
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.773558e-06
Norm of the params: 15.524729
                Loss: fixed  50 labels. Loss 0.13340. Accuracy 0.954.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22158998
Train loss (w/o reg) on all data: 0.2155237
Test loss (w/o reg) on all data: 0.14805771
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.772981e-05
Norm of the params: 11.014783
              Random: fixed  12 labels. Loss 0.14806. Accuracy 0.958.
### Flips: 208, rs: 10, checks: 104
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10458093
Train loss (w/o reg) on all data: 0.09402375
Test loss (w/o reg) on all data: 0.08155128
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.4588647e-06
Norm of the params: 14.530781
     Influence (LOO): fixed  70 labels. Loss 0.08155. Accuracy 0.985.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046267502
Train loss (w/o reg) on all data: 0.03175503
Test loss (w/o reg) on all data: 0.092130035
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.615759e-06
Norm of the params: 17.036709
                Loss: fixed  95 labels. Loss 0.09213. Accuracy 0.973.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21660554
Train loss (w/o reg) on all data: 0.21019559
Test loss (w/o reg) on all data: 0.14580894
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.2866102e-05
Norm of the params: 11.322508
              Random: fixed  15 labels. Loss 0.14581. Accuracy 0.958.
### Flips: 208, rs: 10, checks: 156
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053652506
Train loss (w/o reg) on all data: 0.04351364
Test loss (w/o reg) on all data: 0.057072915
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7169756e-06
Norm of the params: 14.23999
     Influence (LOO): fixed  94 labels. Loss 0.05707. Accuracy 0.985.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018465389
Train loss (w/o reg) on all data: 0.009011591
Test loss (w/o reg) on all data: 0.021059843
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4288838e-06
Norm of the params: 13.750489
                Loss: fixed 109 labels. Loss 0.02106. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2052961
Train loss (w/o reg) on all data: 0.19837512
Test loss (w/o reg) on all data: 0.13378581
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.6856484e-05
Norm of the params: 11.765185
              Random: fixed  23 labels. Loss 0.13379. Accuracy 0.962.
### Flips: 208, rs: 10, checks: 208
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026369335
Train loss (w/o reg) on all data: 0.018092122
Test loss (w/o reg) on all data: 0.025037957
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.619752e-06
Norm of the params: 12.866401
     Influence (LOO): fixed 108 labels. Loss 0.02504. Accuracy 0.992.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015003497
Train loss (w/o reg) on all data: 0.0068788077
Test loss (w/o reg) on all data: 0.021055149
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.169692e-07
Norm of the params: 12.747305
                Loss: fixed 113 labels. Loss 0.02106. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19550821
Train loss (w/o reg) on all data: 0.18821305
Test loss (w/o reg) on all data: 0.12341074
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 7.638863e-06
Norm of the params: 12.079046
              Random: fixed  30 labels. Loss 0.12341. Accuracy 0.969.
### Flips: 208, rs: 10, checks: 260
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011169191
Train loss (w/o reg) on all data: 0.005562778
Test loss (w/o reg) on all data: 0.016671944
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7209563e-07
Norm of the params: 10.589065
     Influence (LOO): fixed 119 labels. Loss 0.01667. Accuracy 0.992.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00945686
Train loss (w/o reg) on all data: 0.003875052
Test loss (w/o reg) on all data: 0.014068016
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.11671e-06
Norm of the params: 10.565802
                Loss: fixed 118 labels. Loss 0.01407. Accuracy 0.996.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.185288
Train loss (w/o reg) on all data: 0.17763233
Test loss (w/o reg) on all data: 0.11422652
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.973688e-05
Norm of the params: 12.373896
              Random: fixed  36 labels. Loss 0.11423. Accuracy 0.973.
### Flips: 208, rs: 10, checks: 312
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730482
Test loss (w/o reg) on all data: 0.012054657
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.924985e-07
Norm of the params: 9.15311
     Influence (LOO): fixed 122 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009456862
Train loss (w/o reg) on all data: 0.0038752318
Test loss (w/o reg) on all data: 0.014067474
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.437601e-07
Norm of the params: 10.565633
                Loss: fixed 118 labels. Loss 0.01407. Accuracy 0.996.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18076248
Train loss (w/o reg) on all data: 0.17355645
Test loss (w/o reg) on all data: 0.10283098
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.45909e-05
Norm of the params: 12.005026
              Random: fixed  39 labels. Loss 0.10283. Accuracy 0.985.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22045666
Train loss (w/o reg) on all data: 0.2137936
Test loss (w/o reg) on all data: 0.124215655
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8997938e-05
Norm of the params: 11.543876
Flipped loss: 0.12422. Accuracy: 0.973
### Flips: 208, rs: 11, checks: 52
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12789509
Train loss (w/o reg) on all data: 0.119461596
Test loss (w/o reg) on all data: 0.10089841
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.121339e-06
Norm of the params: 12.98729
     Influence (LOO): fixed  44 labels. Loss 0.10090. Accuracy 0.958.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09437404
Train loss (w/o reg) on all data: 0.07873874
Test loss (w/o reg) on all data: 0.08973636
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1338155e-05
Norm of the params: 17.68349
                Loss: fixed  51 labels. Loss 0.08974. Accuracy 0.966.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21440049
Train loss (w/o reg) on all data: 0.20788896
Test loss (w/o reg) on all data: 0.1196945
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.053303e-05
Norm of the params: 11.411861
              Random: fixed   7 labels. Loss 0.11969. Accuracy 0.981.
### Flips: 208, rs: 11, checks: 104
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060161524
Train loss (w/o reg) on all data: 0.051222604
Test loss (w/o reg) on all data: 0.050356276
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.0429961e-05
Norm of the params: 13.370805
     Influence (LOO): fixed  77 labels. Loss 0.05036. Accuracy 0.977.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02172251
Train loss (w/o reg) on all data: 0.011374092
Test loss (w/o reg) on all data: 0.021253219
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.001192e-07
Norm of the params: 14.3863945
                Loss: fixed  91 labels. Loss 0.02125. Accuracy 0.989.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2076847
Train loss (w/o reg) on all data: 0.20123988
Test loss (w/o reg) on all data: 0.11509058
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.0497463e-05
Norm of the params: 11.353246
              Random: fixed  12 labels. Loss 0.11509. Accuracy 0.973.
### Flips: 208, rs: 11, checks: 156
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030952409
Train loss (w/o reg) on all data: 0.023746587
Test loss (w/o reg) on all data: 0.026038142
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.187683e-06
Norm of the params: 12.004851
     Influence (LOO): fixed  91 labels. Loss 0.02604. Accuracy 0.989.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011478962
Train loss (w/o reg) on all data: 0.004648867
Test loss (w/o reg) on all data: 0.013333771
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.738699e-07
Norm of the params: 11.687682
                Loss: fixed  99 labels. Loss 0.01333. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20054033
Train loss (w/o reg) on all data: 0.19451044
Test loss (w/o reg) on all data: 0.10719698
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.772672e-06
Norm of the params: 10.981709
              Random: fixed  17 labels. Loss 0.10720. Accuracy 0.992.
### Flips: 208, rs: 11, checks: 208
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01758441
Train loss (w/o reg) on all data: 0.010936132
Test loss (w/o reg) on all data: 0.017868984
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4034016e-06
Norm of the params: 11.53107
     Influence (LOO): fixed  97 labels. Loss 0.01787. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008825903
Train loss (w/o reg) on all data: 0.0033094357
Test loss (w/o reg) on all data: 0.0110445805
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3483909e-07
Norm of the params: 10.503778
                Loss: fixed 101 labels. Loss 0.01104. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18142283
Train loss (w/o reg) on all data: 0.17501676
Test loss (w/o reg) on all data: 0.09432643
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.3849584e-05
Norm of the params: 11.319065
              Random: fixed  29 labels. Loss 0.09433. Accuracy 0.989.
### Flips: 208, rs: 11, checks: 260
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010170892
Train loss (w/o reg) on all data: 0.0048329127
Test loss (w/o reg) on all data: 0.012899312
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.9476316e-07
Norm of the params: 10.332453
     Influence (LOO): fixed 101 labels. Loss 0.01290. Accuracy 0.992.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008825902
Train loss (w/o reg) on all data: 0.003309455
Test loss (w/o reg) on all data: 0.01104477
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5930992e-07
Norm of the params: 10.503758
                Loss: fixed 101 labels. Loss 0.01104. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17288221
Train loss (w/o reg) on all data: 0.1665253
Test loss (w/o reg) on all data: 0.08923292
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.97404e-06
Norm of the params: 11.275559
              Random: fixed  34 labels. Loss 0.08923. Accuracy 0.985.
### Flips: 208, rs: 11, checks: 312
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01017089
Train loss (w/o reg) on all data: 0.004832764
Test loss (w/o reg) on all data: 0.012901177
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2374126e-07
Norm of the params: 10.332595
     Influence (LOO): fixed 101 labels. Loss 0.01290. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008825902
Train loss (w/o reg) on all data: 0.0033094324
Test loss (w/o reg) on all data: 0.011044615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.57589e-08
Norm of the params: 10.503779
                Loss: fixed 101 labels. Loss 0.01104. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15707356
Train loss (w/o reg) on all data: 0.14941967
Test loss (w/o reg) on all data: 0.09094194
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.4796786e-06
Norm of the params: 12.372461
              Random: fixed  41 labels. Loss 0.09094. Accuracy 0.973.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23358321
Train loss (w/o reg) on all data: 0.2267542
Test loss (w/o reg) on all data: 0.17438327
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 6.0459966e-05
Norm of the params: 11.686748
Flipped loss: 0.17438. Accuracy: 0.931
### Flips: 208, rs: 12, checks: 52
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15513551
Train loss (w/o reg) on all data: 0.14511089
Test loss (w/o reg) on all data: 0.122194275
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.0223843e-05
Norm of the params: 14.159537
     Influence (LOO): fixed  39 labels. Loss 0.12219. Accuracy 0.954.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11200614
Train loss (w/o reg) on all data: 0.09822462
Test loss (w/o reg) on all data: 0.16098756
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.5943437e-06
Norm of the params: 16.602121
                Loss: fixed  51 labels. Loss 0.16099. Accuracy 0.935.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22783203
Train loss (w/o reg) on all data: 0.22089608
Test loss (w/o reg) on all data: 0.17048447
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.4987896e-05
Norm of the params: 11.777906
              Random: fixed   4 labels. Loss 0.17048. Accuracy 0.935.
### Flips: 208, rs: 12, checks: 104
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.099110395
Train loss (w/o reg) on all data: 0.08938628
Test loss (w/o reg) on all data: 0.07967642
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2116435e-05
Norm of the params: 13.94569
     Influence (LOO): fixed  67 labels. Loss 0.07968. Accuracy 0.966.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04038047
Train loss (w/o reg) on all data: 0.026274126
Test loss (w/o reg) on all data: 0.068120375
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3874942e-06
Norm of the params: 16.796635
                Loss: fixed  93 labels. Loss 0.06812. Accuracy 0.977.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22200295
Train loss (w/o reg) on all data: 0.21484566
Test loss (w/o reg) on all data: 0.17124236
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.5533207e-05
Norm of the params: 11.964355
              Random: fixed   7 labels. Loss 0.17124. Accuracy 0.939.
### Flips: 208, rs: 12, checks: 156
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045775555
Train loss (w/o reg) on all data: 0.036870167
Test loss (w/o reg) on all data: 0.044430334
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.7790295e-06
Norm of the params: 13.345702
     Influence (LOO): fixed  93 labels. Loss 0.04443. Accuracy 0.985.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016641987
Train loss (w/o reg) on all data: 0.0078044552
Test loss (w/o reg) on all data: 0.0254666
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6742096e-06
Norm of the params: 13.294761
                Loss: fixed 106 labels. Loss 0.02547. Accuracy 0.989.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21935195
Train loss (w/o reg) on all data: 0.21211167
Test loss (w/o reg) on all data: 0.1611808
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.05835325e-05
Norm of the params: 12.033524
              Random: fixed  12 labels. Loss 0.16118. Accuracy 0.939.
### Flips: 208, rs: 12, checks: 208
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024778908
Train loss (w/o reg) on all data: 0.017089443
Test loss (w/o reg) on all data: 0.022018462
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.0353776e-06
Norm of the params: 12.401181
     Influence (LOO): fixed 104 labels. Loss 0.02202. Accuracy 0.992.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011800736
Train loss (w/o reg) on all data: 0.0051454287
Test loss (w/o reg) on all data: 0.025445743
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0100897e-07
Norm of the params: 11.537164
                Loss: fixed 110 labels. Loss 0.02545. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21428056
Train loss (w/o reg) on all data: 0.20688164
Test loss (w/o reg) on all data: 0.14445578
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.6246257e-05
Norm of the params: 12.164633
              Random: fixed  17 labels. Loss 0.14446. Accuracy 0.947.
### Flips: 208, rs: 12, checks: 260
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010311959
Train loss (w/o reg) on all data: 0.0052795857
Test loss (w/o reg) on all data: 0.014923658
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4886926e-07
Norm of the params: 10.032322
     Influence (LOO): fixed 112 labels. Loss 0.01492. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009994649
Train loss (w/o reg) on all data: 0.004006465
Test loss (w/o reg) on all data: 0.0194582
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1172007e-07
Norm of the params: 10.943661
                Loss: fixed 111 labels. Loss 0.01946. Accuracy 0.992.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20117575
Train loss (w/o reg) on all data: 0.19360311
Test loss (w/o reg) on all data: 0.13951187
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.358007e-05
Norm of the params: 12.306609
              Random: fixed  24 labels. Loss 0.13951. Accuracy 0.958.
### Flips: 208, rs: 12, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002173011
Test loss (w/o reg) on all data: 0.012055208
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0721513e-07
Norm of the params: 9.1531515
     Influence (LOO): fixed 114 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [43] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007376437
Train loss (w/o reg) on all data: 0.0026224984
Test loss (w/o reg) on all data: 0.013233006
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2824126e-07
Norm of the params: 9.750834
                Loss: fixed 113 labels. Loss 0.01323. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18743427
Train loss (w/o reg) on all data: 0.17976967
Test loss (w/o reg) on all data: 0.12844034
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.332154e-06
Norm of the params: 12.38112
              Random: fixed  33 labels. Loss 0.12844. Accuracy 0.966.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21141782
Train loss (w/o reg) on all data: 0.20448565
Test loss (w/o reg) on all data: 0.16107409
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.7028544e-05
Norm of the params: 11.774692
Flipped loss: 0.16107. Accuracy: 0.962
### Flips: 208, rs: 13, checks: 52
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12168267
Train loss (w/o reg) on all data: 0.11063229
Test loss (w/o reg) on all data: 0.1262595
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.8641742e-05
Norm of the params: 14.8663225
     Influence (LOO): fixed  43 labels. Loss 0.12626. Accuracy 0.950.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08703375
Train loss (w/o reg) on all data: 0.0699316
Test loss (w/o reg) on all data: 0.13559741
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 9.218393e-06
Norm of the params: 18.494404
                Loss: fixed  50 labels. Loss 0.13560. Accuracy 0.958.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20957728
Train loss (w/o reg) on all data: 0.20241286
Test loss (w/o reg) on all data: 0.15207703
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.3566008e-05
Norm of the params: 11.970313
              Random: fixed   2 labels. Loss 0.15208. Accuracy 0.958.
### Flips: 208, rs: 13, checks: 104
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06620573
Train loss (w/o reg) on all data: 0.05561551
Test loss (w/o reg) on all data: 0.07300938
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.752609e-06
Norm of the params: 14.553504
     Influence (LOO): fixed  71 labels. Loss 0.07301. Accuracy 0.973.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028470315
Train loss (w/o reg) on all data: 0.016546315
Test loss (w/o reg) on all data: 0.056897562
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.6272347e-06
Norm of the params: 15.442798
                Loss: fixed  85 labels. Loss 0.05690. Accuracy 0.969.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20313615
Train loss (w/o reg) on all data: 0.19572088
Test loss (w/o reg) on all data: 0.14375718
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.04961355e-05
Norm of the params: 12.178063
              Random: fixed   6 labels. Loss 0.14376. Accuracy 0.962.
### Flips: 208, rs: 13, checks: 156
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03267797
Train loss (w/o reg) on all data: 0.022801273
Test loss (w/o reg) on all data: 0.044628173
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.268126e-06
Norm of the params: 14.054679
     Influence (LOO): fixed  88 labels. Loss 0.04463. Accuracy 0.981.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013637908
Train loss (w/o reg) on all data: 0.00591744
Test loss (w/o reg) on all data: 0.01704917
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.6928553e-07
Norm of the params: 12.426156
                Loss: fixed  98 labels. Loss 0.01705. Accuracy 0.989.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18281256
Train loss (w/o reg) on all data: 0.17455786
Test loss (w/o reg) on all data: 0.13123882
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.1657084e-05
Norm of the params: 12.848882
              Random: fixed  17 labels. Loss 0.13124. Accuracy 0.950.
### Flips: 208, rs: 13, checks: 208
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009672626
Train loss (w/o reg) on all data: 0.0039269915
Test loss (w/o reg) on all data: 0.015537365
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.535423e-07
Norm of the params: 10.719733
     Influence (LOO): fixed 102 labels. Loss 0.01554. Accuracy 0.992.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011315547
Train loss (w/o reg) on all data: 0.0046085967
Test loss (w/o reg) on all data: 0.018083164
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3234352e-07
Norm of the params: 11.58184
                Loss: fixed 100 labels. Loss 0.01808. Accuracy 0.989.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17533796
Train loss (w/o reg) on all data: 0.1669312
Test loss (w/o reg) on all data: 0.117259756
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2865255e-05
Norm of the params: 12.966694
              Random: fixed  25 labels. Loss 0.11726. Accuracy 0.954.
### Flips: 208, rs: 13, checks: 260
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008618487
Train loss (w/o reg) on all data: 0.0033137673
Test loss (w/o reg) on all data: 0.014072673
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.592203e-07
Norm of the params: 10.300214
     Influence (LOO): fixed 103 labels. Loss 0.01407. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0111265415
Train loss (w/o reg) on all data: 0.004530029
Test loss (w/o reg) on all data: 0.01688669
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.5228465e-07
Norm of the params: 11.48609
                Loss: fixed 101 labels. Loss 0.01689. Accuracy 0.989.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17533797
Train loss (w/o reg) on all data: 0.16693296
Test loss (w/o reg) on all data: 0.11725284
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.8151362e-05
Norm of the params: 12.965349
              Random: fixed  25 labels. Loss 0.11725. Accuracy 0.954.
### Flips: 208, rs: 13, checks: 312
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075718416
Train loss (w/o reg) on all data: 0.002812501
Test loss (w/o reg) on all data: 0.013496916
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9246333e-07
Norm of the params: 9.756373
     Influence (LOO): fixed 104 labels. Loss 0.01350. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01112654
Train loss (w/o reg) on all data: 0.0045297123
Test loss (w/o reg) on all data: 0.016888445
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0542288e-06
Norm of the params: 11.486363
                Loss: fixed 101 labels. Loss 0.01689. Accuracy 0.989.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16562597
Train loss (w/o reg) on all data: 0.15714473
Test loss (w/o reg) on all data: 0.09911352
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.565937e-05
Norm of the params: 13.024021
              Random: fixed  31 labels. Loss 0.09911. Accuracy 0.962.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20511185
Train loss (w/o reg) on all data: 0.19853427
Test loss (w/o reg) on all data: 0.14487644
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.3527926e-05
Norm of the params: 11.469592
Flipped loss: 0.14488. Accuracy: 0.947
### Flips: 208, rs: 14, checks: 52
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108086616
Train loss (w/o reg) on all data: 0.09526849
Test loss (w/o reg) on all data: 0.10438659
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.708932e-06
Norm of the params: 16.011326
     Influence (LOO): fixed  41 labels. Loss 0.10439. Accuracy 0.969.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076336265
Train loss (w/o reg) on all data: 0.059592742
Test loss (w/o reg) on all data: 0.087479904
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.5172898e-06
Norm of the params: 18.299465
                Loss: fixed  51 labels. Loss 0.08748. Accuracy 0.973.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20195098
Train loss (w/o reg) on all data: 0.19522506
Test loss (w/o reg) on all data: 0.14239565
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.628997e-05
Norm of the params: 11.5982065
              Random: fixed   2 labels. Loss 0.14240. Accuracy 0.947.
### Flips: 208, rs: 14, checks: 104
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.070051536
Train loss (w/o reg) on all data: 0.057359032
Test loss (w/o reg) on all data: 0.0862235
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.5425672e-06
Norm of the params: 15.9326725
     Influence (LOO): fixed  68 labels. Loss 0.08622. Accuracy 0.969.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03892018
Train loss (w/o reg) on all data: 0.024429016
Test loss (w/o reg) on all data: 0.052860286
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.4715955e-06
Norm of the params: 17.024199
                Loss: fixed  77 labels. Loss 0.05286. Accuracy 0.981.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18820697
Train loss (w/o reg) on all data: 0.18142499
Test loss (w/o reg) on all data: 0.13215578
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.0534514e-05
Norm of the params: 11.646436
              Random: fixed  13 labels. Loss 0.13216. Accuracy 0.950.
### Flips: 208, rs: 14, checks: 156
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04010041
Train loss (w/o reg) on all data: 0.030479511
Test loss (w/o reg) on all data: 0.04292587
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.082308e-06
Norm of the params: 13.871482
     Influence (LOO): fixed  86 labels. Loss 0.04293. Accuracy 0.985.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009163516
Train loss (w/o reg) on all data: 0.0035972667
Test loss (w/o reg) on all data: 0.020677187
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.3684795e-07
Norm of the params: 10.551065
                Loss: fixed  97 labels. Loss 0.02068. Accuracy 0.989.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16721442
Train loss (w/o reg) on all data: 0.15873815
Test loss (w/o reg) on all data: 0.12888674
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.686501e-05
Norm of the params: 13.020194
              Random: fixed  21 labels. Loss 0.12889. Accuracy 0.947.
### Flips: 208, rs: 14, checks: 208
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014846334
Train loss (w/o reg) on all data: 0.007958282
Test loss (w/o reg) on all data: 0.02366764
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.006505e-06
Norm of the params: 11.737165
     Influence (LOO): fixed  98 labels. Loss 0.02367. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0085450355
Train loss (w/o reg) on all data: 0.0032871314
Test loss (w/o reg) on all data: 0.020061705
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0192228e-07
Norm of the params: 10.254662
                Loss: fixed  98 labels. Loss 0.02006. Accuracy 0.989.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16091152
Train loss (w/o reg) on all data: 0.15273
Test loss (w/o reg) on all data: 0.102947935
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.2793148e-05
Norm of the params: 12.791806
              Random: fixed  28 labels. Loss 0.10295. Accuracy 0.966.
### Flips: 208, rs: 14, checks: 260
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008674959
Train loss (w/o reg) on all data: 0.0034818982
Test loss (w/o reg) on all data: 0.016870828
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1066299e-07
Norm of the params: 10.191233
     Influence (LOO): fixed 100 labels. Loss 0.01687. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074646445
Train loss (w/o reg) on all data: 0.002815381
Test loss (w/o reg) on all data: 0.015515699
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1815852e-07
Norm of the params: 9.642888
                Loss: fixed 100 labels. Loss 0.01552. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15250957
Train loss (w/o reg) on all data: 0.14443083
Test loss (w/o reg) on all data: 0.09808493
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.728947e-06
Norm of the params: 12.71121
              Random: fixed  33 labels. Loss 0.09808. Accuracy 0.969.
### Flips: 208, rs: 14, checks: 312
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021729288
Test loss (w/o reg) on all data: 0.012054655
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8674734e-07
Norm of the params: 9.153241
     Influence (LOO): fixed 101 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [1] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729674
Test loss (w/o reg) on all data: 0.012053971
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0829623e-07
Norm of the params: 9.153197
                Loss: fixed 101 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14810897
Train loss (w/o reg) on all data: 0.14033483
Test loss (w/o reg) on all data: 0.09660009
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.48034e-05
Norm of the params: 12.469276
              Random: fixed  36 labels. Loss 0.09660. Accuracy 0.973.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2117039
Train loss (w/o reg) on all data: 0.2044875
Test loss (w/o reg) on all data: 0.1469673
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.5938072e-05
Norm of the params: 12.013654
Flipped loss: 0.14697. Accuracy: 0.947
### Flips: 208, rs: 15, checks: 52
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119125485
Train loss (w/o reg) on all data: 0.10875802
Test loss (w/o reg) on all data: 0.10329491
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.4140063e-05
Norm of the params: 14.399633
     Influence (LOO): fixed  41 labels. Loss 0.10329. Accuracy 0.958.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.087003954
Train loss (w/o reg) on all data: 0.07220656
Test loss (w/o reg) on all data: 0.12069925
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.4467909e-06
Norm of the params: 17.203138
                Loss: fixed  50 labels. Loss 0.12070. Accuracy 0.954.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20345342
Train loss (w/o reg) on all data: 0.19661511
Test loss (w/o reg) on all data: 0.12633061
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.753168e-05
Norm of the params: 11.694706
              Random: fixed   7 labels. Loss 0.12633. Accuracy 0.962.
### Flips: 208, rs: 15, checks: 104
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06759197
Train loss (w/o reg) on all data: 0.058612365
Test loss (w/o reg) on all data: 0.050847176
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.662025e-06
Norm of the params: 13.401199
     Influence (LOO): fixed  70 labels. Loss 0.05085. Accuracy 0.989.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018951382
Train loss (w/o reg) on all data: 0.009447267
Test loss (w/o reg) on all data: 0.03947338
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.8273274e-06
Norm of the params: 13.787033
                Loss: fixed  89 labels. Loss 0.03947. Accuracy 0.977.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19704969
Train loss (w/o reg) on all data: 0.18979102
Test loss (w/o reg) on all data: 0.13316216
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.613011e-05
Norm of the params: 12.048793
              Random: fixed  11 labels. Loss 0.13316. Accuracy 0.954.
### Flips: 208, rs: 15, checks: 156
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028072545
Train loss (w/o reg) on all data: 0.020408016
Test loss (w/o reg) on all data: 0.02128625
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.4261805e-06
Norm of the params: 12.381059
     Influence (LOO): fixed  88 labels. Loss 0.02129. Accuracy 0.989.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01155109
Train loss (w/o reg) on all data: 0.0051495438
Test loss (w/o reg) on all data: 0.015936809
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.8582027e-07
Norm of the params: 11.315075
                Loss: fixed  95 labels. Loss 0.01594. Accuracy 0.989.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18729109
Train loss (w/o reg) on all data: 0.17990962
Test loss (w/o reg) on all data: 0.12597135
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.729379e-05
Norm of the params: 12.15029
              Random: fixed  17 labels. Loss 0.12597. Accuracy 0.962.
### Flips: 208, rs: 15, checks: 208
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012247436
Train loss (w/o reg) on all data: 0.006260069
Test loss (w/o reg) on all data: 0.012106508
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1648843e-07
Norm of the params: 10.942912
     Influence (LOO): fixed  97 labels. Loss 0.01211. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008821738
Train loss (w/o reg) on all data: 0.0032996847
Test loss (w/o reg) on all data: 0.01929711
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9888145e-07
Norm of the params: 10.509094
                Loss: fixed  97 labels. Loss 0.01930. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17714185
Train loss (w/o reg) on all data: 0.17024054
Test loss (w/o reg) on all data: 0.11597839
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.128457e-05
Norm of the params: 11.748459
              Random: fixed  24 labels. Loss 0.11598. Accuracy 0.966.
### Flips: 208, rs: 15, checks: 260
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.002172926
Test loss (w/o reg) on all data: 0.012054933
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5532007e-07
Norm of the params: 9.153243
     Influence (LOO): fixed 100 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007400997
Train loss (w/o reg) on all data: 0.002645353
Test loss (w/o reg) on all data: 0.017559966
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.749425e-07
Norm of the params: 9.7525835
                Loss: fixed  98 labels. Loss 0.01756. Accuracy 0.989.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16519265
Train loss (w/o reg) on all data: 0.1578908
Test loss (w/o reg) on all data: 0.10472247
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.119392e-05
Norm of the params: 12.084583
              Random: fixed  30 labels. Loss 0.10472. Accuracy 0.962.
### Flips: 208, rs: 15, checks: 312
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730557
Test loss (w/o reg) on all data: 0.012053752
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.6268017e-07
Norm of the params: 9.153102
     Influence (LOO): fixed 100 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [41] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006932771
Train loss (w/o reg) on all data: 0.0024051473
Test loss (w/o reg) on all data: 0.016262157
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2921119e-07
Norm of the params: 9.515906
                Loss: fixed  99 labels. Loss 0.01626. Accuracy 0.989.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15285471
Train loss (w/o reg) on all data: 0.14586635
Test loss (w/o reg) on all data: 0.0945893
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.665334e-05
Norm of the params: 11.822322
              Random: fixed  37 labels. Loss 0.09459. Accuracy 0.973.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23049082
Train loss (w/o reg) on all data: 0.224356
Test loss (w/o reg) on all data: 0.15633848
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.7761695e-05
Norm of the params: 11.076838
Flipped loss: 0.15634. Accuracy: 0.962
### Flips: 208, rs: 16, checks: 52
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12613015
Train loss (w/o reg) on all data: 0.11690965
Test loss (w/o reg) on all data: 0.09493619
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8467183e-05
Norm of the params: 13.579761
     Influence (LOO): fixed  47 labels. Loss 0.09494. Accuracy 0.966.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0972925
Train loss (w/o reg) on all data: 0.08276543
Test loss (w/o reg) on all data: 0.1129235
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.037589e-06
Norm of the params: 17.045273
                Loss: fixed  52 labels. Loss 0.11292. Accuracy 0.958.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22433135
Train loss (w/o reg) on all data: 0.21793832
Test loss (w/o reg) on all data: 0.1527953
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.7235268e-05
Norm of the params: 11.307543
              Random: fixed   5 labels. Loss 0.15280. Accuracy 0.962.
### Flips: 208, rs: 16, checks: 104
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09136923
Train loss (w/o reg) on all data: 0.081474684
Test loss (w/o reg) on all data: 0.0660948
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.880908e-06
Norm of the params: 14.06737
     Influence (LOO): fixed  65 labels. Loss 0.06609. Accuracy 0.977.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025039557
Train loss (w/o reg) on all data: 0.013719331
Test loss (w/o reg) on all data: 0.060640138
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.3582309e-06
Norm of the params: 15.046744
                Loss: fixed  94 labels. Loss 0.06064. Accuracy 0.969.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22278993
Train loss (w/o reg) on all data: 0.21677327
Test loss (w/o reg) on all data: 0.14635709
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.3105312e-05
Norm of the params: 10.969648
              Random: fixed   8 labels. Loss 0.14636. Accuracy 0.966.
### Flips: 208, rs: 16, checks: 156
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046218336
Train loss (w/o reg) on all data: 0.036782157
Test loss (w/o reg) on all data: 0.0340511
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.3722154e-06
Norm of the params: 13.73767
     Influence (LOO): fixed  87 labels. Loss 0.03405. Accuracy 0.989.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014883641
Train loss (w/o reg) on all data: 0.0068186778
Test loss (w/o reg) on all data: 0.025684997
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1884713e-07
Norm of the params: 12.700364
                Loss: fixed 103 labels. Loss 0.02568. Accuracy 0.989.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21223927
Train loss (w/o reg) on all data: 0.20615748
Test loss (w/o reg) on all data: 0.135448
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.42041845e-05
Norm of the params: 11.028859
              Random: fixed  15 labels. Loss 0.13545. Accuracy 0.969.
### Flips: 208, rs: 16, checks: 208
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023545023
Train loss (w/o reg) on all data: 0.015531709
Test loss (w/o reg) on all data: 0.023474995
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5534268e-06
Norm of the params: 12.659633
     Influence (LOO): fixed  99 labels. Loss 0.02347. Accuracy 0.989.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012618731
Train loss (w/o reg) on all data: 0.00576188
Test loss (w/o reg) on all data: 0.018589282
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2021925e-07
Norm of the params: 11.710552
                Loss: fixed 106 labels. Loss 0.01859. Accuracy 0.992.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20548484
Train loss (w/o reg) on all data: 0.19925489
Test loss (w/o reg) on all data: 0.12947929
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4385948e-05
Norm of the params: 11.162399
              Random: fixed  18 labels. Loss 0.12948. Accuracy 0.966.
### Flips: 208, rs: 16, checks: 260
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009897504
Train loss (w/o reg) on all data: 0.0043064742
Test loss (w/o reg) on all data: 0.015435246
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.384205e-07
Norm of the params: 10.574526
     Influence (LOO): fixed 107 labels. Loss 0.01544. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007376437
Train loss (w/o reg) on all data: 0.0026222249
Test loss (w/o reg) on all data: 0.01323412
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0629661e-06
Norm of the params: 9.751116
                Loss: fixed 109 labels. Loss 0.01323. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19561222
Train loss (w/o reg) on all data: 0.18926652
Test loss (w/o reg) on all data: 0.12516147
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.0317543e-05
Norm of the params: 11.265613
              Random: fixed  24 labels. Loss 0.12516. Accuracy 0.966.
### Flips: 208, rs: 16, checks: 312
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021730072
Test loss (w/o reg) on all data: 0.012054242
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1508352e-07
Norm of the params: 9.153156
     Influence (LOO): fixed 110 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730058
Test loss (w/o reg) on all data: 0.012054213
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.219609e-08
Norm of the params: 9.153156
                Loss: fixed 110 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18351355
Train loss (w/o reg) on all data: 0.17644228
Test loss (w/o reg) on all data: 0.118190065
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.130692e-05
Norm of the params: 11.892247
              Random: fixed  31 labels. Loss 0.11819. Accuracy 0.969.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22251382
Train loss (w/o reg) on all data: 0.2163449
Test loss (w/o reg) on all data: 0.13090539
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.8477069e-05
Norm of the params: 11.107595
Flipped loss: 0.13091. Accuracy: 0.958
### Flips: 208, rs: 17, checks: 52
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12704802
Train loss (w/o reg) on all data: 0.11699706
Test loss (w/o reg) on all data: 0.07472706
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.233857e-06
Norm of the params: 14.1781225
     Influence (LOO): fixed  46 labels. Loss 0.07473. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09001425
Train loss (w/o reg) on all data: 0.073359594
Test loss (w/o reg) on all data: 0.111049086
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.141856e-06
Norm of the params: 18.25084
                Loss: fixed  51 labels. Loss 0.11105. Accuracy 0.958.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21556324
Train loss (w/o reg) on all data: 0.20949593
Test loss (w/o reg) on all data: 0.12573455
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.2792504e-05
Norm of the params: 11.015718
              Random: fixed   5 labels. Loss 0.12573. Accuracy 0.958.
### Flips: 208, rs: 17, checks: 104
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07106756
Train loss (w/o reg) on all data: 0.062019233
Test loss (w/o reg) on all data: 0.0531429
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.855949e-06
Norm of the params: 13.452376
     Influence (LOO): fixed  75 labels. Loss 0.05314. Accuracy 0.985.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031581346
Train loss (w/o reg) on all data: 0.018546825
Test loss (w/o reg) on all data: 0.03552486
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4936822e-06
Norm of the params: 16.14591
                Loss: fixed  90 labels. Loss 0.03552. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20687805
Train loss (w/o reg) on all data: 0.20092756
Test loss (w/o reg) on all data: 0.113771796
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.0485066e-05
Norm of the params: 10.909161
              Random: fixed  12 labels. Loss 0.11377. Accuracy 0.966.
### Flips: 208, rs: 17, checks: 156
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041730564
Train loss (w/o reg) on all data: 0.033530705
Test loss (w/o reg) on all data: 0.03657421
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.7146273e-06
Norm of the params: 12.806139
     Influence (LOO): fixed  90 labels. Loss 0.03657. Accuracy 0.996.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014037342
Train loss (w/o reg) on all data: 0.0061006155
Test loss (w/o reg) on all data: 0.018264817
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.914413e-06
Norm of the params: 12.5989895
                Loss: fixed 101 labels. Loss 0.01826. Accuracy 0.996.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1959136
Train loss (w/o reg) on all data: 0.18970455
Test loss (w/o reg) on all data: 0.10955368
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.191645e-05
Norm of the params: 11.143645
              Random: fixed  18 labels. Loss 0.10955. Accuracy 0.966.
### Flips: 208, rs: 17, checks: 208
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019768307
Train loss (w/o reg) on all data: 0.011590041
Test loss (w/o reg) on all data: 0.02442105
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2762657e-06
Norm of the params: 12.789267
     Influence (LOO): fixed  99 labels. Loss 0.02442. Accuracy 0.996.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01030631
Train loss (w/o reg) on all data: 0.0043425723
Test loss (w/o reg) on all data: 0.012629162
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.125311e-07
Norm of the params: 10.921298
                Loss: fixed 104 labels. Loss 0.01263. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18724976
Train loss (w/o reg) on all data: 0.18087323
Test loss (w/o reg) on all data: 0.10503594
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.98872e-06
Norm of the params: 11.292953
              Random: fixed  25 labels. Loss 0.10504. Accuracy 0.977.
### Flips: 208, rs: 17, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0100366175
Train loss (w/o reg) on all data: 0.0044227615
Test loss (w/o reg) on all data: 0.02067274
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1950284e-06
Norm of the params: 10.59609
     Influence (LOO): fixed 104 labels. Loss 0.02067. Accuracy 0.989.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008764811
Train loss (w/o reg) on all data: 0.0035084458
Test loss (w/o reg) on all data: 0.012249941
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9289178e-06
Norm of the params: 10.253161
                Loss: fixed 105 labels. Loss 0.01225. Accuracy 0.992.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17945708
Train loss (w/o reg) on all data: 0.17250642
Test loss (w/o reg) on all data: 0.10370754
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5277346e-05
Norm of the params: 11.790388
              Random: fixed  31 labels. Loss 0.10371. Accuracy 0.973.
### Flips: 208, rs: 17, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729292
Test loss (w/o reg) on all data: 0.01205514
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.156474e-07
Norm of the params: 9.153239
     Influence (LOO): fixed 106 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172931
Test loss (w/o reg) on all data: 0.012055006
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2470735e-07
Norm of the params: 9.153238
                Loss: fixed 106 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16557954
Train loss (w/o reg) on all data: 0.15778983
Test loss (w/o reg) on all data: 0.09074975
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.9237465e-06
Norm of the params: 12.481753
              Random: fixed  38 labels. Loss 0.09075. Accuracy 0.969.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21860872
Train loss (w/o reg) on all data: 0.21158762
Test loss (w/o reg) on all data: 0.14807168
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1492974e-05
Norm of the params: 11.84998
Flipped loss: 0.14807. Accuracy: 0.962
### Flips: 208, rs: 18, checks: 52
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1313339
Train loss (w/o reg) on all data: 0.122349665
Test loss (w/o reg) on all data: 0.100324616
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.356771e-06
Norm of the params: 13.404652
     Influence (LOO): fixed  42 labels. Loss 0.10032. Accuracy 0.969.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091732316
Train loss (w/o reg) on all data: 0.07578526
Test loss (w/o reg) on all data: 0.1295798
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.3370325e-05
Norm of the params: 17.858923
                Loss: fixed  51 labels. Loss 0.12958. Accuracy 0.947.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21198885
Train loss (w/o reg) on all data: 0.20493038
Test loss (w/o reg) on all data: 0.14268462
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.2275597e-05
Norm of the params: 11.881477
              Random: fixed   5 labels. Loss 0.14268. Accuracy 0.958.
### Flips: 208, rs: 18, checks: 104
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06891739
Train loss (w/o reg) on all data: 0.0583458
Test loss (w/o reg) on all data: 0.06352711
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.508233e-06
Norm of the params: 14.540696
     Influence (LOO): fixed  72 labels. Loss 0.06353. Accuracy 0.985.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02846113
Train loss (w/o reg) on all data: 0.017252032
Test loss (w/o reg) on all data: 0.041838836
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5035358e-06
Norm of the params: 14.972707
                Loss: fixed  88 labels. Loss 0.04184. Accuracy 0.985.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20476253
Train loss (w/o reg) on all data: 0.1977519
Test loss (w/o reg) on all data: 0.13941124
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.3810311e-05
Norm of the params: 11.841153
              Random: fixed  10 labels. Loss 0.13941. Accuracy 0.954.
### Flips: 208, rs: 18, checks: 156
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037538406
Train loss (w/o reg) on all data: 0.028916266
Test loss (w/o reg) on all data: 0.038102392
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7348393e-06
Norm of the params: 13.131747
     Influence (LOO): fixed  89 labels. Loss 0.03810. Accuracy 0.992.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010941352
Train loss (w/o reg) on all data: 0.0047419355
Test loss (w/o reg) on all data: 0.02154594
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4993334e-07
Norm of the params: 11.135005
                Loss: fixed  99 labels. Loss 0.02155. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19170682
Train loss (w/o reg) on all data: 0.18434739
Test loss (w/o reg) on all data: 0.11947133
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4085438e-05
Norm of the params: 12.132136
              Random: fixed  19 labels. Loss 0.11947. Accuracy 0.958.
### Flips: 208, rs: 18, checks: 208
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011096894
Train loss (w/o reg) on all data: 0.0049675186
Test loss (w/o reg) on all data: 0.021606117
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.6401715e-07
Norm of the params: 11.071924
     Influence (LOO): fixed 100 labels. Loss 0.02161. Accuracy 0.992.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008829618
Train loss (w/o reg) on all data: 0.00351007
Test loss (w/o reg) on all data: 0.015597799
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0501633e-06
Norm of the params: 10.3146
                Loss: fixed 101 labels. Loss 0.01560. Accuracy 0.996.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18735795
Train loss (w/o reg) on all data: 0.17991933
Test loss (w/o reg) on all data: 0.11384192
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.136432e-05
Norm of the params: 12.197228
              Random: fixed  22 labels. Loss 0.11384. Accuracy 0.954.
### Flips: 208, rs: 18, checks: 260
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729625
Test loss (w/o reg) on all data: 0.012054662
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4292968e-07
Norm of the params: 9.153202
     Influence (LOO): fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00882962
Train loss (w/o reg) on all data: 0.003510088
Test loss (w/o reg) on all data: 0.015597267
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3444214e-07
Norm of the params: 10.314584
                Loss: fixed 101 labels. Loss 0.01560. Accuracy 0.996.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1774078
Train loss (w/o reg) on all data: 0.16951723
Test loss (w/o reg) on all data: 0.10939368
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.722412e-05
Norm of the params: 12.562296
              Random: fixed  28 labels. Loss 0.10939. Accuracy 0.962.
### Flips: 208, rs: 18, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729425
Test loss (w/o reg) on all data: 0.012054309
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8297363e-07
Norm of the params: 9.153224
     Influence (LOO): fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729441
Test loss (w/o reg) on all data: 0.012054233
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.554125e-07
Norm of the params: 9.153223
                Loss: fixed 103 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16609474
Train loss (w/o reg) on all data: 0.15760602
Test loss (w/o reg) on all data: 0.10514457
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.570755e-05
Norm of the params: 13.029749
              Random: fixed  33 labels. Loss 0.10514. Accuracy 0.962.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20047964
Train loss (w/o reg) on all data: 0.19228446
Test loss (w/o reg) on all data: 0.14744508
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.3524815e-05
Norm of the params: 12.802485
Flipped loss: 0.14745. Accuracy: 0.943
### Flips: 208, rs: 19, checks: 52
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11809193
Train loss (w/o reg) on all data: 0.106583074
Test loss (w/o reg) on all data: 0.11543107
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3546319e-05
Norm of the params: 15.171592
     Influence (LOO): fixed  40 labels. Loss 0.11543. Accuracy 0.969.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07839349
Train loss (w/o reg) on all data: 0.0603249
Test loss (w/o reg) on all data: 0.10099136
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.7035477e-06
Norm of the params: 19.00978
                Loss: fixed  50 labels. Loss 0.10099. Accuracy 0.958.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1966224
Train loss (w/o reg) on all data: 0.1888356
Test loss (w/o reg) on all data: 0.13323049
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.7226313e-05
Norm of the params: 12.479414
              Random: fixed   6 labels. Loss 0.13323. Accuracy 0.958.
### Flips: 208, rs: 19, checks: 104
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063989714
Train loss (w/o reg) on all data: 0.053453993
Test loss (w/o reg) on all data: 0.058722634
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7038332e-05
Norm of the params: 14.5160055
     Influence (LOO): fixed  68 labels. Loss 0.05872. Accuracy 0.985.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03046082
Train loss (w/o reg) on all data: 0.017885748
Test loss (w/o reg) on all data: 0.057219565
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6884336e-06
Norm of the params: 15.858796
                Loss: fixed  80 labels. Loss 0.05722. Accuracy 0.977.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19335777
Train loss (w/o reg) on all data: 0.18562227
Test loss (w/o reg) on all data: 0.12532942
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.1150604e-05
Norm of the params: 12.438246
              Random: fixed   9 labels. Loss 0.12533. Accuracy 0.962.
### Flips: 208, rs: 19, checks: 156
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036419615
Train loss (w/o reg) on all data: 0.025955448
Test loss (w/o reg) on all data: 0.04201979
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.8488065e-06
Norm of the params: 14.46663
     Influence (LOO): fixed  81 labels. Loss 0.04202. Accuracy 0.981.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013569594
Train loss (w/o reg) on all data: 0.0061046244
Test loss (w/o reg) on all data: 0.037828933
Train acc on all data:  1.0
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1116396e-06
Norm of the params: 12.218814
                Loss: fixed  92 labels. Loss 0.03783. Accuracy 0.977.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18919133
Train loss (w/o reg) on all data: 0.18139276
Test loss (w/o reg) on all data: 0.11774586
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.5452988e-05
Norm of the params: 12.488852
              Random: fixed  12 labels. Loss 0.11775. Accuracy 0.962.
### Flips: 208, rs: 19, checks: 208
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015933076
Train loss (w/o reg) on all data: 0.00834826
Test loss (w/o reg) on all data: 0.023101067
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.7709844e-06
Norm of the params: 12.316506
     Influence (LOO): fixed  94 labels. Loss 0.02310. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009460383
Train loss (w/o reg) on all data: 0.0038615826
Test loss (w/o reg) on all data: 0.014457813
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1413436e-07
Norm of the params: 10.581872
                Loss: fixed  97 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18448392
Train loss (w/o reg) on all data: 0.17695537
Test loss (w/o reg) on all data: 0.107028134
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.9109752e-05
Norm of the params: 12.270728
              Random: fixed  19 labels. Loss 0.10703. Accuracy 0.969.
### Flips: 208, rs: 19, checks: 260
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009460382
Train loss (w/o reg) on all data: 0.0038617207
Test loss (w/o reg) on all data: 0.014457887
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2432768e-07
Norm of the params: 10.581741
     Influence (LOO): fixed  97 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009460382
Train loss (w/o reg) on all data: 0.0038617197
Test loss (w/o reg) on all data: 0.014457997
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3609433e-07
Norm of the params: 10.581741
                Loss: fixed  97 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17104585
Train loss (w/o reg) on all data: 0.16345266
Test loss (w/o reg) on all data: 0.10876113
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.4470639e-05
Norm of the params: 12.323311
              Random: fixed  26 labels. Loss 0.10876. Accuracy 0.947.
### Flips: 208, rs: 19, checks: 312
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008370904
Train loss (w/o reg) on all data: 0.003409709
Test loss (w/o reg) on all data: 0.01298457
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.8071884e-07
Norm of the params: 9.96112
     Influence (LOO): fixed  98 labels. Loss 0.01298. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009460381
Train loss (w/o reg) on all data: 0.0038615365
Test loss (w/o reg) on all data: 0.014457464
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.141664e-07
Norm of the params: 10.581914
                Loss: fixed  97 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1566135
Train loss (w/o reg) on all data: 0.14798036
Test loss (w/o reg) on all data: 0.11458778
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.971663e-06
Norm of the params: 13.140116
              Random: fixed  32 labels. Loss 0.11459. Accuracy 0.958.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22043103
Train loss (w/o reg) on all data: 0.21243566
Test loss (w/o reg) on all data: 0.18196374
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 4.1270545e-05
Norm of the params: 12.645453
Flipped loss: 0.18196. Accuracy: 0.939
### Flips: 208, rs: 20, checks: 52
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13917483
Train loss (w/o reg) on all data: 0.1288947
Test loss (w/o reg) on all data: 0.11354974
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.81014e-06
Norm of the params: 14.338849
     Influence (LOO): fixed  42 labels. Loss 0.11355. Accuracy 0.958.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095864736
Train loss (w/o reg) on all data: 0.080791496
Test loss (w/o reg) on all data: 0.1258093
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.2539275e-05
Norm of the params: 17.362741
                Loss: fixed  51 labels. Loss 0.12581. Accuracy 0.947.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20951645
Train loss (w/o reg) on all data: 0.20101568
Test loss (w/o reg) on all data: 0.18024653
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 6.81353e-05
Norm of the params: 13.038999
              Random: fixed   6 labels. Loss 0.18025. Accuracy 0.939.
### Flips: 208, rs: 20, checks: 104
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.091281086
Train loss (w/o reg) on all data: 0.081056975
Test loss (w/o reg) on all data: 0.09619954
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.5577984e-06
Norm of the params: 14.299731
     Influence (LOO): fixed  65 labels. Loss 0.09620. Accuracy 0.958.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028717956
Train loss (w/o reg) on all data: 0.016315982
Test loss (w/o reg) on all data: 0.038094137
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0399248e-06
Norm of the params: 15.7492695
                Loss: fixed  90 labels. Loss 0.03809. Accuracy 0.981.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20733684
Train loss (w/o reg) on all data: 0.19984226
Test loss (w/o reg) on all data: 0.15782228
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.6890117e-05
Norm of the params: 12.243019
              Random: fixed  13 labels. Loss 0.15782. Accuracy 0.947.
### Flips: 208, rs: 20, checks: 156
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04428059
Train loss (w/o reg) on all data: 0.034274522
Test loss (w/o reg) on all data: 0.036994882
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.493209e-06
Norm of the params: 14.146424
     Influence (LOO): fixed  89 labels. Loss 0.03699. Accuracy 0.981.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016104214
Train loss (w/o reg) on all data: 0.007698645
Test loss (w/o reg) on all data: 0.026944999
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.115105e-07
Norm of the params: 12.965778
                Loss: fixed  98 labels. Loss 0.02694. Accuracy 0.985.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20331132
Train loss (w/o reg) on all data: 0.1959195
Test loss (w/o reg) on all data: 0.15189154
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.27553585e-05
Norm of the params: 12.158807
              Random: fixed  16 labels. Loss 0.15189. Accuracy 0.947.
### Flips: 208, rs: 20, checks: 208
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022028293
Train loss (w/o reg) on all data: 0.014568485
Test loss (w/o reg) on all data: 0.022990275
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0030313e-06
Norm of the params: 12.214589
     Influence (LOO): fixed 100 labels. Loss 0.02299. Accuracy 0.989.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014185527
Train loss (w/o reg) on all data: 0.0064549656
Test loss (w/o reg) on all data: 0.016361196
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2130056e-06
Norm of the params: 12.434278
                Loss: fixed 102 labels. Loss 0.01636. Accuracy 0.989.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19631462
Train loss (w/o reg) on all data: 0.18858637
Test loss (w/o reg) on all data: 0.15211923
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.1212312e-05
Norm of the params: 12.432412
              Random: fixed  19 labels. Loss 0.15212. Accuracy 0.943.
### Flips: 208, rs: 20, checks: 260
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009210212
Train loss (w/o reg) on all data: 0.0044514053
Test loss (w/o reg) on all data: 0.013358528
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.3450093e-07
Norm of the params: 9.755826
     Influence (LOO): fixed 107 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011382788
Train loss (w/o reg) on all data: 0.004859331
Test loss (w/o reg) on all data: 0.015920201
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.762948e-07
Norm of the params: 11.422309
                Loss: fixed 104 labels. Loss 0.01592. Accuracy 0.989.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17752619
Train loss (w/o reg) on all data: 0.16890241
Test loss (w/o reg) on all data: 0.13437541
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.924729e-05
Norm of the params: 13.133
              Random: fixed  29 labels. Loss 0.13438. Accuracy 0.954.
### Flips: 208, rs: 20, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021729602
Test loss (w/o reg) on all data: 0.012054913
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.16387604e-07
Norm of the params: 9.153208
     Influence (LOO): fixed 108 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007651869
Train loss (w/o reg) on all data: 0.0027779771
Test loss (w/o reg) on all data: 0.014521644
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.993946e-07
Norm of the params: 9.873087
                Loss: fixed 107 labels. Loss 0.01452. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17109126
Train loss (w/o reg) on all data: 0.1619649
Test loss (w/o reg) on all data: 0.13549134
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.5442226e-05
Norm of the params: 13.510273
              Random: fixed  32 labels. Loss 0.13549. Accuracy 0.947.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2219491
Train loss (w/o reg) on all data: 0.2154093
Test loss (w/o reg) on all data: 0.15088247
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.019895e-05
Norm of the params: 11.436615
Flipped loss: 0.15088. Accuracy: 0.950
### Flips: 208, rs: 21, checks: 52
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14319561
Train loss (w/o reg) on all data: 0.13263461
Test loss (w/o reg) on all data: 0.09849412
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.017212e-05
Norm of the params: 14.533409
     Influence (LOO): fixed  42 labels. Loss 0.09849. Accuracy 0.977.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10669723
Train loss (w/o reg) on all data: 0.09189517
Test loss (w/o reg) on all data: 0.11623672
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.7172702e-05
Norm of the params: 17.205849
                Loss: fixed  49 labels. Loss 0.11624. Accuracy 0.943.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21645465
Train loss (w/o reg) on all data: 0.20990764
Test loss (w/o reg) on all data: 0.14128132
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.1772888e-05
Norm of the params: 11.442914
              Random: fixed   5 labels. Loss 0.14128. Accuracy 0.954.
### Flips: 208, rs: 21, checks: 104
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0810256
Train loss (w/o reg) on all data: 0.069088355
Test loss (w/o reg) on all data: 0.07090752
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.5241022e-06
Norm of the params: 15.45137
     Influence (LOO): fixed  73 labels. Loss 0.07091. Accuracy 0.966.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034967877
Train loss (w/o reg) on all data: 0.021361856
Test loss (w/o reg) on all data: 0.06358656
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.3219346e-06
Norm of the params: 16.496073
                Loss: fixed  90 labels. Loss 0.06359. Accuracy 0.973.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20256199
Train loss (w/o reg) on all data: 0.19470719
Test loss (w/o reg) on all data: 0.13453017
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.3357975e-05
Norm of the params: 12.533803
              Random: fixed  15 labels. Loss 0.13453. Accuracy 0.954.
### Flips: 208, rs: 21, checks: 156
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03866053
Train loss (w/o reg) on all data: 0.027745465
Test loss (w/o reg) on all data: 0.07310406
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.2220855e-06
Norm of the params: 14.775023
     Influence (LOO): fixed  94 labels. Loss 0.07310. Accuracy 0.977.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026997592
Train loss (w/o reg) on all data: 0.015576924
Test loss (w/o reg) on all data: 0.036847036
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.2342054e-07
Norm of the params: 15.113349
                Loss: fixed 100 labels. Loss 0.03685. Accuracy 0.977.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19570644
Train loss (w/o reg) on all data: 0.18860619
Test loss (w/o reg) on all data: 0.12102026
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.613359e-05
Norm of the params: 11.916589
              Random: fixed  21 labels. Loss 0.12102. Accuracy 0.969.
### Flips: 208, rs: 21, checks: 208
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018275827
Train loss (w/o reg) on all data: 0.009586783
Test loss (w/o reg) on all data: 0.03975261
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2994661e-06
Norm of the params: 13.182598
     Influence (LOO): fixed 108 labels. Loss 0.03975. Accuracy 0.985.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016890496
Train loss (w/o reg) on all data: 0.00814569
Test loss (w/o reg) on all data: 0.020234305
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9805855e-06
Norm of the params: 13.224831
                Loss: fixed 107 labels. Loss 0.02023. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19036354
Train loss (w/o reg) on all data: 0.18350495
Test loss (w/o reg) on all data: 0.11386331
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.3470879e-05
Norm of the params: 11.712032
              Random: fixed  26 labels. Loss 0.11386. Accuracy 0.969.
### Flips: 208, rs: 21, checks: 260
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00896694
Train loss (w/o reg) on all data: 0.0035014704
Test loss (w/o reg) on all data: 0.023119897
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.9518053e-07
Norm of the params: 10.455113
     Influence (LOO): fixed 114 labels. Loss 0.02312. Accuracy 0.989.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014060251
Train loss (w/o reg) on all data: 0.006533771
Test loss (w/o reg) on all data: 0.018776108
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.146074e-06
Norm of the params: 12.269051
                Loss: fixed 111 labels. Loss 0.01878. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17389715
Train loss (w/o reg) on all data: 0.16691916
Test loss (w/o reg) on all data: 0.1069779
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.841631e-06
Norm of the params: 11.813543
              Random: fixed  35 labels. Loss 0.10698. Accuracy 0.973.
### Flips: 208, rs: 21, checks: 312
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007929786
Train loss (w/o reg) on all data: 0.0030199776
Test loss (w/o reg) on all data: 0.019056583
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.4703442e-07
Norm of the params: 9.909398
     Influence (LOO): fixed 115 labels. Loss 0.01906. Accuracy 0.992.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01258243
Train loss (w/o reg) on all data: 0.0057872874
Test loss (w/o reg) on all data: 0.02399817
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.287701e-07
Norm of the params: 11.657738
                Loss: fixed 112 labels. Loss 0.02400. Accuracy 0.989.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1629146
Train loss (w/o reg) on all data: 0.1553907
Test loss (w/o reg) on all data: 0.10662253
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5848902e-05
Norm of the params: 12.266958
              Random: fixed  42 labels. Loss 0.10662. Accuracy 0.973.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22710021
Train loss (w/o reg) on all data: 0.22086696
Test loss (w/o reg) on all data: 0.1579942
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4096141e-05
Norm of the params: 11.165343
Flipped loss: 0.15799. Accuracy: 0.966
### Flips: 208, rs: 22, checks: 52
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14121272
Train loss (w/o reg) on all data: 0.12939459
Test loss (w/o reg) on all data: 0.13069892
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.8826154e-05
Norm of the params: 15.374087
     Influence (LOO): fixed  40 labels. Loss 0.13070. Accuracy 0.962.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.102519736
Train loss (w/o reg) on all data: 0.08789241
Test loss (w/o reg) on all data: 0.1515385
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.3271032e-05
Norm of the params: 17.103989
                Loss: fixed  49 labels. Loss 0.15154. Accuracy 0.947.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2190329
Train loss (w/o reg) on all data: 0.2124468
Test loss (w/o reg) on all data: 0.14911644
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 9.7586835e-06
Norm of the params: 11.47702
              Random: fixed   6 labels. Loss 0.14912. Accuracy 0.958.
### Flips: 208, rs: 22, checks: 104
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0837572
Train loss (w/o reg) on all data: 0.073028296
Test loss (w/o reg) on all data: 0.09224066
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.7431936e-06
Norm of the params: 14.648485
     Influence (LOO): fixed  70 labels. Loss 0.09224. Accuracy 0.962.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039179277
Train loss (w/o reg) on all data: 0.02416477
Test loss (w/o reg) on all data: 0.07361088
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.544598e-06
Norm of the params: 17.328882
                Loss: fixed  87 labels. Loss 0.07361. Accuracy 0.977.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2090936
Train loss (w/o reg) on all data: 0.20264366
Test loss (w/o reg) on all data: 0.14569768
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.1218912e-05
Norm of the params: 11.357764
              Random: fixed  12 labels. Loss 0.14570. Accuracy 0.954.
### Flips: 208, rs: 22, checks: 156
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044837404
Train loss (w/o reg) on all data: 0.035787072
Test loss (w/o reg) on all data: 0.06584164
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.7844075e-06
Norm of the params: 13.453872
     Influence (LOO): fixed  92 labels. Loss 0.06584. Accuracy 0.969.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015218818
Train loss (w/o reg) on all data: 0.0071500316
Test loss (w/o reg) on all data: 0.029157927
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.4465982e-06
Norm of the params: 12.703374
                Loss: fixed 104 labels. Loss 0.02916. Accuracy 0.981.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19881906
Train loss (w/o reg) on all data: 0.19186321
Test loss (w/o reg) on all data: 0.14230773
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.2570555e-05
Norm of the params: 11.794789
              Random: fixed  17 labels. Loss 0.14231. Accuracy 0.947.
### Flips: 208, rs: 22, checks: 208
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021615773
Train loss (w/o reg) on all data: 0.013697782
Test loss (w/o reg) on all data: 0.052659437
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1516337e-06
Norm of the params: 12.58411
     Influence (LOO): fixed 101 labels. Loss 0.05266. Accuracy 0.973.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015218817
Train loss (w/o reg) on all data: 0.007149962
Test loss (w/o reg) on all data: 0.029158922
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.105818e-06
Norm of the params: 12.703429
                Loss: fixed 104 labels. Loss 0.02916. Accuracy 0.981.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19077517
Train loss (w/o reg) on all data: 0.18345398
Test loss (w/o reg) on all data: 0.13100968
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.51480735e-05
Norm of the params: 12.100578
              Random: fixed  24 labels. Loss 0.13101. Accuracy 0.966.
### Flips: 208, rs: 22, checks: 260
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013257088
Train loss (w/o reg) on all data: 0.007688968
Test loss (w/o reg) on all data: 0.01686956
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1261221e-06
Norm of the params: 10.552838
     Influence (LOO): fixed 107 labels. Loss 0.01687. Accuracy 0.996.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012294443
Train loss (w/o reg) on all data: 0.0051591448
Test loss (w/o reg) on all data: 0.024467427
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.820098e-07
Norm of the params: 11.945961
                Loss: fixed 106 labels. Loss 0.02447. Accuracy 0.985.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18427333
Train loss (w/o reg) on all data: 0.1765346
Test loss (w/o reg) on all data: 0.13034695
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2303717e-05
Norm of the params: 12.440854
              Random: fixed  27 labels. Loss 0.13035. Accuracy 0.954.
### Flips: 208, rs: 22, checks: 312
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009922648
Train loss (w/o reg) on all data: 0.0044791545
Test loss (w/o reg) on all data: 0.014380088
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.9604606e-07
Norm of the params: 10.4340725
     Influence (LOO): fixed 108 labels. Loss 0.01438. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074066725
Train loss (w/o reg) on all data: 0.0026580994
Test loss (w/o reg) on all data: 0.013140734
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7627464e-07
Norm of the params: 9.745331
                Loss: fixed 109 labels. Loss 0.01314. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17129757
Train loss (w/o reg) on all data: 0.16363473
Test loss (w/o reg) on all data: 0.12808065
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.718155e-06
Norm of the params: 12.379687
              Random: fixed  34 labels. Loss 0.12808. Accuracy 0.954.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22610736
Train loss (w/o reg) on all data: 0.21979776
Test loss (w/o reg) on all data: 0.14213914
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.491067e-05
Norm of the params: 11.233524
Flipped loss: 0.14214. Accuracy: 0.973
### Flips: 208, rs: 23, checks: 52
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13568988
Train loss (w/o reg) on all data: 0.1269798
Test loss (w/o reg) on all data: 0.09501646
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.7080094e-05
Norm of the params: 13.198549
     Influence (LOO): fixed  45 labels. Loss 0.09502. Accuracy 0.958.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09681451
Train loss (w/o reg) on all data: 0.07917771
Test loss (w/o reg) on all data: 0.11368974
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.0034715e-05
Norm of the params: 18.78127
                Loss: fixed  52 labels. Loss 0.11369. Accuracy 0.958.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21108507
Train loss (w/o reg) on all data: 0.20416203
Test loss (w/o reg) on all data: 0.12814856
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6045276e-05
Norm of the params: 11.7669325
              Random: fixed   9 labels. Loss 0.12815. Accuracy 0.969.
### Flips: 208, rs: 23, checks: 104
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0775073
Train loss (w/o reg) on all data: 0.068855725
Test loss (w/o reg) on all data: 0.07866148
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.860799e-06
Norm of the params: 13.154145
     Influence (LOO): fixed  74 labels. Loss 0.07866. Accuracy 0.962.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026427697
Train loss (w/o reg) on all data: 0.015248161
Test loss (w/o reg) on all data: 0.05585664
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2190962e-06
Norm of the params: 14.95295
                Loss: fixed  94 labels. Loss 0.05586. Accuracy 0.977.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19931018
Train loss (w/o reg) on all data: 0.19235578
Test loss (w/o reg) on all data: 0.1154843
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.002636e-05
Norm of the params: 11.793554
              Random: fixed  17 labels. Loss 0.11548. Accuracy 0.973.
### Flips: 208, rs: 23, checks: 156
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03501946
Train loss (w/o reg) on all data: 0.026973851
Test loss (w/o reg) on all data: 0.041518986
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0952932e-06
Norm of the params: 12.685119
     Influence (LOO): fixed  94 labels. Loss 0.04152. Accuracy 0.985.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015269983
Train loss (w/o reg) on all data: 0.0068866564
Test loss (w/o reg) on all data: 0.03291557
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4222874e-06
Norm of the params: 12.948611
                Loss: fixed 101 labels. Loss 0.03292. Accuracy 0.985.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18378495
Train loss (w/o reg) on all data: 0.17594938
Test loss (w/o reg) on all data: 0.11820433
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9047642e-05
Norm of the params: 12.518441
              Random: fixed  22 labels. Loss 0.11820. Accuracy 0.973.
### Flips: 208, rs: 23, checks: 208
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013012424
Train loss (w/o reg) on all data: 0.006297489
Test loss (w/o reg) on all data: 0.017001033
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.395161e-07
Norm of the params: 11.588732
     Influence (LOO): fixed 104 labels. Loss 0.01700. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010433794
Train loss (w/o reg) on all data: 0.0042823968
Test loss (w/o reg) on all data: 0.015100448
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.702075e-07
Norm of the params: 11.091796
                Loss: fixed 104 labels. Loss 0.01510. Accuracy 0.992.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17041714
Train loss (w/o reg) on all data: 0.16239396
Test loss (w/o reg) on all data: 0.113059826
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3760199e-05
Norm of the params: 12.66743
              Random: fixed  28 labels. Loss 0.11306. Accuracy 0.981.
### Flips: 208, rs: 23, checks: 260
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010372749
Train loss (w/o reg) on all data: 0.004773184
Test loss (w/o reg) on all data: 0.019226575
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0434031e-06
Norm of the params: 10.582594
     Influence (LOO): fixed 106 labels. Loss 0.01923. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00886751
Train loss (w/o reg) on all data: 0.0034626462
Test loss (w/o reg) on all data: 0.014244986
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8734506e-06
Norm of the params: 10.396983
                Loss: fixed 106 labels. Loss 0.01424. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16949043
Train loss (w/o reg) on all data: 0.16249514
Test loss (w/o reg) on all data: 0.106605574
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7798902e-05
Norm of the params: 11.828182
              Random: fixed  32 labels. Loss 0.10661. Accuracy 0.985.
### Flips: 208, rs: 23, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008055276
Train loss (w/o reg) on all data: 0.003082597
Test loss (w/o reg) on all data: 0.014463114
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.680774e-07
Norm of the params: 9.972641
     Influence (LOO): fixed 107 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008055276
Train loss (w/o reg) on all data: 0.0030825979
Test loss (w/o reg) on all data: 0.014463145
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3322477e-07
Norm of the params: 9.972641
                Loss: fixed 107 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16467822
Train loss (w/o reg) on all data: 0.15788335
Test loss (w/o reg) on all data: 0.10835863
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6252929e-05
Norm of the params: 11.657509
              Random: fixed  35 labels. Loss 0.10836. Accuracy 0.977.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20706528
Train loss (w/o reg) on all data: 0.19721226
Test loss (w/o reg) on all data: 0.14170691
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.575871e-05
Norm of the params: 14.037822
Flipped loss: 0.14171. Accuracy: 0.947
### Flips: 208, rs: 24, checks: 52
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12265047
Train loss (w/o reg) on all data: 0.10680918
Test loss (w/o reg) on all data: 0.092355326
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.0343165e-05
Norm of the params: 17.799603
     Influence (LOO): fixed  40 labels. Loss 0.09236. Accuracy 0.962.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08764269
Train loss (w/o reg) on all data: 0.06933084
Test loss (w/o reg) on all data: 0.107028954
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.2047165e-05
Norm of the params: 19.13732
                Loss: fixed  51 labels. Loss 0.10703. Accuracy 0.958.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2016254
Train loss (w/o reg) on all data: 0.19163227
Test loss (w/o reg) on all data: 0.1385491
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.2403415e-05
Norm of the params: 14.137281
              Random: fixed   4 labels. Loss 0.13855. Accuracy 0.950.
### Flips: 208, rs: 24, checks: 104
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0733459
Train loss (w/o reg) on all data: 0.058362104
Test loss (w/o reg) on all data: 0.072583444
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.9076575e-06
Norm of the params: 17.311148
     Influence (LOO): fixed  68 labels. Loss 0.07258. Accuracy 0.969.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029647592
Train loss (w/o reg) on all data: 0.016640557
Test loss (w/o reg) on all data: 0.03782117
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1173648e-06
Norm of the params: 16.128878
                Loss: fixed  86 labels. Loss 0.03782. Accuracy 0.985.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19686997
Train loss (w/o reg) on all data: 0.18775709
Test loss (w/o reg) on all data: 0.13142842
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.109469e-05
Norm of the params: 13.500286
              Random: fixed  13 labels. Loss 0.13143. Accuracy 0.962.
### Flips: 208, rs: 24, checks: 156
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043730304
Train loss (w/o reg) on all data: 0.032363895
Test loss (w/o reg) on all data: 0.03782923
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.645906e-06
Norm of the params: 15.077407
     Influence (LOO): fixed  86 labels. Loss 0.03783. Accuracy 0.989.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017605245
Train loss (w/o reg) on all data: 0.008642927
Test loss (w/o reg) on all data: 0.021252284
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5356304e-07
Norm of the params: 13.388291
                Loss: fixed  97 labels. Loss 0.02125. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18501456
Train loss (w/o reg) on all data: 0.17618425
Test loss (w/o reg) on all data: 0.12605822
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.2523934e-05
Norm of the params: 13.289325
              Random: fixed  21 labels. Loss 0.12606. Accuracy 0.962.
### Flips: 208, rs: 24, checks: 208
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018342586
Train loss (w/o reg) on all data: 0.010492518
Test loss (w/o reg) on all data: 0.019825226
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.168634e-06
Norm of the params: 12.530019
     Influence (LOO): fixed 100 labels. Loss 0.01983. Accuracy 0.996.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017148912
Train loss (w/o reg) on all data: 0.008428155
Test loss (w/o reg) on all data: 0.022204254
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.524009e-07
Norm of the params: 13.206632
                Loss: fixed  98 labels. Loss 0.02220. Accuracy 0.992.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1782707
Train loss (w/o reg) on all data: 0.16898489
Test loss (w/o reg) on all data: 0.12626958
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.4907962e-05
Norm of the params: 13.627768
              Random: fixed  24 labels. Loss 0.12627. Accuracy 0.966.
### Flips: 208, rs: 24, checks: 260
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008889291
Train loss (w/o reg) on all data: 0.0035534406
Test loss (w/o reg) on all data: 0.013288523
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.3637093e-07
Norm of the params: 10.330393
     Influence (LOO): fixed 104 labels. Loss 0.01329. Accuracy 0.992.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009715089
Train loss (w/o reg) on all data: 0.003921487
Test loss (w/o reg) on all data: 0.013805093
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.0355027e-07
Norm of the params: 10.764387
                Loss: fixed 103 labels. Loss 0.01381. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17569645
Train loss (w/o reg) on all data: 0.16693477
Test loss (w/o reg) on all data: 0.11951683
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.325498e-05
Norm of the params: 13.237574
              Random: fixed  28 labels. Loss 0.11952. Accuracy 0.969.
### Flips: 208, rs: 24, checks: 312
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007651869
Train loss (w/o reg) on all data: 0.0027778097
Test loss (w/o reg) on all data: 0.014517249
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2417403e-06
Norm of the params: 9.873257
     Influence (LOO): fixed 105 labels. Loss 0.01452. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00888929
Train loss (w/o reg) on all data: 0.00355353
Test loss (w/o reg) on all data: 0.013287964
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.274481e-07
Norm of the params: 10.330305
                Loss: fixed 104 labels. Loss 0.01329. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16509716
Train loss (w/o reg) on all data: 0.15599723
Test loss (w/o reg) on all data: 0.10994495
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.336274e-06
Norm of the params: 13.490684
              Random: fixed  34 labels. Loss 0.10994. Accuracy 0.966.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21200457
Train loss (w/o reg) on all data: 0.20636187
Test loss (w/o reg) on all data: 0.12892933
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.176644e-05
Norm of the params: 10.62327
Flipped loss: 0.12893. Accuracy: 0.966
### Flips: 208, rs: 25, checks: 52
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1151356
Train loss (w/o reg) on all data: 0.10623484
Test loss (w/o reg) on all data: 0.09181366
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4410161e-05
Norm of the params: 13.342236
     Influence (LOO): fixed  41 labels. Loss 0.09181. Accuracy 0.969.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07642061
Train loss (w/o reg) on all data: 0.059814163
Test loss (w/o reg) on all data: 0.099881046
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2445321e-05
Norm of the params: 18.224407
                Loss: fixed  49 labels. Loss 0.09988. Accuracy 0.954.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20425971
Train loss (w/o reg) on all data: 0.19883877
Test loss (w/o reg) on all data: 0.1254418
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.168289e-06
Norm of the params: 10.412438
              Random: fixed   5 labels. Loss 0.12544. Accuracy 0.962.
### Flips: 208, rs: 25, checks: 104
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05026889
Train loss (w/o reg) on all data: 0.042227507
Test loss (w/o reg) on all data: 0.045248475
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.64294e-06
Norm of the params: 12.681785
     Influence (LOO): fixed  74 labels. Loss 0.04525. Accuracy 0.989.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01753511
Train loss (w/o reg) on all data: 0.010303827
Test loss (w/o reg) on all data: 0.023164071
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.760779e-07
Norm of the params: 12.026039
                Loss: fixed  87 labels. Loss 0.02316. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1977554
Train loss (w/o reg) on all data: 0.19259778
Test loss (w/o reg) on all data: 0.124354996
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.2171046e-05
Norm of the params: 10.156391
              Random: fixed   9 labels. Loss 0.12435. Accuracy 0.958.
### Flips: 208, rs: 25, checks: 156
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022263326
Train loss (w/o reg) on all data: 0.015163608
Test loss (w/o reg) on all data: 0.026159247
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7885859e-06
Norm of the params: 11.916139
     Influence (LOO): fixed  86 labels. Loss 0.02616. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729209
Test loss (w/o reg) on all data: 0.012054652
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0550277e-07
Norm of the params: 9.153249
                Loss: fixed  93 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19094564
Train loss (w/o reg) on all data: 0.18551715
Test loss (w/o reg) on all data: 0.11479799
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.5884684e-05
Norm of the params: 10.419682
              Random: fixed  14 labels. Loss 0.11480. Accuracy 0.962.
### Flips: 208, rs: 25, checks: 208
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729823
Test loss (w/o reg) on all data: 0.012052393
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.580427e-07
Norm of the params: 9.15318
     Influence (LOO): fixed  93 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021729812
Test loss (w/o reg) on all data: 0.012052603
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0626223e-07
Norm of the params: 9.153181
                Loss: fixed  93 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18226108
Train loss (w/o reg) on all data: 0.17658438
Test loss (w/o reg) on all data: 0.11250908
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.0515842e-05
Norm of the params: 10.655235
              Random: fixed  18 labels. Loss 0.11251. Accuracy 0.958.
### Flips: 208, rs: 25, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729488
Test loss (w/o reg) on all data: 0.012054336
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3849373e-07
Norm of the params: 9.15322
     Influence (LOO): fixed  93 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021729488
Test loss (w/o reg) on all data: 0.012054547
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.584905e-07
Norm of the params: 9.153218
                Loss: fixed  93 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17261286
Train loss (w/o reg) on all data: 0.16643573
Test loss (w/o reg) on all data: 0.11509091
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.7053898e-05
Norm of the params: 11.1149645
              Random: fixed  25 labels. Loss 0.11509. Accuracy 0.954.
### Flips: 208, rs: 25, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021729134
Test loss (w/o reg) on all data: 0.01205411
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.9041366e-07
Norm of the params: 9.153255
     Influence (LOO): fixed  93 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021729146
Test loss (w/o reg) on all data: 0.012054234
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3573642e-07
Norm of the params: 9.1532545
                Loss: fixed  93 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16048671
Train loss (w/o reg) on all data: 0.15443234
Test loss (w/o reg) on all data: 0.11301517
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.0732292e-05
Norm of the params: 11.003971
              Random: fixed  31 labels. Loss 0.11302. Accuracy 0.954.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2070849
Train loss (w/o reg) on all data: 0.19907497
Test loss (w/o reg) on all data: 0.11965766
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.6124402e-05
Norm of the params: 12.656959
Flipped loss: 0.11966. Accuracy: 0.954
### Flips: 208, rs: 26, checks: 52
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1366981
Train loss (w/o reg) on all data: 0.12655106
Test loss (w/o reg) on all data: 0.08700582
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.4216905e-05
Norm of the params: 14.245721
     Influence (LOO): fixed  39 labels. Loss 0.08701. Accuracy 0.962.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08859603
Train loss (w/o reg) on all data: 0.07142392
Test loss (w/o reg) on all data: 0.07521754
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.7773847e-06
Norm of the params: 18.532196
                Loss: fixed  50 labels. Loss 0.07522. Accuracy 0.977.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2003048
Train loss (w/o reg) on all data: 0.19259544
Test loss (w/o reg) on all data: 0.11282109
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.45377335e-05
Norm of the params: 12.417224
              Random: fixed   5 labels. Loss 0.11282. Accuracy 0.958.
### Flips: 208, rs: 26, checks: 104
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07449041
Train loss (w/o reg) on all data: 0.06186596
Test loss (w/o reg) on all data: 0.05273289
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.978641e-06
Norm of the params: 15.889904
     Influence (LOO): fixed  69 labels. Loss 0.05273. Accuracy 0.969.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038811777
Train loss (w/o reg) on all data: 0.023817156
Test loss (w/o reg) on all data: 0.038873106
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6547005e-06
Norm of the params: 17.317402
                Loss: fixed  81 labels. Loss 0.03887. Accuracy 0.985.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19591744
Train loss (w/o reg) on all data: 0.18794923
Test loss (w/o reg) on all data: 0.10305874
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.7114983e-05
Norm of the params: 12.62396
              Random: fixed  10 labels. Loss 0.10306. Accuracy 0.958.
### Flips: 208, rs: 26, checks: 156
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036437556
Train loss (w/o reg) on all data: 0.025454449
Test loss (w/o reg) on all data: 0.04594885
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6872037e-06
Norm of the params: 14.821004
     Influence (LOO): fixed  90 labels. Loss 0.04595. Accuracy 0.981.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023255534
Train loss (w/o reg) on all data: 0.013041581
Test loss (w/o reg) on all data: 0.025690949
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7671764e-06
Norm of the params: 14.292623
                Loss: fixed  94 labels. Loss 0.02569. Accuracy 0.989.
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18533449
Train loss (w/o reg) on all data: 0.17707627
Test loss (w/o reg) on all data: 0.09674797
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.1520267e-05
Norm of the params: 12.851637
              Random: fixed  20 labels. Loss 0.09675. Accuracy 0.969.
### Flips: 208, rs: 26, checks: 208
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012569312
Train loss (w/o reg) on all data: 0.00631155
Test loss (w/o reg) on all data: 0.015003284
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.453038e-07
Norm of the params: 11.18728
     Influence (LOO): fixed 102 labels. Loss 0.01500. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010522828
Train loss (w/o reg) on all data: 0.004245146
Test loss (w/o reg) on all data: 0.012579569
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.2710477e-07
Norm of the params: 11.205072
                Loss: fixed 101 labels. Loss 0.01258. Accuracy 0.996.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17843315
Train loss (w/o reg) on all data: 0.17032385
Test loss (w/o reg) on all data: 0.09326712
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.5713935e-05
Norm of the params: 12.735233
              Random: fixed  25 labels. Loss 0.09327. Accuracy 0.966.
### Flips: 208, rs: 26, checks: 260
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009644585
Train loss (w/o reg) on all data: 0.004283997
Test loss (w/o reg) on all data: 0.011745153
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.192035e-07
Norm of the params: 10.354311
     Influence (LOO): fixed 103 labels. Loss 0.01175. Accuracy 0.996.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075551923
Train loss (w/o reg) on all data: 0.002768475
Test loss (w/o reg) on all data: 0.012676843
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0814048e-07
Norm of the params: 9.784393
                Loss: fixed 104 labels. Loss 0.01268. Accuracy 0.992.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1719766
Train loss (w/o reg) on all data: 0.16317718
Test loss (w/o reg) on all data: 0.091926776
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.135483e-06
Norm of the params: 13.266057
              Random: fixed  30 labels. Loss 0.09193. Accuracy 0.973.
### Flips: 208, rs: 26, checks: 312
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.002172917
Test loss (w/o reg) on all data: 0.012054764
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.47202e-07
Norm of the params: 9.153254
     Influence (LOO): fixed 105 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729171
Test loss (w/o reg) on all data: 0.01205483
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9570976e-07
Norm of the params: 9.153254
                Loss: fixed 105 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16696972
Train loss (w/o reg) on all data: 0.1578038
Test loss (w/o reg) on all data: 0.089727186
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.620471e-05
Norm of the params: 13.539508
              Random: fixed  33 labels. Loss 0.08973. Accuracy 0.981.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22864582
Train loss (w/o reg) on all data: 0.22100171
Test loss (w/o reg) on all data: 0.15765877
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.0178438e-05
Norm of the params: 12.364551
Flipped loss: 0.15766. Accuracy: 0.958
### Flips: 208, rs: 27, checks: 52
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14286998
Train loss (w/o reg) on all data: 0.1323863
Test loss (w/o reg) on all data: 0.12860902
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.9098805e-05
Norm of the params: 14.480111
     Influence (LOO): fixed  40 labels. Loss 0.12861. Accuracy 0.950.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11208356
Train loss (w/o reg) on all data: 0.09731161
Test loss (w/o reg) on all data: 0.1198907
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.5911064e-06
Norm of the params: 17.188341
                Loss: fixed  51 labels. Loss 0.11989. Accuracy 0.962.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22421522
Train loss (w/o reg) on all data: 0.2164334
Test loss (w/o reg) on all data: 0.15531285
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.71734e-06
Norm of the params: 12.475427
              Random: fixed   3 labels. Loss 0.15531. Accuracy 0.954.
### Flips: 208, rs: 27, checks: 104
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083578974
Train loss (w/o reg) on all data: 0.0725799
Test loss (w/o reg) on all data: 0.060501892
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.788203e-06
Norm of the params: 14.831774
     Influence (LOO): fixed  70 labels. Loss 0.06050. Accuracy 0.981.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027813498
Train loss (w/o reg) on all data: 0.017068395
Test loss (w/o reg) on all data: 0.033518333
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.916703e-06
Norm of the params: 14.659537
                Loss: fixed  96 labels. Loss 0.03352. Accuracy 0.996.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20812523
Train loss (w/o reg) on all data: 0.1994003
Test loss (w/o reg) on all data: 0.15102246
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.2147683e-05
Norm of the params: 13.209793
              Random: fixed  11 labels. Loss 0.15102. Accuracy 0.947.
### Flips: 208, rs: 27, checks: 156
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03905896
Train loss (w/o reg) on all data: 0.028373254
Test loss (w/o reg) on all data: 0.026068214
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.9574205e-06
Norm of the params: 14.618965
     Influence (LOO): fixed  90 labels. Loss 0.02607. Accuracy 0.996.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008838476
Train loss (w/o reg) on all data: 0.0035984232
Test loss (w/o reg) on all data: 0.016889071
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.764735e-07
Norm of the params: 10.237239
                Loss: fixed 107 labels. Loss 0.01689. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19881842
Train loss (w/o reg) on all data: 0.18985006
Test loss (w/o reg) on all data: 0.15085539
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.4530056e-05
Norm of the params: 13.392799
              Random: fixed  17 labels. Loss 0.15086. Accuracy 0.947.
### Flips: 208, rs: 27, checks: 208
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015175046
Train loss (w/o reg) on all data: 0.009582979
Test loss (w/o reg) on all data: 0.012607404
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.675123e-07
Norm of the params: 10.575508
     Influence (LOO): fixed 104 labels. Loss 0.01261. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729576
Test loss (w/o reg) on all data: 0.012054966
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3681607e-07
Norm of the params: 9.153209
                Loss: fixed 108 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19382167
Train loss (w/o reg) on all data: 0.1845838
Test loss (w/o reg) on all data: 0.1476118
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.2040399e-05
Norm of the params: 13.592549
              Random: fixed  20 labels. Loss 0.14761. Accuracy 0.939.
### Flips: 208, rs: 27, checks: 260
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009210211
Train loss (w/o reg) on all data: 0.004451507
Test loss (w/o reg) on all data: 0.013356424
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.167957e-07
Norm of the params: 9.755721
     Influence (LOO): fixed 107 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.002172904
Test loss (w/o reg) on all data: 0.012055188
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9625116e-07
Norm of the params: 9.153267
                Loss: fixed 108 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18506604
Train loss (w/o reg) on all data: 0.17541736
Test loss (w/o reg) on all data: 0.13810301
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.2501392e-05
Norm of the params: 13.891491
              Random: fixed  26 labels. Loss 0.13810. Accuracy 0.950.
### Flips: 208, rs: 27, checks: 312
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00921021
Train loss (w/o reg) on all data: 0.004451406
Test loss (w/o reg) on all data: 0.013357835
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.163708e-07
Norm of the params: 9.755823
     Influence (LOO): fixed 107 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00636202
Train loss (w/o reg) on all data: 0.0021728568
Test loss (w/o reg) on all data: 0.012055038
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3306784e-07
Norm of the params: 9.153319
                Loss: fixed 108 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16784526
Train loss (w/o reg) on all data: 0.15886836
Test loss (w/o reg) on all data: 0.11811402
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.9455658e-05
Norm of the params: 13.399186
              Random: fixed  38 labels. Loss 0.11811. Accuracy 0.962.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21446292
Train loss (w/o reg) on all data: 0.20517471
Test loss (w/o reg) on all data: 0.15683737
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.5326633e-05
Norm of the params: 13.629536
Flipped loss: 0.15684. Accuracy: 0.939
### Flips: 208, rs: 28, checks: 52
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13835225
Train loss (w/o reg) on all data: 0.12629
Test loss (w/o reg) on all data: 0.14340831
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.9090357e-06
Norm of the params: 15.532061
     Influence (LOO): fixed  40 labels. Loss 0.14341. Accuracy 0.947.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106365845
Train loss (w/o reg) on all data: 0.08801393
Test loss (w/o reg) on all data: 0.112617694
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.2154458e-06
Norm of the params: 19.158243
                Loss: fixed  49 labels. Loss 0.11262. Accuracy 0.947.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20845924
Train loss (w/o reg) on all data: 0.19918424
Test loss (w/o reg) on all data: 0.14834307
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.757088e-05
Norm of the params: 13.619842
              Random: fixed   6 labels. Loss 0.14834. Accuracy 0.943.
### Flips: 208, rs: 28, checks: 104
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083497494
Train loss (w/o reg) on all data: 0.07268579
Test loss (w/o reg) on all data: 0.059786133
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.815084e-06
Norm of the params: 14.7049
     Influence (LOO): fixed  71 labels. Loss 0.05979. Accuracy 0.977.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041718453
Train loss (w/o reg) on all data: 0.027431056
Test loss (w/o reg) on all data: 0.042478226
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7422698e-06
Norm of the params: 16.904083
                Loss: fixed  88 labels. Loss 0.04248. Accuracy 0.989.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20024581
Train loss (w/o reg) on all data: 0.19021404
Test loss (w/o reg) on all data: 0.14221536
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.537073e-06
Norm of the params: 14.16459
              Random: fixed  11 labels. Loss 0.14222. Accuracy 0.950.
### Flips: 208, rs: 28, checks: 156
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04468119
Train loss (w/o reg) on all data: 0.03453633
Test loss (w/o reg) on all data: 0.038705356
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2003221e-06
Norm of the params: 14.244201
     Influence (LOO): fixed  89 labels. Loss 0.03871. Accuracy 0.992.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023347108
Train loss (w/o reg) on all data: 0.01225365
Test loss (w/o reg) on all data: 0.030302763
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9421805e-06
Norm of the params: 14.895273
                Loss: fixed 101 labels. Loss 0.03030. Accuracy 0.989.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1965736
Train loss (w/o reg) on all data: 0.18664192
Test loss (w/o reg) on all data: 0.13880824
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.9062843e-05
Norm of the params: 14.093747
              Random: fixed  14 labels. Loss 0.13881. Accuracy 0.950.
### Flips: 208, rs: 28, checks: 208
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025038898
Train loss (w/o reg) on all data: 0.015940288
Test loss (w/o reg) on all data: 0.025255993
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0785276e-06
Norm of the params: 13.489708
     Influence (LOO): fixed 101 labels. Loss 0.02526. Accuracy 0.992.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017040215
Train loss (w/o reg) on all data: 0.00829881
Test loss (w/o reg) on all data: 0.023800945
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2929817e-06
Norm of the params: 13.2222595
                Loss: fixed 106 labels. Loss 0.02380. Accuracy 0.989.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19456515
Train loss (w/o reg) on all data: 0.18612018
Test loss (w/o reg) on all data: 0.13357913
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.566649e-05
Norm of the params: 12.996133
              Random: fixed  17 labels. Loss 0.13358. Accuracy 0.958.
### Flips: 208, rs: 28, checks: 260
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014728224
Train loss (w/o reg) on all data: 0.007742963
Test loss (w/o reg) on all data: 0.020660996
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4953914e-07
Norm of the params: 11.819696
     Influence (LOO): fixed 109 labels. Loss 0.02066. Accuracy 0.996.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013583783
Train loss (w/o reg) on all data: 0.0061358954
Test loss (w/o reg) on all data: 0.019881435
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.4181784e-07
Norm of the params: 12.204825
                Loss: fixed 109 labels. Loss 0.01988. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18336603
Train loss (w/o reg) on all data: 0.17529772
Test loss (w/o reg) on all data: 0.114891954
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.595052e-05
Norm of the params: 12.703002
              Random: fixed  25 labels. Loss 0.11489. Accuracy 0.966.
### Flips: 208, rs: 28, checks: 312
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011133393
Train loss (w/o reg) on all data: 0.0053544496
Test loss (w/o reg) on all data: 0.013428766
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.460653e-07
Norm of the params: 10.750761
     Influence (LOO): fixed 112 labels. Loss 0.01343. Accuracy 0.996.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011026057
Train loss (w/o reg) on all data: 0.0045895027
Test loss (w/o reg) on all data: 0.016546953
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5369962e-07
Norm of the params: 11.345973
                Loss: fixed 110 labels. Loss 0.01655. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17301957
Train loss (w/o reg) on all data: 0.16479047
Test loss (w/o reg) on all data: 0.105667636
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1415608e-05
Norm of the params: 12.8289585
              Random: fixed  31 labels. Loss 0.10567. Accuracy 0.962.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20159918
Train loss (w/o reg) on all data: 0.19226006
Test loss (w/o reg) on all data: 0.11314261
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.2682707e-05
Norm of the params: 13.6668415
Flipped loss: 0.11314. Accuracy: 0.973
### Flips: 208, rs: 29, checks: 52
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1118767
Train loss (w/o reg) on all data: 0.09968925
Test loss (w/o reg) on all data: 0.087962136
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.367772e-05
Norm of the params: 15.612458
     Influence (LOO): fixed  42 labels. Loss 0.08796. Accuracy 0.969.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08504385
Train loss (w/o reg) on all data: 0.06786091
Test loss (w/o reg) on all data: 0.061002616
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.5001127e-06
Norm of the params: 18.538034
                Loss: fixed  52 labels. Loss 0.06100. Accuracy 0.981.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.197341
Train loss (w/o reg) on all data: 0.1883776
Test loss (w/o reg) on all data: 0.106709085
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.937073e-05
Norm of the params: 13.389095
              Random: fixed   5 labels. Loss 0.10671. Accuracy 0.981.
### Flips: 208, rs: 29, checks: 104
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061915714
Train loss (w/o reg) on all data: 0.048941765
Test loss (w/o reg) on all data: 0.05231267
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.336078e-06
Norm of the params: 16.108353
     Influence (LOO): fixed  66 labels. Loss 0.05231. Accuracy 0.981.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023497272
Train loss (w/o reg) on all data: 0.013777722
Test loss (w/o reg) on all data: 0.025523545
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.2694833e-06
Norm of the params: 13.942417
                Loss: fixed  88 labels. Loss 0.02552. Accuracy 0.989.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18562342
Train loss (w/o reg) on all data: 0.17647605
Test loss (w/o reg) on all data: 0.09907453
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0606551e-05
Norm of the params: 13.525807
              Random: fixed  12 labels. Loss 0.09907. Accuracy 0.981.
### Flips: 208, rs: 29, checks: 156
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034665205
Train loss (w/o reg) on all data: 0.023020085
Test loss (w/o reg) on all data: 0.02733552
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3298029e-06
Norm of the params: 15.261142
     Influence (LOO): fixed  81 labels. Loss 0.02734. Accuracy 0.989.
Using normal model
LBFGS training took [79] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010992597
Train loss (w/o reg) on all data: 0.004570075
Test loss (w/o reg) on all data: 0.023249835
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.87379e-07
Norm of the params: 11.333598
                Loss: fixed  94 labels. Loss 0.02325. Accuracy 0.989.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18461585
Train loss (w/o reg) on all data: 0.17551354
Test loss (w/o reg) on all data: 0.09907212
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8759098e-05
Norm of the params: 13.492455
              Random: fixed  13 labels. Loss 0.09907. Accuracy 0.981.
### Flips: 208, rs: 29, checks: 208
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021316089
Train loss (w/o reg) on all data: 0.013091247
Test loss (w/o reg) on all data: 0.023024853
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1990157e-06
Norm of the params: 12.825632
     Influence (LOO): fixed  90 labels. Loss 0.02302. Accuracy 0.996.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.0021732198
Test loss (w/o reg) on all data: 0.012055724
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.7170224e-07
Norm of the params: 9.152923
                Loss: fixed  97 labels. Loss 0.01206. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1763026
Train loss (w/o reg) on all data: 0.16683948
Test loss (w/o reg) on all data: 0.093702056
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7969658e-05
Norm of the params: 13.757264
              Random: fixed  18 labels. Loss 0.09370. Accuracy 0.985.
### Flips: 208, rs: 29, checks: 260
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.002172986
Test loss (w/o reg) on all data: 0.01205451
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6076404e-07
Norm of the params: 9.153176
     Influence (LOO): fixed  97 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620196
Train loss (w/o reg) on all data: 0.002172988
Test loss (w/o reg) on all data: 0.012054581
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6128706e-07
Norm of the params: 9.153176
                Loss: fixed  97 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16119555
Train loss (w/o reg) on all data: 0.15119773
Test loss (w/o reg) on all data: 0.07550075
Train acc on all data:  0.938872970391595
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4947602e-05
Norm of the params: 14.140587
              Random: fixed  27 labels. Loss 0.07550. Accuracy 0.992.
### Flips: 208, rs: 29, checks: 312
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730019
Test loss (w/o reg) on all data: 0.012054871
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9999646e-07
Norm of the params: 9.15316
     Influence (LOO): fixed  97 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362018
Train loss (w/o reg) on all data: 0.0021730012
Test loss (w/o reg) on all data: 0.012054919
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7703557e-07
Norm of the params: 9.153161
                Loss: fixed  97 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15508527
Train loss (w/o reg) on all data: 0.14541644
Test loss (w/o reg) on all data: 0.07307964
Train acc on all data:  0.9407831900668577
Test acc on all data:   1.0
Norm of the mean of gradients: 1.2096332e-05
Norm of the params: 13.905984
              Random: fixed  31 labels. Loss 0.07308. Accuracy 1.000.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23315063
Train loss (w/o reg) on all data: 0.2272907
Test loss (w/o reg) on all data: 0.14837249
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.7584028e-05
Norm of the params: 10.825821
Flipped loss: 0.14837. Accuracy: 0.966
### Flips: 208, rs: 30, checks: 52
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13707413
Train loss (w/o reg) on all data: 0.12625678
Test loss (w/o reg) on all data: 0.1152008
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3369565e-05
Norm of the params: 14.708735
     Influence (LOO): fixed  43 labels. Loss 0.11520. Accuracy 0.962.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.093366496
Train loss (w/o reg) on all data: 0.07644144
Test loss (w/o reg) on all data: 0.11729539
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.1557482e-06
Norm of the params: 18.398401
                Loss: fixed  52 labels. Loss 0.11730. Accuracy 0.954.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22706549
Train loss (w/o reg) on all data: 0.22133413
Test loss (w/o reg) on all data: 0.13741744
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.8190326e-05
Norm of the params: 10.706408
              Random: fixed   5 labels. Loss 0.13742. Accuracy 0.958.
### Flips: 208, rs: 30, checks: 104
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07802835
Train loss (w/o reg) on all data: 0.06697965
Test loss (w/o reg) on all data: 0.05642804
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7684896e-06
Norm of the params: 14.865198
     Influence (LOO): fixed  74 labels. Loss 0.05643. Accuracy 0.992.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034465007
Train loss (w/o reg) on all data: 0.02092076
Test loss (w/o reg) on all data: 0.049205277
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.488729e-06
Norm of the params: 16.458582
                Loss: fixed  89 labels. Loss 0.04921. Accuracy 0.989.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22137962
Train loss (w/o reg) on all data: 0.21573673
Test loss (w/o reg) on all data: 0.13292418
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.3587384e-05
Norm of the params: 10.623452
              Random: fixed  11 labels. Loss 0.13292. Accuracy 0.969.
### Flips: 208, rs: 30, checks: 156
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0509982
Train loss (w/o reg) on all data: 0.04118785
Test loss (w/o reg) on all data: 0.03167701
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5758234e-06
Norm of the params: 14.007392
     Influence (LOO): fixed  90 labels. Loss 0.03168. Accuracy 0.989.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018885659
Train loss (w/o reg) on all data: 0.009079291
Test loss (w/o reg) on all data: 0.024048224
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5017549e-06
Norm of the params: 14.004549
                Loss: fixed 101 labels. Loss 0.02405. Accuracy 0.989.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20852472
Train loss (w/o reg) on all data: 0.20236647
Test loss (w/o reg) on all data: 0.12483738
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.524568e-05
Norm of the params: 11.097966
              Random: fixed  18 labels. Loss 0.12484. Accuracy 0.966.
### Flips: 208, rs: 30, checks: 208
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02032554
Train loss (w/o reg) on all data: 0.013351416
Test loss (w/o reg) on all data: 0.013685292
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0869879e-06
Norm of the params: 11.810271
     Influence (LOO): fixed 106 labels. Loss 0.01369. Accuracy 0.996.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010502208
Train loss (w/o reg) on all data: 0.004173946
Test loss (w/o reg) on all data: 0.0211438
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3859673e-06
Norm of the params: 11.250122
                Loss: fixed 109 labels. Loss 0.02114. Accuracy 0.989.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2011774
Train loss (w/o reg) on all data: 0.1950131
Test loss (w/o reg) on all data: 0.12360942
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.6692358e-05
Norm of the params: 11.103416
              Random: fixed  23 labels. Loss 0.12361. Accuracy 0.962.
### Flips: 208, rs: 30, checks: 260
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009912949
Train loss (w/o reg) on all data: 0.004688294
Test loss (w/o reg) on all data: 0.013906862
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.6683767e-07
Norm of the params: 10.222187
     Influence (LOO): fixed 111 labels. Loss 0.01391. Accuracy 0.992.
Using normal model
LBFGS training took [53] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01050221
Train loss (w/o reg) on all data: 0.00417396
Test loss (w/o reg) on all data: 0.021146508
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.3436024e-07
Norm of the params: 11.250111
                Loss: fixed 109 labels. Loss 0.02115. Accuracy 0.989.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1922045
Train loss (w/o reg) on all data: 0.18539855
Test loss (w/o reg) on all data: 0.12782252
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.7809634e-05
Norm of the params: 11.66701
              Random: fixed  29 labels. Loss 0.12782. Accuracy 0.973.
### Flips: 208, rs: 30, checks: 312
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009210211
Train loss (w/o reg) on all data: 0.0044512916
Test loss (w/o reg) on all data: 0.0133583695
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3173342e-07
Norm of the params: 9.755941
     Influence (LOO): fixed 112 labels. Loss 0.01336. Accuracy 0.992.
Using normal model
LBFGS training took [50] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00836267
Train loss (w/o reg) on all data: 0.0030758702
Test loss (w/o reg) on all data: 0.011289681
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.1209816e-07
Norm of the params: 10.282802
                Loss: fixed 111 labels. Loss 0.01129. Accuracy 0.996.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18532504
Train loss (w/o reg) on all data: 0.17871043
Test loss (w/o reg) on all data: 0.12561935
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5219652e-05
Norm of the params: 11.501832
              Random: fixed  34 labels. Loss 0.12562. Accuracy 0.969.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19947441
Train loss (w/o reg) on all data: 0.1902484
Test loss (w/o reg) on all data: 0.14502338
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.1353848e-05
Norm of the params: 13.583823
Flipped loss: 0.14502. Accuracy: 0.947
### Flips: 208, rs: 31, checks: 52
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11874138
Train loss (w/o reg) on all data: 0.107222944
Test loss (w/o reg) on all data: 0.0866371
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.796334e-06
Norm of the params: 15.177902
     Influence (LOO): fixed  39 labels. Loss 0.08664. Accuracy 0.973.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07735958
Train loss (w/o reg) on all data: 0.060566656
Test loss (w/o reg) on all data: 0.06691905
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.05322915e-05
Norm of the params: 18.32644
                Loss: fixed  50 labels. Loss 0.06692. Accuracy 0.977.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19287232
Train loss (w/o reg) on all data: 0.18326265
Test loss (w/o reg) on all data: 0.14229529
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.0351714e-05
Norm of the params: 13.863382
              Random: fixed   4 labels. Loss 0.14230. Accuracy 0.950.
### Flips: 208, rs: 31, checks: 104
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062004372
Train loss (w/o reg) on all data: 0.04954052
Test loss (w/o reg) on all data: 0.04582889
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.2412e-06
Norm of the params: 15.788509
     Influence (LOO): fixed  69 labels. Loss 0.04583. Accuracy 0.981.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027902732
Train loss (w/o reg) on all data: 0.01587445
Test loss (w/o reg) on all data: 0.034232534
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.542514e-07
Norm of the params: 15.510179
                Loss: fixed  82 labels. Loss 0.03423. Accuracy 0.989.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1885355
Train loss (w/o reg) on all data: 0.17936027
Test loss (w/o reg) on all data: 0.13791512
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.1565187e-06
Norm of the params: 13.546382
              Random: fixed   8 labels. Loss 0.13792. Accuracy 0.958.
### Flips: 208, rs: 31, checks: 156
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030688092
Train loss (w/o reg) on all data: 0.020602167
Test loss (w/o reg) on all data: 0.019531516
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1095823e-06
Norm of the params: 14.202764
     Influence (LOO): fixed  87 labels. Loss 0.01953. Accuracy 0.992.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02211252
Train loss (w/o reg) on all data: 0.012832681
Test loss (w/o reg) on all data: 0.016132046
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1429632e-06
Norm of the params: 13.623391
                Loss: fixed  90 labels. Loss 0.01613. Accuracy 0.996.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18321447
Train loss (w/o reg) on all data: 0.17385913
Test loss (w/o reg) on all data: 0.12454976
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.5733898e-05
Norm of the params: 13.678697
              Random: fixed  14 labels. Loss 0.12455. Accuracy 0.966.
### Flips: 208, rs: 31, checks: 208
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020231152
Train loss (w/o reg) on all data: 0.012089383
Test loss (w/o reg) on all data: 0.023267386
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8176912e-06
Norm of the params: 12.760697
     Influence (LOO): fixed  93 labels. Loss 0.02327. Accuracy 0.992.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010525249
Train loss (w/o reg) on all data: 0.0045523616
Test loss (w/o reg) on all data: 0.011393562
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.937883e-07
Norm of the params: 10.929674
                Loss: fixed  97 labels. Loss 0.01139. Accuracy 0.992.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1775812
Train loss (w/o reg) on all data: 0.16856316
Test loss (w/o reg) on all data: 0.11797267
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.5263145e-05
Norm of the params: 13.429859
              Random: fixed  19 labels. Loss 0.11797. Accuracy 0.969.
### Flips: 208, rs: 31, checks: 260
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021728673
Test loss (w/o reg) on all data: 0.0120548885
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5503652e-07
Norm of the params: 9.153305
     Influence (LOO): fixed 100 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075953947
Train loss (w/o reg) on all data: 0.0028998796
Test loss (w/o reg) on all data: 0.010884092
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9580794e-07
Norm of the params: 9.690733
                Loss: fixed  99 labels. Loss 0.01088. Accuracy 0.996.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17143834
Train loss (w/o reg) on all data: 0.16248894
Test loss (w/o reg) on all data: 0.1161107
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.9437716e-05
Norm of the params: 13.378634
              Random: fixed  22 labels. Loss 0.11611. Accuracy 0.969.
### Flips: 208, rs: 31, checks: 312
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620186
Train loss (w/o reg) on all data: 0.0021730005
Test loss (w/o reg) on all data: 0.012054747
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.2410885e-07
Norm of the params: 9.153161
     Influence (LOO): fixed 100 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [0] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021730007
Test loss (w/o reg) on all data: 0.0120546445
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6262726e-07
Norm of the params: 9.153162
                Loss: fixed 100 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16299771
Train loss (w/o reg) on all data: 0.1542916
Test loss (w/o reg) on all data: 0.1036145
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.3241322e-05
Norm of the params: 13.195539
              Random: fixed  28 labels. Loss 0.10361. Accuracy 0.977.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21095182
Train loss (w/o reg) on all data: 0.20391908
Test loss (w/o reg) on all data: 0.14503941
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.1414078e-05
Norm of the params: 11.859793
Flipped loss: 0.14504. Accuracy: 0.947
### Flips: 208, rs: 32, checks: 52
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1148704
Train loss (w/o reg) on all data: 0.105197586
Test loss (w/o reg) on all data: 0.08915423
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.577865e-06
Norm of the params: 13.908856
     Influence (LOO): fixed  44 labels. Loss 0.08915. Accuracy 0.958.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.072778754
Train loss (w/o reg) on all data: 0.057195093
Test loss (w/o reg) on all data: 0.10891351
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.8151505e-06
Norm of the params: 17.654268
                Loss: fixed  52 labels. Loss 0.10891. Accuracy 0.954.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20595184
Train loss (w/o reg) on all data: 0.1987152
Test loss (w/o reg) on all data: 0.1430785
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.270923e-05
Norm of the params: 12.030495
              Random: fixed   5 labels. Loss 0.14308. Accuracy 0.954.
### Flips: 208, rs: 32, checks: 104
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075564764
Train loss (w/o reg) on all data: 0.065733895
Test loss (w/o reg) on all data: 0.061386954
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1284507e-05
Norm of the params: 14.022034
     Influence (LOO): fixed  65 labels. Loss 0.06139. Accuracy 0.966.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029738128
Train loss (w/o reg) on all data: 0.01787954
Test loss (w/o reg) on all data: 0.025567478
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7770095e-06
Norm of the params: 15.400383
                Loss: fixed  83 labels. Loss 0.02557. Accuracy 0.992.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2037159
Train loss (w/o reg) on all data: 0.19655383
Test loss (w/o reg) on all data: 0.13967031
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.1133949e-05
Norm of the params: 11.968363
              Random: fixed   7 labels. Loss 0.13967. Accuracy 0.947.
### Flips: 208, rs: 32, checks: 156
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035206094
Train loss (w/o reg) on all data: 0.026669811
Test loss (w/o reg) on all data: 0.028431755
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9528425e-06
Norm of the params: 13.066202
     Influence (LOO): fixed  88 labels. Loss 0.02843. Accuracy 0.992.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016749838
Train loss (w/o reg) on all data: 0.008201371
Test loss (w/o reg) on all data: 0.019916087
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0339372e-06
Norm of the params: 13.075524
                Loss: fixed  95 labels. Loss 0.01992. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20085345
Train loss (w/o reg) on all data: 0.19366528
Test loss (w/o reg) on all data: 0.13688397
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 0.00011635358
Norm of the params: 11.990136
              Random: fixed   9 labels. Loss 0.13688. Accuracy 0.958.
### Flips: 208, rs: 32, checks: 208
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022936277
Train loss (w/o reg) on all data: 0.015913302
Test loss (w/o reg) on all data: 0.025526242
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8100683e-06
Norm of the params: 11.851561
     Influence (LOO): fixed  96 labels. Loss 0.02553. Accuracy 0.989.
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012673111
Train loss (w/o reg) on all data: 0.0058163987
Test loss (w/o reg) on all data: 0.016187852
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.5391764e-07
Norm of the params: 11.710433
                Loss: fixed  99 labels. Loss 0.01619. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19988075
Train loss (w/o reg) on all data: 0.19260152
Test loss (w/o reg) on all data: 0.13555577
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 9.967844e-06
Norm of the params: 12.065842
              Random: fixed  10 labels. Loss 0.13556. Accuracy 0.958.
### Flips: 208, rs: 32, checks: 260
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008005807
Train loss (w/o reg) on all data: 0.0030481855
Test loss (w/o reg) on all data: 0.009513487
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2125027e-07
Norm of the params: 9.957532
     Influence (LOO): fixed 103 labels. Loss 0.00951. Accuracy 0.992.
Using normal model
LBFGS training took [45] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007824826
Train loss (w/o reg) on all data: 0.0029210113
Test loss (w/o reg) on all data: 0.008974264
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5683975e-07
Norm of the params: 9.903348
                Loss: fixed 104 labels. Loss 0.00897. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18254101
Train loss (w/o reg) on all data: 0.17450549
Test loss (w/o reg) on all data: 0.13502121
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 6.879788e-05
Norm of the params: 12.677161
              Random: fixed  20 labels. Loss 0.13502. Accuracy 0.943.
### Flips: 208, rs: 32, checks: 312
Using normal model
LBFGS training took [61] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008005807
Train loss (w/o reg) on all data: 0.0030482656
Test loss (w/o reg) on all data: 0.009513144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.544011e-07
Norm of the params: 9.957452
     Influence (LOO): fixed 103 labels. Loss 0.00951. Accuracy 0.992.
Using normal model
LBFGS training took [49] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007824826
Train loss (w/o reg) on all data: 0.002921026
Test loss (w/o reg) on all data: 0.008974672
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8367097e-07
Norm of the params: 9.903334
                Loss: fixed 104 labels. Loss 0.00897. Accuracy 0.992.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17870782
Train loss (w/o reg) on all data: 0.17067112
Test loss (w/o reg) on all data: 0.12719345
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 8.891907e-06
Norm of the params: 12.67809
              Random: fixed  23 labels. Loss 0.12719. Accuracy 0.954.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24048726
Train loss (w/o reg) on all data: 0.23383787
Test loss (w/o reg) on all data: 0.18160251
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.79304e-05
Norm of the params: 11.532035
Flipped loss: 0.18160. Accuracy: 0.958
### Flips: 208, rs: 33, checks: 52
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15797897
Train loss (w/o reg) on all data: 0.14627089
Test loss (w/o reg) on all data: 0.1312359
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0676125e-05
Norm of the params: 15.302341
     Influence (LOO): fixed  42 labels. Loss 0.13124. Accuracy 0.969.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11967764
Train loss (w/o reg) on all data: 0.10598214
Test loss (w/o reg) on all data: 0.17616892
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.2549119e-05
Norm of the params: 16.550228
                Loss: fixed  50 labels. Loss 0.17617. Accuracy 0.935.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22975184
Train loss (w/o reg) on all data: 0.22292991
Test loss (w/o reg) on all data: 0.17663242
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.5279227e-05
Norm of the params: 11.680688
              Random: fixed   9 labels. Loss 0.17663. Accuracy 0.958.
### Flips: 208, rs: 33, checks: 104
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10772562
Train loss (w/o reg) on all data: 0.09794934
Test loss (w/o reg) on all data: 0.08536265
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.000692e-06
Norm of the params: 13.983045
     Influence (LOO): fixed  72 labels. Loss 0.08536. Accuracy 0.981.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04098765
Train loss (w/o reg) on all data: 0.027764995
Test loss (w/o reg) on all data: 0.08011919
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.495147e-06
Norm of the params: 16.262014
                Loss: fixed  98 labels. Loss 0.08012. Accuracy 0.981.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2233171
Train loss (w/o reg) on all data: 0.21605203
Test loss (w/o reg) on all data: 0.17169987
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.9345527e-05
Norm of the params: 12.054111
              Random: fixed  14 labels. Loss 0.17170. Accuracy 0.958.
### Flips: 208, rs: 33, checks: 156
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04810069
Train loss (w/o reg) on all data: 0.039021492
Test loss (w/o reg) on all data: 0.05790307
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.8277212e-06
Norm of the params: 13.475309
     Influence (LOO): fixed  96 labels. Loss 0.05790. Accuracy 0.977.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016754523
Train loss (w/o reg) on all data: 0.008332448
Test loss (w/o reg) on all data: 0.035955325
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.601436e-07
Norm of the params: 12.978502
                Loss: fixed 110 labels. Loss 0.03596. Accuracy 0.985.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2145812
Train loss (w/o reg) on all data: 0.20665176
Test loss (w/o reg) on all data: 0.17106666
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.8487087e-05
Norm of the params: 12.593207
              Random: fixed  18 labels. Loss 0.17107. Accuracy 0.962.
### Flips: 208, rs: 33, checks: 208
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020735372
Train loss (w/o reg) on all data: 0.01392375
Test loss (w/o reg) on all data: 0.03143084
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.900767e-07
Norm of the params: 11.671865
     Influence (LOO): fixed 110 labels. Loss 0.03143. Accuracy 0.989.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009580636
Train loss (w/o reg) on all data: 0.0038935922
Test loss (w/o reg) on all data: 0.016407372
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.2449966e-07
Norm of the params: 10.664937
                Loss: fixed 114 labels. Loss 0.01641. Accuracy 0.989.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20699854
Train loss (w/o reg) on all data: 0.19924946
Test loss (w/o reg) on all data: 0.16744997
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.685351e-05
Norm of the params: 12.449164
              Random: fixed  24 labels. Loss 0.16745. Accuracy 0.954.
### Flips: 208, rs: 33, checks: 260
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009232422
Train loss (w/o reg) on all data: 0.0039342055
Test loss (w/o reg) on all data: 0.016214075
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.997673e-07
Norm of the params: 10.293899
     Influence (LOO): fixed 116 labels. Loss 0.01621. Accuracy 0.992.
Using normal model
LBFGS training took [58] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008816605
Train loss (w/o reg) on all data: 0.0033417908
Test loss (w/o reg) on all data: 0.01688705
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7426513e-06
Norm of the params: 10.464047
                Loss: fixed 115 labels. Loss 0.01689. Accuracy 0.989.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19961786
Train loss (w/o reg) on all data: 0.19143604
Test loss (w/o reg) on all data: 0.15383978
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.2033902e-05
Norm of the params: 12.792045
              Random: fixed  29 labels. Loss 0.15384. Accuracy 0.962.
### Flips: 208, rs: 33, checks: 312
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729555
Test loss (w/o reg) on all data: 0.012053737
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4233358e-07
Norm of the params: 9.153211
     Influence (LOO): fixed 118 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008816606
Train loss (w/o reg) on all data: 0.0033416504
Test loss (w/o reg) on all data: 0.016886314
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.218719e-07
Norm of the params: 10.464183
                Loss: fixed 115 labels. Loss 0.01689. Accuracy 0.989.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18908449
Train loss (w/o reg) on all data: 0.18098052
Test loss (w/o reg) on all data: 0.14626637
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.460892e-06
Norm of the params: 12.731035
              Random: fixed  36 labels. Loss 0.14627. Accuracy 0.962.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21172732
Train loss (w/o reg) on all data: 0.20544548
Test loss (w/o reg) on all data: 0.12822805
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6302618e-05
Norm of the params: 11.208776
Flipped loss: 0.12823. Accuracy: 0.969
### Flips: 208, rs: 34, checks: 52
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11739398
Train loss (w/o reg) on all data: 0.107683025
Test loss (w/o reg) on all data: 0.07935057
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.547023e-06
Norm of the params: 13.936251
     Influence (LOO): fixed  44 labels. Loss 0.07935. Accuracy 0.977.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07963407
Train loss (w/o reg) on all data: 0.064375676
Test loss (w/o reg) on all data: 0.10958528
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0212671e-05
Norm of the params: 17.469053
                Loss: fixed  50 labels. Loss 0.10959. Accuracy 0.966.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1983314
Train loss (w/o reg) on all data: 0.19144714
Test loss (w/o reg) on all data: 0.11250944
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0477732e-05
Norm of the params: 11.733936
              Random: fixed   7 labels. Loss 0.11251. Accuracy 0.966.
### Flips: 208, rs: 34, checks: 104
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05183602
Train loss (w/o reg) on all data: 0.039987314
Test loss (w/o reg) on all data: 0.052112903
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.722955e-06
Norm of the params: 15.393964
     Influence (LOO): fixed  71 labels. Loss 0.05211. Accuracy 0.981.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028504968
Train loss (w/o reg) on all data: 0.01775786
Test loss (w/o reg) on all data: 0.034160055
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5786622e-06
Norm of the params: 14.660906
                Loss: fixed  86 labels. Loss 0.03416. Accuracy 0.989.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1920091
Train loss (w/o reg) on all data: 0.18529873
Test loss (w/o reg) on all data: 0.0955638
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1043648e-05
Norm of the params: 11.5848
              Random: fixed  15 labels. Loss 0.09556. Accuracy 0.962.
### Flips: 208, rs: 34, checks: 156
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030816995
Train loss (w/o reg) on all data: 0.022232026
Test loss (w/o reg) on all data: 0.022623105
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.7572967e-06
Norm of the params: 13.103411
     Influence (LOO): fixed  88 labels. Loss 0.02262. Accuracy 0.989.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020205695
Train loss (w/o reg) on all data: 0.011700453
Test loss (w/o reg) on all data: 0.021954926
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.9808756e-06
Norm of the params: 13.042425
                Loss: fixed  94 labels. Loss 0.02195. Accuracy 0.989.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18117744
Train loss (w/o reg) on all data: 0.17412578
Test loss (w/o reg) on all data: 0.09019601
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.5664533e-05
Norm of the params: 11.875739
              Random: fixed  22 labels. Loss 0.09020. Accuracy 0.969.
### Flips: 208, rs: 34, checks: 208
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019277684
Train loss (w/o reg) on all data: 0.011294597
Test loss (w/o reg) on all data: 0.015108119
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8878437e-06
Norm of the params: 12.635733
     Influence (LOO): fixed  95 labels. Loss 0.01511. Accuracy 0.992.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011955451
Train loss (w/o reg) on all data: 0.0052261674
Test loss (w/o reg) on all data: 0.021666553
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.3652798e-07
Norm of the params: 11.601107
                Loss: fixed  99 labels. Loss 0.02167. Accuracy 0.989.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17263015
Train loss (w/o reg) on all data: 0.16483878
Test loss (w/o reg) on all data: 0.08447087
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1490921e-05
Norm of the params: 12.4830885
              Random: fixed  27 labels. Loss 0.08447. Accuracy 0.981.
### Flips: 208, rs: 34, checks: 260
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011952266
Train loss (w/o reg) on all data: 0.0060358667
Test loss (w/o reg) on all data: 0.011862133
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.137137e-06
Norm of the params: 10.877867
     Influence (LOO): fixed 100 labels. Loss 0.01186. Accuracy 0.992.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009358311
Train loss (w/o reg) on all data: 0.003852225
Test loss (w/o reg) on all data: 0.015522818
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8762273e-07
Norm of the params: 10.49389
                Loss: fixed 101 labels. Loss 0.01552. Accuracy 0.989.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17053787
Train loss (w/o reg) on all data: 0.16288507
Test loss (w/o reg) on all data: 0.080747865
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4363635e-05
Norm of the params: 12.371584
              Random: fixed  30 labels. Loss 0.08075. Accuracy 0.981.
### Flips: 208, rs: 34, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010541444
Train loss (w/o reg) on all data: 0.0052486397
Test loss (w/o reg) on all data: 0.011331186
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7342523e-07
Norm of the params: 10.288639
     Influence (LOO): fixed 102 labels. Loss 0.01133. Accuracy 0.992.
Using normal model
LBFGS training took [36] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0071565206
Train loss (w/o reg) on all data: 0.0025722086
Test loss (w/o reg) on all data: 0.016341262
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5328657e-07
Norm of the params: 9.575293
                Loss: fixed 103 labels. Loss 0.01634. Accuracy 0.989.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16462405
Train loss (w/o reg) on all data: 0.15715465
Test loss (w/o reg) on all data: 0.07781001
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1440939e-05
Norm of the params: 12.222442
              Random: fixed  34 labels. Loss 0.07781. Accuracy 0.977.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20896082
Train loss (w/o reg) on all data: 0.20144142
Test loss (w/o reg) on all data: 0.12700486
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.4087127e-05
Norm of the params: 12.263275
Flipped loss: 0.12700. Accuracy: 0.969
### Flips: 208, rs: 35, checks: 52
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1235116
Train loss (w/o reg) on all data: 0.11198707
Test loss (w/o reg) on all data: 0.09079012
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5697475e-05
Norm of the params: 15.181914
     Influence (LOO): fixed  40 labels. Loss 0.09079. Accuracy 0.969.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08012285
Train loss (w/o reg) on all data: 0.064805485
Test loss (w/o reg) on all data: 0.08197036
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.5729227e-06
Norm of the params: 17.502777
                Loss: fixed  51 labels. Loss 0.08197. Accuracy 0.973.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1961286
Train loss (w/o reg) on all data: 0.18831079
Test loss (w/o reg) on all data: 0.1156205
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.7867152e-05
Norm of the params: 12.504252
              Random: fixed   7 labels. Loss 0.11562. Accuracy 0.973.
### Flips: 208, rs: 35, checks: 104
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051321648
Train loss (w/o reg) on all data: 0.03937709
Test loss (w/o reg) on all data: 0.041012272
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.595777e-06
Norm of the params: 15.456104
     Influence (LOO): fixed  74 labels. Loss 0.04101. Accuracy 0.981.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025326453
Train loss (w/o reg) on all data: 0.014394015
Test loss (w/o reg) on all data: 0.012222639
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.543969e-07
Norm of the params: 14.786777
                Loss: fixed  89 labels. Loss 0.01222. Accuracy 0.996.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19100907
Train loss (w/o reg) on all data: 0.18310705
Test loss (w/o reg) on all data: 0.11444082
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.3804874e-05
Norm of the params: 12.571413
              Random: fixed  11 labels. Loss 0.11444. Accuracy 0.977.
### Flips: 208, rs: 35, checks: 156
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025317311
Train loss (w/o reg) on all data: 0.016760627
Test loss (w/o reg) on all data: 0.023088235
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.796044e-07
Norm of the params: 13.081807
     Influence (LOO): fixed  91 labels. Loss 0.02309. Accuracy 0.992.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01588731
Train loss (w/o reg) on all data: 0.0078114215
Test loss (w/o reg) on all data: 0.014882761
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5859076e-06
Norm of the params: 12.708965
                Loss: fixed  95 labels. Loss 0.01488. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18816207
Train loss (w/o reg) on all data: 0.18011084
Test loss (w/o reg) on all data: 0.10803947
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.085112e-05
Norm of the params: 12.68955
              Random: fixed  14 labels. Loss 0.10804. Accuracy 0.981.
### Flips: 208, rs: 35, checks: 208
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016133502
Train loss (w/o reg) on all data: 0.008937944
Test loss (w/o reg) on all data: 0.0236383
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4229632e-06
Norm of the params: 11.9963
     Influence (LOO): fixed  97 labels. Loss 0.02364. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009604899
Train loss (w/o reg) on all data: 0.003991401
Test loss (w/o reg) on all data: 0.012300226
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.3200408e-07
Norm of the params: 10.595753
                Loss: fixed  98 labels. Loss 0.01230. Accuracy 0.992.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18740028
Train loss (w/o reg) on all data: 0.17955369
Test loss (w/o reg) on all data: 0.10587999
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4658403e-05
Norm of the params: 12.527241
              Random: fixed  17 labels. Loss 0.10588. Accuracy 0.981.
### Flips: 208, rs: 35, checks: 260
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009025987
Train loss (w/o reg) on all data: 0.0038992637
Test loss (w/o reg) on all data: 0.017265722
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0196453e-07
Norm of the params: 10.125932
     Influence (LOO): fixed 101 labels. Loss 0.01727. Accuracy 0.992.
Using normal model
LBFGS training took [51] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008056695
Train loss (w/o reg) on all data: 0.0030190651
Test loss (w/o reg) on all data: 0.010702341
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.8070976e-07
Norm of the params: 10.0375595
                Loss: fixed 100 labels. Loss 0.01070. Accuracy 0.996.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18405919
Train loss (w/o reg) on all data: 0.17618346
Test loss (w/o reg) on all data: 0.10296716
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.9452516e-05
Norm of the params: 12.550483
              Random: fixed  20 labels. Loss 0.10297. Accuracy 0.977.
### Flips: 208, rs: 35, checks: 312
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620177
Train loss (w/o reg) on all data: 0.0021728864
Test loss (w/o reg) on all data: 0.012054924
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9105143e-07
Norm of the params: 9.153285
     Influence (LOO): fixed 102 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [38] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0077379476
Train loss (w/o reg) on all data: 0.0028791707
Test loss (w/o reg) on all data: 0.0107901655
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.0827825e-06
Norm of the params: 9.857765
                Loss: fixed 101 labels. Loss 0.01079. Accuracy 0.996.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17427957
Train loss (w/o reg) on all data: 0.16626762
Test loss (w/o reg) on all data: 0.10380513
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1521858e-05
Norm of the params: 12.658555
              Random: fixed  25 labels. Loss 0.10381. Accuracy 0.973.
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19844182
Train loss (w/o reg) on all data: 0.18826154
Test loss (w/o reg) on all data: 0.11397944
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.5297131e-05
Norm of the params: 14.269045
Flipped loss: 0.11398. Accuracy: 0.969
### Flips: 208, rs: 36, checks: 52
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11278397
Train loss (w/o reg) on all data: 0.099217504
Test loss (w/o reg) on all data: 0.07099717
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.7510847e-05
Norm of the params: 16.472076
     Influence (LOO): fixed  42 labels. Loss 0.07100. Accuracy 0.973.
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07113536
Train loss (w/o reg) on all data: 0.052467316
Test loss (w/o reg) on all data: 0.095410965
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.7575847e-06
Norm of the params: 19.322546
                Loss: fixed  52 labels. Loss 0.09541. Accuracy 0.958.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18982519
Train loss (w/o reg) on all data: 0.1794663
Test loss (w/o reg) on all data: 0.10972827
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.0701784e-05
Norm of the params: 14.393666
              Random: fixed   8 labels. Loss 0.10973. Accuracy 0.969.
### Flips: 208, rs: 36, checks: 104
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04846538
Train loss (w/o reg) on all data: 0.03632345
Test loss (w/o reg) on all data: 0.03695329
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.642446e-06
Norm of the params: 15.583279
     Influence (LOO): fixed  76 labels. Loss 0.03695. Accuracy 0.989.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029527642
Train loss (w/o reg) on all data: 0.016192691
Test loss (w/o reg) on all data: 0.054256607
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.8282834e-06
Norm of the params: 16.330923
                Loss: fixed  80 labels. Loss 0.05426. Accuracy 0.981.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18640691
Train loss (w/o reg) on all data: 0.17655686
Test loss (w/o reg) on all data: 0.10658252
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.7609036e-05
Norm of the params: 14.035704
              Random: fixed  12 labels. Loss 0.10658. Accuracy 0.969.
### Flips: 208, rs: 36, checks: 156
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024497416
Train loss (w/o reg) on all data: 0.014234278
Test loss (w/o reg) on all data: 0.024358181
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0561318e-06
Norm of the params: 14.326993
     Influence (LOO): fixed  89 labels. Loss 0.02436. Accuracy 0.989.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014374608
Train loss (w/o reg) on all data: 0.0063144607
Test loss (w/o reg) on all data: 0.017234702
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.966523e-07
Norm of the params: 12.696572
                Loss: fixed  93 labels. Loss 0.01723. Accuracy 0.996.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16705768
Train loss (w/o reg) on all data: 0.157591
Test loss (w/o reg) on all data: 0.095385425
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4898054e-05
Norm of the params: 13.759857
              Random: fixed  23 labels. Loss 0.09539. Accuracy 0.977.
### Flips: 208, rs: 36, checks: 208
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020009182
Train loss (w/o reg) on all data: 0.0110008465
Test loss (w/o reg) on all data: 0.017666234
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1543503e-06
Norm of the params: 13.422621
     Influence (LOO): fixed  93 labels. Loss 0.01767. Accuracy 0.992.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012966001
Train loss (w/o reg) on all data: 0.0056473603
Test loss (w/o reg) on all data: 0.016308937
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2214755e-06
Norm of the params: 12.098463
                Loss: fixed  96 labels. Loss 0.01631. Accuracy 0.992.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16148463
Train loss (w/o reg) on all data: 0.15209049
Test loss (w/o reg) on all data: 0.09072824
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.2575357e-05
Norm of the params: 13.707029
              Random: fixed  28 labels. Loss 0.09073. Accuracy 0.977.
### Flips: 208, rs: 36, checks: 260
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012582004
Train loss (w/o reg) on all data: 0.0062616314
Test loss (w/o reg) on all data: 0.008568693
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.1081782e-07
Norm of the params: 11.243107
     Influence (LOO): fixed  98 labels. Loss 0.00857. Accuracy 0.996.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0119703
Train loss (w/o reg) on all data: 0.0051285885
Test loss (w/o reg) on all data: 0.0071294243
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.3491755e-07
Norm of the params: 11.697617
                Loss: fixed  98 labels. Loss 0.00713. Accuracy 1.000.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15365373
Train loss (w/o reg) on all data: 0.14350669
Test loss (w/o reg) on all data: 0.09410715
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.192859e-06
Norm of the params: 14.245722
              Random: fixed  31 labels. Loss 0.09411. Accuracy 0.973.
### Flips: 208, rs: 36, checks: 312
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010973788
Train loss (w/o reg) on all data: 0.005245844
Test loss (w/o reg) on all data: 0.008753965
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.327393e-07
Norm of the params: 10.703218
     Influence (LOO): fixed  99 labels. Loss 0.00875. Accuracy 0.996.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011970298
Train loss (w/o reg) on all data: 0.0051288307
Test loss (w/o reg) on all data: 0.0071292836
Train acc on all data:  1.0
Test acc on all data:   1.0
Norm of the mean of gradients: 4.922684e-07
Norm of the params: 11.697409
                Loss: fixed  98 labels. Loss 0.00713. Accuracy 1.000.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14086552
Train loss (w/o reg) on all data: 0.13008429
Test loss (w/o reg) on all data: 0.09109922
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.941573e-06
Norm of the params: 14.684161
              Random: fixed  38 labels. Loss 0.09110. Accuracy 0.973.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21134509
Train loss (w/o reg) on all data: 0.20443815
Test loss (w/o reg) on all data: 0.13652892
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.637269e-05
Norm of the params: 11.753244
Flipped loss: 0.13653. Accuracy: 0.966
### Flips: 208, rs: 37, checks: 52
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13442962
Train loss (w/o reg) on all data: 0.12523082
Test loss (w/o reg) on all data: 0.104151525
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8107752e-05
Norm of the params: 13.563781
     Influence (LOO): fixed  42 labels. Loss 0.10415. Accuracy 0.966.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09666087
Train loss (w/o reg) on all data: 0.08231304
Test loss (w/o reg) on all data: 0.09808649
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.8919264e-06
Norm of the params: 16.939795
                Loss: fixed  48 labels. Loss 0.09809. Accuracy 0.958.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20195067
Train loss (w/o reg) on all data: 0.19442697
Test loss (w/o reg) on all data: 0.12068409
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.0190133e-05
Norm of the params: 12.26678
              Random: fixed   8 labels. Loss 0.12068. Accuracy 0.969.
### Flips: 208, rs: 37, checks: 104
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0767707
Train loss (w/o reg) on all data: 0.06701691
Test loss (w/o reg) on all data: 0.07620292
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0045727e-05
Norm of the params: 13.966957
     Influence (LOO): fixed  71 labels. Loss 0.07620. Accuracy 0.966.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03391333
Train loss (w/o reg) on all data: 0.020927394
Test loss (w/o reg) on all data: 0.043942843
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1097405e-06
Norm of the params: 16.115791
                Loss: fixed  87 labels. Loss 0.04394. Accuracy 0.977.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1992433
Train loss (w/o reg) on all data: 0.19172749
Test loss (w/o reg) on all data: 0.11970221
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.03017555e-05
Norm of the params: 12.260362
              Random: fixed  11 labels. Loss 0.11970. Accuracy 0.973.
### Flips: 208, rs: 37, checks: 156
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03562589
Train loss (w/o reg) on all data: 0.027656242
Test loss (w/o reg) on all data: 0.033008184
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7537142e-06
Norm of the params: 12.625092
     Influence (LOO): fixed  92 labels. Loss 0.03301. Accuracy 0.985.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020724617
Train loss (w/o reg) on all data: 0.011625598
Test loss (w/o reg) on all data: 0.029643528
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0630243e-06
Norm of the params: 13.49001
                Loss: fixed  98 labels. Loss 0.02964. Accuracy 0.985.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1852652
Train loss (w/o reg) on all data: 0.17757256
Test loss (w/o reg) on all data: 0.1111594
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.4318932e-05
Norm of the params: 12.403732
              Random: fixed  19 labels. Loss 0.11116. Accuracy 0.958.
### Flips: 208, rs: 37, checks: 208
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023563571
Train loss (w/o reg) on all data: 0.01598704
Test loss (w/o reg) on all data: 0.023276104
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 8.331999e-06
Norm of the params: 12.309777
     Influence (LOO): fixed 100 labels. Loss 0.02328. Accuracy 0.996.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01507725
Train loss (w/o reg) on all data: 0.006949255
Test loss (w/o reg) on all data: 0.02648711
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.5022217e-07
Norm of the params: 12.749898
                Loss: fixed 101 labels. Loss 0.02649. Accuracy 0.985.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17633067
Train loss (w/o reg) on all data: 0.16764823
Test loss (w/o reg) on all data: 0.102301106
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.0050134e-05
Norm of the params: 13.177592
              Random: fixed  24 labels. Loss 0.10230. Accuracy 0.973.
### Flips: 208, rs: 37, checks: 260
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01294545
Train loss (w/o reg) on all data: 0.0068005905
Test loss (w/o reg) on all data: 0.017291099
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1401851e-07
Norm of the params: 11.0859
     Influence (LOO): fixed 105 labels. Loss 0.01729. Accuracy 0.992.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010073043
Train loss (w/o reg) on all data: 0.0040522385
Test loss (w/o reg) on all data: 0.0133087095
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.801226e-07
Norm of the params: 10.973427
                Loss: fixed 106 labels. Loss 0.01331. Accuracy 0.992.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16490617
Train loss (w/o reg) on all data: 0.15478279
Test loss (w/o reg) on all data: 0.09707094
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.8606307e-05
Norm of the params: 14.229121
              Random: fixed  30 labels. Loss 0.09707. Accuracy 0.969.
### Flips: 208, rs: 37, checks: 312
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007851765
Train loss (w/o reg) on all data: 0.0030548144
Test loss (w/o reg) on all data: 0.011449365
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.535069e-07
Norm of the params: 9.794847
     Influence (LOO): fixed 108 labels. Loss 0.01145. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008472963
Train loss (w/o reg) on all data: 0.0033283276
Test loss (w/o reg) on all data: 0.013517573
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0320504e-06
Norm of the params: 10.143605
                Loss: fixed 107 labels. Loss 0.01352. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15323375
Train loss (w/o reg) on all data: 0.14331353
Test loss (w/o reg) on all data: 0.08928502
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.0240262e-05
Norm of the params: 14.085609
              Random: fixed  37 labels. Loss 0.08929. Accuracy 0.985.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21300699
Train loss (w/o reg) on all data: 0.20576245
Test loss (w/o reg) on all data: 0.15638311
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.206851e-05
Norm of the params: 12.037068
Flipped loss: 0.15638. Accuracy: 0.954
### Flips: 208, rs: 38, checks: 52
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12726338
Train loss (w/o reg) on all data: 0.116484866
Test loss (w/o reg) on all data: 0.11627228
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.3559316e-05
Norm of the params: 14.682318
     Influence (LOO): fixed  40 labels. Loss 0.11627. Accuracy 0.958.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09596716
Train loss (w/o reg) on all data: 0.08055199
Test loss (w/o reg) on all data: 0.13220353
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.040276e-06
Norm of the params: 17.55857
                Loss: fixed  48 labels. Loss 0.13220. Accuracy 0.947.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20166032
Train loss (w/o reg) on all data: 0.19440779
Test loss (w/o reg) on all data: 0.14893849
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4163797e-05
Norm of the params: 12.043696
              Random: fixed   7 labels. Loss 0.14894. Accuracy 0.958.
### Flips: 208, rs: 38, checks: 104
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07677556
Train loss (w/o reg) on all data: 0.06510867
Test loss (w/o reg) on all data: 0.076168954
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.740748e-06
Norm of the params: 15.275395
     Influence (LOO): fixed  71 labels. Loss 0.07617. Accuracy 0.962.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030644149
Train loss (w/o reg) on all data: 0.017993124
Test loss (w/o reg) on all data: 0.0612396
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.3374902e-06
Norm of the params: 15.906618
                Loss: fixed  88 labels. Loss 0.06124. Accuracy 0.966.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1894767
Train loss (w/o reg) on all data: 0.18200088
Test loss (w/o reg) on all data: 0.14206241
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 9.868021e-06
Norm of the params: 12.22769
              Random: fixed  15 labels. Loss 0.14206. Accuracy 0.958.
### Flips: 208, rs: 38, checks: 156
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043070525
Train loss (w/o reg) on all data: 0.03181471
Test loss (w/o reg) on all data: 0.07915131
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.4928114e-06
Norm of the params: 15.003878
     Influence (LOO): fixed  86 labels. Loss 0.07915. Accuracy 0.962.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02128521
Train loss (w/o reg) on all data: 0.011131046
Test loss (w/o reg) on all data: 0.03941704
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8204246e-06
Norm of the params: 14.25073
                Loss: fixed  97 labels. Loss 0.03942. Accuracy 0.985.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18833122
Train loss (w/o reg) on all data: 0.18086644
Test loss (w/o reg) on all data: 0.13533306
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.1594222e-05
Norm of the params: 12.218657
              Random: fixed  18 labels. Loss 0.13533. Accuracy 0.950.
### Flips: 208, rs: 38, checks: 208
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023941599
Train loss (w/o reg) on all data: 0.01451173
Test loss (w/o reg) on all data: 0.041829355
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.049805e-07
Norm of the params: 13.733077
     Influence (LOO): fixed  97 labels. Loss 0.04183. Accuracy 0.981.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014107043
Train loss (w/o reg) on all data: 0.0064893723
Test loss (w/o reg) on all data: 0.03142579
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.1756525e-07
Norm of the params: 12.343152
                Loss: fixed 102 labels. Loss 0.03143. Accuracy 0.989.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17850688
Train loss (w/o reg) on all data: 0.17117468
Test loss (w/o reg) on all data: 0.12465695
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.977113e-05
Norm of the params: 12.109675
              Random: fixed  24 labels. Loss 0.12466. Accuracy 0.962.
### Flips: 208, rs: 38, checks: 260
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016066091
Train loss (w/o reg) on all data: 0.008981934
Test loss (w/o reg) on all data: 0.030795464
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.109883e-07
Norm of the params: 11.903072
     Influence (LOO): fixed 103 labels. Loss 0.03080. Accuracy 0.989.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011090752
Train loss (w/o reg) on all data: 0.004849928
Test loss (w/o reg) on all data: 0.017693857
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.8977364e-07
Norm of the params: 11.172129
                Loss: fixed 105 labels. Loss 0.01769. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17235823
Train loss (w/o reg) on all data: 0.16488041
Test loss (w/o reg) on all data: 0.109478965
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.4408637e-05
Norm of the params: 12.229321
              Random: fixed  30 labels. Loss 0.10948. Accuracy 0.969.
### Flips: 208, rs: 38, checks: 312
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009303676
Train loss (w/o reg) on all data: 0.003684606
Test loss (w/o reg) on all data: 0.014798453
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.2351124e-07
Norm of the params: 10.601009
     Influence (LOO): fixed 106 labels. Loss 0.01480. Accuracy 0.992.
Using normal model
LBFGS training took [40] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620205
Train loss (w/o reg) on all data: 0.0021729642
Test loss (w/o reg) on all data: 0.012054681
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2667854e-08
Norm of the params: 9.153203
                Loss: fixed 108 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15808406
Train loss (w/o reg) on all data: 0.1494794
Test loss (w/o reg) on all data: 0.10647326
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.4625178e-05
Norm of the params: 13.118432
              Random: fixed  35 labels. Loss 0.10647. Accuracy 0.966.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22793967
Train loss (w/o reg) on all data: 0.22138923
Test loss (w/o reg) on all data: 0.12563865
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.8385464e-05
Norm of the params: 11.445896
Flipped loss: 0.12564. Accuracy: 0.981
### Flips: 208, rs: 39, checks: 52
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.129257
Train loss (w/o reg) on all data: 0.11867672
Test loss (w/o reg) on all data: 0.07889173
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.19885e-06
Norm of the params: 14.546661
     Influence (LOO): fixed  44 labels. Loss 0.07889. Accuracy 0.981.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09304615
Train loss (w/o reg) on all data: 0.07587295
Test loss (w/o reg) on all data: 0.083206825
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.1342395e-05
Norm of the params: 18.532782
                Loss: fixed  52 labels. Loss 0.08321. Accuracy 0.969.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21802467
Train loss (w/o reg) on all data: 0.21110733
Test loss (w/o reg) on all data: 0.11865446
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.250829e-05
Norm of the params: 11.762095
              Random: fixed   6 labels. Loss 0.11865. Accuracy 0.985.
### Flips: 208, rs: 39, checks: 104
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0821814
Train loss (w/o reg) on all data: 0.071767926
Test loss (w/o reg) on all data: 0.04380474
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.549628e-06
Norm of the params: 14.431546
     Influence (LOO): fixed  68 labels. Loss 0.04380. Accuracy 0.989.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027030379
Train loss (w/o reg) on all data: 0.013828066
Test loss (w/o reg) on all data: 0.016450502
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9329602e-06
Norm of the params: 16.249502
                Loss: fixed  90 labels. Loss 0.01645. Accuracy 0.992.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21068583
Train loss (w/o reg) on all data: 0.20353366
Test loss (w/o reg) on all data: 0.11145105
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1748789e-05
Norm of the params: 11.96007
              Random: fixed  11 labels. Loss 0.11145. Accuracy 0.981.
### Flips: 208, rs: 39, checks: 156
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038322076
Train loss (w/o reg) on all data: 0.029625572
Test loss (w/o reg) on all data: 0.026619041
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.91965e-06
Norm of the params: 13.188253
     Influence (LOO): fixed  91 labels. Loss 0.02662. Accuracy 0.989.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0139695965
Train loss (w/o reg) on all data: 0.006155715
Test loss (w/o reg) on all data: 0.012422098
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4584152e-06
Norm of the params: 12.501104
                Loss: fixed 102 labels. Loss 0.01242. Accuracy 0.996.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20660667
Train loss (w/o reg) on all data: 0.19949387
Test loss (w/o reg) on all data: 0.105940595
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.810737e-06
Norm of the params: 11.927107
              Random: fixed  14 labels. Loss 0.10594. Accuracy 0.985.
### Flips: 208, rs: 39, checks: 208
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015923746
Train loss (w/o reg) on all data: 0.008875915
Test loss (w/o reg) on all data: 0.012623534
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7862666e-06
Norm of the params: 11.872516
     Influence (LOO): fixed 102 labels. Loss 0.01262. Accuracy 0.992.
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008031076
Train loss (w/o reg) on all data: 0.00297408
Test loss (w/o reg) on all data: 0.01499705
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0300068e-06
Norm of the params: 10.056835
                Loss: fixed 106 labels. Loss 0.01500. Accuracy 0.992.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18995732
Train loss (w/o reg) on all data: 0.18210132
Test loss (w/o reg) on all data: 0.09330776
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9142099e-05
Norm of the params: 12.53475
              Random: fixed  22 labels. Loss 0.09331. Accuracy 0.981.
### Flips: 208, rs: 39, checks: 260
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01027125
Train loss (w/o reg) on all data: 0.0048391907
Test loss (w/o reg) on all data: 0.015142848
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.203231e-06
Norm of the params: 10.423108
     Influence (LOO): fixed 105 labels. Loss 0.01514. Accuracy 0.992.
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008031078
Train loss (w/o reg) on all data: 0.002974202
Test loss (w/o reg) on all data: 0.014996987
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9378475e-07
Norm of the params: 10.056716
                Loss: fixed 106 labels. Loss 0.01500. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18686435
Train loss (w/o reg) on all data: 0.17980792
Test loss (w/o reg) on all data: 0.09280857
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.899776e-05
Norm of the params: 11.879761
              Random: fixed  26 labels. Loss 0.09281. Accuracy 0.977.
### Flips: 208, rs: 39, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0063620172
Train loss (w/o reg) on all data: 0.0021730175
Test loss (w/o reg) on all data: 0.012054636
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.0491244e-07
Norm of the params: 9.153141
     Influence (LOO): fixed 107 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008031076
Train loss (w/o reg) on all data: 0.0029741046
Test loss (w/o reg) on all data: 0.014997423
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.861514e-07
Norm of the params: 10.05681
                Loss: fixed 106 labels. Loss 0.01500. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18082362
Train loss (w/o reg) on all data: 0.1736452
Test loss (w/o reg) on all data: 0.093031384
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.7349131e-05
Norm of the params: 11.982014
              Random: fixed  30 labels. Loss 0.09303. Accuracy 0.977.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24182343
Train loss (w/o reg) on all data: 0.23411883
Test loss (w/o reg) on all data: 0.19369285
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.2752891e-05
Norm of the params: 12.413381
Flipped loss: 0.19369. Accuracy: 0.924
### Flips: 260, rs: 0, checks: 52
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16680232
Train loss (w/o reg) on all data: 0.15373065
Test loss (w/o reg) on all data: 0.13765615
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 9.710755e-06
Norm of the params: 16.168901
     Influence (LOO): fixed  39 labels. Loss 0.13766. Accuracy 0.950.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12638211
Train loss (w/o reg) on all data: 0.10905165
Test loss (w/o reg) on all data: 0.14834243
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.9314564e-05
Norm of the params: 18.617443
                Loss: fixed  51 labels. Loss 0.14834. Accuracy 0.931.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23084408
Train loss (w/o reg) on all data: 0.22205016
Test loss (w/o reg) on all data: 0.19125855
Train acc on all data:  0.897803247373448
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 3.6162186e-05
Norm of the params: 13.261917
              Random: fixed   7 labels. Loss 0.19126. Accuracy 0.916.
### Flips: 260, rs: 0, checks: 104
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.119551025
Train loss (w/o reg) on all data: 0.106019005
Test loss (w/o reg) on all data: 0.09541105
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.125507e-05
Norm of the params: 16.451153
     Influence (LOO): fixed  70 labels. Loss 0.09541. Accuracy 0.962.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081885844
Train loss (w/o reg) on all data: 0.06359454
Test loss (w/o reg) on all data: 0.084706195
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.8398955e-06
Norm of the params: 19.126577
                Loss: fixed  85 labels. Loss 0.08471. Accuracy 0.962.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22394699
Train loss (w/o reg) on all data: 0.21503238
Test loss (w/o reg) on all data: 0.18673462
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 2.8040797e-05
Norm of the params: 13.352601
              Random: fixed  13 labels. Loss 0.18673. Accuracy 0.924.
### Flips: 260, rs: 0, checks: 156
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081781805
Train loss (w/o reg) on all data: 0.06802962
Test loss (w/o reg) on all data: 0.0699012
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.230054e-06
Norm of the params: 16.584442
     Influence (LOO): fixed  92 labels. Loss 0.06990. Accuracy 0.973.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040480148
Train loss (w/o reg) on all data: 0.025453554
Test loss (w/o reg) on all data: 0.044511832
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.4467673e-06
Norm of the params: 17.335855
                Loss: fixed 113 labels. Loss 0.04451. Accuracy 0.985.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2220833
Train loss (w/o reg) on all data: 0.21318473
Test loss (w/o reg) on all data: 0.18383573
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 0.00011690293
Norm of the params: 13.340595
              Random: fixed  15 labels. Loss 0.18384. Accuracy 0.931.
### Flips: 260, rs: 0, checks: 208
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061226603
Train loss (w/o reg) on all data: 0.050052796
Test loss (w/o reg) on all data: 0.05167631
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1087588e-05
Norm of the params: 14.94912
     Influence (LOO): fixed 108 labels. Loss 0.05168. Accuracy 0.985.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025386807
Train loss (w/o reg) on all data: 0.013576081
Test loss (w/o reg) on all data: 0.02116188
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.554417e-07
Norm of the params: 15.369271
                Loss: fixed 121 labels. Loss 0.02116. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21764866
Train loss (w/o reg) on all data: 0.20863187
Test loss (w/o reg) on all data: 0.18000963
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.8171817e-05
Norm of the params: 13.428912
              Random: fixed  19 labels. Loss 0.18001. Accuracy 0.947.
### Flips: 260, rs: 0, checks: 260
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031176675
Train loss (w/o reg) on all data: 0.021492139
Test loss (w/o reg) on all data: 0.032170944
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6502423e-06
Norm of the params: 13.917282
     Influence (LOO): fixed 123 labels. Loss 0.03217. Accuracy 0.985.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016514698
Train loss (w/o reg) on all data: 0.0073445146
Test loss (w/o reg) on all data: 0.017895171
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.839887e-07
Norm of the params: 13.542661
                Loss: fixed 126 labels. Loss 0.01790. Accuracy 0.996.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21048392
Train loss (w/o reg) on all data: 0.20167454
Test loss (w/o reg) on all data: 0.16942726
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.9541192e-05
Norm of the params: 13.273579
              Random: fixed  28 labels. Loss 0.16943. Accuracy 0.947.
### Flips: 260, rs: 0, checks: 312
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021747775
Train loss (w/o reg) on all data: 0.013736769
Test loss (w/o reg) on all data: 0.025163893
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5881766e-06
Norm of the params: 12.65781
     Influence (LOO): fixed 128 labels. Loss 0.02516. Accuracy 0.989.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012412516
Train loss (w/o reg) on all data: 0.005176134
Test loss (w/o reg) on all data: 0.013187082
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2219667e-06
Norm of the params: 12.030281
                Loss: fixed 131 labels. Loss 0.01319. Accuracy 0.992.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19780387
Train loss (w/o reg) on all data: 0.1892915
Test loss (w/o reg) on all data: 0.16957633
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.6824324e-05
Norm of the params: 13.047879
              Random: fixed  38 labels. Loss 0.16958. Accuracy 0.939.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26342767
Train loss (w/o reg) on all data: 0.25689766
Test loss (w/o reg) on all data: 0.21205021
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 3.316908e-05
Norm of the params: 11.428037
Flipped loss: 0.21205. Accuracy: 0.912
### Flips: 260, rs: 1, checks: 52
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19816625
Train loss (w/o reg) on all data: 0.18727574
Test loss (w/o reg) on all data: 0.18414202
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.0017656e-05
Norm of the params: 14.758394
     Influence (LOO): fixed  39 labels. Loss 0.18414. Accuracy 0.935.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14502983
Train loss (w/o reg) on all data: 0.13047342
Test loss (w/o reg) on all data: 0.19984543
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 2.605377e-05
Norm of the params: 17.06248
                Loss: fixed  52 labels. Loss 0.19985. Accuracy 0.916.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25458077
Train loss (w/o reg) on all data: 0.24810077
Test loss (w/o reg) on all data: 0.20014468
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.9004161e-05
Norm of the params: 11.384188
              Random: fixed   9 labels. Loss 0.20014. Accuracy 0.920.
### Flips: 260, rs: 1, checks: 104
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14695725
Train loss (w/o reg) on all data: 0.13653037
Test loss (w/o reg) on all data: 0.14140847
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 7.41305e-06
Norm of the params: 14.440831
     Influence (LOO): fixed  68 labels. Loss 0.14141. Accuracy 0.939.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07189016
Train loss (w/o reg) on all data: 0.05237408
Test loss (w/o reg) on all data: 0.18387365
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.5886822e-05
Norm of the params: 19.756557
                Loss: fixed  93 labels. Loss 0.18387. Accuracy 0.931.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24455817
Train loss (w/o reg) on all data: 0.23826255
Test loss (w/o reg) on all data: 0.17646703
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 4.698366e-05
Norm of the params: 11.221077
              Random: fixed  17 labels. Loss 0.17647. Accuracy 0.927.
### Flips: 260, rs: 1, checks: 156
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09592732
Train loss (w/o reg) on all data: 0.08480308
Test loss (w/o reg) on all data: 0.08342125
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 9.83967e-06
Norm of the params: 14.915924
     Influence (LOO): fixed  95 labels. Loss 0.08342. Accuracy 0.962.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04153833
Train loss (w/o reg) on all data: 0.025991635
Test loss (w/o reg) on all data: 0.09471007
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.7292923e-06
Norm of the params: 17.633318
                Loss: fixed 116 labels. Loss 0.09471. Accuracy 0.966.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23574175
Train loss (w/o reg) on all data: 0.22918077
Test loss (w/o reg) on all data: 0.16262802
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 9.738074e-05
Norm of the params: 11.455115
              Random: fixed  25 labels. Loss 0.16263. Accuracy 0.943.
### Flips: 260, rs: 1, checks: 208
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062704116
Train loss (w/o reg) on all data: 0.05211221
Test loss (w/o reg) on all data: 0.05547131
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.0527146e-06
Norm of the params: 14.554659
     Influence (LOO): fixed 115 labels. Loss 0.05547. Accuracy 0.981.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023669533
Train loss (w/o reg) on all data: 0.013222105
Test loss (w/o reg) on all data: 0.053795066
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.8177108e-06
Norm of the params: 14.455053
                Loss: fixed 131 labels. Loss 0.05380. Accuracy 0.981.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22401015
Train loss (w/o reg) on all data: 0.21781889
Test loss (w/o reg) on all data: 0.15291445
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.336458e-05
Norm of the params: 11.127686
              Random: fixed  36 labels. Loss 0.15291. Accuracy 0.950.
### Flips: 260, rs: 1, checks: 260
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03218328
Train loss (w/o reg) on all data: 0.02295762
Test loss (w/o reg) on all data: 0.037037287
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.061296e-06
Norm of the params: 13.583563
     Influence (LOO): fixed 126 labels. Loss 0.03704. Accuracy 0.985.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015905762
Train loss (w/o reg) on all data: 0.00756856
Test loss (w/o reg) on all data: 0.023665017
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.927854e-07
Norm of the params: 12.912941
                Loss: fixed 137 labels. Loss 0.02367. Accuracy 0.989.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2060759
Train loss (w/o reg) on all data: 0.19839479
Test loss (w/o reg) on all data: 0.14718582
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.1286048e-05
Norm of the params: 12.3944435
              Random: fixed  47 labels. Loss 0.14719. Accuracy 0.947.
### Flips: 260, rs: 1, checks: 312
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015484767
Train loss (w/o reg) on all data: 0.008237035
Test loss (w/o reg) on all data: 0.014312419
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.091497e-07
Norm of the params: 12.039711
     Influence (LOO): fixed 136 labels. Loss 0.01431. Accuracy 0.992.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015613595
Train loss (w/o reg) on all data: 0.0074932035
Test loss (w/o reg) on all data: 0.020462204
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3838842e-06
Norm of the params: 12.743934
                Loss: fixed 138 labels. Loss 0.02046. Accuracy 0.989.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19402774
Train loss (w/o reg) on all data: 0.1854614
Test loss (w/o reg) on all data: 0.147246
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.1774984e-05
Norm of the params: 13.089184
              Random: fixed  54 labels. Loss 0.14725. Accuracy 0.939.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2502447
Train loss (w/o reg) on all data: 0.24407496
Test loss (w/o reg) on all data: 0.16128859
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.0286838e-05
Norm of the params: 11.108343
Flipped loss: 0.16129. Accuracy: 0.947
### Flips: 260, rs: 2, checks: 52
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17450204
Train loss (w/o reg) on all data: 0.16447163
Test loss (w/o reg) on all data: 0.13543056
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 0.00011011098
Norm of the params: 14.163629
     Influence (LOO): fixed  41 labels. Loss 0.13543. Accuracy 0.943.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12075083
Train loss (w/o reg) on all data: 0.10531621
Test loss (w/o reg) on all data: 0.15378506
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 9.9733925e-06
Norm of the params: 17.569643
                Loss: fixed  52 labels. Loss 0.15379. Accuracy 0.927.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24343336
Train loss (w/o reg) on all data: 0.23695979
Test loss (w/o reg) on all data: 0.16031419
Train acc on all data:  0.894937917860554
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.2840557e-05
Norm of the params: 11.378544
              Random: fixed   4 labels. Loss 0.16031. Accuracy 0.950.
### Flips: 260, rs: 2, checks: 104
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10477466
Train loss (w/o reg) on all data: 0.09406689
Test loss (w/o reg) on all data: 0.09771213
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4994998e-05
Norm of the params: 14.634051
     Influence (LOO): fixed  77 labels. Loss 0.09771. Accuracy 0.958.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04974088
Train loss (w/o reg) on all data: 0.03345924
Test loss (w/o reg) on all data: 0.07124037
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.8967406e-06
Norm of the params: 18.045301
                Loss: fixed  94 labels. Loss 0.07124. Accuracy 0.969.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22899462
Train loss (w/o reg) on all data: 0.2218678
Test loss (w/o reg) on all data: 0.15186323
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.3859784e-05
Norm of the params: 11.938862
              Random: fixed  15 labels. Loss 0.15186. Accuracy 0.950.
### Flips: 260, rs: 2, checks: 156
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053170517
Train loss (w/o reg) on all data: 0.042097844
Test loss (w/o reg) on all data: 0.04902441
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.306444e-06
Norm of the params: 14.881313
     Influence (LOO): fixed  99 labels. Loss 0.04902. Accuracy 0.977.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028705433
Train loss (w/o reg) on all data: 0.016039297
Test loss (w/o reg) on all data: 0.034958873
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.62905e-06
Norm of the params: 15.916115
                Loss: fixed 114 labels. Loss 0.03496. Accuracy 0.989.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21947144
Train loss (w/o reg) on all data: 0.213745
Test loss (w/o reg) on all data: 0.14265536
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 9.01634e-06
Norm of the params: 10.701812
              Random: fixed  22 labels. Loss 0.14266. Accuracy 0.947.
### Flips: 260, rs: 2, checks: 208
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027481303
Train loss (w/o reg) on all data: 0.017615711
Test loss (w/o reg) on all data: 0.029365176
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.736307e-06
Norm of the params: 14.046773
     Influence (LOO): fixed 114 labels. Loss 0.02937. Accuracy 0.989.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023744414
Train loss (w/o reg) on all data: 0.012491204
Test loss (w/o reg) on all data: 0.030158965
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.7767414e-06
Norm of the params: 15.00214
                Loss: fixed 118 labels. Loss 0.03016. Accuracy 0.989.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20390065
Train loss (w/o reg) on all data: 0.19810705
Test loss (w/o reg) on all data: 0.13225903
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.1401664e-05
Norm of the params: 10.764384
              Random: fixed  30 labels. Loss 0.13226. Accuracy 0.943.
### Flips: 260, rs: 2, checks: 260
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018795457
Train loss (w/o reg) on all data: 0.011113833
Test loss (w/o reg) on all data: 0.016825825
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.513996e-07
Norm of the params: 12.394856
     Influence (LOO): fixed 124 labels. Loss 0.01683. Accuracy 0.992.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014304448
Train loss (w/o reg) on all data: 0.0063403994
Test loss (w/o reg) on all data: 0.0182526
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.603613e-07
Norm of the params: 12.620656
                Loss: fixed 124 labels. Loss 0.01825. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20030108
Train loss (w/o reg) on all data: 0.19436508
Test loss (w/o reg) on all data: 0.12574826
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.7879831e-05
Norm of the params: 10.895863
              Random: fixed  35 labels. Loss 0.12575. Accuracy 0.950.
### Flips: 260, rs: 2, checks: 312
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011441346
Train loss (w/o reg) on all data: 0.006089602
Test loss (w/o reg) on all data: 0.015768366
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.8259964e-07
Norm of the params: 10.345767
     Influence (LOO): fixed 129 labels. Loss 0.01577. Accuracy 0.992.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014304446
Train loss (w/o reg) on all data: 0.0063407877
Test loss (w/o reg) on all data: 0.018251363
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2617467e-06
Norm of the params: 12.620348
                Loss: fixed 124 labels. Loss 0.01825. Accuracy 0.992.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18956047
Train loss (w/o reg) on all data: 0.18321796
Test loss (w/o reg) on all data: 0.12866624
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.934253e-05
Norm of the params: 11.262779
              Random: fixed  42 labels. Loss 0.12867. Accuracy 0.962.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25166947
Train loss (w/o reg) on all data: 0.24616061
Test loss (w/o reg) on all data: 0.17016263
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.4218746e-05
Norm of the params: 10.496531
Flipped loss: 0.17016. Accuracy: 0.943
### Flips: 260, rs: 3, checks: 52
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17753886
Train loss (w/o reg) on all data: 0.16706932
Test loss (w/o reg) on all data: 0.1281858
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.0950052e-05
Norm of the params: 14.470342
     Influence (LOO): fixed  38 labels. Loss 0.12819. Accuracy 0.954.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13368027
Train loss (w/o reg) on all data: 0.12044047
Test loss (w/o reg) on all data: 0.14843771
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.9331868e-05
Norm of the params: 16.272552
                Loss: fixed  51 labels. Loss 0.14844. Accuracy 0.931.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24811992
Train loss (w/o reg) on all data: 0.24255213
Test loss (w/o reg) on all data: 0.16440554
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.7207689e-05
Norm of the params: 10.552521
              Random: fixed   4 labels. Loss 0.16441. Accuracy 0.943.
### Flips: 260, rs: 3, checks: 104
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.120986246
Train loss (w/o reg) on all data: 0.10990713
Test loss (w/o reg) on all data: 0.086110674
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.6299474e-05
Norm of the params: 14.885642
     Influence (LOO): fixed  71 labels. Loss 0.08611. Accuracy 0.962.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05550192
Train loss (w/o reg) on all data: 0.039404273
Test loss (w/o reg) on all data: 0.09409076
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.6041134e-06
Norm of the params: 17.943047
                Loss: fixed  94 labels. Loss 0.09409. Accuracy 0.966.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23393689
Train loss (w/o reg) on all data: 0.22827733
Test loss (w/o reg) on all data: 0.15568358
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.4920727e-05
Norm of the params: 10.639143
              Random: fixed  14 labels. Loss 0.15568. Accuracy 0.954.
### Flips: 260, rs: 3, checks: 156
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07010298
Train loss (w/o reg) on all data: 0.059553206
Test loss (w/o reg) on all data: 0.06532838
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1148765e-05
Norm of the params: 14.525684
     Influence (LOO): fixed  98 labels. Loss 0.06533. Accuracy 0.969.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031241052
Train loss (w/o reg) on all data: 0.01883048
Test loss (w/o reg) on all data: 0.041795634
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.1972442e-06
Norm of the params: 15.754728
                Loss: fixed 115 labels. Loss 0.04180. Accuracy 0.989.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22738302
Train loss (w/o reg) on all data: 0.22161521
Test loss (w/o reg) on all data: 0.14885391
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.791866e-05
Norm of the params: 10.740391
              Random: fixed  19 labels. Loss 0.14885. Accuracy 0.958.
### Flips: 260, rs: 3, checks: 208
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035429582
Train loss (w/o reg) on all data: 0.02576615
Test loss (w/o reg) on all data: 0.046444427
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.62457e-06
Norm of the params: 13.90211
     Influence (LOO): fixed 117 labels. Loss 0.04644. Accuracy 0.977.
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013216636
Train loss (w/o reg) on all data: 0.005751308
Test loss (w/o reg) on all data: 0.029753784
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.7228673e-06
Norm of the params: 12.219107
                Loss: fixed 128 labels. Loss 0.02975. Accuracy 0.985.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22400175
Train loss (w/o reg) on all data: 0.21823308
Test loss (w/o reg) on all data: 0.14580205
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.5464076e-05
Norm of the params: 10.7412
              Random: fixed  22 labels. Loss 0.14580. Accuracy 0.958.
### Flips: 260, rs: 3, checks: 260
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022896437
Train loss (w/o reg) on all data: 0.0153556615
Test loss (w/o reg) on all data: 0.028452955
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.3087714e-06
Norm of the params: 12.280696
     Influence (LOO): fixed 125 labels. Loss 0.02845. Accuracy 0.989.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012285056
Train loss (w/o reg) on all data: 0.005268812
Test loss (w/o reg) on all data: 0.027594203
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.846471e-07
Norm of the params: 11.84588
                Loss: fixed 129 labels. Loss 0.02759. Accuracy 0.989.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21563876
Train loss (w/o reg) on all data: 0.20967203
Test loss (w/o reg) on all data: 0.13164428
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.3218784e-05
Norm of the params: 10.924033
              Random: fixed  28 labels. Loss 0.13164. Accuracy 0.954.
### Flips: 260, rs: 3, checks: 312
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008929482
Train loss (w/o reg) on all data: 0.0037918282
Test loss (w/o reg) on all data: 0.014905005
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4678663e-07
Norm of the params: 10.136719
     Influence (LOO): fixed 132 labels. Loss 0.01491. Accuracy 0.992.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010938775
Train loss (w/o reg) on all data: 0.0044235666
Test loss (w/o reg) on all data: 0.022034118
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0951268e-06
Norm of the params: 11.415085
                Loss: fixed 130 labels. Loss 0.02203. Accuracy 0.989.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20703007
Train loss (w/o reg) on all data: 0.20071392
Test loss (w/o reg) on all data: 0.124862
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1295411e-05
Norm of the params: 11.239348
              Random: fixed  35 labels. Loss 0.12486. Accuracy 0.966.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24835175
Train loss (w/o reg) on all data: 0.2434568
Test loss (w/o reg) on all data: 0.17250973
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.3365944e-05
Norm of the params: 9.894398
Flipped loss: 0.17251. Accuracy: 0.954
### Flips: 260, rs: 4, checks: 52
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17854115
Train loss (w/o reg) on all data: 0.1696667
Test loss (w/o reg) on all data: 0.1493015
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.8025343e-05
Norm of the params: 13.322505
     Influence (LOO): fixed  36 labels. Loss 0.14930. Accuracy 0.943.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12945887
Train loss (w/o reg) on all data: 0.1156422
Test loss (w/o reg) on all data: 0.15092288
Train acc on all data:  0.9465138490926457
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.6408153e-06
Norm of the params: 16.62328
                Loss: fixed  49 labels. Loss 0.15092. Accuracy 0.950.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24555457
Train loss (w/o reg) on all data: 0.24059203
Test loss (w/o reg) on all data: 0.16130756
Train acc on all data:  0.89207258834766
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.185665e-05
Norm of the params: 9.962463
              Random: fixed   6 labels. Loss 0.16131. Accuracy 0.950.
### Flips: 260, rs: 4, checks: 104
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11545801
Train loss (w/o reg) on all data: 0.10501585
Test loss (w/o reg) on all data: 0.1244787
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.3160253e-05
Norm of the params: 14.45141
     Influence (LOO): fixed  69 labels. Loss 0.12448. Accuracy 0.943.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053033747
Train loss (w/o reg) on all data: 0.038641497
Test loss (w/o reg) on all data: 0.10270606
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.2198747e-06
Norm of the params: 16.965996
                Loss: fixed  91 labels. Loss 0.10271. Accuracy 0.962.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23797591
Train loss (w/o reg) on all data: 0.23291554
Test loss (w/o reg) on all data: 0.14831896
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.7498976e-05
Norm of the params: 10.060196
              Random: fixed  13 labels. Loss 0.14832. Accuracy 0.962.
### Flips: 260, rs: 4, checks: 156
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05790792
Train loss (w/o reg) on all data: 0.0491228
Test loss (w/o reg) on all data: 0.053507537
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.516736e-06
Norm of the params: 13.255279
     Influence (LOO): fixed 103 labels. Loss 0.05351. Accuracy 0.989.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0239814
Train loss (w/o reg) on all data: 0.014437892
Test loss (w/o reg) on all data: 0.08049083
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7464085e-06
Norm of the params: 13.815577
                Loss: fixed 112 labels. Loss 0.08049. Accuracy 0.977.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22436538
Train loss (w/o reg) on all data: 0.2189167
Test loss (w/o reg) on all data: 0.15064102
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.417831e-05
Norm of the params: 10.439046
              Random: fixed  21 labels. Loss 0.15064. Accuracy 0.954.
### Flips: 260, rs: 4, checks: 208
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02134075
Train loss (w/o reg) on all data: 0.013367269
Test loss (w/o reg) on all data: 0.04019177
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.24657e-06
Norm of the params: 12.628128
     Influence (LOO): fixed 118 labels. Loss 0.04019. Accuracy 0.989.
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012118856
Train loss (w/o reg) on all data: 0.0052222805
Test loss (w/o reg) on all data: 0.02862008
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3329528e-06
Norm of the params: 11.744425
                Loss: fixed 121 labels. Loss 0.02862. Accuracy 0.989.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21461579
Train loss (w/o reg) on all data: 0.20822394
Test loss (w/o reg) on all data: 0.1482933
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.7907856e-05
Norm of the params: 11.306505
              Random: fixed  27 labels. Loss 0.14829. Accuracy 0.962.
### Flips: 260, rs: 4, checks: 260
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009203981
Train loss (w/o reg) on all data: 0.0038243022
Test loss (w/o reg) on all data: 0.016626472
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.38076e-07
Norm of the params: 10.372732
     Influence (LOO): fixed 124 labels. Loss 0.01663. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0095423
Train loss (w/o reg) on all data: 0.004058176
Test loss (w/o reg) on all data: 0.018136233
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.063537e-07
Norm of the params: 10.47294
                Loss: fixed 124 labels. Loss 0.01814. Accuracy 0.989.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20334458
Train loss (w/o reg) on all data: 0.19594629
Test loss (w/o reg) on all data: 0.12865584
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2555309e-05
Norm of the params: 12.164125
              Random: fixed  34 labels. Loss 0.12866. Accuracy 0.966.
### Flips: 260, rs: 4, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0068053915
Train loss (w/o reg) on all data: 0.0024352456
Test loss (w/o reg) on all data: 0.010979281
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.7688105e-07
Norm of the params: 9.348953
     Influence (LOO): fixed 127 labels. Loss 0.01098. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008168558
Train loss (w/o reg) on all data: 0.0032151937
Test loss (w/o reg) on all data: 0.014969451
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2374304e-07
Norm of the params: 9.953255
                Loss: fixed 126 labels. Loss 0.01497. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19532819
Train loss (w/o reg) on all data: 0.18771183
Test loss (w/o reg) on all data: 0.12020513
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.6329455e-05
Norm of the params: 12.342083
              Random: fixed  39 labels. Loss 0.12021. Accuracy 0.966.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23854361
Train loss (w/o reg) on all data: 0.23147935
Test loss (w/o reg) on all data: 0.16899651
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.9647317e-05
Norm of the params: 11.886353
Flipped loss: 0.16900. Accuracy: 0.935
### Flips: 260, rs: 5, checks: 52
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15879406
Train loss (w/o reg) on all data: 0.14784911
Test loss (w/o reg) on all data: 0.1407345
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.106148e-05
Norm of the params: 14.795229
     Influence (LOO): fixed  43 labels. Loss 0.14073. Accuracy 0.947.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12180472
Train loss (w/o reg) on all data: 0.1051123
Test loss (w/o reg) on all data: 0.15627727
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 7.803985e-06
Norm of the params: 18.271519
                Loss: fixed  51 labels. Loss 0.15628. Accuracy 0.927.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23419455
Train loss (w/o reg) on all data: 0.22722451
Test loss (w/o reg) on all data: 0.16364346
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.8747534e-05
Norm of the params: 11.806808
              Random: fixed   5 labels. Loss 0.16364. Accuracy 0.935.
### Flips: 260, rs: 5, checks: 104
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09838438
Train loss (w/o reg) on all data: 0.086482055
Test loss (w/o reg) on all data: 0.116124585
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.0572597e-05
Norm of the params: 15.428756
     Influence (LOO): fixed  73 labels. Loss 0.11612. Accuracy 0.950.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063070655
Train loss (w/o reg) on all data: 0.046007853
Test loss (w/o reg) on all data: 0.09260811
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.8552525e-06
Norm of the params: 18.473118
                Loss: fixed  89 labels. Loss 0.09261. Accuracy 0.950.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22487642
Train loss (w/o reg) on all data: 0.21775
Test loss (w/o reg) on all data: 0.16986115
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 4.5183835e-05
Norm of the params: 11.938528
              Random: fixed  12 labels. Loss 0.16986. Accuracy 0.927.
### Flips: 260, rs: 5, checks: 156
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06846884
Train loss (w/o reg) on all data: 0.055208925
Test loss (w/o reg) on all data: 0.092522345
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.1845498e-06
Norm of the params: 16.28491
     Influence (LOO): fixed  89 labels. Loss 0.09252. Accuracy 0.947.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023192216
Train loss (w/o reg) on all data: 0.012383865
Test loss (w/o reg) on all data: 0.05001567
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.6257965e-06
Norm of the params: 14.702619
                Loss: fixed 112 labels. Loss 0.05002. Accuracy 0.985.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.219975
Train loss (w/o reg) on all data: 0.21288161
Test loss (w/o reg) on all data: 0.16216587
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 3.5478057e-05
Norm of the params: 11.910827
              Random: fixed  17 labels. Loss 0.16217. Accuracy 0.927.
### Flips: 260, rs: 5, checks: 208
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045444753
Train loss (w/o reg) on all data: 0.035361417
Test loss (w/o reg) on all data: 0.04399768
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.423015e-06
Norm of the params: 14.200942
     Influence (LOO): fixed 107 labels. Loss 0.04400. Accuracy 0.985.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014231549
Train loss (w/o reg) on all data: 0.006214205
Test loss (w/o reg) on all data: 0.020521186
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.937602e-07
Norm of the params: 12.662815
                Loss: fixed 119 labels. Loss 0.02052. Accuracy 0.989.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2091193
Train loss (w/o reg) on all data: 0.20146339
Test loss (w/o reg) on all data: 0.16081148
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.7092862e-05
Norm of the params: 12.3741
              Random: fixed  24 labels. Loss 0.16081. Accuracy 0.920.
### Flips: 260, rs: 5, checks: 260
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021887384
Train loss (w/o reg) on all data: 0.0140231205
Test loss (w/o reg) on all data: 0.024207188
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.4475715e-06
Norm of the params: 12.541342
     Influence (LOO): fixed 119 labels. Loss 0.02421. Accuracy 0.989.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011179289
Train loss (w/o reg) on all data: 0.0047398303
Test loss (w/o reg) on all data: 0.016614556
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.3521804e-07
Norm of the params: 11.348533
                Loss: fixed 122 labels. Loss 0.01661. Accuracy 0.992.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20532584
Train loss (w/o reg) on all data: 0.1975388
Test loss (w/o reg) on all data: 0.1569271
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.2387385e-05
Norm of the params: 12.479625
              Random: fixed  26 labels. Loss 0.15693. Accuracy 0.927.
### Flips: 260, rs: 5, checks: 312
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012512456
Train loss (w/o reg) on all data: 0.006133274
Test loss (w/o reg) on all data: 0.013390728
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9384876e-06
Norm of the params: 11.295293
     Influence (LOO): fixed 123 labels. Loss 0.01339. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010568963
Train loss (w/o reg) on all data: 0.004269428
Test loss (w/o reg) on all data: 0.017699799
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2352393e-06
Norm of the params: 11.224559
                Loss: fixed 123 labels. Loss 0.01770. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19635744
Train loss (w/o reg) on all data: 0.18812953
Test loss (w/o reg) on all data: 0.15222456
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.2938267e-05
Norm of the params: 12.828026
              Random: fixed  32 labels. Loss 0.15222. Accuracy 0.947.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25414455
Train loss (w/o reg) on all data: 0.24734735
Test loss (w/o reg) on all data: 0.1787476
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.778448e-05
Norm of the params: 11.659491
Flipped loss: 0.17875. Accuracy: 0.950
### Flips: 260, rs: 6, checks: 52
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18225126
Train loss (w/o reg) on all data: 0.17212151
Test loss (w/o reg) on all data: 0.14619532
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.5169698e-05
Norm of the params: 14.233586
     Influence (LOO): fixed  39 labels. Loss 0.14620. Accuracy 0.950.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13248591
Train loss (w/o reg) on all data: 0.11578854
Test loss (w/o reg) on all data: 0.1568043
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.875803e-06
Norm of the params: 18.274225
                Loss: fixed  52 labels. Loss 0.15680. Accuracy 0.950.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24632537
Train loss (w/o reg) on all data: 0.2390594
Test loss (w/o reg) on all data: 0.17778707
Train acc on all data:  0.89207258834766
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.9495793e-05
Norm of the params: 12.054854
              Random: fixed   4 labels. Loss 0.17779. Accuracy 0.950.
### Flips: 260, rs: 6, checks: 104
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12474991
Train loss (w/o reg) on all data: 0.11244679
Test loss (w/o reg) on all data: 0.08962695
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.451276e-05
Norm of the params: 15.686372
     Influence (LOO): fixed  70 labels. Loss 0.08963. Accuracy 0.969.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059322685
Train loss (w/o reg) on all data: 0.041083775
Test loss (w/o reg) on all data: 0.09022394
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.7593854e-06
Norm of the params: 19.099167
                Loss: fixed  92 labels. Loss 0.09022. Accuracy 0.966.
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24035
Train loss (w/o reg) on all data: 0.23268948
Test loss (w/o reg) on all data: 0.17917612
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 9.834564e-06
Norm of the params: 12.377808
              Random: fixed   8 labels. Loss 0.17918. Accuracy 0.943.
### Flips: 260, rs: 6, checks: 156
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07749743
Train loss (w/o reg) on all data: 0.06507053
Test loss (w/o reg) on all data: 0.053865317
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.4604608e-06
Norm of the params: 15.765087
     Influence (LOO): fixed  97 labels. Loss 0.05387. Accuracy 0.981.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026092364
Train loss (w/o reg) on all data: 0.01441655
Test loss (w/o reg) on all data: 0.039844736
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8266526e-06
Norm of the params: 15.281239
                Loss: fixed 118 labels. Loss 0.03984. Accuracy 0.989.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23007688
Train loss (w/o reg) on all data: 0.2224138
Test loss (w/o reg) on all data: 0.16981015
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.0453181e-05
Norm of the params: 12.379889
              Random: fixed  18 labels. Loss 0.16981. Accuracy 0.935.
### Flips: 260, rs: 6, checks: 208
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040471777
Train loss (w/o reg) on all data: 0.028837768
Test loss (w/o reg) on all data: 0.035546307
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.914226e-06
Norm of the params: 15.253859
     Influence (LOO): fixed 116 labels. Loss 0.03555. Accuracy 0.989.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017957782
Train loss (w/o reg) on all data: 0.008355415
Test loss (w/o reg) on all data: 0.036996752
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.295335e-07
Norm of the params: 13.858115
                Loss: fixed 124 labels. Loss 0.03700. Accuracy 0.989.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21712576
Train loss (w/o reg) on all data: 0.20897554
Test loss (w/o reg) on all data: 0.16268115
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.005816e-05
Norm of the params: 12.767323
              Random: fixed  28 labels. Loss 0.16268. Accuracy 0.947.
### Flips: 260, rs: 6, checks: 260
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017864585
Train loss (w/o reg) on all data: 0.00984153
Test loss (w/o reg) on all data: 0.027655786
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.979455e-07
Norm of the params: 12.667325
     Influence (LOO): fixed 128 labels. Loss 0.02766. Accuracy 0.989.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015989978
Train loss (w/o reg) on all data: 0.007327256
Test loss (w/o reg) on all data: 0.025425987
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0936088e-06
Norm of the params: 13.162616
                Loss: fixed 126 labels. Loss 0.02543. Accuracy 0.992.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20191972
Train loss (w/o reg) on all data: 0.19296387
Test loss (w/o reg) on all data: 0.1555225
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.9339239e-05
Norm of the params: 13.383461
              Random: fixed  37 labels. Loss 0.15552. Accuracy 0.943.
### Flips: 260, rs: 6, checks: 312
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014146418
Train loss (w/o reg) on all data: 0.0072318944
Test loss (w/o reg) on all data: 0.018635975
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.276055e-07
Norm of the params: 11.759697
     Influence (LOO): fixed 132 labels. Loss 0.01864. Accuracy 0.992.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016008213
Train loss (w/o reg) on all data: 0.0073773586
Test loss (w/o reg) on all data: 0.022861518
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.1197334e-06
Norm of the params: 13.138381
                Loss: fixed 127 labels. Loss 0.02286. Accuracy 0.992.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19277012
Train loss (w/o reg) on all data: 0.18378823
Test loss (w/o reg) on all data: 0.13879333
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.811859e-06
Norm of the params: 13.402912
              Random: fixed  44 labels. Loss 0.13879. Accuracy 0.954.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25515276
Train loss (w/o reg) on all data: 0.24755839
Test loss (w/o reg) on all data: 0.18218407
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.2807386e-05
Norm of the params: 12.324262
Flipped loss: 0.18218. Accuracy: 0.947
### Flips: 260, rs: 7, checks: 52
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18656367
Train loss (w/o reg) on all data: 0.17679538
Test loss (w/o reg) on all data: 0.13262223
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.0949766e-05
Norm of the params: 13.97733
     Influence (LOO): fixed  41 labels. Loss 0.13262. Accuracy 0.962.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13804384
Train loss (w/o reg) on all data: 0.12237801
Test loss (w/o reg) on all data: 0.13979198
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 6.7955834e-06
Norm of the params: 17.700754
                Loss: fixed  50 labels. Loss 0.13979. Accuracy 0.935.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25230542
Train loss (w/o reg) on all data: 0.24520014
Test loss (w/o reg) on all data: 0.1738814
Train acc on all data:  0.894937917860554
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.2104785e-05
Norm of the params: 11.9208
              Random: fixed   5 labels. Loss 0.17388. Accuracy 0.950.
### Flips: 260, rs: 7, checks: 104
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10370554
Train loss (w/o reg) on all data: 0.094028495
Test loss (w/o reg) on all data: 0.07610781
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.318591e-06
Norm of the params: 13.9119005
     Influence (LOO): fixed  82 labels. Loss 0.07611. Accuracy 0.977.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061749816
Train loss (w/o reg) on all data: 0.043404933
Test loss (w/o reg) on all data: 0.10223586
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.3307126e-06
Norm of the params: 19.154573
                Loss: fixed  96 labels. Loss 0.10224. Accuracy 0.962.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24421814
Train loss (w/o reg) on all data: 0.23698768
Test loss (w/o reg) on all data: 0.164916
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.1889134e-05
Norm of the params: 12.025354
              Random: fixed  12 labels. Loss 0.16492. Accuracy 0.958.
### Flips: 260, rs: 7, checks: 156
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056157008
Train loss (w/o reg) on all data: 0.046340004
Test loss (w/o reg) on all data: 0.054784972
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.0259076e-06
Norm of the params: 14.012142
     Influence (LOO): fixed 107 labels. Loss 0.05478. Accuracy 0.985.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03542743
Train loss (w/o reg) on all data: 0.021248374
Test loss (w/o reg) on all data: 0.050097
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0933933e-06
Norm of the params: 16.839869
                Loss: fixed 115 labels. Loss 0.05010. Accuracy 0.977.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23302482
Train loss (w/o reg) on all data: 0.2255616
Test loss (w/o reg) on all data: 0.15701151
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.7849266e-05
Norm of the params: 12.21738
              Random: fixed  21 labels. Loss 0.15701. Accuracy 0.954.
### Flips: 260, rs: 7, checks: 208
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033490013
Train loss (w/o reg) on all data: 0.026025524
Test loss (w/o reg) on all data: 0.035804443
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0141379e-06
Norm of the params: 12.218421
     Influence (LOO): fixed 121 labels. Loss 0.03580. Accuracy 0.985.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018525086
Train loss (w/o reg) on all data: 0.008854762
Test loss (w/o reg) on all data: 0.029546868
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.253743e-06
Norm of the params: 13.907067
                Loss: fixed 127 labels. Loss 0.02955. Accuracy 0.989.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22492264
Train loss (w/o reg) on all data: 0.21740498
Test loss (w/o reg) on all data: 0.14494707
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 9.205954e-06
Norm of the params: 12.261867
              Random: fixed  28 labels. Loss 0.14495. Accuracy 0.958.
### Flips: 260, rs: 7, checks: 260
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021476008
Train loss (w/o reg) on all data: 0.014439189
Test loss (w/o reg) on all data: 0.021230403
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.554411e-07
Norm of the params: 11.863236
     Influence (LOO): fixed 128 labels. Loss 0.02123. Accuracy 0.989.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014933371
Train loss (w/o reg) on all data: 0.006672198
Test loss (w/o reg) on all data: 0.030204615
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1526665e-06
Norm of the params: 12.853929
                Loss: fixed 129 labels. Loss 0.03020. Accuracy 0.992.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21100274
Train loss (w/o reg) on all data: 0.20335679
Test loss (w/o reg) on all data: 0.1256828
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.954998e-06
Norm of the params: 12.366048
              Random: fixed  37 labels. Loss 0.12568. Accuracy 0.966.
### Flips: 260, rs: 7, checks: 312
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015600705
Train loss (w/o reg) on all data: 0.009461985
Test loss (w/o reg) on all data: 0.01754425
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.758136e-07
Norm of the params: 11.080361
     Influence (LOO): fixed 132 labels. Loss 0.01754. Accuracy 0.992.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009287424
Train loss (w/o reg) on all data: 0.0035289093
Test loss (w/o reg) on all data: 0.016124995
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1928337e-06
Norm of the params: 10.731743
                Loss: fixed 134 labels. Loss 0.01612. Accuracy 0.989.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19935302
Train loss (w/o reg) on all data: 0.19189148
Test loss (w/o reg) on all data: 0.12297045
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.8691459e-05
Norm of the params: 12.216007
              Random: fixed  44 labels. Loss 0.12297. Accuracy 0.958.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23508877
Train loss (w/o reg) on all data: 0.22764115
Test loss (w/o reg) on all data: 0.17297666
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 9.2868904e-05
Norm of the params: 12.2046
Flipped loss: 0.17298. Accuracy: 0.920
### Flips: 260, rs: 8, checks: 52
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1579923
Train loss (w/o reg) on all data: 0.14678451
Test loss (w/o reg) on all data: 0.12405736
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.3731908e-05
Norm of the params: 14.971829
     Influence (LOO): fixed  43 labels. Loss 0.12406. Accuracy 0.939.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12717542
Train loss (w/o reg) on all data: 0.112791985
Test loss (w/o reg) on all data: 0.12716714
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 9.756201e-06
Norm of the params: 16.9608
                Loss: fixed  48 labels. Loss 0.12717. Accuracy 0.947.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2293146
Train loss (w/o reg) on all data: 0.22230542
Test loss (w/o reg) on all data: 0.16183469
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 4.824426e-05
Norm of the params: 11.839918
              Random: fixed   6 labels. Loss 0.16183. Accuracy 0.931.
### Flips: 260, rs: 8, checks: 104
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11782785
Train loss (w/o reg) on all data: 0.103918225
Test loss (w/o reg) on all data: 0.11022645
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 8.248063e-06
Norm of the params: 16.679104
     Influence (LOO): fixed  65 labels. Loss 0.11023. Accuracy 0.947.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061316
Train loss (w/o reg) on all data: 0.04478405
Test loss (w/o reg) on all data: 0.07337291
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.2720118e-05
Norm of the params: 18.183481
                Loss: fixed  85 labels. Loss 0.07337. Accuracy 0.973.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21288003
Train loss (w/o reg) on all data: 0.20527185
Test loss (w/o reg) on all data: 0.14017963
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.2819087e-05
Norm of the params: 12.33546
              Random: fixed  19 labels. Loss 0.14018. Accuracy 0.950.
### Flips: 260, rs: 8, checks: 156
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07963848
Train loss (w/o reg) on all data: 0.06684264
Test loss (w/o reg) on all data: 0.054506347
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.9522276e-05
Norm of the params: 15.997399
     Influence (LOO): fixed  89 labels. Loss 0.05451. Accuracy 0.977.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041411635
Train loss (w/o reg) on all data: 0.026596038
Test loss (w/o reg) on all data: 0.05448679
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.4292198e-06
Norm of the params: 17.213715
                Loss: fixed 104 labels. Loss 0.05449. Accuracy 0.973.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20937508
Train loss (w/o reg) on all data: 0.20154952
Test loss (w/o reg) on all data: 0.13952532
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.098777e-05
Norm of the params: 12.510453
              Random: fixed  23 labels. Loss 0.13953. Accuracy 0.954.
### Flips: 260, rs: 8, checks: 208
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052531697
Train loss (w/o reg) on all data: 0.040055715
Test loss (w/o reg) on all data: 0.03879679
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6321715e-06
Norm of the params: 15.79619
     Influence (LOO): fixed 105 labels. Loss 0.03880. Accuracy 0.989.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02429444
Train loss (w/o reg) on all data: 0.013072619
Test loss (w/o reg) on all data: 0.029972013
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.761021e-07
Norm of the params: 14.981201
                Loss: fixed 116 labels. Loss 0.02997. Accuracy 0.989.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20413174
Train loss (w/o reg) on all data: 0.19651924
Test loss (w/o reg) on all data: 0.13113022
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.3444423e-05
Norm of the params: 12.338962
              Random: fixed  30 labels. Loss 0.13113. Accuracy 0.954.
### Flips: 260, rs: 8, checks: 260
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028642435
Train loss (w/o reg) on all data: 0.01820827
Test loss (w/o reg) on all data: 0.026649356
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.306712e-07
Norm of the params: 14.445874
     Influence (LOO): fixed 118 labels. Loss 0.02665. Accuracy 0.985.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015919605
Train loss (w/o reg) on all data: 0.0075523607
Test loss (w/o reg) on all data: 0.024393337
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.929387e-06
Norm of the params: 12.936185
                Loss: fixed 124 labels. Loss 0.02439. Accuracy 0.989.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19775704
Train loss (w/o reg) on all data: 0.19008255
Test loss (w/o reg) on all data: 0.12667018
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.2113398e-05
Norm of the params: 12.389098
              Random: fixed  35 labels. Loss 0.12667. Accuracy 0.962.
### Flips: 260, rs: 8, checks: 312
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017228682
Train loss (w/o reg) on all data: 0.010225373
Test loss (w/o reg) on all data: 0.019693704
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0636444e-06
Norm of the params: 11.834955
     Influence (LOO): fixed 127 labels. Loss 0.01969. Accuracy 0.989.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012295846
Train loss (w/o reg) on all data: 0.005302411
Test loss (w/o reg) on all data: 0.021513632
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6300921e-06
Norm of the params: 11.82661
                Loss: fixed 127 labels. Loss 0.02151. Accuracy 0.989.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19193116
Train loss (w/o reg) on all data: 0.18418583
Test loss (w/o reg) on all data: 0.12153418
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.2129518e-05
Norm of the params: 12.446148
              Random: fixed  40 labels. Loss 0.12153. Accuracy 0.966.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23811622
Train loss (w/o reg) on all data: 0.23158507
Test loss (w/o reg) on all data: 0.16483185
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.2742224e-05
Norm of the params: 11.429037
Flipped loss: 0.16483. Accuracy: 0.950
### Flips: 260, rs: 9, checks: 52
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15057917
Train loss (w/o reg) on all data: 0.13898598
Test loss (w/o reg) on all data: 0.13419272
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.1146966e-05
Norm of the params: 15.227081
     Influence (LOO): fixed  40 labels. Loss 0.13419. Accuracy 0.950.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12493305
Train loss (w/o reg) on all data: 0.11129532
Test loss (w/o reg) on all data: 0.14191659
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.8579827e-06
Norm of the params: 16.515285
                Loss: fixed  50 labels. Loss 0.14192. Accuracy 0.950.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22903426
Train loss (w/o reg) on all data: 0.22283317
Test loss (w/o reg) on all data: 0.1477074
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.286658e-05
Norm of the params: 11.136504
              Random: fixed   9 labels. Loss 0.14771. Accuracy 0.950.
### Flips: 260, rs: 9, checks: 104
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09506566
Train loss (w/o reg) on all data: 0.08403088
Test loss (w/o reg) on all data: 0.0673051
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.0901138e-05
Norm of the params: 14.855828
     Influence (LOO): fixed  73 labels. Loss 0.06731. Accuracy 0.977.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045215394
Train loss (w/o reg) on all data: 0.030813212
Test loss (w/o reg) on all data: 0.07199425
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.1218829e-06
Norm of the params: 16.971848
                Loss: fixed  95 labels. Loss 0.07199. Accuracy 0.973.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22330108
Train loss (w/o reg) on all data: 0.21698092
Test loss (w/o reg) on all data: 0.14624983
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.672891e-06
Norm of the params: 11.242921
              Random: fixed  14 labels. Loss 0.14625. Accuracy 0.954.
### Flips: 260, rs: 9, checks: 156
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04200297
Train loss (w/o reg) on all data: 0.030403385
Test loss (w/o reg) on all data: 0.06282644
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.875852e-06
Norm of the params: 15.231272
     Influence (LOO): fixed  97 labels. Loss 0.06283. Accuracy 0.977.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020006066
Train loss (w/o reg) on all data: 0.010417147
Test loss (w/o reg) on all data: 0.019472118
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.015316e-06
Norm of the params: 13.848407
                Loss: fixed 112 labels. Loss 0.01947. Accuracy 0.996.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20901375
Train loss (w/o reg) on all data: 0.20193726
Test loss (w/o reg) on all data: 0.1423719
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.7963135e-05
Norm of the params: 11.896623
              Random: fixed  21 labels. Loss 0.14237. Accuracy 0.958.
### Flips: 260, rs: 9, checks: 208
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029245181
Train loss (w/o reg) on all data: 0.018634958
Test loss (w/o reg) on all data: 0.039855517
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.135695e-05
Norm of the params: 14.567239
     Influence (LOO): fixed 106 labels. Loss 0.03986. Accuracy 0.981.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018178433
Train loss (w/o reg) on all data: 0.008957767
Test loss (w/o reg) on all data: 0.015878012
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.9053517e-06
Norm of the params: 13.579888
                Loss: fixed 114 labels. Loss 0.01588. Accuracy 0.996.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19671455
Train loss (w/o reg) on all data: 0.18933424
Test loss (w/o reg) on all data: 0.13411331
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.6425153e-05
Norm of the params: 12.149325
              Random: fixed  28 labels. Loss 0.13411. Accuracy 0.947.
### Flips: 260, rs: 9, checks: 260
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02304877
Train loss (w/o reg) on all data: 0.014213724
Test loss (w/o reg) on all data: 0.024973443
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0632683e-06
Norm of the params: 13.292889
     Influence (LOO): fixed 112 labels. Loss 0.02497. Accuracy 0.989.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008831767
Train loss (w/o reg) on all data: 0.003354606
Test loss (w/o reg) on all data: 0.00972075
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.8918419e-06
Norm of the params: 10.46629
                Loss: fixed 120 labels. Loss 0.00972. Accuracy 0.996.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18890166
Train loss (w/o reg) on all data: 0.18144982
Test loss (w/o reg) on all data: 0.124458805
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.28520105e-05
Norm of the params: 12.208074
              Random: fixed  34 labels. Loss 0.12446. Accuracy 0.966.
### Flips: 260, rs: 9, checks: 312
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0114766685
Train loss (w/o reg) on all data: 0.0065588313
Test loss (w/o reg) on all data: 0.01476337
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.632475e-07
Norm of the params: 9.917498
     Influence (LOO): fixed 120 labels. Loss 0.01476. Accuracy 0.992.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0074989237
Train loss (w/o reg) on all data: 0.002646164
Test loss (w/o reg) on all data: 0.009038262
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.9004786e-07
Norm of the params: 9.85166
                Loss: fixed 121 labels. Loss 0.00904. Accuracy 0.996.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17680718
Train loss (w/o reg) on all data: 0.16932471
Test loss (w/o reg) on all data: 0.12334458
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.047225e-05
Norm of the params: 12.233131
              Random: fixed  41 labels. Loss 0.12334. Accuracy 0.958.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24804401
Train loss (w/o reg) on all data: 0.24014913
Test loss (w/o reg) on all data: 0.1389013
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.36092485e-05
Norm of the params: 12.565739
Flipped loss: 0.13890. Accuracy: 0.973
### Flips: 260, rs: 10, checks: 52
Using normal model
LBFGS training took [301] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17863996
Train loss (w/o reg) on all data: 0.1653256
Test loss (w/o reg) on all data: 0.11326792
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.8326955e-06
Norm of the params: 16.318314
     Influence (LOO): fixed  38 labels. Loss 0.11327. Accuracy 0.973.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13857158
Train loss (w/o reg) on all data: 0.12188895
Test loss (w/o reg) on all data: 0.09699132
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.5722126e-06
Norm of the params: 18.266161
                Loss: fixed  50 labels. Loss 0.09699. Accuracy 0.962.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23880544
Train loss (w/o reg) on all data: 0.23066407
Test loss (w/o reg) on all data: 0.1307574
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.1819745e-05
Norm of the params: 12.760383
              Random: fixed   7 labels. Loss 0.13076. Accuracy 0.966.
### Flips: 260, rs: 10, checks: 104
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14135993
Train loss (w/o reg) on all data: 0.12852734
Test loss (w/o reg) on all data: 0.076680645
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6206448e-05
Norm of the params: 16.020346
     Influence (LOO): fixed  65 labels. Loss 0.07668. Accuracy 0.977.
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06689152
Train loss (w/o reg) on all data: 0.04876005
Test loss (w/o reg) on all data: 0.059784155
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.3585065e-06
Norm of the params: 19.04283
                Loss: fixed  89 labels. Loss 0.05978. Accuracy 0.985.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23086613
Train loss (w/o reg) on all data: 0.22274925
Test loss (w/o reg) on all data: 0.12587087
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.3719356e-05
Norm of the params: 12.7411785
              Random: fixed  13 labels. Loss 0.12587. Accuracy 0.969.
### Flips: 260, rs: 10, checks: 156
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09846653
Train loss (w/o reg) on all data: 0.08626209
Test loss (w/o reg) on all data: 0.046683047
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.2381275e-06
Norm of the params: 15.62334
     Influence (LOO): fixed  90 labels. Loss 0.04668. Accuracy 0.977.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040017135
Train loss (w/o reg) on all data: 0.023512729
Test loss (w/o reg) on all data: 0.042358056
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6708947e-06
Norm of the params: 18.168327
                Loss: fixed 107 labels. Loss 0.04236. Accuracy 0.977.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22075684
Train loss (w/o reg) on all data: 0.21198858
Test loss (w/o reg) on all data: 0.11284538
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9924593e-05
Norm of the params: 13.242552
              Random: fixed  21 labels. Loss 0.11285. Accuracy 0.973.
### Flips: 260, rs: 10, checks: 208
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067608565
Train loss (w/o reg) on all data: 0.056269497
Test loss (w/o reg) on all data: 0.03785336
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.423264e-06
Norm of the params: 15.059259
     Influence (LOO): fixed 106 labels. Loss 0.03785. Accuracy 0.989.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025084307
Train loss (w/o reg) on all data: 0.014049257
Test loss (w/o reg) on all data: 0.030080503
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5680683e-06
Norm of the params: 14.856009
                Loss: fixed 125 labels. Loss 0.03008. Accuracy 0.989.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20527112
Train loss (w/o reg) on all data: 0.19610421
Test loss (w/o reg) on all data: 0.10501273
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0600061e-05
Norm of the params: 13.54024
              Random: fixed  31 labels. Loss 0.10501. Accuracy 0.966.
### Flips: 260, rs: 10, checks: 260
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040293515
Train loss (w/o reg) on all data: 0.030940007
Test loss (w/o reg) on all data: 0.02367657
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.14113e-06
Norm of the params: 13.67736
     Influence (LOO): fixed 122 labels. Loss 0.02368. Accuracy 0.992.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019045403
Train loss (w/o reg) on all data: 0.009726198
Test loss (w/o reg) on all data: 0.01996771
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.112099e-07
Norm of the params: 13.652255
                Loss: fixed 129 labels. Loss 0.01997. Accuracy 0.992.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20058504
Train loss (w/o reg) on all data: 0.19158655
Test loss (w/o reg) on all data: 0.096154585
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1239324e-05
Norm of the params: 13.415279
              Random: fixed  37 labels. Loss 0.09615. Accuracy 0.973.
### Flips: 260, rs: 10, checks: 312
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019061737
Train loss (w/o reg) on all data: 0.011301575
Test loss (w/o reg) on all data: 0.020938804
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.547592e-06
Norm of the params: 12.458057
     Influence (LOO): fixed 132 labels. Loss 0.02094. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011235336
Train loss (w/o reg) on all data: 0.004897477
Test loss (w/o reg) on all data: 0.022159485
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.756014e-07
Norm of the params: 11.25865
                Loss: fixed 134 labels. Loss 0.02216. Accuracy 0.989.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19500476
Train loss (w/o reg) on all data: 0.18568607
Test loss (w/o reg) on all data: 0.09508265
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.9347273e-05
Norm of the params: 13.651881
              Random: fixed  41 labels. Loss 0.09508. Accuracy 0.973.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22354996
Train loss (w/o reg) on all data: 0.21402502
Test loss (w/o reg) on all data: 0.16882268
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.01939595e-05
Norm of the params: 13.802135
Flipped loss: 0.16882. Accuracy: 0.920
### Flips: 260, rs: 11, checks: 52
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1423502
Train loss (w/o reg) on all data: 0.12886757
Test loss (w/o reg) on all data: 0.12875263
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.7234053e-05
Norm of the params: 16.421099
     Influence (LOO): fixed  42 labels. Loss 0.12875. Accuracy 0.943.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1139359
Train loss (w/o reg) on all data: 0.09471469
Test loss (w/o reg) on all data: 0.1265907
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.924593e-05
Norm of the params: 19.60674
                Loss: fixed  49 labels. Loss 0.12659. Accuracy 0.935.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21792683
Train loss (w/o reg) on all data: 0.20826225
Test loss (w/o reg) on all data: 0.15915526
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 4.927146e-05
Norm of the params: 13.902931
              Random: fixed   7 labels. Loss 0.15916. Accuracy 0.939.
### Flips: 260, rs: 11, checks: 104
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.084626
Train loss (w/o reg) on all data: 0.07044794
Test loss (w/o reg) on all data: 0.086699404
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.9136444e-06
Norm of the params: 16.839275
     Influence (LOO): fixed  69 labels. Loss 0.08670. Accuracy 0.958.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06140349
Train loss (w/o reg) on all data: 0.043173105
Test loss (w/o reg) on all data: 0.095363
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.9495173e-06
Norm of the params: 19.094706
                Loss: fixed  83 labels. Loss 0.09536. Accuracy 0.973.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21227455
Train loss (w/o reg) on all data: 0.20208605
Test loss (w/o reg) on all data: 0.15762405
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.417995e-05
Norm of the params: 14.274809
              Random: fixed  13 labels. Loss 0.15762. Accuracy 0.935.
### Flips: 260, rs: 11, checks: 156
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05968831
Train loss (w/o reg) on all data: 0.046641644
Test loss (w/o reg) on all data: 0.0611778
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5384767e-06
Norm of the params: 16.15343
     Influence (LOO): fixed  90 labels. Loss 0.06118. Accuracy 0.981.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0386343
Train loss (w/o reg) on all data: 0.023691544
Test loss (w/o reg) on all data: 0.066974215
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.0064687e-06
Norm of the params: 17.287426
                Loss: fixed 104 labels. Loss 0.06697. Accuracy 0.977.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20998
Train loss (w/o reg) on all data: 0.19983393
Test loss (w/o reg) on all data: 0.14880519
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.2654767e-05
Norm of the params: 14.245045
              Random: fixed  19 labels. Loss 0.14881. Accuracy 0.950.
### Flips: 260, rs: 11, checks: 208
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042218514
Train loss (w/o reg) on all data: 0.030707369
Test loss (w/o reg) on all data: 0.046099506
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.960841e-06
Norm of the params: 15.173098
     Influence (LOO): fixed 101 labels. Loss 0.04610. Accuracy 0.985.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019913107
Train loss (w/o reg) on all data: 0.010519533
Test loss (w/o reg) on all data: 0.028363897
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2418498e-06
Norm of the params: 13.70662
                Loss: fixed 118 labels. Loss 0.02836. Accuracy 0.989.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20468616
Train loss (w/o reg) on all data: 0.19463149
Test loss (w/o reg) on all data: 0.13738394
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.0970266e-05
Norm of the params: 14.180745
              Random: fixed  24 labels. Loss 0.13738. Accuracy 0.950.
### Flips: 260, rs: 11, checks: 260
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028787386
Train loss (w/o reg) on all data: 0.019070107
Test loss (w/o reg) on all data: 0.031284854
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9972838e-06
Norm of the params: 13.940787
     Influence (LOO): fixed 114 labels. Loss 0.03128. Accuracy 0.989.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012975385
Train loss (w/o reg) on all data: 0.005750037
Test loss (w/o reg) on all data: 0.023521392
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.490117e-07
Norm of the params: 12.021106
                Loss: fixed 122 labels. Loss 0.02352. Accuracy 0.989.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19049044
Train loss (w/o reg) on all data: 0.18117492
Test loss (w/o reg) on all data: 0.12166151
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.844415e-06
Norm of the params: 13.649554
              Random: fixed  36 labels. Loss 0.12166. Accuracy 0.966.
### Flips: 260, rs: 11, checks: 312
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019029958
Train loss (w/o reg) on all data: 0.011378094
Test loss (w/o reg) on all data: 0.024653442
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5200666e-06
Norm of the params: 12.370824
     Influence (LOO): fixed 121 labels. Loss 0.02465. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010776029
Train loss (w/o reg) on all data: 0.0044411756
Test loss (w/o reg) on all data: 0.017972382
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.496922e-06
Norm of the params: 11.25598
                Loss: fixed 123 labels. Loss 0.01797. Accuracy 0.989.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18304726
Train loss (w/o reg) on all data: 0.17384456
Test loss (w/o reg) on all data: 0.114748895
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.3776494e-05
Norm of the params: 13.566653
              Random: fixed  41 labels. Loss 0.11475. Accuracy 0.977.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24264558
Train loss (w/o reg) on all data: 0.23521368
Test loss (w/o reg) on all data: 0.17329419
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.8795628e-05
Norm of the params: 12.191715
Flipped loss: 0.17329. Accuracy: 0.954
### Flips: 260, rs: 12, checks: 52
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16159613
Train loss (w/o reg) on all data: 0.1501706
Test loss (w/o reg) on all data: 0.14553872
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.2867364e-05
Norm of the params: 15.116573
     Influence (LOO): fixed  38 labels. Loss 0.14554. Accuracy 0.924.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12817265
Train loss (w/o reg) on all data: 0.112479255
Test loss (w/o reg) on all data: 0.16554333
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 8.644705e-06
Norm of the params: 17.71632
                Loss: fixed  47 labels. Loss 0.16554. Accuracy 0.931.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23031828
Train loss (w/o reg) on all data: 0.2224046
Test loss (w/o reg) on all data: 0.15935637
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.5941528e-05
Norm of the params: 12.580678
              Random: fixed   8 labels. Loss 0.15936. Accuracy 0.950.
### Flips: 260, rs: 12, checks: 104
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12157259
Train loss (w/o reg) on all data: 0.11005236
Test loss (w/o reg) on all data: 0.12455963
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.3331198e-05
Norm of the params: 15.179084
     Influence (LOO): fixed  65 labels. Loss 0.12456. Accuracy 0.958.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06627099
Train loss (w/o reg) on all data: 0.049338125
Test loss (w/o reg) on all data: 0.11118401
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.7019183e-06
Norm of the params: 18.402643
                Loss: fixed  87 labels. Loss 0.11118. Accuracy 0.973.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22277698
Train loss (w/o reg) on all data: 0.21503448
Test loss (w/o reg) on all data: 0.14662848
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.3461407e-05
Norm of the params: 12.443875
              Random: fixed  15 labels. Loss 0.14663. Accuracy 0.958.
### Flips: 260, rs: 12, checks: 156
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08059479
Train loss (w/o reg) on all data: 0.06972248
Test loss (w/o reg) on all data: 0.09156623
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.9998399e-05
Norm of the params: 14.74606
     Influence (LOO): fixed  92 labels. Loss 0.09157. Accuracy 0.973.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035841756
Train loss (w/o reg) on all data: 0.022783754
Test loss (w/o reg) on all data: 0.050379608
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.4771892e-06
Norm of the params: 16.160446
                Loss: fixed 113 labels. Loss 0.05038. Accuracy 0.985.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2199766
Train loss (w/o reg) on all data: 0.21253061
Test loss (w/o reg) on all data: 0.13087131
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.265096e-05
Norm of the params: 12.203274
              Random: fixed  20 labels. Loss 0.13087. Accuracy 0.969.
### Flips: 260, rs: 12, checks: 208
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050391402
Train loss (w/o reg) on all data: 0.03960837
Test loss (w/o reg) on all data: 0.05880169
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.6896373e-06
Norm of the params: 14.685389
     Influence (LOO): fixed 110 labels. Loss 0.05880. Accuracy 0.981.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0152063025
Train loss (w/o reg) on all data: 0.0069498755
Test loss (w/o reg) on all data: 0.01997056
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.746348e-07
Norm of the params: 12.850235
                Loss: fixed 127 labels. Loss 0.01997. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21247807
Train loss (w/o reg) on all data: 0.20531175
Test loss (w/o reg) on all data: 0.1264073
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.2441614e-05
Norm of the params: 11.971906
              Random: fixed  28 labels. Loss 0.12641. Accuracy 0.977.
### Flips: 260, rs: 12, checks: 260
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027885303
Train loss (w/o reg) on all data: 0.018874465
Test loss (w/o reg) on all data: 0.045297608
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.6419385e-07
Norm of the params: 13.424483
     Influence (LOO): fixed 123 labels. Loss 0.04530. Accuracy 0.981.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009850761
Train loss (w/o reg) on all data: 0.0038871714
Test loss (w/o reg) on all data: 0.023660498
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.186716e-07
Norm of the params: 10.921163
                Loss: fixed 130 labels. Loss 0.02366. Accuracy 0.989.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20488906
Train loss (w/o reg) on all data: 0.19783662
Test loss (w/o reg) on all data: 0.12145332
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.4278553e-05
Norm of the params: 11.876399
              Random: fixed  34 labels. Loss 0.12145. Accuracy 0.981.
### Flips: 260, rs: 12, checks: 312
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013549851
Train loss (w/o reg) on all data: 0.00694111
Test loss (w/o reg) on all data: 0.029933373
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.2244536e-07
Norm of the params: 11.496731
     Influence (LOO): fixed 131 labels. Loss 0.02993. Accuracy 0.989.
Using normal model
LBFGS training took [56] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007934282
Train loss (w/o reg) on all data: 0.0028760363
Test loss (w/o reg) on all data: 0.021921106
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.4582513e-07
Norm of the params: 10.058078
                Loss: fixed 132 labels. Loss 0.02192. Accuracy 0.989.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19400838
Train loss (w/o reg) on all data: 0.18652947
Test loss (w/o reg) on all data: 0.10721773
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.3405237e-05
Norm of the params: 12.230216
              Random: fixed  41 labels. Loss 0.10722. Accuracy 0.981.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25601393
Train loss (w/o reg) on all data: 0.24835773
Test loss (w/o reg) on all data: 0.18634425
Train acc on all data:  0.874880611270296
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.7683435e-05
Norm of the params: 12.374321
Flipped loss: 0.18634. Accuracy: 0.935
### Flips: 260, rs: 13, checks: 52
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19023024
Train loss (w/o reg) on all data: 0.1798972
Test loss (w/o reg) on all data: 0.15031567
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.631564e-05
Norm of the params: 14.375697
     Influence (LOO): fixed  41 labels. Loss 0.15032. Accuracy 0.947.
Using normal model
LBFGS training took [148] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14902887
Train loss (w/o reg) on all data: 0.13193497
Test loss (w/o reg) on all data: 0.16691495
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.0846159e-05
Norm of the params: 18.489946
                Loss: fixed  51 labels. Loss 0.16691. Accuracy 0.935.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25356662
Train loss (w/o reg) on all data: 0.24636771
Test loss (w/o reg) on all data: 0.18112576
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 5.505197e-05
Norm of the params: 11.999091
              Random: fixed   4 labels. Loss 0.18113. Accuracy 0.924.
### Flips: 260, rs: 13, checks: 104
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1343884
Train loss (w/o reg) on all data: 0.122086436
Test loss (w/o reg) on all data: 0.11674564
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.4701899e-05
Norm of the params: 15.6856365
     Influence (LOO): fixed  72 labels. Loss 0.11675. Accuracy 0.954.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07578961
Train loss (w/o reg) on all data: 0.05722187
Test loss (w/o reg) on all data: 0.09193476
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.6181416e-06
Norm of the params: 19.270569
                Loss: fixed  94 labels. Loss 0.09193. Accuracy 0.969.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24724627
Train loss (w/o reg) on all data: 0.23990816
Test loss (w/o reg) on all data: 0.17077208
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.2358552e-05
Norm of the params: 12.114547
              Random: fixed   9 labels. Loss 0.17077. Accuracy 0.943.
### Flips: 260, rs: 13, checks: 156
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09224677
Train loss (w/o reg) on all data: 0.08030455
Test loss (w/o reg) on all data: 0.06640828
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.517048e-06
Norm of the params: 15.454592
     Influence (LOO): fixed  96 labels. Loss 0.06641. Accuracy 0.985.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035843264
Train loss (w/o reg) on all data: 0.022018047
Test loss (w/o reg) on all data: 0.031896576
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.3591876e-06
Norm of the params: 16.62842
                Loss: fixed 119 labels. Loss 0.03190. Accuracy 0.992.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2328379
Train loss (w/o reg) on all data: 0.22534701
Test loss (w/o reg) on all data: 0.16391899
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.4330266e-05
Norm of the params: 12.240012
              Random: fixed  20 labels. Loss 0.16392. Accuracy 0.950.
### Flips: 260, rs: 13, checks: 208
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.050751
Train loss (w/o reg) on all data: 0.041763525
Test loss (w/o reg) on all data: 0.03484569
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.4197704e-06
Norm of the params: 13.407069
     Influence (LOO): fixed 119 labels. Loss 0.03485. Accuracy 0.985.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02138403
Train loss (w/o reg) on all data: 0.011144683
Test loss (w/o reg) on all data: 0.017461516
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.412996e-06
Norm of the params: 14.310379
                Loss: fixed 130 labels. Loss 0.01746. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22757652
Train loss (w/o reg) on all data: 0.21991697
Test loss (w/o reg) on all data: 0.15050095
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.812853e-06
Norm of the params: 12.377039
              Random: fixed  26 labels. Loss 0.15050. Accuracy 0.962.
### Flips: 260, rs: 13, checks: 260
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024955453
Train loss (w/o reg) on all data: 0.017320512
Test loss (w/o reg) on all data: 0.023788467
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1652633e-06
Norm of the params: 12.357137
     Influence (LOO): fixed 131 labels. Loss 0.02379. Accuracy 0.992.
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01604292
Train loss (w/o reg) on all data: 0.008265492
Test loss (w/o reg) on all data: 0.017157761
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.665863e-07
Norm of the params: 12.471911
                Loss: fixed 134 labels. Loss 0.01716. Accuracy 0.992.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21899937
Train loss (w/o reg) on all data: 0.21129169
Test loss (w/o reg) on all data: 0.13736184
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.1924052e-05
Norm of the params: 12.415868
              Random: fixed  34 labels. Loss 0.13736. Accuracy 0.962.
### Flips: 260, rs: 13, checks: 312
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015853152
Train loss (w/o reg) on all data: 0.009097597
Test loss (w/o reg) on all data: 0.023851408
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.622164e-07
Norm of the params: 11.62373
     Influence (LOO): fixed 135 labels. Loss 0.02385. Accuracy 0.992.
Using normal model
LBFGS training took [62] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007737946
Train loss (w/o reg) on all data: 0.0028791223
Test loss (w/o reg) on all data: 0.010790906
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.9349455e-07
Norm of the params: 9.857813
                Loss: fixed 139 labels. Loss 0.01079. Accuracy 0.996.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19798216
Train loss (w/o reg) on all data: 0.18914036
Test loss (w/o reg) on all data: 0.1232309
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.758555e-05
Norm of the params: 13.29797
              Random: fixed  45 labels. Loss 0.12323. Accuracy 0.958.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23633088
Train loss (w/o reg) on all data: 0.22781034
Test loss (w/o reg) on all data: 0.17847943
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.3812723e-05
Norm of the params: 13.054155
Flipped loss: 0.17848. Accuracy: 0.947
### Flips: 260, rs: 14, checks: 52
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16285877
Train loss (w/o reg) on all data: 0.14993683
Test loss (w/o reg) on all data: 0.1744723
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.0416589e-05
Norm of the params: 16.076038
     Influence (LOO): fixed  36 labels. Loss 0.17447. Accuracy 0.927.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1146355
Train loss (w/o reg) on all data: 0.09540395
Test loss (w/o reg) on all data: 0.14821677
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.2369051e-05
Norm of the params: 19.612015
                Loss: fixed  50 labels. Loss 0.14822. Accuracy 0.947.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2297359
Train loss (w/o reg) on all data: 0.22221918
Test loss (w/o reg) on all data: 0.17979762
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.1265363e-05
Norm of the params: 12.261083
              Random: fixed  10 labels. Loss 0.17980. Accuracy 0.939.
### Flips: 260, rs: 14, checks: 104
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.122290626
Train loss (w/o reg) on all data: 0.10776383
Test loss (w/o reg) on all data: 0.1045753
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.2285132e-05
Norm of the params: 17.045116
     Influence (LOO): fixed  68 labels. Loss 0.10458. Accuracy 0.962.
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06904478
Train loss (w/o reg) on all data: 0.048671205
Test loss (w/o reg) on all data: 0.12083147
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.22824e-06
Norm of the params: 20.185926
                Loss: fixed  81 labels. Loss 0.12083. Accuracy 0.958.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22867785
Train loss (w/o reg) on all data: 0.2216135
Test loss (w/o reg) on all data: 0.16571255
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.33621215e-05
Norm of the params: 11.886433
              Random: fixed  16 labels. Loss 0.16571. Accuracy 0.954.
### Flips: 260, rs: 14, checks: 156
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0691708
Train loss (w/o reg) on all data: 0.055057574
Test loss (w/o reg) on all data: 0.06825648
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2357627e-05
Norm of the params: 16.800734
     Influence (LOO): fixed  96 labels. Loss 0.06826. Accuracy 0.985.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044582386
Train loss (w/o reg) on all data: 0.028417623
Test loss (w/o reg) on all data: 0.066846654
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7654725e-06
Norm of the params: 17.980412
                Loss: fixed 104 labels. Loss 0.06685. Accuracy 0.977.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22624053
Train loss (w/o reg) on all data: 0.2192614
Test loss (w/o reg) on all data: 0.15250552
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.8461683e-05
Norm of the params: 11.814513
              Random: fixed  21 labels. Loss 0.15251. Accuracy 0.958.
### Flips: 260, rs: 14, checks: 208
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03973597
Train loss (w/o reg) on all data: 0.030638784
Test loss (w/o reg) on all data: 0.03758565
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.2777792e-06
Norm of the params: 13.488651
     Influence (LOO): fixed 116 labels. Loss 0.03759. Accuracy 0.985.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019909382
Train loss (w/o reg) on all data: 0.009825782
Test loss (w/o reg) on all data: 0.019680101
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3038439e-06
Norm of the params: 14.201127
                Loss: fixed 119 labels. Loss 0.01968. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22335309
Train loss (w/o reg) on all data: 0.21680896
Test loss (w/o reg) on all data: 0.1477285
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.8565005e-05
Norm of the params: 11.440394
              Random: fixed  26 labels. Loss 0.14773. Accuracy 0.958.
### Flips: 260, rs: 14, checks: 260
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014239006
Train loss (w/o reg) on all data: 0.007363967
Test loss (w/o reg) on all data: 0.020760149
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.875611e-07
Norm of the params: 11.726073
     Influence (LOO): fixed 128 labels. Loss 0.02076. Accuracy 0.989.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014596878
Train loss (w/o reg) on all data: 0.006634101
Test loss (w/o reg) on all data: 0.01828166
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.612843e-07
Norm of the params: 12.619648
                Loss: fixed 126 labels. Loss 0.01828. Accuracy 0.992.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22186913
Train loss (w/o reg) on all data: 0.21562153
Test loss (w/o reg) on all data: 0.13200115
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.1024184e-05
Norm of the params: 11.17819
              Random: fixed  33 labels. Loss 0.13200. Accuracy 0.973.
### Flips: 260, rs: 14, checks: 312
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012078272
Train loss (w/o reg) on all data: 0.0059431144
Test loss (w/o reg) on all data: 0.020683663
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1263838e-07
Norm of the params: 11.077145
     Influence (LOO): fixed 129 labels. Loss 0.02068. Accuracy 0.989.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012899496
Train loss (w/o reg) on all data: 0.0058167316
Test loss (w/o reg) on all data: 0.014895842
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8004492e-06
Norm of the params: 11.901903
                Loss: fixed 128 labels. Loss 0.01490. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20889716
Train loss (w/o reg) on all data: 0.20251353
Test loss (w/o reg) on all data: 0.117747985
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7605844e-05
Norm of the params: 11.299228
              Random: fixed  41 labels. Loss 0.11775. Accuracy 0.977.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24607828
Train loss (w/o reg) on all data: 0.2372337
Test loss (w/o reg) on all data: 0.22331952
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.8931297709923665
Norm of the mean of gradients: 2.3121638e-05
Norm of the params: 13.300069
Flipped loss: 0.22332. Accuracy: 0.893
### Flips: 260, rs: 15, checks: 52
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1730368
Train loss (w/o reg) on all data: 0.16245566
Test loss (w/o reg) on all data: 0.14226799
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.1807286e-05
Norm of the params: 14.547256
     Influence (LOO): fixed  41 labels. Loss 0.14227. Accuracy 0.943.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12897936
Train loss (w/o reg) on all data: 0.11299661
Test loss (w/o reg) on all data: 0.21001866
Train acc on all data:  0.956064947468959
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 7.0186643e-06
Norm of the params: 17.878893
                Loss: fixed  51 labels. Loss 0.21002. Accuracy 0.916.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23743857
Train loss (w/o reg) on all data: 0.22896276
Test loss (w/o reg) on all data: 0.19752344
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 8.8535235e-05
Norm of the params: 13.01984
              Random: fixed   9 labels. Loss 0.19752. Accuracy 0.924.
### Flips: 260, rs: 15, checks: 104
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12310956
Train loss (w/o reg) on all data: 0.1098428
Test loss (w/o reg) on all data: 0.10762305
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.8085733e-05
Norm of the params: 16.28911
     Influence (LOO): fixed  72 labels. Loss 0.10762. Accuracy 0.954.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06561489
Train loss (w/o reg) on all data: 0.04781067
Test loss (w/o reg) on all data: 0.104031466
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.939128e-06
Norm of the params: 18.870197
                Loss: fixed  90 labels. Loss 0.10403. Accuracy 0.962.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23207986
Train loss (w/o reg) on all data: 0.22387345
Test loss (w/o reg) on all data: 0.19036293
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.5911852e-05
Norm of the params: 12.811249
              Random: fixed  14 labels. Loss 0.19036. Accuracy 0.920.
### Flips: 260, rs: 15, checks: 156
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.095229305
Train loss (w/o reg) on all data: 0.08216673
Test loss (w/o reg) on all data: 0.06747898
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.3207388e-06
Norm of the params: 16.163277
     Influence (LOO): fixed  90 labels. Loss 0.06748. Accuracy 0.981.
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043723904
Train loss (w/o reg) on all data: 0.027628198
Test loss (w/o reg) on all data: 0.054861736
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5508455e-06
Norm of the params: 17.941965
                Loss: fixed 110 labels. Loss 0.05486. Accuracy 0.985.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22600907
Train loss (w/o reg) on all data: 0.21797413
Test loss (w/o reg) on all data: 0.18715067
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 3.855518e-05
Norm of the params: 12.676713
              Random: fixed  19 labels. Loss 0.18715. Accuracy 0.927.
### Flips: 260, rs: 15, checks: 208
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054796085
Train loss (w/o reg) on all data: 0.042997584
Test loss (w/o reg) on all data: 0.06589286
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.3839093e-06
Norm of the params: 15.361316
     Influence (LOO): fixed 110 labels. Loss 0.06589. Accuracy 0.977.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034373324
Train loss (w/o reg) on all data: 0.0205358
Test loss (w/o reg) on all data: 0.04200666
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.4053274e-07
Norm of the params: 16.63582
                Loss: fixed 119 labels. Loss 0.04201. Accuracy 0.989.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22471973
Train loss (w/o reg) on all data: 0.21700405
Test loss (w/o reg) on all data: 0.16920568
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.3703439e-05
Norm of the params: 12.422314
              Random: fixed  24 labels. Loss 0.16921. Accuracy 0.947.
### Flips: 260, rs: 15, checks: 260
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040161066
Train loss (w/o reg) on all data: 0.029434225
Test loss (w/o reg) on all data: 0.054688897
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.1438537e-06
Norm of the params: 14.647078
     Influence (LOO): fixed 119 labels. Loss 0.05469. Accuracy 0.977.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026780147
Train loss (w/o reg) on all data: 0.015251023
Test loss (w/o reg) on all data: 0.038170617
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.478036e-06
Norm of the params: 15.184943
                Loss: fixed 125 labels. Loss 0.03817. Accuracy 0.989.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21277498
Train loss (w/o reg) on all data: 0.20462087
Test loss (w/o reg) on all data: 0.16940701
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.3969565e-05
Norm of the params: 12.770364
              Random: fixed  30 labels. Loss 0.16941. Accuracy 0.943.
### Flips: 260, rs: 15, checks: 312
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017796325
Train loss (w/o reg) on all data: 0.009816525
Test loss (w/o reg) on all data: 0.029432159
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0894842e-06
Norm of the params: 12.633131
     Influence (LOO): fixed 131 labels. Loss 0.02943. Accuracy 0.989.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015542149
Train loss (w/o reg) on all data: 0.0070560705
Test loss (w/o reg) on all data: 0.018069752
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.5224325e-07
Norm of the params: 13.027722
                Loss: fixed 131 labels. Loss 0.01807. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19895996
Train loss (w/o reg) on all data: 0.19047247
Test loss (w/o reg) on all data: 0.15869239
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.1169938e-05
Norm of the params: 13.028808
              Random: fixed  38 labels. Loss 0.15869. Accuracy 0.935.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24151139
Train loss (w/o reg) on all data: 0.2351407
Test loss (w/o reg) on all data: 0.18310918
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.717316e-05
Norm of the params: 11.287774
Flipped loss: 0.18311. Accuracy: 0.939
### Flips: 260, rs: 16, checks: 52
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15642488
Train loss (w/o reg) on all data: 0.14665963
Test loss (w/o reg) on all data: 0.13609023
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.5907515e-05
Norm of the params: 13.9751625
     Influence (LOO): fixed  41 labels. Loss 0.13609. Accuracy 0.950.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12562728
Train loss (w/o reg) on all data: 0.11126002
Test loss (w/o reg) on all data: 0.1506189
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.0104652e-05
Norm of the params: 16.951256
                Loss: fixed  49 labels. Loss 0.15062. Accuracy 0.954.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23514068
Train loss (w/o reg) on all data: 0.22830983
Test loss (w/o reg) on all data: 0.17003812
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.296303e-05
Norm of the params: 11.688337
              Random: fixed   6 labels. Loss 0.17004. Accuracy 0.943.
### Flips: 260, rs: 16, checks: 104
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10172437
Train loss (w/o reg) on all data: 0.08970644
Test loss (w/o reg) on all data: 0.08573672
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0967395e-05
Norm of the params: 15.503502
     Influence (LOO): fixed  72 labels. Loss 0.08574. Accuracy 0.966.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05438097
Train loss (w/o reg) on all data: 0.037194215
Test loss (w/o reg) on all data: 0.09679972
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.580762e-06
Norm of the params: 18.540094
                Loss: fixed  90 labels. Loss 0.09680. Accuracy 0.958.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23148596
Train loss (w/o reg) on all data: 0.2249448
Test loss (w/o reg) on all data: 0.16899365
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.2885632e-05
Norm of the params: 11.4378
              Random: fixed  11 labels. Loss 0.16899. Accuracy 0.950.
### Flips: 260, rs: 16, checks: 156
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060448267
Train loss (w/o reg) on all data: 0.047213487
Test loss (w/o reg) on all data: 0.07347884
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.1590576e-05
Norm of the params: 16.269468
     Influence (LOO): fixed  92 labels. Loss 0.07348. Accuracy 0.969.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027407859
Train loss (w/o reg) on all data: 0.015372075
Test loss (w/o reg) on all data: 0.04195269
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.188668e-06
Norm of the params: 15.515015
                Loss: fixed 108 labels. Loss 0.04195. Accuracy 0.989.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2236753
Train loss (w/o reg) on all data: 0.21712437
Test loss (w/o reg) on all data: 0.16227412
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.337054e-05
Norm of the params: 11.446333
              Random: fixed  19 labels. Loss 0.16227. Accuracy 0.950.
### Flips: 260, rs: 16, checks: 208
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030480469
Train loss (w/o reg) on all data: 0.021010436
Test loss (w/o reg) on all data: 0.04896887
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4234532e-06
Norm of the params: 13.762291
     Influence (LOO): fixed 112 labels. Loss 0.04897. Accuracy 0.973.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018577747
Train loss (w/o reg) on all data: 0.009778467
Test loss (w/o reg) on all data: 0.040249318
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.328866e-07
Norm of the params: 13.265958
                Loss: fixed 117 labels. Loss 0.04025. Accuracy 0.989.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21830063
Train loss (w/o reg) on all data: 0.21172163
Test loss (w/o reg) on all data: 0.15573047
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.5664932e-05
Norm of the params: 11.470835
              Random: fixed  25 labels. Loss 0.15573. Accuracy 0.958.
### Flips: 260, rs: 16, checks: 260
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01031423
Train loss (w/o reg) on all data: 0.0043729274
Test loss (w/o reg) on all data: 0.0144611
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7418816e-07
Norm of the params: 10.900737
     Influence (LOO): fixed 124 labels. Loss 0.01446. Accuracy 0.992.
Using normal model
LBFGS training took [34] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00920343
Train loss (w/o reg) on all data: 0.003768138
Test loss (w/o reg) on all data: 0.0137116695
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.093047e-07
Norm of the params: 10.42621
                Loss: fixed 124 labels. Loss 0.01371. Accuracy 0.992.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20739831
Train loss (w/o reg) on all data: 0.20125358
Test loss (w/o reg) on all data: 0.14698793
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.8203518e-05
Norm of the params: 11.085792
              Random: fixed  33 labels. Loss 0.14699. Accuracy 0.958.
### Flips: 260, rs: 16, checks: 312
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.00734019
Train loss (w/o reg) on all data: 0.0027836354
Test loss (w/o reg) on all data: 0.011959967
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6593025e-07
Norm of the params: 9.546261
     Influence (LOO): fixed 126 labels. Loss 0.01196. Accuracy 0.992.
Using normal model
LBFGS training took [46] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021729998
Test loss (w/o reg) on all data: 0.012054823
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8623976e-07
Norm of the params: 9.153163
                Loss: fixed 127 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19592687
Train loss (w/o reg) on all data: 0.189458
Test loss (w/o reg) on all data: 0.14123128
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.1608272e-05
Norm of the params: 11.374429
              Random: fixed  39 labels. Loss 0.14123. Accuracy 0.958.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24140303
Train loss (w/o reg) on all data: 0.2348803
Test loss (w/o reg) on all data: 0.15679307
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 0.000114920294
Norm of the params: 11.421676
Flipped loss: 0.15679. Accuracy: 0.969
### Flips: 260, rs: 17, checks: 52
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16137674
Train loss (w/o reg) on all data: 0.14991345
Test loss (w/o reg) on all data: 0.13633476
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.120561e-06
Norm of the params: 15.141528
     Influence (LOO): fixed  44 labels. Loss 0.13633. Accuracy 0.954.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12045402
Train loss (w/o reg) on all data: 0.10120499
Test loss (w/o reg) on all data: 0.13277976
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.6244683e-05
Norm of the params: 19.620924
                Loss: fixed  49 labels. Loss 0.13278. Accuracy 0.943.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23773861
Train loss (w/o reg) on all data: 0.23137952
Test loss (w/o reg) on all data: 0.15182191
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.3659224e-05
Norm of the params: 11.277493
              Random: fixed   4 labels. Loss 0.15182. Accuracy 0.973.
### Flips: 260, rs: 17, checks: 104
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.105700426
Train loss (w/o reg) on all data: 0.093233414
Test loss (w/o reg) on all data: 0.0979093
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.834923e-05
Norm of the params: 15.790513
     Influence (LOO): fixed  73 labels. Loss 0.09791. Accuracy 0.969.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051048346
Train loss (w/o reg) on all data: 0.033205748
Test loss (w/o reg) on all data: 0.06993865
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.9890802e-06
Norm of the params: 18.890526
                Loss: fixed  94 labels. Loss 0.06994. Accuracy 0.981.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22756852
Train loss (w/o reg) on all data: 0.22096805
Test loss (w/o reg) on all data: 0.1467131
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.976501e-05
Norm of the params: 11.489534
              Random: fixed  11 labels. Loss 0.14671. Accuracy 0.950.
### Flips: 260, rs: 17, checks: 156
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060817562
Train loss (w/o reg) on all data: 0.048938148
Test loss (w/o reg) on all data: 0.063780986
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.610717e-06
Norm of the params: 15.4139
     Influence (LOO): fixed  96 labels. Loss 0.06378. Accuracy 0.985.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030223925
Train loss (w/o reg) on all data: 0.016774481
Test loss (w/o reg) on all data: 0.0315696
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.4950498e-05
Norm of the params: 16.40088
                Loss: fixed 109 labels. Loss 0.03157. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22287048
Train loss (w/o reg) on all data: 0.2163647
Test loss (w/o reg) on all data: 0.13863583
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 4.5646084e-05
Norm of the params: 11.406829
              Random: fixed  16 labels. Loss 0.13864. Accuracy 0.966.
### Flips: 260, rs: 17, checks: 208
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033599902
Train loss (w/o reg) on all data: 0.023939067
Test loss (w/o reg) on all data: 0.027979042
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.3712138e-06
Norm of the params: 13.900242
     Influence (LOO): fixed 111 labels. Loss 0.02798. Accuracy 0.996.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018786173
Train loss (w/o reg) on all data: 0.009051794
Test loss (w/o reg) on all data: 0.017709326
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6820655e-06
Norm of the params: 13.95305
                Loss: fixed 117 labels. Loss 0.01771. Accuracy 0.996.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21075487
Train loss (w/o reg) on all data: 0.20362489
Test loss (w/o reg) on all data: 0.11994834
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3560302e-05
Norm of the params: 11.941508
              Random: fixed  25 labels. Loss 0.11995. Accuracy 0.969.
### Flips: 260, rs: 17, checks: 260
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02543518
Train loss (w/o reg) on all data: 0.016378364
Test loss (w/o reg) on all data: 0.025494697
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0702914e-06
Norm of the params: 13.45869
     Influence (LOO): fixed 116 labels. Loss 0.02549. Accuracy 0.992.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01400771
Train loss (w/o reg) on all data: 0.0063563264
Test loss (w/o reg) on all data: 0.017989846
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.2402534e-07
Norm of the params: 12.370435
                Loss: fixed 120 labels. Loss 0.01799. Accuracy 0.996.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20015071
Train loss (w/o reg) on all data: 0.19299513
Test loss (w/o reg) on all data: 0.1141224
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.3879514e-05
Norm of the params: 11.962927
              Random: fixed  35 labels. Loss 0.11412. Accuracy 0.969.
### Flips: 260, rs: 17, checks: 312
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010263214
Train loss (w/o reg) on all data: 0.0048615197
Test loss (w/o reg) on all data: 0.020195698
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.0692474e-07
Norm of the params: 10.393935
     Influence (LOO): fixed 124 labels. Loss 0.02020. Accuracy 0.985.
Using normal model
LBFGS training took [55] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010127188
Train loss (w/o reg) on all data: 0.0041441345
Test loss (w/o reg) on all data: 0.015488521
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2686274e-07
Norm of the params: 10.938971
                Loss: fixed 123 labels. Loss 0.01549. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18943316
Train loss (w/o reg) on all data: 0.18194969
Test loss (w/o reg) on all data: 0.107978314
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4558208e-05
Norm of the params: 12.233947
              Random: fixed  42 labels. Loss 0.10798. Accuracy 0.973.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2586077
Train loss (w/o reg) on all data: 0.25240323
Test loss (w/o reg) on all data: 0.17762095
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.635395e-05
Norm of the params: 11.13953
Flipped loss: 0.17762. Accuracy: 0.954
### Flips: 260, rs: 18, checks: 52
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1766428
Train loss (w/o reg) on all data: 0.16713476
Test loss (w/o reg) on all data: 0.13728173
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 6.1358373e-06
Norm of the params: 13.7898855
     Influence (LOO): fixed  40 labels. Loss 0.13728. Accuracy 0.939.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13997503
Train loss (w/o reg) on all data: 0.12619583
Test loss (w/o reg) on all data: 0.16646744
Train acc on all data:  0.9407831900668577
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.9206578e-05
Norm of the params: 16.600716
                Loss: fixed  51 labels. Loss 0.16647. Accuracy 0.935.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25197715
Train loss (w/o reg) on all data: 0.24571449
Test loss (w/o reg) on all data: 0.17250995
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.3718647e-05
Norm of the params: 11.191671
              Random: fixed   5 labels. Loss 0.17251. Accuracy 0.958.
### Flips: 260, rs: 18, checks: 104
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12846002
Train loss (w/o reg) on all data: 0.11698267
Test loss (w/o reg) on all data: 0.09764482
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.9120414e-05
Norm of the params: 15.150808
     Influence (LOO): fixed  69 labels. Loss 0.09764. Accuracy 0.969.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.059074678
Train loss (w/o reg) on all data: 0.042883255
Test loss (w/o reg) on all data: 0.085512005
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.813181e-06
Norm of the params: 17.995235
                Loss: fixed  92 labels. Loss 0.08551. Accuracy 0.969.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25330275
Train loss (w/o reg) on all data: 0.2475974
Test loss (w/o reg) on all data: 0.16547406
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.3267878e-05
Norm of the params: 10.682093
              Random: fixed   9 labels. Loss 0.16547. Accuracy 0.973.
### Flips: 260, rs: 18, checks: 156
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08639903
Train loss (w/o reg) on all data: 0.07401547
Test loss (w/o reg) on all data: 0.07154895
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.56925e-06
Norm of the params: 15.737575
     Influence (LOO): fixed  94 labels. Loss 0.07155. Accuracy 0.969.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030493543
Train loss (w/o reg) on all data: 0.01779119
Test loss (w/o reg) on all data: 0.03481406
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.2843357e-06
Norm of the params: 15.938854
                Loss: fixed 115 labels. Loss 0.03481. Accuracy 0.985.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24452619
Train loss (w/o reg) on all data: 0.23850447
Test loss (w/o reg) on all data: 0.15626991
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3670354e-05
Norm of the params: 10.97426
              Random: fixed  15 labels. Loss 0.15627. Accuracy 0.969.
### Flips: 260, rs: 18, checks: 208
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04829976
Train loss (w/o reg) on all data: 0.03839376
Test loss (w/o reg) on all data: 0.03931409
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2154574e-05
Norm of the params: 14.075512
     Influence (LOO): fixed 116 labels. Loss 0.03931. Accuracy 0.981.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021836217
Train loss (w/o reg) on all data: 0.0118613085
Test loss (w/o reg) on all data: 0.028439417
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.224637e-07
Norm of the params: 14.124383
                Loss: fixed 124 labels. Loss 0.02844. Accuracy 0.989.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23151797
Train loss (w/o reg) on all data: 0.22472069
Test loss (w/o reg) on all data: 0.14368558
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.0422356e-05
Norm of the params: 11.659576
              Random: fixed  26 labels. Loss 0.14369. Accuracy 0.962.
### Flips: 260, rs: 18, checks: 260
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024034554
Train loss (w/o reg) on all data: 0.016840769
Test loss (w/o reg) on all data: 0.026319457
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.318077e-06
Norm of the params: 11.994819
     Influence (LOO): fixed 129 labels. Loss 0.02632. Accuracy 0.992.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01787403
Train loss (w/o reg) on all data: 0.009224746
Test loss (w/o reg) on all data: 0.019805687
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.3098592e-06
Norm of the params: 13.152402
                Loss: fixed 127 labels. Loss 0.01981. Accuracy 0.996.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2254072
Train loss (w/o reg) on all data: 0.21858978
Test loss (w/o reg) on all data: 0.13545875
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.9578301e-05
Norm of the params: 11.67683
              Random: fixed  31 labels. Loss 0.13546. Accuracy 0.969.
### Flips: 260, rs: 18, checks: 312
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013108114
Train loss (w/o reg) on all data: 0.0064413035
Test loss (w/o reg) on all data: 0.019632185
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.50071e-06
Norm of the params: 11.547131
     Influence (LOO): fixed 134 labels. Loss 0.01963. Accuracy 0.989.
Using normal model
LBFGS training took [72] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015888471
Train loss (w/o reg) on all data: 0.008109524
Test loss (w/o reg) on all data: 0.020050956
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.955367e-07
Norm of the params: 12.473128
                Loss: fixed 130 labels. Loss 0.02005. Accuracy 0.992.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2186897
Train loss (w/o reg) on all data: 0.21145718
Test loss (w/o reg) on all data: 0.12515351
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6091648e-05
Norm of the params: 12.027066
              Random: fixed  37 labels. Loss 0.12515. Accuracy 0.973.
Using normal model
LBFGS training took [288] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22540541
Train loss (w/o reg) on all data: 0.21696004
Test loss (w/o reg) on all data: 0.16463967
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.0659968e-05
Norm of the params: 12.996436
Flipped loss: 0.16464. Accuracy: 0.935
### Flips: 260, rs: 19, checks: 52
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14783104
Train loss (w/o reg) on all data: 0.1352518
Test loss (w/o reg) on all data: 0.12560867
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.3715234e-05
Norm of the params: 15.8614235
     Influence (LOO): fixed  39 labels. Loss 0.12561. Accuracy 0.950.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11891844
Train loss (w/o reg) on all data: 0.102550164
Test loss (w/o reg) on all data: 0.13683969
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 6.5896725e-06
Norm of the params: 18.093246
                Loss: fixed  44 labels. Loss 0.13684. Accuracy 0.947.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22558342
Train loss (w/o reg) on all data: 0.2172911
Test loss (w/o reg) on all data: 0.16311684
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.888489e-05
Norm of the params: 12.878132
              Random: fixed   1 labels. Loss 0.16312. Accuracy 0.935.
### Flips: 260, rs: 19, checks: 104
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.088002816
Train loss (w/o reg) on all data: 0.07337839
Test loss (w/o reg) on all data: 0.117307
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.8904497e-05
Norm of the params: 17.102295
     Influence (LOO): fixed  70 labels. Loss 0.11731. Accuracy 0.947.
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06803216
Train loss (w/o reg) on all data: 0.051015444
Test loss (w/o reg) on all data: 0.10034781
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.9718115e-06
Norm of the params: 18.448153
                Loss: fixed  80 labels. Loss 0.10035. Accuracy 0.958.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22427997
Train loss (w/o reg) on all data: 0.21654296
Test loss (w/o reg) on all data: 0.14921711
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.3366108e-05
Norm of the params: 12.439457
              Random: fixed   9 labels. Loss 0.14922. Accuracy 0.954.
### Flips: 260, rs: 19, checks: 156
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.074905604
Train loss (w/o reg) on all data: 0.060905855
Test loss (w/o reg) on all data: 0.0946142
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.855531e-06
Norm of the params: 16.73305
     Influence (LOO): fixed  85 labels. Loss 0.09461. Accuracy 0.947.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040422603
Train loss (w/o reg) on all data: 0.026351798
Test loss (w/o reg) on all data: 0.04627131
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.1683852e-06
Norm of the params: 16.775461
                Loss: fixed 106 labels. Loss 0.04627. Accuracy 0.977.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2204084
Train loss (w/o reg) on all data: 0.21317622
Test loss (w/o reg) on all data: 0.14119563
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.305009e-05
Norm of the params: 12.026784
              Random: fixed  16 labels. Loss 0.14120. Accuracy 0.954.
### Flips: 260, rs: 19, checks: 208
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05481279
Train loss (w/o reg) on all data: 0.042156734
Test loss (w/o reg) on all data: 0.05665422
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7380046e-06
Norm of the params: 15.90978
     Influence (LOO): fixed 104 labels. Loss 0.05665. Accuracy 0.977.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025162792
Train loss (w/o reg) on all data: 0.014782251
Test loss (w/o reg) on all data: 0.03747084
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.660678e-07
Norm of the params: 14.408706
                Loss: fixed 117 labels. Loss 0.03747. Accuracy 0.985.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21574022
Train loss (w/o reg) on all data: 0.20847791
Test loss (w/o reg) on all data: 0.14218567
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.2559896e-05
Norm of the params: 12.051804
              Random: fixed  19 labels. Loss 0.14219. Accuracy 0.950.
### Flips: 260, rs: 19, checks: 260
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03574958
Train loss (w/o reg) on all data: 0.025906306
Test loss (w/o reg) on all data: 0.03068591
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7059663e-05
Norm of the params: 14.030875
     Influence (LOO): fixed 116 labels. Loss 0.03069. Accuracy 0.992.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019951358
Train loss (w/o reg) on all data: 0.010536118
Test loss (w/o reg) on all data: 0.03649799
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.914846e-06
Norm of the params: 13.722421
                Loss: fixed 119 labels. Loss 0.03650. Accuracy 0.989.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21235533
Train loss (w/o reg) on all data: 0.20540881
Test loss (w/o reg) on all data: 0.13295506
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4759275e-05
Norm of the params: 11.786879
              Random: fixed  25 labels. Loss 0.13296. Accuracy 0.958.
### Flips: 260, rs: 19, checks: 312
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019155208
Train loss (w/o reg) on all data: 0.0112617975
Test loss (w/o reg) on all data: 0.032214914
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.6747276e-06
Norm of the params: 12.564564
     Influence (LOO): fixed 125 labels. Loss 0.03221. Accuracy 0.989.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011568798
Train loss (w/o reg) on all data: 0.0048448327
Test loss (w/o reg) on all data: 0.03061459
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.712353e-07
Norm of the params: 11.59652
                Loss: fixed 126 labels. Loss 0.03061. Accuracy 0.989.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20159878
Train loss (w/o reg) on all data: 0.19458203
Test loss (w/o reg) on all data: 0.13410035
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.3153756e-05
Norm of the params: 11.846302
              Random: fixed  33 labels. Loss 0.13410. Accuracy 0.943.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24928318
Train loss (w/o reg) on all data: 0.24302582
Test loss (w/o reg) on all data: 0.17226171
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 3.865917e-05
Norm of the params: 11.186916
Flipped loss: 0.17226. Accuracy: 0.927
### Flips: 260, rs: 20, checks: 52
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1759044
Train loss (w/o reg) on all data: 0.16533114
Test loss (w/o reg) on all data: 0.15291089
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.4161795e-05
Norm of the params: 14.541842
     Influence (LOO): fixed  38 labels. Loss 0.15291. Accuracy 0.935.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1322701
Train loss (w/o reg) on all data: 0.11784928
Test loss (w/o reg) on all data: 0.1669248
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 6.1073086e-05
Norm of the params: 16.982822
                Loss: fixed  49 labels. Loss 0.16692. Accuracy 0.935.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24225418
Train loss (w/o reg) on all data: 0.2357751
Test loss (w/o reg) on all data: 0.16046815
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.7342609e-05
Norm of the params: 11.383389
              Random: fixed   8 labels. Loss 0.16047. Accuracy 0.935.
### Flips: 260, rs: 20, checks: 104
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.121724434
Train loss (w/o reg) on all data: 0.11072732
Test loss (w/o reg) on all data: 0.10347989
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.2179325e-05
Norm of the params: 14.830454
     Influence (LOO): fixed  70 labels. Loss 0.10348. Accuracy 0.947.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055376664
Train loss (w/o reg) on all data: 0.03923568
Test loss (w/o reg) on all data: 0.11885021
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.1842984e-05
Norm of the params: 17.967182
                Loss: fixed  93 labels. Loss 0.11885. Accuracy 0.943.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2371699
Train loss (w/o reg) on all data: 0.2306961
Test loss (w/o reg) on all data: 0.15930827
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.8245904e-05
Norm of the params: 11.378761
              Random: fixed  14 labels. Loss 0.15931. Accuracy 0.947.
### Flips: 260, rs: 20, checks: 156
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.076041564
Train loss (w/o reg) on all data: 0.06588009
Test loss (w/o reg) on all data: 0.06853145
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.3999845e-06
Norm of the params: 14.255858
     Influence (LOO): fixed  95 labels. Loss 0.06853. Accuracy 0.973.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025568724
Train loss (w/o reg) on all data: 0.014040784
Test loss (w/o reg) on all data: 0.046103407
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4790146e-06
Norm of the params: 15.184162
                Loss: fixed 117 labels. Loss 0.04610. Accuracy 0.977.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22649506
Train loss (w/o reg) on all data: 0.22001952
Test loss (w/o reg) on all data: 0.14627615
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.0541176e-05
Norm of the params: 11.380276
              Random: fixed  21 labels. Loss 0.14628. Accuracy 0.954.
### Flips: 260, rs: 20, checks: 208
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03149355
Train loss (w/o reg) on all data: 0.021983895
Test loss (w/o reg) on all data: 0.02249116
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.2238276e-05
Norm of the params: 13.791051
     Influence (LOO): fixed 116 labels. Loss 0.02249. Accuracy 0.989.
Using normal model
LBFGS training took [80] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013488309
Train loss (w/o reg) on all data: 0.006040163
Test loss (w/o reg) on all data: 0.028003344
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.0478116e-06
Norm of the params: 12.205037
                Loss: fixed 125 labels. Loss 0.02800. Accuracy 0.985.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21786702
Train loss (w/o reg) on all data: 0.21081027
Test loss (w/o reg) on all data: 0.14274111
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.4413092e-05
Norm of the params: 11.880016
              Random: fixed  27 labels. Loss 0.14274. Accuracy 0.950.
### Flips: 260, rs: 20, checks: 260
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02034027
Train loss (w/o reg) on all data: 0.0133086955
Test loss (w/o reg) on all data: 0.016703518
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.6705817e-06
Norm of the params: 11.858814
     Influence (LOO): fixed 126 labels. Loss 0.01670. Accuracy 0.996.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011481917
Train loss (w/o reg) on all data: 0.00489579
Test loss (w/o reg) on all data: 0.020092633
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6263319e-06
Norm of the params: 11.477044
                Loss: fixed 127 labels. Loss 0.02009. Accuracy 0.992.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21101972
Train loss (w/o reg) on all data: 0.20376107
Test loss (w/o reg) on all data: 0.13985907
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.3903785e-05
Norm of the params: 12.048785
              Random: fixed  32 labels. Loss 0.13986. Accuracy 0.958.
### Flips: 260, rs: 20, checks: 312
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010522975
Train loss (w/o reg) on all data: 0.0048191417
Test loss (w/o reg) on all data: 0.015151974
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.2666142e-07
Norm of the params: 10.680668
     Influence (LOO): fixed 131 labels. Loss 0.01515. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0090962965
Train loss (w/o reg) on all data: 0.0037990375
Test loss (w/o reg) on all data: 0.011369529
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.8047265e-07
Norm of the params: 10.292969
                Loss: fixed 130 labels. Loss 0.01137. Accuracy 0.996.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20253524
Train loss (w/o reg) on all data: 0.19495331
Test loss (w/o reg) on all data: 0.12724613
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.061178e-05
Norm of the params: 12.314162
              Random: fixed  40 labels. Loss 0.12725. Accuracy 0.966.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25962543
Train loss (w/o reg) on all data: 0.25436005
Test loss (w/o reg) on all data: 0.16204669
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.94531e-05
Norm of the params: 10.261944
Flipped loss: 0.16205. Accuracy: 0.950
### Flips: 260, rs: 21, checks: 52
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18149267
Train loss (w/o reg) on all data: 0.17283061
Test loss (w/o reg) on all data: 0.1134111
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.5772654e-05
Norm of the params: 13.162117
     Influence (LOO): fixed  43 labels. Loss 0.11341. Accuracy 0.954.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1434401
Train loss (w/o reg) on all data: 0.13139193
Test loss (w/o reg) on all data: 0.122806
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.2565727e-05
Norm of the params: 15.522997
                Loss: fixed  51 labels. Loss 0.12281. Accuracy 0.950.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2539217
Train loss (w/o reg) on all data: 0.24879259
Test loss (w/o reg) on all data: 0.15732011
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.9476169e-05
Norm of the params: 10.12827
              Random: fixed   7 labels. Loss 0.15732. Accuracy 0.958.
### Flips: 260, rs: 21, checks: 104
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13707322
Train loss (w/o reg) on all data: 0.12572195
Test loss (w/o reg) on all data: 0.09805916
Train acc on all data:  0.9398280802292264
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.7846387e-05
Norm of the params: 15.067361
     Influence (LOO): fixed  68 labels. Loss 0.09806. Accuracy 0.962.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.066212766
Train loss (w/o reg) on all data: 0.05153617
Test loss (w/o reg) on all data: 0.07142622
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.481858e-06
Norm of the params: 17.132776
                Loss: fixed  93 labels. Loss 0.07143. Accuracy 0.966.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25109044
Train loss (w/o reg) on all data: 0.24610229
Test loss (w/o reg) on all data: 0.15471481
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.5986492e-05
Norm of the params: 9.988153
              Random: fixed  10 labels. Loss 0.15471. Accuracy 0.966.
### Flips: 260, rs: 21, checks: 156
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09634173
Train loss (w/o reg) on all data: 0.08582452
Test loss (w/o reg) on all data: 0.065527424
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.307471e-06
Norm of the params: 14.503245
     Influence (LOO): fixed  93 labels. Loss 0.06553. Accuracy 0.981.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030534953
Train loss (w/o reg) on all data: 0.019305618
Test loss (w/o reg) on all data: 0.03009982
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.006174e-06
Norm of the params: 14.986217
                Loss: fixed 117 labels. Loss 0.03010. Accuracy 0.985.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24109857
Train loss (w/o reg) on all data: 0.23596926
Test loss (w/o reg) on all data: 0.14380674
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.6378373e-05
Norm of the params: 10.128483
              Random: fixed  18 labels. Loss 0.14381. Accuracy 0.977.
### Flips: 260, rs: 21, checks: 208
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058004893
Train loss (w/o reg) on all data: 0.04838833
Test loss (w/o reg) on all data: 0.046164542
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.961114e-06
Norm of the params: 13.868358
     Influence (LOO): fixed 113 labels. Loss 0.04616. Accuracy 0.977.
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018338881
Train loss (w/o reg) on all data: 0.009263034
Test loss (w/o reg) on all data: 0.023514355
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.764012e-07
Norm of the params: 13.472822
                Loss: fixed 129 labels. Loss 0.02351. Accuracy 0.989.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2307952
Train loss (w/o reg) on all data: 0.22555393
Test loss (w/o reg) on all data: 0.13521034
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5632488e-05
Norm of the params: 10.238426
              Random: fixed  25 labels. Loss 0.13521. Accuracy 0.977.
### Flips: 260, rs: 21, checks: 260
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029373601
Train loss (w/o reg) on all data: 0.022883236
Test loss (w/o reg) on all data: 0.01966539
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.0179368e-06
Norm of the params: 11.3933
     Influence (LOO): fixed 128 labels. Loss 0.01967. Accuracy 0.992.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009327635
Train loss (w/o reg) on all data: 0.0036814525
Test loss (w/o reg) on all data: 0.009853321
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.8998605e-07
Norm of the params: 10.6265545
                Loss: fixed 135 labels. Loss 0.00985. Accuracy 0.996.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22613707
Train loss (w/o reg) on all data: 0.22077754
Test loss (w/o reg) on all data: 0.11974637
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.0345763e-05
Norm of the params: 10.353286
              Random: fixed  33 labels. Loss 0.11975. Accuracy 0.989.
### Flips: 260, rs: 21, checks: 312
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020173231
Train loss (w/o reg) on all data: 0.013921335
Test loss (w/o reg) on all data: 0.016601278
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.6446626e-06
Norm of the params: 11.182035
     Influence (LOO): fixed 132 labels. Loss 0.01660. Accuracy 0.996.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.007517296
Train loss (w/o reg) on all data: 0.0027335316
Test loss (w/o reg) on all data: 0.011539685
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.231954e-07
Norm of the params: 9.781374
                Loss: fixed 137 labels. Loss 0.01154. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2187425
Train loss (w/o reg) on all data: 0.21367823
Test loss (w/o reg) on all data: 0.11630466
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.813385e-05
Norm of the params: 10.064074
              Random: fixed  41 labels. Loss 0.11630. Accuracy 0.985.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24849908
Train loss (w/o reg) on all data: 0.24131334
Test loss (w/o reg) on all data: 0.18410704
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 3.5383553e-05
Norm of the params: 11.988113
Flipped loss: 0.18411. Accuracy: 0.927
### Flips: 260, rs: 22, checks: 52
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17098348
Train loss (w/o reg) on all data: 0.1602436
Test loss (w/o reg) on all data: 0.1431531
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.1679139e-05
Norm of the params: 14.6559725
     Influence (LOO): fixed  42 labels. Loss 0.14315. Accuracy 0.950.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13180175
Train loss (w/o reg) on all data: 0.11399304
Test loss (w/o reg) on all data: 0.15179513
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.9223797e-05
Norm of the params: 18.87258
                Loss: fixed  51 labels. Loss 0.15180. Accuracy 0.935.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24113466
Train loss (w/o reg) on all data: 0.2339949
Test loss (w/o reg) on all data: 0.16708298
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.6821332e-05
Norm of the params: 11.949698
              Random: fixed   8 labels. Loss 0.16708. Accuracy 0.943.
### Flips: 260, rs: 22, checks: 104
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11904918
Train loss (w/o reg) on all data: 0.10693786
Test loss (w/o reg) on all data: 0.09728877
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.702216e-06
Norm of the params: 15.563618
     Influence (LOO): fixed  69 labels. Loss 0.09729. Accuracy 0.969.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06442008
Train loss (w/o reg) on all data: 0.04685479
Test loss (w/o reg) on all data: 0.08381398
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.9866655e-06
Norm of the params: 18.743155
                Loss: fixed  90 labels. Loss 0.08381. Accuracy 0.958.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23774543
Train loss (w/o reg) on all data: 0.23070383
Test loss (w/o reg) on all data: 0.1609224
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.460141e-05
Norm of the params: 11.867275
              Random: fixed  12 labels. Loss 0.16092. Accuracy 0.943.
### Flips: 260, rs: 22, checks: 156
Using normal model
LBFGS training took [194] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.082371205
Train loss (w/o reg) on all data: 0.071796365
Test loss (w/o reg) on all data: 0.060143303
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0721467e-05
Norm of the params: 14.542928
     Influence (LOO): fixed  95 labels. Loss 0.06014. Accuracy 0.981.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029217653
Train loss (w/o reg) on all data: 0.016657488
Test loss (w/o reg) on all data: 0.057929143
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.3068925e-06
Norm of the params: 15.849393
                Loss: fixed 114 labels. Loss 0.05793. Accuracy 0.973.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22494934
Train loss (w/o reg) on all data: 0.21670295
Test loss (w/o reg) on all data: 0.14292033
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.8666175e-05
Norm of the params: 12.842424
              Random: fixed  19 labels. Loss 0.14292. Accuracy 0.954.
### Flips: 260, rs: 22, checks: 208
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043545462
Train loss (w/o reg) on all data: 0.03556057
Test loss (w/o reg) on all data: 0.04165964
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.368161e-06
Norm of the params: 12.637162
     Influence (LOO): fixed 114 labels. Loss 0.04166. Accuracy 0.973.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021401118
Train loss (w/o reg) on all data: 0.011385975
Test loss (w/o reg) on all data: 0.024469834
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9455675e-06
Norm of the params: 14.152841
                Loss: fixed 120 labels. Loss 0.02447. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21869664
Train loss (w/o reg) on all data: 0.20993051
Test loss (w/o reg) on all data: 0.14352648
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.2206528e-05
Norm of the params: 13.240947
              Random: fixed  22 labels. Loss 0.14353. Accuracy 0.954.
### Flips: 260, rs: 22, checks: 260
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019846605
Train loss (w/o reg) on all data: 0.012726308
Test loss (w/o reg) on all data: 0.025127087
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.3268682e-07
Norm of the params: 11.933397
     Influence (LOO): fixed 123 labels. Loss 0.02513. Accuracy 0.989.
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014161902
Train loss (w/o reg) on all data: 0.006459435
Test loss (w/o reg) on all data: 0.018713756
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.674977e-07
Norm of the params: 12.411661
                Loss: fixed 124 labels. Loss 0.01871. Accuracy 0.992.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20815293
Train loss (w/o reg) on all data: 0.19904791
Test loss (w/o reg) on all data: 0.13631806
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.005433e-05
Norm of the params: 13.49446
              Random: fixed  30 labels. Loss 0.13632. Accuracy 0.962.
### Flips: 260, rs: 22, checks: 312
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009397754
Train loss (w/o reg) on all data: 0.004234437
Test loss (w/o reg) on all data: 0.015204979
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.907789e-07
Norm of the params: 10.162005
     Influence (LOO): fixed 128 labels. Loss 0.01520. Accuracy 0.992.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010101447
Train loss (w/o reg) on all data: 0.004137737
Test loss (w/o reg) on all data: 0.016469173
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.1384865e-07
Norm of the params: 10.921273
                Loss: fixed 127 labels. Loss 0.01647. Accuracy 0.992.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18830083
Train loss (w/o reg) on all data: 0.17792432
Test loss (w/o reg) on all data: 0.12578127
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.21698e-05
Norm of the params: 14.405911
              Random: fixed  40 labels. Loss 0.12578. Accuracy 0.966.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22718066
Train loss (w/o reg) on all data: 0.21766075
Test loss (w/o reg) on all data: 0.18404357
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.486547e-05
Norm of the params: 13.798486
Flipped loss: 0.18404. Accuracy: 0.939
### Flips: 260, rs: 23, checks: 52
Using normal model
LBFGS training took [175] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15020254
Train loss (w/o reg) on all data: 0.13580222
Test loss (w/o reg) on all data: 0.1749034
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.7906284e-05
Norm of the params: 16.970753
     Influence (LOO): fixed  38 labels. Loss 0.17490. Accuracy 0.943.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115072295
Train loss (w/o reg) on all data: 0.09649078
Test loss (w/o reg) on all data: 0.15663803
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.7735556e-05
Norm of the params: 19.277716
                Loss: fixed  50 labels. Loss 0.15664. Accuracy 0.943.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21413781
Train loss (w/o reg) on all data: 0.20334886
Test loss (w/o reg) on all data: 0.17748982
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 9.5225994e-05
Norm of the params: 14.689421
              Random: fixed   8 labels. Loss 0.17749. Accuracy 0.943.
### Flips: 260, rs: 23, checks: 104
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.100903705
Train loss (w/o reg) on all data: 0.08484806
Test loss (w/o reg) on all data: 0.1408168
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.5558069e-05
Norm of the params: 17.919619
     Influence (LOO): fixed  66 labels. Loss 0.14082. Accuracy 0.954.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054102514
Train loss (w/o reg) on all data: 0.035580404
Test loss (w/o reg) on all data: 0.115273565
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7056565e-06
Norm of the params: 19.246876
                Loss: fixed  86 labels. Loss 0.11527. Accuracy 0.966.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2075626
Train loss (w/o reg) on all data: 0.19763632
Test loss (w/o reg) on all data: 0.15536179
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.4955995e-05
Norm of the params: 14.089907
              Random: fixed  17 labels. Loss 0.15536. Accuracy 0.954.
### Flips: 260, rs: 23, checks: 156
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057231657
Train loss (w/o reg) on all data: 0.04160389
Test loss (w/o reg) on all data: 0.09528015
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4558769e-05
Norm of the params: 17.679235
     Influence (LOO): fixed  92 labels. Loss 0.09528. Accuracy 0.966.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035278924
Train loss (w/o reg) on all data: 0.020826658
Test loss (w/o reg) on all data: 0.045034997
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.272567e-06
Norm of the params: 17.001331
                Loss: fixed 104 labels. Loss 0.04503. Accuracy 0.977.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20023376
Train loss (w/o reg) on all data: 0.18978669
Test loss (w/o reg) on all data: 0.15300018
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.761006e-06
Norm of the params: 14.454802
              Random: fixed  22 labels. Loss 0.15300. Accuracy 0.958.
### Flips: 260, rs: 23, checks: 208
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035958376
Train loss (w/o reg) on all data: 0.02418073
Test loss (w/o reg) on all data: 0.045444563
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.718179e-06
Norm of the params: 15.3477335
     Influence (LOO): fixed 111 labels. Loss 0.04544. Accuracy 0.981.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02487886
Train loss (w/o reg) on all data: 0.0124454275
Test loss (w/o reg) on all data: 0.038449023
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4932399e-06
Norm of the params: 15.769232
                Loss: fixed 113 labels. Loss 0.03845. Accuracy 0.985.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20082302
Train loss (w/o reg) on all data: 0.19097975
Test loss (w/o reg) on all data: 0.13489626
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4602864e-05
Norm of the params: 14.030874
              Random: fixed  27 labels. Loss 0.13490. Accuracy 0.958.
### Flips: 260, rs: 23, checks: 260
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022828806
Train loss (w/o reg) on all data: 0.013941382
Test loss (w/o reg) on all data: 0.057208076
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.048765e-07
Norm of the params: 13.332235
     Influence (LOO): fixed 120 labels. Loss 0.05721. Accuracy 0.981.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021467393
Train loss (w/o reg) on all data: 0.010235355
Test loss (w/o reg) on all data: 0.03828206
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.863436e-06
Norm of the params: 14.988021
                Loss: fixed 116 labels. Loss 0.03828. Accuracy 0.985.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19428656
Train loss (w/o reg) on all data: 0.18455224
Test loss (w/o reg) on all data: 0.118975855
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1076559e-05
Norm of the params: 13.953009
              Random: fixed  35 labels. Loss 0.11898. Accuracy 0.973.
### Flips: 260, rs: 23, checks: 312
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020117387
Train loss (w/o reg) on all data: 0.011795782
Test loss (w/o reg) on all data: 0.042155024
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.6170204e-07
Norm of the params: 12.900856
     Influence (LOO): fixed 125 labels. Loss 0.04216. Accuracy 0.985.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016861022
Train loss (w/o reg) on all data: 0.007737353
Test loss (w/o reg) on all data: 0.02619236
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.153799e-07
Norm of the params: 13.508269
                Loss: fixed 123 labels. Loss 0.02619. Accuracy 0.996.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18077877
Train loss (w/o reg) on all data: 0.17171882
Test loss (w/o reg) on all data: 0.11033221
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.6666574e-05
Norm of the params: 13.4610195
              Random: fixed  46 labels. Loss 0.11033. Accuracy 0.977.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25334948
Train loss (w/o reg) on all data: 0.24685217
Test loss (w/o reg) on all data: 0.17985423
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.2849131e-05
Norm of the params: 11.399392
Flipped loss: 0.17985. Accuracy: 0.935
### Flips: 260, rs: 24, checks: 52
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17816912
Train loss (w/o reg) on all data: 0.16778703
Test loss (w/o reg) on all data: 0.13871108
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.7574723e-05
Norm of the params: 14.409775
     Influence (LOO): fixed  38 labels. Loss 0.13871. Accuracy 0.954.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13848141
Train loss (w/o reg) on all data: 0.12277809
Test loss (w/o reg) on all data: 0.16548443
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.4531044e-05
Norm of the params: 17.721918
                Loss: fixed  49 labels. Loss 0.16548. Accuracy 0.931.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24890673
Train loss (w/o reg) on all data: 0.24233246
Test loss (w/o reg) on all data: 0.17849673
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.758226e-05
Norm of the params: 11.466708
              Random: fixed   4 labels. Loss 0.17850. Accuracy 0.943.
### Flips: 260, rs: 24, checks: 104
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12696601
Train loss (w/o reg) on all data: 0.11461868
Test loss (w/o reg) on all data: 0.09435831
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 9.0211715e-06
Norm of the params: 15.714534
     Influence (LOO): fixed  69 labels. Loss 0.09436. Accuracy 0.966.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07397873
Train loss (w/o reg) on all data: 0.055267785
Test loss (w/o reg) on all data: 0.07618416
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.3931806e-06
Norm of the params: 19.34474
                Loss: fixed  91 labels. Loss 0.07618. Accuracy 0.977.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24088483
Train loss (w/o reg) on all data: 0.23392326
Test loss (w/o reg) on all data: 0.16528592
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.6428492e-05
Norm of the params: 11.799636
              Random: fixed  10 labels. Loss 0.16529. Accuracy 0.939.
### Flips: 260, rs: 24, checks: 156
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08088889
Train loss (w/o reg) on all data: 0.06837773
Test loss (w/o reg) on all data: 0.06296231
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.6449842e-06
Norm of the params: 15.818441
     Influence (LOO): fixed  95 labels. Loss 0.06296. Accuracy 0.973.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.041912533
Train loss (w/o reg) on all data: 0.028197976
Test loss (w/o reg) on all data: 0.03515296
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.2083901e-06
Norm of the params: 16.56174
                Loss: fixed 114 labels. Loss 0.03515. Accuracy 0.989.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23138441
Train loss (w/o reg) on all data: 0.22467913
Test loss (w/o reg) on all data: 0.15681638
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.904905e-05
Norm of the params: 11.580403
              Random: fixed  18 labels. Loss 0.15682. Accuracy 0.947.
### Flips: 260, rs: 24, checks: 208
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04912699
Train loss (w/o reg) on all data: 0.038885072
Test loss (w/o reg) on all data: 0.03456129
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.559273e-06
Norm of the params: 14.312174
     Influence (LOO): fixed 114 labels. Loss 0.03456. Accuracy 0.992.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029331606
Train loss (w/o reg) on all data: 0.017487539
Test loss (w/o reg) on all data: 0.026587432
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 7.3318424e-06
Norm of the params: 15.39095
                Loss: fixed 121 labels. Loss 0.02659. Accuracy 0.996.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22811005
Train loss (w/o reg) on all data: 0.22174764
Test loss (w/o reg) on all data: 0.1457526
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.012487e-05
Norm of the params: 11.280432
              Random: fixed  23 labels. Loss 0.14575. Accuracy 0.969.
### Flips: 260, rs: 24, checks: 260
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028662276
Train loss (w/o reg) on all data: 0.01954841
Test loss (w/o reg) on all data: 0.034835223
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2563361e-06
Norm of the params: 13.501012
     Influence (LOO): fixed 123 labels. Loss 0.03484. Accuracy 0.985.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01769975
Train loss (w/o reg) on all data: 0.008540575
Test loss (w/o reg) on all data: 0.022439921
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5872698e-06
Norm of the params: 13.53453
                Loss: fixed 129 labels. Loss 0.02244. Accuracy 0.992.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2157908
Train loss (w/o reg) on all data: 0.208917
Test loss (w/o reg) on all data: 0.14066662
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.8038896e-05
Norm of the params: 11.725004
              Random: fixed  31 labels. Loss 0.14067. Accuracy 0.962.
### Flips: 260, rs: 24, checks: 312
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018755153
Train loss (w/o reg) on all data: 0.01207083
Test loss (w/o reg) on all data: 0.03007827
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0795386e-06
Norm of the params: 11.562286
     Influence (LOO): fixed 133 labels. Loss 0.03008. Accuracy 0.992.
Using normal model
LBFGS training took [66] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012999205
Train loss (w/o reg) on all data: 0.0058355993
Test loss (w/o reg) on all data: 0.019881468
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2598058e-06
Norm of the params: 11.969633
                Loss: fixed 132 labels. Loss 0.01988. Accuracy 0.992.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20413639
Train loss (w/o reg) on all data: 0.19742857
Test loss (w/o reg) on all data: 0.13314585
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.267206e-05
Norm of the params: 11.582588
              Random: fixed  43 labels. Loss 0.13315. Accuracy 0.966.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24780557
Train loss (w/o reg) on all data: 0.24213447
Test loss (w/o reg) on all data: 0.23620342
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.8778625954198473
Norm of the mean of gradients: 5.5503657e-05
Norm of the params: 10.649975
Flipped loss: 0.23620. Accuracy: 0.878
### Flips: 260, rs: 25, checks: 52
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16278172
Train loss (w/o reg) on all data: 0.15190683
Test loss (w/o reg) on all data: 0.22975065
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.8969465648854962
Norm of the mean of gradients: 1.1465599e-05
Norm of the params: 14.7478
     Influence (LOO): fixed  39 labels. Loss 0.22975. Accuracy 0.897.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12333546
Train loss (w/o reg) on all data: 0.109150805
Test loss (w/o reg) on all data: 0.2422956
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 5.1888226e-05
Norm of the params: 16.843191
                Loss: fixed  48 labels. Loss 0.24230. Accuracy 0.901.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24678294
Train loss (w/o reg) on all data: 0.24123123
Test loss (w/o reg) on all data: 0.2343781
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.8854961832061069
Norm of the mean of gradients: 1.1323046e-05
Norm of the params: 10.537277
              Random: fixed   3 labels. Loss 0.23438. Accuracy 0.885.
### Flips: 260, rs: 25, checks: 104
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1198702
Train loss (w/o reg) on all data: 0.108091384
Test loss (w/o reg) on all data: 0.17068687
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.6291397e-05
Norm of the params: 15.348496
     Influence (LOO): fixed  66 labels. Loss 0.17069. Accuracy 0.931.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058669813
Train loss (w/o reg) on all data: 0.043582648
Test loss (w/o reg) on all data: 0.15908642
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.9414567e-06
Norm of the params: 17.370762
                Loss: fixed  87 labels. Loss 0.15909. Accuracy 0.950.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24208
Train loss (w/o reg) on all data: 0.2365526
Test loss (w/o reg) on all data: 0.22299643
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 4.679442e-05
Norm of the params: 10.51419
              Random: fixed   8 labels. Loss 0.22300. Accuracy 0.908.
### Flips: 260, rs: 25, checks: 156
Using normal model
LBFGS training took [179] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.078482
Train loss (w/o reg) on all data: 0.06713851
Test loss (w/o reg) on all data: 0.12201012
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.2912733e-06
Norm of the params: 15.062199
     Influence (LOO): fixed  88 labels. Loss 0.12201. Accuracy 0.962.
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030979745
Train loss (w/o reg) on all data: 0.01696716
Test loss (w/o reg) on all data: 0.10228447
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.7389842e-06
Norm of the params: 16.740719
                Loss: fixed 106 labels. Loss 0.10228. Accuracy 0.950.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23360372
Train loss (w/o reg) on all data: 0.22804594
Test loss (w/o reg) on all data: 0.20045172
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 2.4252466e-05
Norm of the params: 10.543034
              Random: fixed  19 labels. Loss 0.20045. Accuracy 0.901.
### Flips: 260, rs: 25, checks: 208
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048090115
Train loss (w/o reg) on all data: 0.037999645
Test loss (w/o reg) on all data: 0.07546803
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.637539e-06
Norm of the params: 14.205965
     Influence (LOO): fixed 108 labels. Loss 0.07547. Accuracy 0.977.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026393883
Train loss (w/o reg) on all data: 0.014826439
Test loss (w/o reg) on all data: 0.07073216
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.7579548e-06
Norm of the params: 15.210157
                Loss: fixed 113 labels. Loss 0.07073. Accuracy 0.973.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22955346
Train loss (w/o reg) on all data: 0.22404252
Test loss (w/o reg) on all data: 0.186074
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.21704325e-05
Norm of the params: 10.498508
              Random: fixed  23 labels. Loss 0.18607. Accuracy 0.916.
### Flips: 260, rs: 25, checks: 260
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02500669
Train loss (w/o reg) on all data: 0.015174772
Test loss (w/o reg) on all data: 0.04391241
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8358265e-06
Norm of the params: 14.022779
     Influence (LOO): fixed 120 labels. Loss 0.04391. Accuracy 0.989.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019819088
Train loss (w/o reg) on all data: 0.010102679
Test loss (w/o reg) on all data: 0.039872255
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.9589375e-07
Norm of the params: 13.940165
                Loss: fixed 121 labels. Loss 0.03987. Accuracy 0.989.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22080359
Train loss (w/o reg) on all data: 0.21501444
Test loss (w/o reg) on all data: 0.18416831
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 4.1686533e-05
Norm of the params: 10.76024
              Random: fixed  29 labels. Loss 0.18417. Accuracy 0.916.
### Flips: 260, rs: 25, checks: 312
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01753676
Train loss (w/o reg) on all data: 0.0098063275
Test loss (w/o reg) on all data: 0.039138433
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.529002e-07
Norm of the params: 12.434173
     Influence (LOO): fixed 127 labels. Loss 0.03914. Accuracy 0.992.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016805202
Train loss (w/o reg) on all data: 0.008188027
Test loss (w/o reg) on all data: 0.0234556
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2278861e-06
Norm of the params: 13.127967
                Loss: fixed 126 labels. Loss 0.02346. Accuracy 0.992.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21151434
Train loss (w/o reg) on all data: 0.20589665
Test loss (w/o reg) on all data: 0.1619039
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.5884287e-05
Norm of the params: 10.599716
              Random: fixed  40 labels. Loss 0.16190. Accuracy 0.931.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24624383
Train loss (w/o reg) on all data: 0.24081765
Test loss (w/o reg) on all data: 0.13684206
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.287433e-05
Norm of the params: 10.417476
Flipped loss: 0.13684. Accuracy: 0.969
### Flips: 260, rs: 26, checks: 52
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1573217
Train loss (w/o reg) on all data: 0.1457111
Test loss (w/o reg) on all data: 0.09491752
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.1502355e-05
Norm of the params: 15.238513
     Influence (LOO): fixed  42 labels. Loss 0.09492. Accuracy 0.973.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.123064764
Train loss (w/o reg) on all data: 0.108314015
Test loss (w/o reg) on all data: 0.10682636
Train acc on all data:  0.9426934097421203
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.1686086e-05
Norm of the params: 17.176
                Loss: fixed  50 labels. Loss 0.10683. Accuracy 0.958.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23766874
Train loss (w/o reg) on all data: 0.23177451
Test loss (w/o reg) on all data: 0.13105549
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.3489442e-05
Norm of the params: 10.857465
              Random: fixed   6 labels. Loss 0.13106. Accuracy 0.969.
### Flips: 260, rs: 26, checks: 104
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10251759
Train loss (w/o reg) on all data: 0.08988981
Test loss (w/o reg) on all data: 0.07457654
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.0603712e-06
Norm of the params: 15.891999
     Influence (LOO): fixed  74 labels. Loss 0.07458. Accuracy 0.977.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053667367
Train loss (w/o reg) on all data: 0.039813936
Test loss (w/o reg) on all data: 0.038030546
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3604955e-06
Norm of the params: 16.64538
                Loss: fixed  94 labels. Loss 0.03803. Accuracy 0.985.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23445062
Train loss (w/o reg) on all data: 0.22864243
Test loss (w/o reg) on all data: 0.12835711
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 0.0001236835
Norm of the params: 10.777931
              Random: fixed  10 labels. Loss 0.12836. Accuracy 0.969.
### Flips: 260, rs: 26, checks: 156
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07110062
Train loss (w/o reg) on all data: 0.0601385
Test loss (w/o reg) on all data: 0.053127356
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6673657e-05
Norm of the params: 14.806837
     Influence (LOO): fixed  94 labels. Loss 0.05313. Accuracy 0.969.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026296977
Train loss (w/o reg) on all data: 0.014799724
Test loss (w/o reg) on all data: 0.024186876
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4646073e-06
Norm of the params: 15.163939
                Loss: fixed 113 labels. Loss 0.02419. Accuracy 0.985.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2321396
Train loss (w/o reg) on all data: 0.22622387
Test loss (w/o reg) on all data: 0.12346347
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.2777504e-05
Norm of the params: 10.877254
              Random: fixed  11 labels. Loss 0.12346. Accuracy 0.973.
### Flips: 260, rs: 26, checks: 208
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05721985
Train loss (w/o reg) on all data: 0.047267307
Test loss (w/o reg) on all data: 0.030829
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3917029e-06
Norm of the params: 14.1085415
     Influence (LOO): fixed 104 labels. Loss 0.03083. Accuracy 0.992.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016171137
Train loss (w/o reg) on all data: 0.007684041
Test loss (w/o reg) on all data: 0.021821631
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.2910314e-07
Norm of the params: 13.028505
                Loss: fixed 119 labels. Loss 0.02182. Accuracy 0.992.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2247965
Train loss (w/o reg) on all data: 0.2189004
Test loss (w/o reg) on all data: 0.11846352
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.0561363e-05
Norm of the params: 10.859202
              Random: fixed  17 labels. Loss 0.11846. Accuracy 0.969.
### Flips: 260, rs: 26, checks: 260
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022417128
Train loss (w/o reg) on all data: 0.014422867
Test loss (w/o reg) on all data: 0.019238623
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0481053e-06
Norm of the params: 12.644573
     Influence (LOO): fixed 119 labels. Loss 0.01924. Accuracy 0.996.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011117976
Train loss (w/o reg) on all data: 0.004588948
Test loss (w/o reg) on all data: 0.013557291
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3914404e-07
Norm of the params: 11.427185
                Loss: fixed 123 labels. Loss 0.01356. Accuracy 0.992.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21639487
Train loss (w/o reg) on all data: 0.2106385
Test loss (w/o reg) on all data: 0.111593336
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.6298229e-05
Norm of the params: 10.729751
              Random: fixed  26 labels. Loss 0.11159. Accuracy 0.977.
### Flips: 260, rs: 26, checks: 312
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010222657
Train loss (w/o reg) on all data: 0.004692772
Test loss (w/o reg) on all data: 0.014832762
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0028086e-06
Norm of the params: 10.516544
     Influence (LOO): fixed 126 labels. Loss 0.01483. Accuracy 0.992.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0076635894
Train loss (w/o reg) on all data: 0.0027395943
Test loss (w/o reg) on all data: 0.012514115
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7126247e-07
Norm of the params: 9.923704
                Loss: fixed 126 labels. Loss 0.01251. Accuracy 0.992.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19960637
Train loss (w/o reg) on all data: 0.1932794
Test loss (w/o reg) on all data: 0.09500742
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7513694e-05
Norm of the params: 11.248971
              Random: fixed  36 labels. Loss 0.09501. Accuracy 0.977.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23836078
Train loss (w/o reg) on all data: 0.23078926
Test loss (w/o reg) on all data: 0.19455045
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 3.7020618e-05
Norm of the params: 12.305704
Flipped loss: 0.19455. Accuracy: 0.927
### Flips: 260, rs: 27, checks: 52
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15570149
Train loss (w/o reg) on all data: 0.14432366
Test loss (w/o reg) on all data: 0.17394027
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 2.551823e-05
Norm of the params: 15.084976
     Influence (LOO): fixed  42 labels. Loss 0.17394. Accuracy 0.912.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.117886335
Train loss (w/o reg) on all data: 0.10160239
Test loss (w/o reg) on all data: 0.19063993
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 7.5812427e-06
Norm of the params: 18.046574
                Loss: fixed  51 labels. Loss 0.19064. Accuracy 0.905.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22913559
Train loss (w/o reg) on all data: 0.22108598
Test loss (w/o reg) on all data: 0.18886757
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 3.286344e-05
Norm of the params: 12.688264
              Random: fixed   6 labels. Loss 0.18887. Accuracy 0.924.
### Flips: 260, rs: 27, checks: 104
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10377685
Train loss (w/o reg) on all data: 0.09170984
Test loss (w/o reg) on all data: 0.12688579
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.502283e-06
Norm of the params: 15.535129
     Influence (LOO): fixed  71 labels. Loss 0.12689. Accuracy 0.947.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055875774
Train loss (w/o reg) on all data: 0.038754422
Test loss (w/o reg) on all data: 0.13932827
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 9.4488005e-06
Norm of the params: 18.504784
                Loss: fixed  89 labels. Loss 0.13933. Accuracy 0.954.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22273396
Train loss (w/o reg) on all data: 0.21458903
Test loss (w/o reg) on all data: 0.18010755
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.726362e-05
Norm of the params: 12.763169
              Random: fixed  11 labels. Loss 0.18011. Accuracy 0.935.
### Flips: 260, rs: 27, checks: 156
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061404925
Train loss (w/o reg) on all data: 0.049349494
Test loss (w/o reg) on all data: 0.08606767
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.3088407e-06
Norm of the params: 15.527672
     Influence (LOO): fixed  91 labels. Loss 0.08607. Accuracy 0.969.
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029484358
Train loss (w/o reg) on all data: 0.017294474
Test loss (w/o reg) on all data: 0.061829466
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.878062e-07
Norm of the params: 15.614021
                Loss: fixed 109 labels. Loss 0.06183. Accuracy 0.981.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2109534
Train loss (w/o reg) on all data: 0.20243157
Test loss (w/o reg) on all data: 0.16573913
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.0052523e-05
Norm of the params: 13.055138
              Random: fixed  20 labels. Loss 0.16574. Accuracy 0.943.
### Flips: 260, rs: 27, checks: 208
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031091368
Train loss (w/o reg) on all data: 0.020616464
Test loss (w/o reg) on all data: 0.057344403
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7624358e-06
Norm of the params: 14.474049
     Influence (LOO): fixed 107 labels. Loss 0.05734. Accuracy 0.981.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016765729
Train loss (w/o reg) on all data: 0.008340131
Test loss (w/o reg) on all data: 0.028830402
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9514258e-06
Norm of the params: 12.9812155
                Loss: fixed 116 labels. Loss 0.02883. Accuracy 0.985.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2004382
Train loss (w/o reg) on all data: 0.19143823
Test loss (w/o reg) on all data: 0.15691862
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.2239299e-05
Norm of the params: 13.416392
              Random: fixed  26 labels. Loss 0.15692. Accuracy 0.947.
### Flips: 260, rs: 27, checks: 260
Using normal model
LBFGS training took [68] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013325656
Train loss (w/o reg) on all data: 0.006553397
Test loss (w/o reg) on all data: 0.0136779025
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.605536e-07
Norm of the params: 11.638093
     Influence (LOO): fixed 120 labels. Loss 0.01368. Accuracy 0.996.
Using normal model
LBFGS training took [74] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011138581
Train loss (w/o reg) on all data: 0.004636906
Test loss (w/o reg) on all data: 0.016424762
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6997254e-06
Norm of the params: 11.403224
                Loss: fixed 120 labels. Loss 0.01642. Accuracy 0.989.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19527444
Train loss (w/o reg) on all data: 0.18609805
Test loss (w/o reg) on all data: 0.15408365
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.1935986e-05
Norm of the params: 13.54724
              Random: fixed  30 labels. Loss 0.15408. Accuracy 0.947.
### Flips: 260, rs: 27, checks: 312
Using normal model
LBFGS training took [52] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.006362019
Train loss (w/o reg) on all data: 0.0021728387
Test loss (w/o reg) on all data: 0.012054728
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.535886e-07
Norm of the params: 9.153338
     Influence (LOO): fixed 124 labels. Loss 0.01205. Accuracy 0.992.
Using normal model
LBFGS training took [48] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0075961016
Train loss (w/o reg) on all data: 0.0028792133
Test loss (w/o reg) on all data: 0.013194008
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.340373e-07
Norm of the params: 9.712763
                Loss: fixed 123 labels. Loss 0.01319. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19094335
Train loss (w/o reg) on all data: 0.18185987
Test loss (w/o reg) on all data: 0.14916684
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.4385088e-05
Norm of the params: 13.478491
              Random: fixed  33 labels. Loss 0.14917. Accuracy 0.950.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2242951
Train loss (w/o reg) on all data: 0.21689416
Test loss (w/o reg) on all data: 0.15081888
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.5694475e-05
Norm of the params: 12.166292
Flipped loss: 0.15082. Accuracy: 0.947
### Flips: 260, rs: 28, checks: 52
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13322544
Train loss (w/o reg) on all data: 0.12079328
Test loss (w/o reg) on all data: 0.124748096
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.325496e-05
Norm of the params: 15.768426
     Influence (LOO): fixed  43 labels. Loss 0.12475. Accuracy 0.958.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10687999
Train loss (w/o reg) on all data: 0.0880342
Test loss (w/o reg) on all data: 0.10561231
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.0590665e-05
Norm of the params: 19.414318
                Loss: fixed  49 labels. Loss 0.10561. Accuracy 0.966.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2212129
Train loss (w/o reg) on all data: 0.21332766
Test loss (w/o reg) on all data: 0.15124163
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.435945e-05
Norm of the params: 12.558049
              Random: fixed   3 labels. Loss 0.15124. Accuracy 0.943.
### Flips: 260, rs: 28, checks: 104
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08502616
Train loss (w/o reg) on all data: 0.07031919
Test loss (w/o reg) on all data: 0.11124264
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.2173767e-05
Norm of the params: 17.150494
     Influence (LOO): fixed  72 labels. Loss 0.11124. Accuracy 0.969.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05677837
Train loss (w/o reg) on all data: 0.03949441
Test loss (w/o reg) on all data: 0.058018036
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.22030215e-05
Norm of the params: 18.59245
                Loss: fixed  83 labels. Loss 0.05802. Accuracy 0.969.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21712355
Train loss (w/o reg) on all data: 0.20866786
Test loss (w/o reg) on all data: 0.14765023
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.5108836e-05
Norm of the params: 13.004378
              Random: fixed   6 labels. Loss 0.14765. Accuracy 0.950.
### Flips: 260, rs: 28, checks: 156
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05745908
Train loss (w/o reg) on all data: 0.044150077
Test loss (w/o reg) on all data: 0.06334309
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.3423514e-05
Norm of the params: 16.315027
     Influence (LOO): fixed  90 labels. Loss 0.06334. Accuracy 0.981.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03224992
Train loss (w/o reg) on all data: 0.019695688
Test loss (w/o reg) on all data: 0.0325762
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1281e-06
Norm of the params: 15.84565
                Loss: fixed 103 labels. Loss 0.03258. Accuracy 0.992.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20795524
Train loss (w/o reg) on all data: 0.19900617
Test loss (w/o reg) on all data: 0.14446606
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.0574816e-05
Norm of the params: 13.378395
              Random: fixed  13 labels. Loss 0.14447. Accuracy 0.939.
### Flips: 260, rs: 28, checks: 208
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026897453
Train loss (w/o reg) on all data: 0.017202074
Test loss (w/o reg) on all data: 0.033793237
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.421472e-06
Norm of the params: 13.92507
     Influence (LOO): fixed 110 labels. Loss 0.03379. Accuracy 0.989.
Using normal model
LBFGS training took [75] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021340583
Train loss (w/o reg) on all data: 0.0114901075
Test loss (w/o reg) on all data: 0.018428206
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.1627377e-06
Norm of the params: 14.036008
                Loss: fixed 113 labels. Loss 0.01843. Accuracy 0.996.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20185585
Train loss (w/o reg) on all data: 0.19279951
Test loss (w/o reg) on all data: 0.13812117
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.2740568e-05
Norm of the params: 13.458336
              Random: fixed  19 labels. Loss 0.13812. Accuracy 0.939.
### Flips: 260, rs: 28, checks: 260
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021211833
Train loss (w/o reg) on all data: 0.012309061
Test loss (w/o reg) on all data: 0.030079652
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1754174e-06
Norm of the params: 13.343742
     Influence (LOO): fixed 113 labels. Loss 0.03008. Accuracy 0.992.
Using normal model
LBFGS training took [97] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01619706
Train loss (w/o reg) on all data: 0.00793509
Test loss (w/o reg) on all data: 0.012726191
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.529605e-06
Norm of the params: 12.8545475
                Loss: fixed 117 labels. Loss 0.01273. Accuracy 0.996.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18440962
Train loss (w/o reg) on all data: 0.17461003
Test loss (w/o reg) on all data: 0.123501755
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4768565e-05
Norm of the params: 13.9997015
              Random: fixed  28 labels. Loss 0.12350. Accuracy 0.958.
### Flips: 260, rs: 28, checks: 312
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016540589
Train loss (w/o reg) on all data: 0.008961193
Test loss (w/o reg) on all data: 0.020176811
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.9975525e-07
Norm of the params: 12.312103
     Influence (LOO): fixed 117 labels. Loss 0.02018. Accuracy 0.996.
Using normal model
LBFGS training took [47] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0115137445
Train loss (w/o reg) on all data: 0.0048666284
Test loss (w/o reg) on all data: 0.013354948
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 2.3512134e-06
Norm of the params: 11.530062
                Loss: fixed 120 labels. Loss 0.01335. Accuracy 0.996.
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18191667
Train loss (w/o reg) on all data: 0.17230353
Test loss (w/o reg) on all data: 0.119545214
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.499981e-05
Norm of the params: 13.865888
              Random: fixed  34 labels. Loss 0.11955. Accuracy 0.958.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24832396
Train loss (w/o reg) on all data: 0.24188945
Test loss (w/o reg) on all data: 0.14786026
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.66524e-05
Norm of the params: 11.344179
Flipped loss: 0.14786. Accuracy: 0.962
### Flips: 260, rs: 29, checks: 52
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1661151
Train loss (w/o reg) on all data: 0.1539224
Test loss (w/o reg) on all data: 0.10700502
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.490966e-05
Norm of the params: 15.61583
     Influence (LOO): fixed  42 labels. Loss 0.10701. Accuracy 0.969.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12345274
Train loss (w/o reg) on all data: 0.10729765
Test loss (w/o reg) on all data: 0.10786359
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.0174593e-05
Norm of the params: 17.975033
                Loss: fixed  52 labels. Loss 0.10786. Accuracy 0.939.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24673696
Train loss (w/o reg) on all data: 0.24022262
Test loss (w/o reg) on all data: 0.14742738
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.7012597e-05
Norm of the params: 11.414324
              Random: fixed   1 labels. Loss 0.14743. Accuracy 0.962.
### Flips: 260, rs: 29, checks: 104
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10792862
Train loss (w/o reg) on all data: 0.09562671
Test loss (w/o reg) on all data: 0.08013354
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.229952e-06
Norm of the params: 15.685603
     Influence (LOO): fixed  75 labels. Loss 0.08013. Accuracy 0.977.
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047337905
Train loss (w/o reg) on all data: 0.03196494
Test loss (w/o reg) on all data: 0.052334525
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.0006007e-06
Norm of the params: 17.534517
                Loss: fixed  95 labels. Loss 0.05233. Accuracy 0.981.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23768604
Train loss (w/o reg) on all data: 0.23100452
Test loss (w/o reg) on all data: 0.13814417
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.1583692e-05
Norm of the params: 11.559853
              Random: fixed   8 labels. Loss 0.13814. Accuracy 0.962.
### Flips: 260, rs: 29, checks: 156
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06465921
Train loss (w/o reg) on all data: 0.054855265
Test loss (w/o reg) on all data: 0.044332933
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8825814e-06
Norm of the params: 14.002818
     Influence (LOO): fixed 100 labels. Loss 0.04433. Accuracy 0.985.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022064652
Train loss (w/o reg) on all data: 0.011802975
Test loss (w/o reg) on all data: 0.033382248
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9415595e-06
Norm of the params: 14.325974
                Loss: fixed 114 labels. Loss 0.03338. Accuracy 0.989.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22688636
Train loss (w/o reg) on all data: 0.21986583
Test loss (w/o reg) on all data: 0.1319124
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.014584e-05
Norm of the params: 11.849501
              Random: fixed  16 labels. Loss 0.13191. Accuracy 0.973.
### Flips: 260, rs: 29, checks: 208
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021981642
Train loss (w/o reg) on all data: 0.013567948
Test loss (w/o reg) on all data: 0.039241917
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.384699e-07
Norm of the params: 12.972042
     Influence (LOO): fixed 117 labels. Loss 0.03924. Accuracy 0.981.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013029581
Train loss (w/o reg) on all data: 0.005804723
Test loss (w/o reg) on all data: 0.024382515
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.209266e-07
Norm of the params: 12.020698
                Loss: fixed 121 labels. Loss 0.02438. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21838915
Train loss (w/o reg) on all data: 0.21082257
Test loss (w/o reg) on all data: 0.13511236
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.6242595e-05
Norm of the params: 12.30169
              Random: fixed  21 labels. Loss 0.13511. Accuracy 0.973.
### Flips: 260, rs: 29, checks: 260
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015247218
Train loss (w/o reg) on all data: 0.0091089085
Test loss (w/o reg) on all data: 0.022204995
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.110693e-07
Norm of the params: 11.07999
     Influence (LOO): fixed 122 labels. Loss 0.02220. Accuracy 0.989.
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010583786
Train loss (w/o reg) on all data: 0.0044172336
Test loss (w/o reg) on all data: 0.016095309
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.013953e-06
Norm of the params: 11.105453
                Loss: fixed 123 labels. Loss 0.01610. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20609218
Train loss (w/o reg) on all data: 0.19772734
Test loss (w/o reg) on all data: 0.12028567
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5569778e-05
Norm of the params: 12.934321
              Random: fixed  29 labels. Loss 0.12029. Accuracy 0.973.
### Flips: 260, rs: 29, checks: 312
Using normal model
LBFGS training took [64] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010139596
Train loss (w/o reg) on all data: 0.005022721
Test loss (w/o reg) on all data: 0.0113036465
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.79174e-07
Norm of the params: 10.1161995
     Influence (LOO): fixed 124 labels. Loss 0.01130. Accuracy 0.996.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009217086
Train loss (w/o reg) on all data: 0.0036519768
Test loss (w/o reg) on all data: 0.015065484
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.7964668e-07
Norm of the params: 10.549986
                Loss: fixed 124 labels. Loss 0.01507. Accuracy 0.992.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18924947
Train loss (w/o reg) on all data: 0.180443
Test loss (w/o reg) on all data: 0.10307234
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.299841e-05
Norm of the params: 13.271372
              Random: fixed  39 labels. Loss 0.10307. Accuracy 0.985.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24144822
Train loss (w/o reg) on all data: 0.23513925
Test loss (w/o reg) on all data: 0.1709293
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.7758727e-05
Norm of the params: 11.232959
Flipped loss: 0.17093. Accuracy: 0.935
### Flips: 260, rs: 30, checks: 52
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15985847
Train loss (w/o reg) on all data: 0.14835814
Test loss (w/o reg) on all data: 0.12769353
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.8693728e-05
Norm of the params: 15.165971
     Influence (LOO): fixed  39 labels. Loss 0.12769. Accuracy 0.950.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116457194
Train loss (w/o reg) on all data: 0.1007328
Test loss (w/o reg) on all data: 0.11773662
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.4814885e-05
Norm of the params: 17.733805
                Loss: fixed  52 labels. Loss 0.11774. Accuracy 0.954.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2383019
Train loss (w/o reg) on all data: 0.23203564
Test loss (w/o reg) on all data: 0.16842045
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.5612943e-05
Norm of the params: 11.19488
              Random: fixed   4 labels. Loss 0.16842. Accuracy 0.931.
### Flips: 260, rs: 30, checks: 104
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10473603
Train loss (w/o reg) on all data: 0.09107502
Test loss (w/o reg) on all data: 0.09678139
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.8229856e-06
Norm of the params: 16.529377
     Influence (LOO): fixed  69 labels. Loss 0.09678. Accuracy 0.958.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05607091
Train loss (w/o reg) on all data: 0.037960824
Test loss (w/o reg) on all data: 0.078563385
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.3395795e-06
Norm of the params: 19.031597
                Loss: fixed  89 labels. Loss 0.07856. Accuracy 0.969.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23125245
Train loss (w/o reg) on all data: 0.22455773
Test loss (w/o reg) on all data: 0.15969913
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.4877437e-05
Norm of the params: 11.571278
              Random: fixed   8 labels. Loss 0.15970. Accuracy 0.947.
### Flips: 260, rs: 30, checks: 156
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.081140555
Train loss (w/o reg) on all data: 0.068542965
Test loss (w/o reg) on all data: 0.06202778
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.4566804e-06
Norm of the params: 15.872992
     Influence (LOO): fixed  87 labels. Loss 0.06203. Accuracy 0.973.
Using normal model
LBFGS training took [144] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035027344
Train loss (w/o reg) on all data: 0.020433182
Test loss (w/o reg) on all data: 0.05054434
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.2450753e-06
Norm of the params: 17.08459
                Loss: fixed 106 labels. Loss 0.05054. Accuracy 0.985.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22159421
Train loss (w/o reg) on all data: 0.21474895
Test loss (w/o reg) on all data: 0.15380618
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.7646098e-05
Norm of the params: 11.700659
              Random: fixed  16 labels. Loss 0.15381. Accuracy 0.950.
### Flips: 260, rs: 30, checks: 208
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045647502
Train loss (w/o reg) on all data: 0.033058856
Test loss (w/o reg) on all data: 0.06695204
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.927251e-06
Norm of the params: 15.867355
     Influence (LOO): fixed 106 labels. Loss 0.06695. Accuracy 0.973.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027825706
Train loss (w/o reg) on all data: 0.015992371
Test loss (w/o reg) on all data: 0.032057796
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8546367e-06
Norm of the params: 15.383975
                Loss: fixed 117 labels. Loss 0.03206. Accuracy 0.985.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21619356
Train loss (w/o reg) on all data: 0.2092749
Test loss (w/o reg) on all data: 0.14523235
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.2807395e-05
Norm of the params: 11.763215
              Random: fixed  23 labels. Loss 0.14523. Accuracy 0.954.
### Flips: 260, rs: 30, checks: 260
Using normal model
LBFGS training took [69] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026376765
Train loss (w/o reg) on all data: 0.014970012
Test loss (w/o reg) on all data: 0.03522231
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.4193258e-06
Norm of the params: 15.10414
     Influence (LOO): fixed 116 labels. Loss 0.03522. Accuracy 0.985.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016498234
Train loss (w/o reg) on all data: 0.008304104
Test loss (w/o reg) on all data: 0.021645704
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.312397e-07
Norm of the params: 12.801664
                Loss: fixed 123 labels. Loss 0.02165. Accuracy 0.992.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2129213
Train loss (w/o reg) on all data: 0.20609161
Test loss (w/o reg) on all data: 0.13688943
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3109461e-05
Norm of the params: 11.687338
              Random: fixed  26 labels. Loss 0.13689. Accuracy 0.962.
### Flips: 260, rs: 30, checks: 312
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020954806
Train loss (w/o reg) on all data: 0.011341594
Test loss (w/o reg) on all data: 0.027696952
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.829118e-06
Norm of the params: 13.865937
     Influence (LOO): fixed 121 labels. Loss 0.02770. Accuracy 0.992.
Using normal model
LBFGS training took [57] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010744266
Train loss (w/o reg) on all data: 0.004630105
Test loss (w/o reg) on all data: 0.018046904
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.4198127e-07
Norm of the params: 11.058174
                Loss: fixed 127 labels. Loss 0.01805. Accuracy 0.992.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20059448
Train loss (w/o reg) on all data: 0.19325295
Test loss (w/o reg) on all data: 0.12931217
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.8969755e-05
Norm of the params: 12.117372
              Random: fixed  33 labels. Loss 0.12931. Accuracy 0.969.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23431963
Train loss (w/o reg) on all data: 0.2257844
Test loss (w/o reg) on all data: 0.14728968
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.6646194e-05
Norm of the params: 13.065389
Flipped loss: 0.14729. Accuracy: 0.954
### Flips: 260, rs: 31, checks: 52
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14770114
Train loss (w/o reg) on all data: 0.13609217
Test loss (w/o reg) on all data: 0.104190275
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.7719746e-05
Norm of the params: 15.237439
     Influence (LOO): fixed  43 labels. Loss 0.10419. Accuracy 0.962.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11435435
Train loss (w/o reg) on all data: 0.09699754
Test loss (w/o reg) on all data: 0.09647539
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.1795215e-06
Norm of the params: 18.631592
                Loss: fixed  50 labels. Loss 0.09648. Accuracy 0.958.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21758398
Train loss (w/o reg) on all data: 0.20869191
Test loss (w/o reg) on all data: 0.13337809
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.0028368e-05
Norm of the params: 13.335724
              Random: fixed  13 labels. Loss 0.13338. Accuracy 0.958.
### Flips: 260, rs: 31, checks: 104
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.094924256
Train loss (w/o reg) on all data: 0.08285158
Test loss (w/o reg) on all data: 0.05732612
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.215494e-06
Norm of the params: 15.538777
     Influence (LOO): fixed  71 labels. Loss 0.05733. Accuracy 0.989.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0587508
Train loss (w/o reg) on all data: 0.04242136
Test loss (w/o reg) on all data: 0.042349536
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5411062e-05
Norm of the params: 18.07177
                Loss: fixed  89 labels. Loss 0.04235. Accuracy 0.985.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2088724
Train loss (w/o reg) on all data: 0.1997112
Test loss (w/o reg) on all data: 0.13028976
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.7082786e-05
Norm of the params: 13.536015
              Random: fixed  18 labels. Loss 0.13029. Accuracy 0.950.
### Flips: 260, rs: 31, checks: 156
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061644915
Train loss (w/o reg) on all data: 0.049843013
Test loss (w/o reg) on all data: 0.039763562
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.507235e-06
Norm of the params: 15.363529
     Influence (LOO): fixed  90 labels. Loss 0.03976. Accuracy 0.989.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02740401
Train loss (w/o reg) on all data: 0.015106388
Test loss (w/o reg) on all data: 0.029674115
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.1078492e-06
Norm of the params: 15.682871
                Loss: fixed 111 labels. Loss 0.02967. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20406626
Train loss (w/o reg) on all data: 0.19536193
Test loss (w/o reg) on all data: 0.12467215
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.5292984e-05
Norm of the params: 13.194193
              Random: fixed  24 labels. Loss 0.12467. Accuracy 0.958.
### Flips: 260, rs: 31, checks: 208
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034918346
Train loss (w/o reg) on all data: 0.02630715
Test loss (w/o reg) on all data: 0.031173375
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3125483e-05
Norm of the params: 13.123412
     Influence (LOO): fixed 111 labels. Loss 0.03117. Accuracy 0.985.
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0115527045
Train loss (w/o reg) on all data: 0.0048176204
Test loss (w/o reg) on all data: 0.019851724
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.7387075e-07
Norm of the params: 11.606106
                Loss: fixed 122 labels. Loss 0.01985. Accuracy 0.989.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20434006
Train loss (w/o reg) on all data: 0.19584592
Test loss (w/o reg) on all data: 0.123197496
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.379887e-05
Norm of the params: 13.033913
              Random: fixed  28 labels. Loss 0.12320. Accuracy 0.958.
### Flips: 260, rs: 31, checks: 260
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01748516
Train loss (w/o reg) on all data: 0.010149733
Test loss (w/o reg) on all data: 0.020355871
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3581564e-06
Norm of the params: 12.11233
     Influence (LOO): fixed 120 labels. Loss 0.02036. Accuracy 0.992.
Using normal model
LBFGS training took [84] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010784101
Train loss (w/o reg) on all data: 0.0044032354
Test loss (w/o reg) on all data: 0.01761813
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.250168e-07
Norm of the params: 11.296783
                Loss: fixed 123 labels. Loss 0.01762. Accuracy 0.989.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19866195
Train loss (w/o reg) on all data: 0.19031303
Test loss (w/o reg) on all data: 0.119712844
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.4372857e-05
Norm of the params: 12.922018
              Random: fixed  32 labels. Loss 0.11971. Accuracy 0.958.
### Flips: 260, rs: 31, checks: 312
Using normal model
LBFGS training took [76] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008973278
Train loss (w/o reg) on all data: 0.003692984
Test loss (w/o reg) on all data: 0.015392407
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0000668e-07
Norm of the params: 10.276471
     Influence (LOO): fixed 127 labels. Loss 0.01539. Accuracy 0.992.
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010784103
Train loss (w/o reg) on all data: 0.004403723
Test loss (w/o reg) on all data: 0.0176201
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3569014e-06
Norm of the params: 11.296353
                Loss: fixed 123 labels. Loss 0.01762. Accuracy 0.989.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18506984
Train loss (w/o reg) on all data: 0.17626446
Test loss (w/o reg) on all data: 0.10871151
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.5262765e-05
Norm of the params: 13.27055
              Random: fixed  40 labels. Loss 0.10871. Accuracy 0.966.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24994285
Train loss (w/o reg) on all data: 0.24338028
Test loss (w/o reg) on all data: 0.17421445
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.786144e-05
Norm of the params: 11.4565115
Flipped loss: 0.17421. Accuracy: 0.943
### Flips: 260, rs: 32, checks: 52
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18215166
Train loss (w/o reg) on all data: 0.17110273
Test loss (w/o reg) on all data: 0.13074082
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.33251e-05
Norm of the params: 14.865349
     Influence (LOO): fixed  38 labels. Loss 0.13074. Accuracy 0.947.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13326496
Train loss (w/o reg) on all data: 0.11810242
Test loss (w/o reg) on all data: 0.15641505
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.2564408e-05
Norm of the params: 17.414103
                Loss: fixed  50 labels. Loss 0.15642. Accuracy 0.924.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2412526
Train loss (w/o reg) on all data: 0.23426153
Test loss (w/o reg) on all data: 0.16898973
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.5254793e-05
Norm of the params: 11.824618
              Random: fixed   7 labels. Loss 0.16899. Accuracy 0.924.
### Flips: 260, rs: 32, checks: 104
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13350885
Train loss (w/o reg) on all data: 0.12215845
Test loss (w/o reg) on all data: 0.09703841
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.6443404e-05
Norm of the params: 15.066777
     Influence (LOO): fixed  67 labels. Loss 0.09704. Accuracy 0.969.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0635297
Train loss (w/o reg) on all data: 0.04670956
Test loss (w/o reg) on all data: 0.09137627
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.0344395e-05
Norm of the params: 18.341286
                Loss: fixed  87 labels. Loss 0.09138. Accuracy 0.973.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23947157
Train loss (w/o reg) on all data: 0.23271425
Test loss (w/o reg) on all data: 0.1613418
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.4116117e-05
Norm of the params: 11.625246
              Random: fixed  12 labels. Loss 0.16134. Accuracy 0.943.
### Flips: 260, rs: 32, checks: 156
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09806058
Train loss (w/o reg) on all data: 0.08768136
Test loss (w/o reg) on all data: 0.08049293
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.768059e-05
Norm of the params: 14.407786
     Influence (LOO): fixed  90 labels. Loss 0.08049. Accuracy 0.973.
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.048005242
Train loss (w/o reg) on all data: 0.03280831
Test loss (w/o reg) on all data: 0.08406699
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.15256e-06
Norm of the params: 17.433836
                Loss: fixed 104 labels. Loss 0.08407. Accuracy 0.977.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2321121
Train loss (w/o reg) on all data: 0.22514582
Test loss (w/o reg) on all data: 0.15439118
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.8926805e-05
Norm of the params: 11.80363
              Random: fixed  19 labels. Loss 0.15439. Accuracy 0.947.
### Flips: 260, rs: 32, checks: 208
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047127336
Train loss (w/o reg) on all data: 0.035565738
Test loss (w/o reg) on all data: 0.08107454
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.6006066e-06
Norm of the params: 15.206312
     Influence (LOO): fixed 114 labels. Loss 0.08107. Accuracy 0.973.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033495877
Train loss (w/o reg) on all data: 0.02134242
Test loss (w/o reg) on all data: 0.040852755
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 8.839813e-06
Norm of the params: 15.590675
                Loss: fixed 121 labels. Loss 0.04085. Accuracy 0.989.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2282308
Train loss (w/o reg) on all data: 0.22091417
Test loss (w/o reg) on all data: 0.14781609
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.01322485e-05
Norm of the params: 12.09681
              Random: fixed  23 labels. Loss 0.14782. Accuracy 0.947.
### Flips: 260, rs: 32, checks: 260
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028764851
Train loss (w/o reg) on all data: 0.018250447
Test loss (w/o reg) on all data: 0.048882045
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.5688046e-06
Norm of the params: 14.501313
     Influence (LOO): fixed 125 labels. Loss 0.04888. Accuracy 0.985.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017688327
Train loss (w/o reg) on all data: 0.009347826
Test loss (w/o reg) on all data: 0.024306977
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.584802e-07
Norm of the params: 12.915493
                Loss: fixed 132 labels. Loss 0.02431. Accuracy 0.989.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22063626
Train loss (w/o reg) on all data: 0.21361314
Test loss (w/o reg) on all data: 0.13298437
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.0849839e-05
Norm of the params: 11.851688
              Random: fixed  32 labels. Loss 0.13298. Accuracy 0.973.
### Flips: 260, rs: 32, checks: 312
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01455416
Train loss (w/o reg) on all data: 0.007334537
Test loss (w/o reg) on all data: 0.017693926
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.3181405e-07
Norm of the params: 12.016341
     Influence (LOO): fixed 134 labels. Loss 0.01769. Accuracy 0.989.
Using normal model
LBFGS training took [67] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01372779
Train loss (w/o reg) on all data: 0.00678761
Test loss (w/o reg) on all data: 0.017755467
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.9956574e-06
Norm of the params: 11.781494
                Loss: fixed 136 labels. Loss 0.01776. Accuracy 0.992.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21001308
Train loss (w/o reg) on all data: 0.20273192
Test loss (w/o reg) on all data: 0.123361096
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.096632e-05
Norm of the params: 12.067439
              Random: fixed  39 labels. Loss 0.12336. Accuracy 0.977.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24360698
Train loss (w/o reg) on all data: 0.2371925
Test loss (w/o reg) on all data: 0.19828409
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 3.3831533e-05
Norm of the params: 11.326501
Flipped loss: 0.19828. Accuracy: 0.931
### Flips: 260, rs: 33, checks: 52
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16022919
Train loss (w/o reg) on all data: 0.14825252
Test loss (w/o reg) on all data: 0.13901785
Train acc on all data:  0.938872970391595
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.147514e-05
Norm of the params: 15.476871
     Influence (LOO): fixed  41 labels. Loss 0.13902. Accuracy 0.950.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.109776594
Train loss (w/o reg) on all data: 0.09180018
Test loss (w/o reg) on all data: 0.16751142
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.3964709e-05
Norm of the params: 18.961227
                Loss: fixed  52 labels. Loss 0.16751. Accuracy 0.943.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23798533
Train loss (w/o reg) on all data: 0.2314531
Test loss (w/o reg) on all data: 0.1883195
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.979718e-05
Norm of the params: 11.429979
              Random: fixed   5 labels. Loss 0.18832. Accuracy 0.931.
### Flips: 260, rs: 33, checks: 104
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09695518
Train loss (w/o reg) on all data: 0.08478682
Test loss (w/o reg) on all data: 0.09061612
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.8419993e-06
Norm of the params: 15.600232
     Influence (LOO): fixed  79 labels. Loss 0.09062. Accuracy 0.969.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.039003365
Train loss (w/o reg) on all data: 0.023951959
Test loss (w/o reg) on all data: 0.05159756
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.80811e-06
Norm of the params: 17.350163
                Loss: fixed  96 labels. Loss 0.05160. Accuracy 0.981.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2309377
Train loss (w/o reg) on all data: 0.22428668
Test loss (w/o reg) on all data: 0.19206634
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 7.375185e-06
Norm of the params: 11.533452
              Random: fixed  11 labels. Loss 0.19207. Accuracy 0.931.
### Flips: 260, rs: 33, checks: 156
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060912006
Train loss (w/o reg) on all data: 0.04824661
Test loss (w/o reg) on all data: 0.073390186
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.0604426e-06
Norm of the params: 15.915648
     Influence (LOO): fixed  96 labels. Loss 0.07339. Accuracy 0.981.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02236501
Train loss (w/o reg) on all data: 0.01115224
Test loss (w/o reg) on all data: 0.050241727
Train acc on all data:  1.0
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.23287e-07
Norm of the params: 14.975159
                Loss: fixed 110 labels. Loss 0.05024. Accuracy 0.981.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2176903
Train loss (w/o reg) on all data: 0.21082449
Test loss (w/o reg) on all data: 0.18371382
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 9.305172e-05
Norm of the params: 11.718197
              Random: fixed  19 labels. Loss 0.18371. Accuracy 0.939.
### Flips: 260, rs: 33, checks: 208
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038087327
Train loss (w/o reg) on all data: 0.027147803
Test loss (w/o reg) on all data: 0.033568036
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.6287498e-06
Norm of the params: 14.791567
     Influence (LOO): fixed 109 labels. Loss 0.03357. Accuracy 0.989.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017978044
Train loss (w/o reg) on all data: 0.007950837
Test loss (w/o reg) on all data: 0.0437775
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0216236e-06
Norm of the params: 14.161362
                Loss: fixed 113 labels. Loss 0.04378. Accuracy 0.985.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20828313
Train loss (w/o reg) on all data: 0.20093632
Test loss (w/o reg) on all data: 0.1649789
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.4370365e-05
Norm of the params: 12.121724
              Random: fixed  26 labels. Loss 0.16498. Accuracy 0.950.
### Flips: 260, rs: 33, checks: 260
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018084085
Train loss (w/o reg) on all data: 0.0100321425
Test loss (w/o reg) on all data: 0.016381133
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.80773e-07
Norm of the params: 12.690107
     Influence (LOO): fixed 119 labels. Loss 0.01638. Accuracy 0.992.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017272584
Train loss (w/o reg) on all data: 0.0075816615
Test loss (w/o reg) on all data: 0.041041248
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.07655e-07
Norm of the params: 13.921869
                Loss: fixed 114 labels. Loss 0.04104. Accuracy 0.985.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19893584
Train loss (w/o reg) on all data: 0.19175045
Test loss (w/o reg) on all data: 0.13805954
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.525565e-05
Norm of the params: 11.987816
              Random: fixed  32 labels. Loss 0.13806. Accuracy 0.962.
### Flips: 260, rs: 33, checks: 312
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01499587
Train loss (w/o reg) on all data: 0.008234583
Test loss (w/o reg) on all data: 0.021163754
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8892497e-06
Norm of the params: 11.628661
     Influence (LOO): fixed 124 labels. Loss 0.02116. Accuracy 0.989.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017035626
Train loss (w/o reg) on all data: 0.007686677
Test loss (w/o reg) on all data: 0.040392704
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0961813e-06
Norm of the params: 13.674026
                Loss: fixed 116 labels. Loss 0.04039. Accuracy 0.985.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18852247
Train loss (w/o reg) on all data: 0.18108112
Test loss (w/o reg) on all data: 0.12557669
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 8.477906e-06
Norm of the params: 12.199471
              Random: fixed  40 labels. Loss 0.12558. Accuracy 0.962.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25897083
Train loss (w/o reg) on all data: 0.25268328
Test loss (w/o reg) on all data: 0.16661581
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.1317845e-05
Norm of the params: 11.213882
Flipped loss: 0.16662. Accuracy: 0.962
### Flips: 260, rs: 34, checks: 52
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18642549
Train loss (w/o reg) on all data: 0.17714882
Test loss (w/o reg) on all data: 0.12565726
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.964679e-06
Norm of the params: 13.621071
     Influence (LOO): fixed  41 labels. Loss 0.12566. Accuracy 0.973.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1431764
Train loss (w/o reg) on all data: 0.12867136
Test loss (w/o reg) on all data: 0.13622236
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.0688136e-05
Norm of the params: 17.032349
                Loss: fixed  51 labels. Loss 0.13622. Accuracy 0.947.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24971217
Train loss (w/o reg) on all data: 0.24376948
Test loss (w/o reg) on all data: 0.15192826
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.248161e-05
Norm of the params: 10.902006
              Random: fixed   7 labels. Loss 0.15193. Accuracy 0.969.
### Flips: 260, rs: 34, checks: 104
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1358219
Train loss (w/o reg) on all data: 0.12476402
Test loss (w/o reg) on all data: 0.07989153
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8548899e-05
Norm of the params: 14.8713665
     Influence (LOO): fixed  70 labels. Loss 0.07989. Accuracy 0.985.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062109645
Train loss (w/o reg) on all data: 0.044669654
Test loss (w/o reg) on all data: 0.07828378
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.8676818e-06
Norm of the params: 18.676184
                Loss: fixed  97 labels. Loss 0.07828. Accuracy 0.973.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23547632
Train loss (w/o reg) on all data: 0.2288054
Test loss (w/o reg) on all data: 0.14272988
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.4592269e-05
Norm of the params: 11.550689
              Random: fixed  16 labels. Loss 0.14273. Accuracy 0.962.
### Flips: 260, rs: 34, checks: 156
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08372682
Train loss (w/o reg) on all data: 0.07273123
Test loss (w/o reg) on all data: 0.057667423
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 4.930093e-06
Norm of the params: 14.829431
     Influence (LOO): fixed  95 labels. Loss 0.05767. Accuracy 0.981.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033759825
Train loss (w/o reg) on all data: 0.021492397
Test loss (w/o reg) on all data: 0.03963989
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.959712e-06
Norm of the params: 15.663606
                Loss: fixed 119 labels. Loss 0.03964. Accuracy 0.981.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23176841
Train loss (w/o reg) on all data: 0.22531788
Test loss (w/o reg) on all data: 0.13869515
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.3576638e-05
Norm of the params: 11.358293
              Random: fixed  21 labels. Loss 0.13870. Accuracy 0.973.
### Flips: 260, rs: 34, checks: 208
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044570662
Train loss (w/o reg) on all data: 0.034975383
Test loss (w/o reg) on all data: 0.027236862
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.3807726e-06
Norm of the params: 13.852997
     Influence (LOO): fixed 117 labels. Loss 0.02724. Accuracy 0.992.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019442149
Train loss (w/o reg) on all data: 0.009880327
Test loss (w/o reg) on all data: 0.01611246
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.545876e-06
Norm of the params: 13.828827
                Loss: fixed 127 labels. Loss 0.01611. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22774105
Train loss (w/o reg) on all data: 0.22125213
Test loss (w/o reg) on all data: 0.1368002
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.9387985e-05
Norm of the params: 11.392033
              Random: fixed  25 labels. Loss 0.13680. Accuracy 0.977.
### Flips: 260, rs: 34, checks: 260
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02160785
Train loss (w/o reg) on all data: 0.014583921
Test loss (w/o reg) on all data: 0.017389705
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8682653e-06
Norm of the params: 11.8523655
     Influence (LOO): fixed 129 labels. Loss 0.01739. Accuracy 0.992.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016149249
Train loss (w/o reg) on all data: 0.007771256
Test loss (w/o reg) on all data: 0.017734164
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3961587e-06
Norm of the params: 12.944491
                Loss: fixed 130 labels. Loss 0.01773. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21106865
Train loss (w/o reg) on all data: 0.20454717
Test loss (w/o reg) on all data: 0.12903981
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.0880747e-06
Norm of the params: 11.420578
              Random: fixed  37 labels. Loss 0.12904. Accuracy 0.973.
### Flips: 260, rs: 34, checks: 312
Using normal model
LBFGS training took [87] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012764127
Train loss (w/o reg) on all data: 0.0065624216
Test loss (w/o reg) on all data: 0.017765729
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.0730184e-07
Norm of the params: 11.13706
     Influence (LOO): fixed 133 labels. Loss 0.01777. Accuracy 0.992.
Using normal model
LBFGS training took [63] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010653771
Train loss (w/o reg) on all data: 0.0044072503
Test loss (w/o reg) on all data: 0.017269015
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.427844e-07
Norm of the params: 11.177228
                Loss: fixed 133 labels. Loss 0.01727. Accuracy 0.992.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.195482
Train loss (w/o reg) on all data: 0.18798758
Test loss (w/o reg) on all data: 0.12178479
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6943362e-05
Norm of the params: 12.242894
              Random: fixed  47 labels. Loss 0.12178. Accuracy 0.973.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25500056
Train loss (w/o reg) on all data: 0.24794526
Test loss (w/o reg) on all data: 0.17786948
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.8256087e-05
Norm of the params: 11.878801
Flipped loss: 0.17787. Accuracy: 0.939
### Flips: 260, rs: 35, checks: 52
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18395345
Train loss (w/o reg) on all data: 0.17163017
Test loss (w/o reg) on all data: 0.16443422
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 7.753442e-06
Norm of the params: 15.699224
     Influence (LOO): fixed  40 labels. Loss 0.16443. Accuracy 0.924.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13867491
Train loss (w/o reg) on all data: 0.122658364
Test loss (w/o reg) on all data: 0.14170922
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.4328164e-05
Norm of the params: 17.897799
                Loss: fixed  51 labels. Loss 0.14171. Accuracy 0.939.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24890225
Train loss (w/o reg) on all data: 0.24213016
Test loss (w/o reg) on all data: 0.16829585
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 0.00018209417
Norm of the params: 11.637942
              Random: fixed   7 labels. Loss 0.16830. Accuracy 0.954.
### Flips: 260, rs: 35, checks: 104
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12964733
Train loss (w/o reg) on all data: 0.117146514
Test loss (w/o reg) on all data: 0.08610785
Train acc on all data:  0.9541547277936963
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.7937864e-05
Norm of the params: 15.811904
     Influence (LOO): fixed  70 labels. Loss 0.08611. Accuracy 0.969.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061495937
Train loss (w/o reg) on all data: 0.04452416
Test loss (w/o reg) on all data: 0.08167251
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.5862957e-05
Norm of the params: 18.423779
                Loss: fixed  94 labels. Loss 0.08167. Accuracy 0.969.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2389565
Train loss (w/o reg) on all data: 0.23151793
Test loss (w/o reg) on all data: 0.15507574
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.3730154e-05
Norm of the params: 12.197185
              Random: fixed  13 labels. Loss 0.15508. Accuracy 0.950.
### Flips: 260, rs: 35, checks: 156
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09328552
Train loss (w/o reg) on all data: 0.08093736
Test loss (w/o reg) on all data: 0.0587036
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.6959423e-06
Norm of the params: 15.715064
     Influence (LOO): fixed  94 labels. Loss 0.05870. Accuracy 0.977.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02679061
Train loss (w/o reg) on all data: 0.013885355
Test loss (w/o reg) on all data: 0.035246037
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.7215393e-06
Norm of the params: 16.065649
                Loss: fixed 117 labels. Loss 0.03525. Accuracy 0.989.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22851677
Train loss (w/o reg) on all data: 0.22109395
Test loss (w/o reg) on all data: 0.13479175
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.534623e-05
Norm of the params: 12.184266
              Random: fixed  23 labels. Loss 0.13479. Accuracy 0.966.
### Flips: 260, rs: 35, checks: 208
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047991123
Train loss (w/o reg) on all data: 0.03721761
Test loss (w/o reg) on all data: 0.029341348
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.720198e-06
Norm of the params: 14.6789055
     Influence (LOO): fixed 117 labels. Loss 0.02934. Accuracy 0.996.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021210719
Train loss (w/o reg) on all data: 0.010911857
Test loss (w/o reg) on all data: 0.031863105
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.1100816e-07
Norm of the params: 14.351908
                Loss: fixed 124 labels. Loss 0.03186. Accuracy 0.985.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21708934
Train loss (w/o reg) on all data: 0.21023133
Test loss (w/o reg) on all data: 0.11331616
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1372391e-05
Norm of the params: 11.7115345
              Random: fixed  34 labels. Loss 0.11332. Accuracy 0.985.
### Flips: 260, rs: 35, checks: 260
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028021391
Train loss (w/o reg) on all data: 0.020873802
Test loss (w/o reg) on all data: 0.021101598
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.4181052e-06
Norm of the params: 11.956245
     Influence (LOO): fixed 127 labels. Loss 0.02110. Accuracy 0.996.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0150136035
Train loss (w/o reg) on all data: 0.0067292317
Test loss (w/o reg) on all data: 0.016087841
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 6.986546e-07
Norm of the params: 12.8719635
                Loss: fixed 129 labels. Loss 0.01609. Accuracy 0.989.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2056541
Train loss (w/o reg) on all data: 0.19816984
Test loss (w/o reg) on all data: 0.108492516
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3198504e-05
Norm of the params: 12.234584
              Random: fixed  40 labels. Loss 0.10849. Accuracy 0.985.
### Flips: 260, rs: 35, checks: 312
Using normal model
LBFGS training took [86] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016501708
Train loss (w/o reg) on all data: 0.010083806
Test loss (w/o reg) on all data: 0.01705789
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.693946e-07
Norm of the params: 11.329522
     Influence (LOO): fixed 132 labels. Loss 0.01706. Accuracy 0.992.
Using normal model
LBFGS training took [88] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013904339
Train loss (w/o reg) on all data: 0.0060655237
Test loss (w/o reg) on all data: 0.0153311575
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4526503e-06
Norm of the params: 12.521034
                Loss: fixed 131 labels. Loss 0.01533. Accuracy 0.989.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1953846
Train loss (w/o reg) on all data: 0.18773234
Test loss (w/o reg) on all data: 0.10472532
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.6004795e-06
Norm of the params: 12.371156
              Random: fixed  49 labels. Loss 0.10473. Accuracy 0.981.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25141096
Train loss (w/o reg) on all data: 0.24450634
Test loss (w/o reg) on all data: 0.19773242
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 5.6926845e-05
Norm of the params: 11.7512665
Flipped loss: 0.19773. Accuracy: 0.912
### Flips: 260, rs: 36, checks: 52
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17212632
Train loss (w/o reg) on all data: 0.16177408
Test loss (w/o reg) on all data: 0.15128595
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.1239175e-05
Norm of the params: 14.389049
     Influence (LOO): fixed  40 labels. Loss 0.15129. Accuracy 0.927.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1345817
Train loss (w/o reg) on all data: 0.11976047
Test loss (w/o reg) on all data: 0.18799855
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.4025431e-05
Norm of the params: 17.216984
                Loss: fixed  51 labels. Loss 0.18800. Accuracy 0.927.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2434165
Train loss (w/o reg) on all data: 0.23722757
Test loss (w/o reg) on all data: 0.17832129
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.6150183e-05
Norm of the params: 11.125585
              Random: fixed  12 labels. Loss 0.17832. Accuracy 0.939.
### Flips: 260, rs: 36, checks: 104
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12281385
Train loss (w/o reg) on all data: 0.112214886
Test loss (w/o reg) on all data: 0.11502317
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.438983e-05
Norm of the params: 14.559508
     Influence (LOO): fixed  69 labels. Loss 0.11502. Accuracy 0.947.
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062497646
Train loss (w/o reg) on all data: 0.044827204
Test loss (w/o reg) on all data: 0.147146
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 6.856971e-06
Norm of the params: 18.799171
                Loss: fixed  90 labels. Loss 0.14715. Accuracy 0.947.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23430678
Train loss (w/o reg) on all data: 0.22798033
Test loss (w/o reg) on all data: 0.16665143
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.180231e-05
Norm of the params: 11.24851
              Random: fixed  20 labels. Loss 0.16665. Accuracy 0.947.
### Flips: 260, rs: 36, checks: 156
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07289701
Train loss (w/o reg) on all data: 0.061247066
Test loss (w/o reg) on all data: 0.09999327
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.6471956e-06
Norm of the params: 15.264299
     Influence (LOO): fixed  94 labels. Loss 0.09999. Accuracy 0.958.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025602646
Train loss (w/o reg) on all data: 0.014180202
Test loss (w/o reg) on all data: 0.05262948
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6622595e-06
Norm of the params: 15.114527
                Loss: fixed 114 labels. Loss 0.05263. Accuracy 0.981.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2208553
Train loss (w/o reg) on all data: 0.21404988
Test loss (w/o reg) on all data: 0.16295393
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.6871662e-05
Norm of the params: 11.666555
              Random: fixed  30 labels. Loss 0.16295. Accuracy 0.943.
### Flips: 260, rs: 36, checks: 208
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045201793
Train loss (w/o reg) on all data: 0.03549627
Test loss (w/o reg) on all data: 0.047979195
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9492147e-06
Norm of the params: 13.932354
     Influence (LOO): fixed 112 labels. Loss 0.04798. Accuracy 0.981.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015809441
Train loss (w/o reg) on all data: 0.007354261
Test loss (w/o reg) on all data: 0.022717772
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.560226e-07
Norm of the params: 13.003984
                Loss: fixed 122 labels. Loss 0.02272. Accuracy 0.989.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2091906
Train loss (w/o reg) on all data: 0.20246081
Test loss (w/o reg) on all data: 0.15961742
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 4.1353844e-05
Norm of the params: 11.601544
              Random: fixed  40 labels. Loss 0.15962. Accuracy 0.939.
### Flips: 260, rs: 36, checks: 260
Using normal model
LBFGS training took [98] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016206142
Train loss (w/o reg) on all data: 0.00943496
Test loss (w/o reg) on all data: 0.025702083
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.232594e-06
Norm of the params: 11.637166
     Influence (LOO): fixed 128 labels. Loss 0.02570. Accuracy 0.985.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011801198
Train loss (w/o reg) on all data: 0.004947973
Test loss (w/o reg) on all data: 0.014046422
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.796553e-07
Norm of the params: 11.707455
                Loss: fixed 125 labels. Loss 0.01405. Accuracy 0.992.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18525262
Train loss (w/o reg) on all data: 0.17797287
Test loss (w/o reg) on all data: 0.15408528
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.0824434e-05
Norm of the params: 12.066279
              Random: fixed  51 labels. Loss 0.15409. Accuracy 0.939.
### Flips: 260, rs: 36, checks: 312
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009217031
Train loss (w/o reg) on all data: 0.004139431
Test loss (w/o reg) on all data: 0.011992895
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.904599e-07
Norm of the params: 10.077302
     Influence (LOO): fixed 132 labels. Loss 0.01199. Accuracy 0.992.
Using normal model
LBFGS training took [60] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0100163985
Train loss (w/o reg) on all data: 0.0043039564
Test loss (w/o reg) on all data: 0.014567683
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.415985e-07
Norm of the params: 10.688725
                Loss: fixed 129 labels. Loss 0.01457. Accuracy 0.996.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17121239
Train loss (w/o reg) on all data: 0.16358303
Test loss (w/o reg) on all data: 0.1390526
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.7468208e-05
Norm of the params: 12.352625
              Random: fixed  58 labels. Loss 0.13905. Accuracy 0.943.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25719732
Train loss (w/o reg) on all data: 0.25211585
Test loss (w/o reg) on all data: 0.17067337
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 9.8243705e-05
Norm of the params: 10.081152
Flipped loss: 0.17067. Accuracy: 0.950
### Flips: 260, rs: 37, checks: 52
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17570513
Train loss (w/o reg) on all data: 0.16534078
Test loss (w/o reg) on all data: 0.12931873
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.2596006e-05
Norm of the params: 14.397471
     Influence (LOO): fixed  42 labels. Loss 0.12932. Accuracy 0.950.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13315544
Train loss (w/o reg) on all data: 0.12080245
Test loss (w/o reg) on all data: 0.16170922
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.772778e-05
Norm of the params: 15.718138
                Loss: fixed  51 labels. Loss 0.16171. Accuracy 0.927.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24568042
Train loss (w/o reg) on all data: 0.23977762
Test loss (w/o reg) on all data: 0.1669108
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.4831493e-05
Norm of the params: 10.865356
              Random: fixed   7 labels. Loss 0.16691. Accuracy 0.950.
### Flips: 260, rs: 37, checks: 104
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.118663915
Train loss (w/o reg) on all data: 0.105103105
Test loss (w/o reg) on all data: 0.103058726
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.7577331e-05
Norm of the params: 16.468641
     Influence (LOO): fixed  71 labels. Loss 0.10306. Accuracy 0.958.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058572426
Train loss (w/o reg) on all data: 0.041167736
Test loss (w/o reg) on all data: 0.12260243
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.4503177e-06
Norm of the params: 18.657274
                Loss: fixed  93 labels. Loss 0.12260. Accuracy 0.958.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24037638
Train loss (w/o reg) on all data: 0.23488355
Test loss (w/o reg) on all data: 0.16171868
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.6968537e-05
Norm of the params: 10.481251
              Random: fixed  13 labels. Loss 0.16172. Accuracy 0.954.
### Flips: 260, rs: 37, checks: 156
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08365835
Train loss (w/o reg) on all data: 0.07031753
Test loss (w/o reg) on all data: 0.07505588
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.27149e-06
Norm of the params: 16.334518
     Influence (LOO): fixed  94 labels. Loss 0.07506. Accuracy 0.977.
Using normal model
LBFGS training took [113] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027318325
Train loss (w/o reg) on all data: 0.016237656
Test loss (w/o reg) on all data: 0.06605365
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.4624454e-06
Norm of the params: 14.886685
                Loss: fixed 118 labels. Loss 0.06605. Accuracy 0.962.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23087047
Train loss (w/o reg) on all data: 0.2254481
Test loss (w/o reg) on all data: 0.15293968
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.874444e-05
Norm of the params: 10.413801
              Random: fixed  21 labels. Loss 0.15294. Accuracy 0.958.
### Flips: 260, rs: 37, checks: 208
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044543475
Train loss (w/o reg) on all data: 0.032505546
Test loss (w/o reg) on all data: 0.04300453
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5121026e-06
Norm of the params: 15.516399
     Influence (LOO): fixed 115 labels. Loss 0.04300. Accuracy 0.989.
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0152598
Train loss (w/o reg) on all data: 0.007648731
Test loss (w/o reg) on all data: 0.017690327
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.0791343e-07
Norm of the params: 12.337803
                Loss: fixed 128 labels. Loss 0.01769. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21570909
Train loss (w/o reg) on all data: 0.21020447
Test loss (w/o reg) on all data: 0.13748111
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.530643e-05
Norm of the params: 10.4924965
              Random: fixed  32 labels. Loss 0.13748. Accuracy 0.958.
### Flips: 260, rs: 37, checks: 260
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029309064
Train loss (w/o reg) on all data: 0.019497663
Test loss (w/o reg) on all data: 0.029509263
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5064387e-06
Norm of the params: 14.0081415
     Influence (LOO): fixed 122 labels. Loss 0.02951. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011417272
Train loss (w/o reg) on all data: 0.0048264046
Test loss (w/o reg) on all data: 0.01924402
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.3545084e-07
Norm of the params: 11.4811735
                Loss: fixed 132 labels. Loss 0.01924. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21495737
Train loss (w/o reg) on all data: 0.20949396
Test loss (w/o reg) on all data: 0.1351265
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.069612e-05
Norm of the params: 10.453146
              Random: fixed  34 labels. Loss 0.13513. Accuracy 0.962.
### Flips: 260, rs: 37, checks: 312
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020779882
Train loss (w/o reg) on all data: 0.012907591
Test loss (w/o reg) on all data: 0.023887958
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.5848633e-07
Norm of the params: 12.547741
     Influence (LOO): fixed 128 labels. Loss 0.02389. Accuracy 0.992.
Using normal model
LBFGS training took [65] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009255122
Train loss (w/o reg) on all data: 0.003621994
Test loss (w/o reg) on all data: 0.014520756
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.6025452e-06
Norm of the params: 10.614263
                Loss: fixed 133 labels. Loss 0.01452. Accuracy 0.992.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21050689
Train loss (w/o reg) on all data: 0.20523773
Test loss (w/o reg) on all data: 0.1279788
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4043779e-05
Norm of the params: 10.265631
              Random: fixed  38 labels. Loss 0.12798. Accuracy 0.958.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23668797
Train loss (w/o reg) on all data: 0.23040892
Test loss (w/o reg) on all data: 0.18124576
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 9.166478e-06
Norm of the params: 11.206292
Flipped loss: 0.18125. Accuracy: 0.931
### Flips: 260, rs: 38, checks: 52
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16061206
Train loss (w/o reg) on all data: 0.15064259
Test loss (w/o reg) on all data: 0.13617735
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.8629637e-05
Norm of the params: 14.120537
     Influence (LOO): fixed  42 labels. Loss 0.13618. Accuracy 0.947.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12037557
Train loss (w/o reg) on all data: 0.104986124
Test loss (w/o reg) on all data: 0.17108987
Train acc on all data:  0.9531996179560649
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 7.553933e-06
Norm of the params: 17.543915
                Loss: fixed  51 labels. Loss 0.17109. Accuracy 0.943.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23127303
Train loss (w/o reg) on all data: 0.22506046
Test loss (w/o reg) on all data: 0.17773074
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.7672073e-05
Norm of the params: 11.14681
              Random: fixed   4 labels. Loss 0.17773. Accuracy 0.927.
### Flips: 260, rs: 38, checks: 104
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0933759
Train loss (w/o reg) on all data: 0.08074001
Test loss (w/o reg) on all data: 0.101028994
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.810951e-05
Norm of the params: 15.897097
     Influence (LOO): fixed  76 labels. Loss 0.10103. Accuracy 0.947.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.058332816
Train loss (w/o reg) on all data: 0.040936824
Test loss (w/o reg) on all data: 0.119057134
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.2984211e-06
Norm of the params: 18.652609
                Loss: fixed  86 labels. Loss 0.11906. Accuracy 0.950.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22339694
Train loss (w/o reg) on all data: 0.2170621
Test loss (w/o reg) on all data: 0.1553178
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.4291141e-05
Norm of the params: 11.255967
              Random: fixed  10 labels. Loss 0.15532. Accuracy 0.947.
### Flips: 260, rs: 38, checks: 156
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06652948
Train loss (w/o reg) on all data: 0.055217218
Test loss (w/o reg) on all data: 0.05209136
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.721032e-06
Norm of the params: 15.041453
     Influence (LOO): fixed  92 labels. Loss 0.05209. Accuracy 0.989.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031396378
Train loss (w/o reg) on all data: 0.017811093
Test loss (w/o reg) on all data: 0.058503456
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.8497127e-06
Norm of the params: 16.483496
                Loss: fixed 109 labels. Loss 0.05850. Accuracy 0.973.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21341124
Train loss (w/o reg) on all data: 0.20628615
Test loss (w/o reg) on all data: 0.14232606
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.7512487e-05
Norm of the params: 11.937412
              Random: fixed  18 labels. Loss 0.14233. Accuracy 0.958.
### Flips: 260, rs: 38, checks: 208
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04299617
Train loss (w/o reg) on all data: 0.032218743
Test loss (w/o reg) on all data: 0.032809388
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.719476e-06
Norm of the params: 14.681573
     Influence (LOO): fixed 105 labels. Loss 0.03281. Accuracy 0.989.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021603655
Train loss (w/o reg) on all data: 0.011715246
Test loss (w/o reg) on all data: 0.02680087
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.224972e-07
Norm of the params: 14.063008
                Loss: fixed 118 labels. Loss 0.02680. Accuracy 0.992.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20881702
Train loss (w/o reg) on all data: 0.20163406
Test loss (w/o reg) on all data: 0.1337807
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2207071e-05
Norm of the params: 11.985787
              Random: fixed  23 labels. Loss 0.13378. Accuracy 0.954.
### Flips: 260, rs: 38, checks: 260
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02554499
Train loss (w/o reg) on all data: 0.015797036
Test loss (w/o reg) on all data: 0.028082773
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.764278e-06
Norm of the params: 13.962775
     Influence (LOO): fixed 115 labels. Loss 0.02808. Accuracy 0.989.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013126672
Train loss (w/o reg) on all data: 0.0061179306
Test loss (w/o reg) on all data: 0.014434636
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.4112722e-07
Norm of the params: 11.839545
                Loss: fixed 124 labels. Loss 0.01443. Accuracy 0.996.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20246539
Train loss (w/o reg) on all data: 0.19524068
Test loss (w/o reg) on all data: 0.120943084
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.9744638e-06
Norm of the params: 12.020575
              Random: fixed  28 labels. Loss 0.12094. Accuracy 0.969.
### Flips: 260, rs: 38, checks: 312
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01885873
Train loss (w/o reg) on all data: 0.011141087
Test loss (w/o reg) on all data: 0.016681474
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.090387e-06
Norm of the params: 12.4238825
     Influence (LOO): fixed 121 labels. Loss 0.01668. Accuracy 0.992.
Using normal model
LBFGS training took [77] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.009736199
Train loss (w/o reg) on all data: 0.0038374034
Test loss (w/o reg) on all data: 0.011526708
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.588812e-07
Norm of the params: 10.861672
                Loss: fixed 125 labels. Loss 0.01153. Accuracy 0.996.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19283213
Train loss (w/o reg) on all data: 0.1856205
Test loss (w/o reg) on all data: 0.111095116
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.2341431e-05
Norm of the params: 12.009681
              Random: fixed  36 labels. Loss 0.11110. Accuracy 0.969.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24247879
Train loss (w/o reg) on all data: 0.23538269
Test loss (w/o reg) on all data: 0.17775318
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.19655515e-05
Norm of the params: 11.913104
Flipped loss: 0.17775. Accuracy: 0.935
### Flips: 260, rs: 39, checks: 52
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17114432
Train loss (w/o reg) on all data: 0.15923558
Test loss (w/o reg) on all data: 0.13585196
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.0788844e-05
Norm of the params: 15.432909
     Influence (LOO): fixed  36 labels. Loss 0.13585. Accuracy 0.950.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13000037
Train loss (w/o reg) on all data: 0.11483093
Test loss (w/o reg) on all data: 0.13061683
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.3547476e-05
Norm of the params: 17.41806
                Loss: fixed  49 labels. Loss 0.13062. Accuracy 0.939.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2348957
Train loss (w/o reg) on all data: 0.22751847
Test loss (w/o reg) on all data: 0.16008948
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.5948444e-05
Norm of the params: 12.1468
              Random: fixed   7 labels. Loss 0.16009. Accuracy 0.947.
### Flips: 260, rs: 39, checks: 104
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11435657
Train loss (w/o reg) on all data: 0.10319487
Test loss (w/o reg) on all data: 0.08801523
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.9662613e-05
Norm of the params: 14.941019
     Influence (LOO): fixed  71 labels. Loss 0.08802. Accuracy 0.966.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05995422
Train loss (w/o reg) on all data: 0.041976877
Test loss (w/o reg) on all data: 0.075572014
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.105727e-06
Norm of the params: 18.961721
                Loss: fixed  89 labels. Loss 0.07557. Accuracy 0.969.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22863013
Train loss (w/o reg) on all data: 0.22112736
Test loss (w/o reg) on all data: 0.14345868
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.7323597e-05
Norm of the params: 12.249702
              Random: fixed  13 labels. Loss 0.14346. Accuracy 0.962.
### Flips: 260, rs: 39, checks: 156
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.067060575
Train loss (w/o reg) on all data: 0.055928312
Test loss (w/o reg) on all data: 0.056335486
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.33128e-05
Norm of the params: 14.921301
     Influence (LOO): fixed 100 labels. Loss 0.05634. Accuracy 0.977.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034158543
Train loss (w/o reg) on all data: 0.019995162
Test loss (w/o reg) on all data: 0.035080515
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.4721161e-06
Norm of the params: 16.830555
                Loss: fixed 108 labels. Loss 0.03508. Accuracy 0.985.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22651787
Train loss (w/o reg) on all data: 0.21876532
Test loss (w/o reg) on all data: 0.1400797
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 2.13429e-05
Norm of the params: 12.45195
              Random: fixed  16 labels. Loss 0.14008. Accuracy 0.958.
### Flips: 260, rs: 39, checks: 208
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03697218
Train loss (w/o reg) on all data: 0.028360264
Test loss (w/o reg) on all data: 0.038813133
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0364076e-06
Norm of the params: 13.123961
     Influence (LOO): fixed 116 labels. Loss 0.03881. Accuracy 0.989.
Using normal model
LBFGS training took [99] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020479202
Train loss (w/o reg) on all data: 0.010099917
Test loss (w/o reg) on all data: 0.019095443
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.7616892e-06
Norm of the params: 14.407836
                Loss: fixed 118 labels. Loss 0.01910. Accuracy 0.989.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21790002
Train loss (w/o reg) on all data: 0.2101896
Test loss (w/o reg) on all data: 0.14279959
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.1012112e-05
Norm of the params: 12.418075
              Random: fixed  23 labels. Loss 0.14280. Accuracy 0.954.
### Flips: 260, rs: 39, checks: 260
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019801136
Train loss (w/o reg) on all data: 0.013487221
Test loss (w/o reg) on all data: 0.022506043
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 6.696648e-07
Norm of the params: 11.237362
     Influence (LOO): fixed 127 labels. Loss 0.02251. Accuracy 0.996.
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019277371
Train loss (w/o reg) on all data: 0.010236532
Test loss (w/o reg) on all data: 0.020694397
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.2889104e-07
Norm of the params: 13.446814
                Loss: fixed 123 labels. Loss 0.02069. Accuracy 0.992.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21011636
Train loss (w/o reg) on all data: 0.20218056
Test loss (w/o reg) on all data: 0.1272688
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.0873362e-05
Norm of the params: 12.598245
              Random: fixed  29 labels. Loss 0.12727. Accuracy 0.958.
### Flips: 260, rs: 39, checks: 312
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.008869581
Train loss (w/o reg) on all data: 0.004091459
Test loss (w/o reg) on all data: 0.011885286
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8241755e-07
Norm of the params: 9.775604
     Influence (LOO): fixed 131 labels. Loss 0.01189. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010467866
Train loss (w/o reg) on all data: 0.004293815
Test loss (w/o reg) on all data: 0.015448443
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8096301e-07
Norm of the params: 11.112202
                Loss: fixed 130 labels. Loss 0.01545. Accuracy 0.992.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19984274
Train loss (w/o reg) on all data: 0.19118965
Test loss (w/o reg) on all data: 0.12030996
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.244167e-05
Norm of the params: 13.155297
              Random: fixed  36 labels. Loss 0.12031. Accuracy 0.973.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26422918
Train loss (w/o reg) on all data: 0.2573496
Test loss (w/o reg) on all data: 0.22947603
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.8969465648854962
Norm of the mean of gradients: 1.2056725e-05
Norm of the params: 11.729944
Flipped loss: 0.22948. Accuracy: 0.897
### Flips: 312, rs: 0, checks: 52
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20280309
Train loss (w/o reg) on all data: 0.19025993
Test loss (w/o reg) on all data: 0.19690727
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.8969465648854962
Norm of the mean of gradients: 1.6779977e-05
Norm of the params: 15.83866
     Influence (LOO): fixed  35 labels. Loss 0.19691. Accuracy 0.897.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16281529
Train loss (w/o reg) on all data: 0.1464676
Test loss (w/o reg) on all data: 0.21872021
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.8893129770992366
Norm of the mean of gradients: 3.439503e-05
Norm of the params: 18.081867
                Loss: fixed  45 labels. Loss 0.21872. Accuracy 0.889.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2546724
Train loss (w/o reg) on all data: 0.24751592
Test loss (w/o reg) on all data: 0.20888327
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.2073869e-05
Norm of the params: 11.963677
              Random: fixed  12 labels. Loss 0.20888. Accuracy 0.920.
### Flips: 312, rs: 0, checks: 104
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15634365
Train loss (w/o reg) on all data: 0.14233895
Test loss (w/o reg) on all data: 0.168831
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 1.1684421e-05
Norm of the params: 16.736015
     Influence (LOO): fixed  65 labels. Loss 0.16883. Accuracy 0.912.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10671036
Train loss (w/o reg) on all data: 0.0867429
Test loss (w/o reg) on all data: 0.17778642
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 3.201726e-05
Norm of the params: 19.983725
                Loss: fixed  82 labels. Loss 0.17779. Accuracy 0.924.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2515516
Train loss (w/o reg) on all data: 0.24424031
Test loss (w/o reg) on all data: 0.19447352
Train acc on all data:  0.874880611270296
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 2.649462e-05
Norm of the params: 12.092376
              Random: fixed  19 labels. Loss 0.19447. Accuracy 0.908.
### Flips: 312, rs: 0, checks: 156
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11314731
Train loss (w/o reg) on all data: 0.09838582
Test loss (w/o reg) on all data: 0.13614865
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.0969968e-05
Norm of the params: 17.182253
     Influence (LOO): fixed  94 labels. Loss 0.13615. Accuracy 0.943.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07735449
Train loss (w/o reg) on all data: 0.058974836
Test loss (w/o reg) on all data: 0.109730385
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.819882e-06
Norm of the params: 19.172716
                Loss: fixed 113 labels. Loss 0.10973. Accuracy 0.950.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24295437
Train loss (w/o reg) on all data: 0.23545468
Test loss (w/o reg) on all data: 0.1745106
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.8728668e-05
Norm of the params: 12.247198
              Random: fixed  28 labels. Loss 0.17451. Accuracy 0.931.
### Flips: 312, rs: 0, checks: 208
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086759046
Train loss (w/o reg) on all data: 0.07247843
Test loss (w/o reg) on all data: 0.10970005
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.2875173e-05
Norm of the params: 16.900066
     Influence (LOO): fixed 116 labels. Loss 0.10970. Accuracy 0.947.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055240784
Train loss (w/o reg) on all data: 0.036914267
Test loss (w/o reg) on all data: 0.076734945
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.7668857e-06
Norm of the params: 19.144983
                Loss: fixed 128 labels. Loss 0.07673. Accuracy 0.973.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23708087
Train loss (w/o reg) on all data: 0.2291365
Test loss (w/o reg) on all data: 0.17009746
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.2210304e-05
Norm of the params: 12.605056
              Random: fixed  33 labels. Loss 0.17010. Accuracy 0.935.
### Flips: 312, rs: 0, checks: 260
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.062893875
Train loss (w/o reg) on all data: 0.050573338
Test loss (w/o reg) on all data: 0.07801929
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.3204023e-05
Norm of the params: 15.6974745
     Influence (LOO): fixed 134 labels. Loss 0.07802. Accuracy 0.962.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042559482
Train loss (w/o reg) on all data: 0.0264564
Test loss (w/o reg) on all data: 0.061403215
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.0260268e-06
Norm of the params: 17.946074
                Loss: fixed 141 labels. Loss 0.06140. Accuracy 0.981.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22881553
Train loss (w/o reg) on all data: 0.22057757
Test loss (w/o reg) on all data: 0.16366035
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.012208e-05
Norm of the params: 12.835858
              Random: fixed  41 labels. Loss 0.16366. Accuracy 0.935.
### Flips: 312, rs: 0, checks: 312
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047418345
Train loss (w/o reg) on all data: 0.03547588
Test loss (w/o reg) on all data: 0.060440794
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.893386e-06
Norm of the params: 15.45475
     Influence (LOO): fixed 144 labels. Loss 0.06044. Accuracy 0.969.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028191332
Train loss (w/o reg) on all data: 0.016047271
Test loss (w/o reg) on all data: 0.03149155
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.0048442e-06
Norm of the params: 15.584647
                Loss: fixed 154 labels. Loss 0.03149. Accuracy 0.989.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22599244
Train loss (w/o reg) on all data: 0.21857159
Test loss (w/o reg) on all data: 0.1485654
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.4610259e-05
Norm of the params: 12.182653
              Random: fixed  48 labels. Loss 0.14857. Accuracy 0.935.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2822308
Train loss (w/o reg) on all data: 0.2764423
Test loss (w/o reg) on all data: 0.23272231
Train acc on all data:  0.8624641833810889
Test acc on all data:   0.8816793893129771
Norm of the mean of gradients: 3.5384193e-05
Norm of the params: 10.7596445
Flipped loss: 0.23272. Accuracy: 0.882
### Flips: 312, rs: 1, checks: 52
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21640062
Train loss (w/o reg) on all data: 0.20494317
Test loss (w/o reg) on all data: 0.20151792
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.24277785e-05
Norm of the params: 15.137671
     Influence (LOO): fixed  38 labels. Loss 0.20152. Accuracy 0.908.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17419995
Train loss (w/o reg) on all data: 0.15964223
Test loss (w/o reg) on all data: 0.22003466
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.8702290076335878
Norm of the mean of gradients: 2.3233903e-05
Norm of the params: 17.063246
                Loss: fixed  48 labels. Loss 0.22003. Accuracy 0.870.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26608616
Train loss (w/o reg) on all data: 0.2591121
Test loss (w/o reg) on all data: 0.21361804
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 1.4508165e-05
Norm of the params: 11.810236
              Random: fixed  10 labels. Loss 0.21362. Accuracy 0.901.
### Flips: 312, rs: 1, checks: 104
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17098492
Train loss (w/o reg) on all data: 0.15837213
Test loss (w/o reg) on all data: 0.14550933
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 3.3007724e-05
Norm of the params: 15.882564
     Influence (LOO): fixed  65 labels. Loss 0.14551. Accuracy 0.927.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11664072
Train loss (w/o reg) on all data: 0.09989482
Test loss (w/o reg) on all data: 0.18420835
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 3.487412e-05
Norm of the params: 18.300762
                Loss: fixed  89 labels. Loss 0.18421. Accuracy 0.916.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26640087
Train loss (w/o reg) on all data: 0.25980633
Test loss (w/o reg) on all data: 0.20722772
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.7740341e-05
Norm of the params: 11.484367
              Random: fixed  15 labels. Loss 0.20723. Accuracy 0.920.
### Flips: 312, rs: 1, checks: 156
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12765417
Train loss (w/o reg) on all data: 0.114141814
Test loss (w/o reg) on all data: 0.119785346
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.21273815e-05
Norm of the params: 16.439194
     Influence (LOO): fixed  92 labels. Loss 0.11979. Accuracy 0.943.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08196082
Train loss (w/o reg) on all data: 0.06463345
Test loss (w/o reg) on all data: 0.15751043
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.0162814e-06
Norm of the params: 18.615784
                Loss: fixed 115 labels. Loss 0.15751. Accuracy 0.947.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26045004
Train loss (w/o reg) on all data: 0.2537885
Test loss (w/o reg) on all data: 0.19901112
Train acc on all data:  0.87774594078319
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 0.00012263135
Norm of the params: 11.542563
              Random: fixed  23 labels. Loss 0.19901. Accuracy 0.916.
### Flips: 312, rs: 1, checks: 208
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09756891
Train loss (w/o reg) on all data: 0.08386252
Test loss (w/o reg) on all data: 0.10243273
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.5233143e-05
Norm of the params: 16.5568
     Influence (LOO): fixed 113 labels. Loss 0.10243. Accuracy 0.943.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05493667
Train loss (w/o reg) on all data: 0.03883063
Test loss (w/o reg) on all data: 0.09163885
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.419811e-06
Norm of the params: 17.947725
                Loss: fixed 135 labels. Loss 0.09164. Accuracy 0.966.
Using normal model
LBFGS training took [311] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24684599
Train loss (w/o reg) on all data: 0.23939344
Test loss (w/o reg) on all data: 0.1761253
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.691676e-05
Norm of the params: 12.208648
              Random: fixed  36 labels. Loss 0.17613. Accuracy 0.935.
### Flips: 312, rs: 1, checks: 260
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07482605
Train loss (w/o reg) on all data: 0.061017208
Test loss (w/o reg) on all data: 0.09077092
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.189159e-06
Norm of the params: 16.618567
     Influence (LOO): fixed 126 labels. Loss 0.09077. Accuracy 0.958.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026320942
Train loss (w/o reg) on all data: 0.014133812
Test loss (w/o reg) on all data: 0.044322714
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.488182e-07
Norm of the params: 15.612259
                Loss: fixed 153 labels. Loss 0.04432. Accuracy 0.981.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23933351
Train loss (w/o reg) on all data: 0.2317322
Test loss (w/o reg) on all data: 0.16893962
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.4655e-05
Norm of the params: 12.329892
              Random: fixed  43 labels. Loss 0.16894. Accuracy 0.935.
### Flips: 312, rs: 1, checks: 312
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03980325
Train loss (w/o reg) on all data: 0.029076036
Test loss (w/o reg) on all data: 0.060761746
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 9.129896e-06
Norm of the params: 14.64733
     Influence (LOO): fixed 146 labels. Loss 0.06076. Accuracy 0.969.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022816608
Train loss (w/o reg) on all data: 0.011757798
Test loss (w/o reg) on all data: 0.038305778
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.9859933e-06
Norm of the params: 14.871994
                Loss: fixed 156 labels. Loss 0.03831. Accuracy 0.981.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22639184
Train loss (w/o reg) on all data: 0.21793099
Test loss (w/o reg) on all data: 0.15746514
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 6.1828905e-05
Norm of the params: 13.008339
              Random: fixed  50 labels. Loss 0.15747. Accuracy 0.939.
Using normal model
LBFGS training took [269] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2886174
Train loss (w/o reg) on all data: 0.28244165
Test loss (w/o reg) on all data: 0.23295127
Train acc on all data:  0.8595988538681948
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 0.00012805108
Norm of the params: 11.113738
Flipped loss: 0.23295. Accuracy: 0.905
### Flips: 312, rs: 2, checks: 52
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22684707
Train loss (w/o reg) on all data: 0.21666355
Test loss (w/o reg) on all data: 0.20481287
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.51808745e-05
Norm of the params: 14.271308
     Influence (LOO): fixed  35 labels. Loss 0.20481. Accuracy 0.920.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18345082
Train loss (w/o reg) on all data: 0.16968058
Test loss (w/o reg) on all data: 0.23109189
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.8969465648854962
Norm of the mean of gradients: 2.4627734e-05
Norm of the params: 16.59532
                Loss: fixed  49 labels. Loss 0.23109. Accuracy 0.897.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28106385
Train loss (w/o reg) on all data: 0.27502343
Test loss (w/o reg) on all data: 0.21925296
Train acc on all data:  0.8672397325692455
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 0.00010125452
Norm of the params: 10.991279
              Random: fixed   9 labels. Loss 0.21925. Accuracy 0.905.
### Flips: 312, rs: 2, checks: 104
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17590302
Train loss (w/o reg) on all data: 0.16404106
Test loss (w/o reg) on all data: 0.18194017
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.692468e-05
Norm of the params: 15.402576
     Influence (LOO): fixed  66 labels. Loss 0.18194. Accuracy 0.935.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11494521
Train loss (w/o reg) on all data: 0.09539375
Test loss (w/o reg) on all data: 0.18187277
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 6.028598e-06
Norm of the params: 19.774458
                Loss: fixed  89 labels. Loss 0.18187. Accuracy 0.924.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.275439
Train loss (w/o reg) on all data: 0.27033344
Test loss (w/o reg) on all data: 0.20875967
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.7257926e-05
Norm of the params: 10.105002
              Random: fixed  18 labels. Loss 0.20876. Accuracy 0.920.
### Flips: 312, rs: 2, checks: 156
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13390501
Train loss (w/o reg) on all data: 0.121386126
Test loss (w/o reg) on all data: 0.16042876
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.532122e-05
Norm of the params: 15.823323
     Influence (LOO): fixed  93 labels. Loss 0.16043. Accuracy 0.935.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07400516
Train loss (w/o reg) on all data: 0.054763023
Test loss (w/o reg) on all data: 0.11594275
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.6801447e-06
Norm of the params: 19.617407
                Loss: fixed 120 labels. Loss 0.11594. Accuracy 0.958.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27063978
Train loss (w/o reg) on all data: 0.26569325
Test loss (w/o reg) on all data: 0.1955655
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.474066e-05
Norm of the params: 9.946387
              Random: fixed  26 labels. Loss 0.19557. Accuracy 0.935.
### Flips: 312, rs: 2, checks: 208
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10121238
Train loss (w/o reg) on all data: 0.09036494
Test loss (w/o reg) on all data: 0.124682315
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.166042e-05
Norm of the params: 14.72918
     Influence (LOO): fixed 116 labels. Loss 0.12468. Accuracy 0.950.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.044364847
Train loss (w/o reg) on all data: 0.026247576
Test loss (w/o reg) on all data: 0.088324845
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.7495453e-06
Norm of the params: 19.035372
                Loss: fixed 139 labels. Loss 0.08832. Accuracy 0.973.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26016024
Train loss (w/o reg) on all data: 0.2547439
Test loss (w/o reg) on all data: 0.18644042
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 5.4398355e-05
Norm of the params: 10.408009
              Random: fixed  35 labels. Loss 0.18644. Accuracy 0.939.
### Flips: 312, rs: 2, checks: 260
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07112372
Train loss (w/o reg) on all data: 0.061862346
Test loss (w/o reg) on all data: 0.06099637
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.162049e-06
Norm of the params: 13.609831
     Influence (LOO): fixed 140 labels. Loss 0.06100. Accuracy 0.969.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03293039
Train loss (w/o reg) on all data: 0.018764842
Test loss (w/o reg) on all data: 0.049192894
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.2184255e-06
Norm of the params: 16.831842
                Loss: fixed 151 labels. Loss 0.04919. Accuracy 0.989.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25355536
Train loss (w/o reg) on all data: 0.24793944
Test loss (w/o reg) on all data: 0.17581974
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.7693616e-05
Norm of the params: 10.59805
              Random: fixed  43 labels. Loss 0.17582. Accuracy 0.947.
### Flips: 312, rs: 2, checks: 312
Using normal model
LBFGS training took [130] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045102827
Train loss (w/o reg) on all data: 0.03575752
Test loss (w/o reg) on all data: 0.048808984
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.3271203e-06
Norm of the params: 13.671362
     Influence (LOO): fixed 154 labels. Loss 0.04881. Accuracy 0.985.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02587376
Train loss (w/o reg) on all data: 0.013575028
Test loss (w/o reg) on all data: 0.036580607
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.8701317e-06
Norm of the params: 15.6835785
                Loss: fixed 159 labels. Loss 0.03658. Accuracy 0.992.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25080237
Train loss (w/o reg) on all data: 0.24520959
Test loss (w/o reg) on all data: 0.17055914
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.3651213e-05
Norm of the params: 10.576168
              Random: fixed  47 labels. Loss 0.17056. Accuracy 0.950.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26976976
Train loss (w/o reg) on all data: 0.26288417
Test loss (w/o reg) on all data: 0.22163473
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.8931297709923665
Norm of the mean of gradients: 2.4055942e-05
Norm of the params: 11.735061
Flipped loss: 0.22163. Accuracy: 0.893
### Flips: 312, rs: 3, checks: 52
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19197057
Train loss (w/o reg) on all data: 0.18046367
Test loss (w/o reg) on all data: 0.18726142
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 3.8411185e-05
Norm of the params: 15.170303
     Influence (LOO): fixed  43 labels. Loss 0.18726. Accuracy 0.901.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16513209
Train loss (w/o reg) on all data: 0.14862713
Test loss (w/o reg) on all data: 0.20581499
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 8.370284e-06
Norm of the params: 18.168629
                Loss: fixed  50 labels. Loss 0.20581. Accuracy 0.905.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2663813
Train loss (w/o reg) on all data: 0.26016423
Test loss (w/o reg) on all data: 0.20354706
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.248578e-05
Norm of the params: 11.150842
              Random: fixed  10 labels. Loss 0.20355. Accuracy 0.908.
### Flips: 312, rs: 3, checks: 104
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15330233
Train loss (w/o reg) on all data: 0.14074397
Test loss (w/o reg) on all data: 0.15688165
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.9927485e-05
Norm of the params: 15.848256
     Influence (LOO): fixed  70 labels. Loss 0.15688. Accuracy 0.924.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09618378
Train loss (w/o reg) on all data: 0.07547788
Test loss (w/o reg) on all data: 0.149016
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.2119211e-05
Norm of the params: 20.349888
                Loss: fixed  90 labels. Loss 0.14902. Accuracy 0.931.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2591976
Train loss (w/o reg) on all data: 0.2525091
Test loss (w/o reg) on all data: 0.20429592
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 3.661513e-05
Norm of the params: 11.565913
              Random: fixed  17 labels. Loss 0.20430. Accuracy 0.916.
### Flips: 312, rs: 3, checks: 156
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.124324165
Train loss (w/o reg) on all data: 0.111129075
Test loss (w/o reg) on all data: 0.11974424
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.737725e-05
Norm of the params: 16.245054
     Influence (LOO): fixed  91 labels. Loss 0.11974. Accuracy 0.958.
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0541998
Train loss (w/o reg) on all data: 0.03741203
Test loss (w/o reg) on all data: 0.07322645
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.167915e-06
Norm of the params: 18.32363
                Loss: fixed 121 labels. Loss 0.07323. Accuracy 0.969.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25367162
Train loss (w/o reg) on all data: 0.24710421
Test loss (w/o reg) on all data: 0.17982116
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.745954e-05
Norm of the params: 11.460713
              Random: fixed  25 labels. Loss 0.17982. Accuracy 0.924.
### Flips: 312, rs: 3, checks: 208
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07505193
Train loss (w/o reg) on all data: 0.06343358
Test loss (w/o reg) on all data: 0.08602924
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.4678346e-06
Norm of the params: 15.243593
     Influence (LOO): fixed 118 labels. Loss 0.08603. Accuracy 0.958.
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036720075
Train loss (w/o reg) on all data: 0.02343937
Test loss (w/o reg) on all data: 0.066814624
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.1186306e-06
Norm of the params: 16.29767
                Loss: fixed 136 labels. Loss 0.06681. Accuracy 0.981.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25005838
Train loss (w/o reg) on all data: 0.24343257
Test loss (w/o reg) on all data: 0.16947246
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 6.226468e-05
Norm of the params: 11.511563
              Random: fixed  29 labels. Loss 0.16947. Accuracy 0.935.
### Flips: 312, rs: 3, checks: 260
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.045546666
Train loss (w/o reg) on all data: 0.03623972
Test loss (w/o reg) on all data: 0.059721652
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.1126003e-06
Norm of the params: 13.643272
     Influence (LOO): fixed 134 labels. Loss 0.05972. Accuracy 0.966.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022574488
Train loss (w/o reg) on all data: 0.012301401
Test loss (w/o reg) on all data: 0.04604849
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.554616e-06
Norm of the params: 14.333938
                Loss: fixed 142 labels. Loss 0.04605. Accuracy 0.977.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24385083
Train loss (w/o reg) on all data: 0.23706114
Test loss (w/o reg) on all data: 0.17125489
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.3421157e-05
Norm of the params: 11.653061
              Random: fixed  35 labels. Loss 0.17125. Accuracy 0.935.
### Flips: 312, rs: 3, checks: 312
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028094156
Train loss (w/o reg) on all data: 0.020200333
Test loss (w/o reg) on all data: 0.029875917
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.7662956e-06
Norm of the params: 12.56489
     Influence (LOO): fixed 145 labels. Loss 0.02988. Accuracy 0.989.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020436551
Train loss (w/o reg) on all data: 0.011111481
Test loss (w/o reg) on all data: 0.042337686
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.9833602e-06
Norm of the params: 13.656552
                Loss: fixed 145 labels. Loss 0.04234. Accuracy 0.981.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23820049
Train loss (w/o reg) on all data: 0.23158583
Test loss (w/o reg) on all data: 0.16179089
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.3005649e-05
Norm of the params: 11.501868
              Random: fixed  40 labels. Loss 0.16179. Accuracy 0.943.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27511594
Train loss (w/o reg) on all data: 0.2699845
Test loss (w/o reg) on all data: 0.19540481
Train acc on all data:  0.8653295128939829
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 5.1667903e-05
Norm of the params: 10.13057
Flipped loss: 0.19540. Accuracy: 0.939
### Flips: 312, rs: 4, checks: 52
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19667915
Train loss (w/o reg) on all data: 0.18590523
Test loss (w/o reg) on all data: 0.16343069
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 8.022717e-06
Norm of the params: 14.679179
     Influence (LOO): fixed  41 labels. Loss 0.16343. Accuracy 0.935.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15523352
Train loss (w/o reg) on all data: 0.14138247
Test loss (w/o reg) on all data: 0.17996973
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.0733052e-05
Norm of the params: 16.64395
                Loss: fixed  52 labels. Loss 0.17997. Accuracy 0.931.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2690326
Train loss (w/o reg) on all data: 0.2634005
Test loss (w/o reg) on all data: 0.19003151
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.430929e-05
Norm of the params: 10.613292
              Random: fixed   5 labels. Loss 0.19003. Accuracy 0.939.
### Flips: 312, rs: 4, checks: 104
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16044581
Train loss (w/o reg) on all data: 0.14859219
Test loss (w/o reg) on all data: 0.12725629
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.726675e-05
Norm of the params: 15.397157
     Influence (LOO): fixed  67 labels. Loss 0.12726. Accuracy 0.969.
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08505539
Train loss (w/o reg) on all data: 0.06632106
Test loss (w/o reg) on all data: 0.15420598
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 5.3605136e-06
Norm of the params: 19.356823
                Loss: fixed  94 labels. Loss 0.15421. Accuracy 0.947.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25805292
Train loss (w/o reg) on all data: 0.25168523
Test loss (w/o reg) on all data: 0.17152776
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.4597284e-05
Norm of the params: 11.285113
              Random: fixed  15 labels. Loss 0.17153. Accuracy 0.947.
### Flips: 312, rs: 4, checks: 156
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12048928
Train loss (w/o reg) on all data: 0.107241504
Test loss (w/o reg) on all data: 0.09546726
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.7823053e-06
Norm of the params: 16.277452
     Influence (LOO): fixed  90 labels. Loss 0.09547. Accuracy 0.969.
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04436335
Train loss (w/o reg) on all data: 0.027750935
Test loss (w/o reg) on all data: 0.06487879
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.1300924e-06
Norm of the params: 18.227678
                Loss: fixed 123 labels. Loss 0.06488. Accuracy 0.977.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25262785
Train loss (w/o reg) on all data: 0.24681279
Test loss (w/o reg) on all data: 0.16984679
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 6.2206054e-05
Norm of the params: 10.784313
              Random: fixed  22 labels. Loss 0.16985. Accuracy 0.958.
### Flips: 312, rs: 4, checks: 208
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08704078
Train loss (w/o reg) on all data: 0.0756783
Test loss (w/o reg) on all data: 0.07068809
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.6195665e-05
Norm of the params: 15.074803
     Influence (LOO): fixed 111 labels. Loss 0.07069. Accuracy 0.966.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028999709
Train loss (w/o reg) on all data: 0.01572895
Test loss (w/o reg) on all data: 0.03949019
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.1417285e-06
Norm of the params: 16.291569
                Loss: fixed 135 labels. Loss 0.03949. Accuracy 0.985.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24453913
Train loss (w/o reg) on all data: 0.2381222
Test loss (w/o reg) on all data: 0.16230293
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.2491744e-05
Norm of the params: 11.32866
              Random: fixed  26 labels. Loss 0.16230. Accuracy 0.966.
### Flips: 312, rs: 4, checks: 260
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047978822
Train loss (w/o reg) on all data: 0.038164146
Test loss (w/o reg) on all data: 0.0404319
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 2.4453923e-06
Norm of the params: 14.010479
     Influence (LOO): fixed 131 labels. Loss 0.04043. Accuracy 0.977.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019381668
Train loss (w/o reg) on all data: 0.009182092
Test loss (w/o reg) on all data: 0.01783065
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.5767874e-06
Norm of the params: 14.282561
                Loss: fixed 144 labels. Loss 0.01783. Accuracy 0.992.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23458976
Train loss (w/o reg) on all data: 0.22880098
Test loss (w/o reg) on all data: 0.14950916
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.6728616e-05
Norm of the params: 10.759897
              Random: fixed  38 labels. Loss 0.14951. Accuracy 0.962.
### Flips: 312, rs: 4, checks: 312
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019100234
Train loss (w/o reg) on all data: 0.0120183155
Test loss (w/o reg) on all data: 0.024160923
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.906858e-07
Norm of the params: 11.901192
     Influence (LOO): fixed 146 labels. Loss 0.02416. Accuracy 0.992.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018259801
Train loss (w/o reg) on all data: 0.008554482
Test loss (w/o reg) on all data: 0.017839901
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.074813e-07
Norm of the params: 13.932207
                Loss: fixed 146 labels. Loss 0.01784. Accuracy 0.992.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22052138
Train loss (w/o reg) on all data: 0.214478
Test loss (w/o reg) on all data: 0.14999795
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.6202204e-05
Norm of the params: 10.9939785
              Random: fixed  48 labels. Loss 0.15000. Accuracy 0.958.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27000538
Train loss (w/o reg) on all data: 0.26399523
Test loss (w/o reg) on all data: 0.22971356
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 4.5789144e-05
Norm of the params: 10.963702
Flipped loss: 0.22971. Accuracy: 0.908
### Flips: 312, rs: 5, checks: 52
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19662474
Train loss (w/o reg) on all data: 0.18470035
Test loss (w/o reg) on all data: 0.19804858
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.8931297709923665
Norm of the mean of gradients: 1.9277988e-05
Norm of the params: 15.4430485
     Influence (LOO): fixed  39 labels. Loss 0.19805. Accuracy 0.893.
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1596316
Train loss (w/o reg) on all data: 0.14341277
Test loss (w/o reg) on all data: 0.19348133
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 2.8416938e-05
Norm of the params: 18.010454
                Loss: fixed  47 labels. Loss 0.19348. Accuracy 0.908.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25884303
Train loss (w/o reg) on all data: 0.25210118
Test loss (w/o reg) on all data: 0.2209692
Train acc on all data:  0.87774594078319
Test acc on all data:   0.8931297709923665
Norm of the mean of gradients: 1.7419947e-05
Norm of the params: 11.611942
              Random: fixed   8 labels. Loss 0.22097. Accuracy 0.893.
### Flips: 312, rs: 5, checks: 104
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15359712
Train loss (w/o reg) on all data: 0.13976952
Test loss (w/o reg) on all data: 0.14544529
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.0355748e-05
Norm of the params: 16.62985
     Influence (LOO): fixed  68 labels. Loss 0.14545. Accuracy 0.931.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09503296
Train loss (w/o reg) on all data: 0.07546459
Test loss (w/o reg) on all data: 0.17172116
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.5550495e-06
Norm of the params: 19.783009
                Loss: fixed  88 labels. Loss 0.17172. Accuracy 0.935.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25002235
Train loss (w/o reg) on all data: 0.24214862
Test loss (w/o reg) on all data: 0.21193305
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 6.002476e-05
Norm of the params: 12.5488825
              Random: fixed  15 labels. Loss 0.21193. Accuracy 0.908.
### Flips: 312, rs: 5, checks: 156
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12210148
Train loss (w/o reg) on all data: 0.10804102
Test loss (w/o reg) on all data: 0.13734369
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 6.4487544e-06
Norm of the params: 16.769293
     Influence (LOO): fixed  89 labels. Loss 0.13734. Accuracy 0.931.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06042455
Train loss (w/o reg) on all data: 0.041852202
Test loss (w/o reg) on all data: 0.14097832
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.3184081e-05
Norm of the params: 19.272957
                Loss: fixed 111 labels. Loss 0.14098. Accuracy 0.954.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22866635
Train loss (w/o reg) on all data: 0.21941851
Test loss (w/o reg) on all data: 0.1957152
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 2.6674787e-05
Norm of the params: 13.599883
              Random: fixed  28 labels. Loss 0.19572. Accuracy 0.912.
### Flips: 312, rs: 5, checks: 208
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08551378
Train loss (w/o reg) on all data: 0.070465915
Test loss (w/o reg) on all data: 0.0965302
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.3158974e-06
Norm of the params: 17.34812
     Influence (LOO): fixed 114 labels. Loss 0.09653. Accuracy 0.966.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040902454
Train loss (w/o reg) on all data: 0.02550075
Test loss (w/o reg) on all data: 0.075927
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.7834662e-06
Norm of the params: 17.550901
                Loss: fixed 129 labels. Loss 0.07593. Accuracy 0.969.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22317779
Train loss (w/o reg) on all data: 0.21406
Test loss (w/o reg) on all data: 0.1856803
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 7.5877e-06
Norm of the params: 13.503923
              Random: fixed  38 labels. Loss 0.18568. Accuracy 0.927.
### Flips: 312, rs: 5, checks: 260
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051952846
Train loss (w/o reg) on all data: 0.03817279
Test loss (w/o reg) on all data: 0.038569644
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.0388259e-06
Norm of the params: 16.601238
     Influence (LOO): fixed 135 labels. Loss 0.03857. Accuracy 0.989.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031103954
Train loss (w/o reg) on all data: 0.018460037
Test loss (w/o reg) on all data: 0.029259317
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5666105e-06
Norm of the params: 15.902148
                Loss: fixed 142 labels. Loss 0.02926. Accuracy 0.992.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21646386
Train loss (w/o reg) on all data: 0.20675369
Test loss (w/o reg) on all data: 0.18531191
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 8.982487e-06
Norm of the params: 13.935689
              Random: fixed  41 labels. Loss 0.18531. Accuracy 0.912.
### Flips: 312, rs: 5, checks: 312
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03051632
Train loss (w/o reg) on all data: 0.02002314
Test loss (w/o reg) on all data: 0.023703927
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.641431e-06
Norm of the params: 14.486671
     Influence (LOO): fixed 150 labels. Loss 0.02370. Accuracy 0.992.
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019557633
Train loss (w/o reg) on all data: 0.010259358
Test loss (w/o reg) on all data: 0.021003611
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1643365e-06
Norm of the params: 13.636916
                Loss: fixed 151 labels. Loss 0.02100. Accuracy 0.989.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20916514
Train loss (w/o reg) on all data: 0.19922182
Test loss (w/o reg) on all data: 0.16555524
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.3011063e-05
Norm of the params: 14.102003
              Random: fixed  47 labels. Loss 0.16556. Accuracy 0.935.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2820996
Train loss (w/o reg) on all data: 0.27665046
Test loss (w/o reg) on all data: 0.21233237
Train acc on all data:  0.8586437440305635
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 3.5799898e-05
Norm of the params: 10.439482
Flipped loss: 0.21233. Accuracy: 0.924
### Flips: 312, rs: 6, checks: 52
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2121758
Train loss (w/o reg) on all data: 0.20207788
Test loss (w/o reg) on all data: 0.18090802
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.0293762e-05
Norm of the params: 14.211209
     Influence (LOO): fixed  39 labels. Loss 0.18091. Accuracy 0.935.
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18324058
Train loss (w/o reg) on all data: 0.17092274
Test loss (w/o reg) on all data: 0.19184238
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.2734519e-05
Norm of the params: 15.695753
                Loss: fixed  50 labels. Loss 0.19184. Accuracy 0.920.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27417412
Train loss (w/o reg) on all data: 0.2686586
Test loss (w/o reg) on all data: 0.2036549
Train acc on all data:  0.8672397325692455
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 4.780818e-05
Norm of the params: 10.502873
              Random: fixed   7 labels. Loss 0.20365. Accuracy 0.931.
### Flips: 312, rs: 6, checks: 104
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17199954
Train loss (w/o reg) on all data: 0.1590504
Test loss (w/o reg) on all data: 0.15189743
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.0232072e-05
Norm of the params: 16.092936
     Influence (LOO): fixed  64 labels. Loss 0.15190. Accuracy 0.943.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10083632
Train loss (w/o reg) on all data: 0.085082844
Test loss (w/o reg) on all data: 0.13378365
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 9.776962e-06
Norm of the params: 17.750196
                Loss: fixed  94 labels. Loss 0.13378. Accuracy 0.943.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26667464
Train loss (w/o reg) on all data: 0.26107943
Test loss (w/o reg) on all data: 0.18993191
Train acc on all data:  0.8720152817574021
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.070741e-05
Norm of the params: 10.578485
              Random: fixed  15 labels. Loss 0.18993. Accuracy 0.931.
### Flips: 312, rs: 6, checks: 156
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12708478
Train loss (w/o reg) on all data: 0.11240743
Test loss (w/o reg) on all data: 0.11047204
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.7375907e-05
Norm of the params: 17.133211
     Influence (LOO): fixed  91 labels. Loss 0.11047. Accuracy 0.962.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04529498
Train loss (w/o reg) on all data: 0.030549543
Test loss (w/o reg) on all data: 0.0779709
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.2140414e-06
Norm of the params: 17.172909
                Loss: fixed 130 labels. Loss 0.07797. Accuracy 0.969.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26027966
Train loss (w/o reg) on all data: 0.25493312
Test loss (w/o reg) on all data: 0.18505128
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.70169e-05
Norm of the params: 10.340724
              Random: fixed  23 labels. Loss 0.18505. Accuracy 0.939.
### Flips: 312, rs: 6, checks: 208
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0878102
Train loss (w/o reg) on all data: 0.07392139
Test loss (w/o reg) on all data: 0.080801256
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.6433697e-06
Norm of the params: 16.66662
     Influence (LOO): fixed 114 labels. Loss 0.08080. Accuracy 0.973.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023111958
Train loss (w/o reg) on all data: 0.012430364
Test loss (w/o reg) on all data: 0.049677435
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7190447e-06
Norm of the params: 14.616152
                Loss: fixed 143 labels. Loss 0.04968. Accuracy 0.985.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25103194
Train loss (w/o reg) on all data: 0.24525715
Test loss (w/o reg) on all data: 0.17521343
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 5.172745e-05
Norm of the params: 10.746896
              Random: fixed  29 labels. Loss 0.17521. Accuracy 0.939.
### Flips: 312, rs: 6, checks: 260
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06429932
Train loss (w/o reg) on all data: 0.05267446
Test loss (w/o reg) on all data: 0.054752298
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.4927096e-06
Norm of the params: 15.247861
     Influence (LOO): fixed 130 labels. Loss 0.05475. Accuracy 0.977.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018329866
Train loss (w/o reg) on all data: 0.009560874
Test loss (w/o reg) on all data: 0.033166096
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 9.83091e-07
Norm of the params: 13.243105
                Loss: fixed 150 labels. Loss 0.03317. Accuracy 0.992.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24487484
Train loss (w/o reg) on all data: 0.23915555
Test loss (w/o reg) on all data: 0.16986488
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.0154266e-05
Norm of the params: 10.695131
              Random: fixed  36 labels. Loss 0.16986. Accuracy 0.943.
### Flips: 312, rs: 6, checks: 312
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03104505
Train loss (w/o reg) on all data: 0.023146393
Test loss (w/o reg) on all data: 0.03272001
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3794923e-06
Norm of the params: 12.568737
     Influence (LOO): fixed 146 labels. Loss 0.03272. Accuracy 0.989.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013301443
Train loss (w/o reg) on all data: 0.0059708715
Test loss (w/o reg) on all data: 0.019956747
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.759943e-07
Norm of the params: 12.108321
                Loss: fixed 154 labels. Loss 0.01996. Accuracy 0.992.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23188108
Train loss (w/o reg) on all data: 0.22587325
Test loss (w/o reg) on all data: 0.16501316
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.9531067e-05
Norm of the params: 10.961601
              Random: fixed  44 labels. Loss 0.16501. Accuracy 0.947.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.266353
Train loss (w/o reg) on all data: 0.25968015
Test loss (w/o reg) on all data: 0.16772787
Train acc on all data:  0.874880611270296
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.1197272e-05
Norm of the params: 11.552359
Flipped loss: 0.16773. Accuracy: 0.962
### Flips: 312, rs: 7, checks: 52
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19786835
Train loss (w/o reg) on all data: 0.18779449
Test loss (w/o reg) on all data: 0.14828336
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.7945956e-05
Norm of the params: 14.194267
     Influence (LOO): fixed  39 labels. Loss 0.14828. Accuracy 0.943.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15581112
Train loss (w/o reg) on all data: 0.13960633
Test loss (w/o reg) on all data: 0.12949994
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 6.983066e-06
Norm of the params: 18.002657
                Loss: fixed  50 labels. Loss 0.12950. Accuracy 0.962.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26393116
Train loss (w/o reg) on all data: 0.25730315
Test loss (w/o reg) on all data: 0.16525787
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 8.898849e-05
Norm of the params: 11.513481
              Random: fixed   5 labels. Loss 0.16526. Accuracy 0.958.
### Flips: 312, rs: 7, checks: 104
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15436085
Train loss (w/o reg) on all data: 0.14222796
Test loss (w/o reg) on all data: 0.12995146
Train acc on all data:  0.9312320916905444
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 0.00010820664
Norm of the params: 15.577473
     Influence (LOO): fixed  66 labels. Loss 0.12995. Accuracy 0.947.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08064613
Train loss (w/o reg) on all data: 0.06276278
Test loss (w/o reg) on all data: 0.07205084
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.0428184e-06
Norm of the params: 18.912083
                Loss: fixed  95 labels. Loss 0.07205. Accuracy 0.977.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25066411
Train loss (w/o reg) on all data: 0.24340439
Test loss (w/o reg) on all data: 0.16126622
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.673605e-05
Norm of the params: 12.049668
              Random: fixed  18 labels. Loss 0.16127. Accuracy 0.954.
### Flips: 312, rs: 7, checks: 156
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11284505
Train loss (w/o reg) on all data: 0.10029233
Test loss (w/o reg) on all data: 0.095231876
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.01364e-06
Norm of the params: 15.844696
     Influence (LOO): fixed  89 labels. Loss 0.09523. Accuracy 0.973.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033471473
Train loss (w/o reg) on all data: 0.018047629
Test loss (w/o reg) on all data: 0.03197381
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.993981e-06
Norm of the params: 17.563509
                Loss: fixed 123 labels. Loss 0.03197. Accuracy 0.992.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24163082
Train loss (w/o reg) on all data: 0.23447011
Test loss (w/o reg) on all data: 0.14495838
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.9333142e-05
Norm of the params: 11.9672165
              Random: fixed  27 labels. Loss 0.14496. Accuracy 0.966.
### Flips: 312, rs: 7, checks: 208
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07569675
Train loss (w/o reg) on all data: 0.062542014
Test loss (w/o reg) on all data: 0.061221007
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.9010392e-06
Norm of the params: 16.220198
     Influence (LOO): fixed 111 labels. Loss 0.06122. Accuracy 0.977.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030158702
Train loss (w/o reg) on all data: 0.015535777
Test loss (w/o reg) on all data: 0.021399356
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 8.039946e-06
Norm of the params: 17.10142
                Loss: fixed 127 labels. Loss 0.02140. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23544765
Train loss (w/o reg) on all data: 0.22814263
Test loss (w/o reg) on all data: 0.14023262
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.534604e-05
Norm of the params: 12.087192
              Random: fixed  33 labels. Loss 0.14023. Accuracy 0.966.
### Flips: 312, rs: 7, checks: 260
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05106047
Train loss (w/o reg) on all data: 0.03937644
Test loss (w/o reg) on all data: 0.046246473
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.6457826e-06
Norm of the params: 15.286614
     Influence (LOO): fixed 129 labels. Loss 0.04625. Accuracy 0.985.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02135069
Train loss (w/o reg) on all data: 0.01028095
Test loss (w/o reg) on all data: 0.01770144
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.4669837e-06
Norm of the params: 14.879341
                Loss: fixed 137 labels. Loss 0.01770. Accuracy 0.992.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23105006
Train loss (w/o reg) on all data: 0.22382657
Test loss (w/o reg) on all data: 0.12805872
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.1044521e-05
Norm of the params: 12.019556
              Random: fixed  38 labels. Loss 0.12806. Accuracy 0.966.
### Flips: 312, rs: 7, checks: 312
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020926401
Train loss (w/o reg) on all data: 0.011347141
Test loss (w/o reg) on all data: 0.02005876
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 4.173954e-06
Norm of the params: 13.841431
     Influence (LOO): fixed 143 labels. Loss 0.02006. Accuracy 0.996.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014120522
Train loss (w/o reg) on all data: 0.0063333274
Test loss (w/o reg) on all data: 0.011373485
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.2388144e-07
Norm of the params: 12.479739
                Loss: fixed 144 labels. Loss 0.01137. Accuracy 0.992.
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22606501
Train loss (w/o reg) on all data: 0.21854421
Test loss (w/o reg) on all data: 0.13240282
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.6183738e-05
Norm of the params: 12.264419
              Random: fixed  43 labels. Loss 0.13240. Accuracy 0.966.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2902276
Train loss (w/o reg) on all data: 0.28599015
Test loss (w/o reg) on all data: 0.2457128
Train acc on all data:  0.8576886341929322
Test acc on all data:   0.8740458015267175
Norm of the mean of gradients: 8.948113e-05
Norm of the params: 9.205922
Flipped loss: 0.24571. Accuracy: 0.874
### Flips: 312, rs: 8, checks: 52
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21422103
Train loss (w/o reg) on all data: 0.20340674
Test loss (w/o reg) on all data: 0.21280335
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 2.5937168e-05
Norm of the params: 14.706658
     Influence (LOO): fixed  40 labels. Loss 0.21280. Accuracy 0.908.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17581105
Train loss (w/o reg) on all data: 0.16155922
Test loss (w/o reg) on all data: 0.21319923
Train acc on all data:  0.9169054441260746
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 8.744872e-06
Norm of the params: 16.883024
                Loss: fixed  50 labels. Loss 0.21320. Accuracy 0.901.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2855105
Train loss (w/o reg) on all data: 0.28101096
Test loss (w/o reg) on all data: 0.2384412
Train acc on all data:  0.8624641833810889
Test acc on all data:   0.8893129770992366
Norm of the mean of gradients: 0.000121207464
Norm of the params: 9.486377
              Random: fixed   7 labels. Loss 0.23844. Accuracy 0.889.
### Flips: 312, rs: 8, checks: 104
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16841874
Train loss (w/o reg) on all data: 0.15629159
Test loss (w/o reg) on all data: 0.16785368
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.8927985e-05
Norm of the params: 15.573791
     Influence (LOO): fixed  67 labels. Loss 0.16785. Accuracy 0.935.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.114299364
Train loss (w/o reg) on all data: 0.09506776
Test loss (w/o reg) on all data: 0.16160579
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 7.1534914e-06
Norm of the params: 19.612038
                Loss: fixed  87 labels. Loss 0.16161. Accuracy 0.935.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28140756
Train loss (w/o reg) on all data: 0.2771034
Test loss (w/o reg) on all data: 0.22523932
Train acc on all data:  0.8662846227316141
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 3.670604e-05
Norm of the params: 9.278126
              Random: fixed  15 labels. Loss 0.22524. Accuracy 0.901.
### Flips: 312, rs: 8, checks: 156
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12977228
Train loss (w/o reg) on all data: 0.11592756
Test loss (w/o reg) on all data: 0.13329205
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.2396223e-05
Norm of the params: 16.640144
     Influence (LOO): fixed  92 labels. Loss 0.13329. Accuracy 0.927.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06591858
Train loss (w/o reg) on all data: 0.047483977
Test loss (w/o reg) on all data: 0.11942746
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.9309682e-05
Norm of the params: 19.201355
                Loss: fixed 117 labels. Loss 0.11943. Accuracy 0.947.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2761362
Train loss (w/o reg) on all data: 0.27183148
Test loss (w/o reg) on all data: 0.21553253
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 6.8421985e-05
Norm of the params: 9.278692
              Random: fixed  23 labels. Loss 0.21553. Accuracy 0.908.
### Flips: 312, rs: 8, checks: 208
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0994903
Train loss (w/o reg) on all data: 0.08585194
Test loss (w/o reg) on all data: 0.105009645
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.699249e-06
Norm of the params: 16.515669
     Influence (LOO): fixed 114 labels. Loss 0.10501. Accuracy 0.950.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042467006
Train loss (w/o reg) on all data: 0.026613519
Test loss (w/o reg) on all data: 0.094336994
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.5026757e-06
Norm of the params: 17.806452
                Loss: fixed 136 labels. Loss 0.09434. Accuracy 0.966.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2647014
Train loss (w/o reg) on all data: 0.2601906
Test loss (w/o reg) on all data: 0.19257672
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.8385806e-05
Norm of the params: 9.498198
              Random: fixed  36 labels. Loss 0.19258. Accuracy 0.950.
### Flips: 312, rs: 8, checks: 260
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06889291
Train loss (w/o reg) on all data: 0.056188826
Test loss (w/o reg) on all data: 0.06437189
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.87863e-06
Norm of the params: 15.939941
     Influence (LOO): fixed 133 labels. Loss 0.06437. Accuracy 0.969.
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030654225
Train loss (w/o reg) on all data: 0.017415501
Test loss (w/o reg) on all data: 0.039640147
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.1366635e-06
Norm of the params: 16.271893
                Loss: fixed 151 labels. Loss 0.03964. Accuracy 0.989.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24892834
Train loss (w/o reg) on all data: 0.24369696
Test loss (w/o reg) on all data: 0.17181043
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.0609168e-05
Norm of the params: 10.228766
              Random: fixed  48 labels. Loss 0.17181. Accuracy 0.954.
### Flips: 312, rs: 8, checks: 312
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05579842
Train loss (w/o reg) on all data: 0.044910617
Test loss (w/o reg) on all data: 0.037550166
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.0987153e-06
Norm of the params: 14.756559
     Influence (LOO): fixed 145 labels. Loss 0.03755. Accuracy 0.992.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02339895
Train loss (w/o reg) on all data: 0.011840056
Test loss (w/o reg) on all data: 0.02709811
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.651646e-06
Norm of the params: 15.204536
                Loss: fixed 158 labels. Loss 0.02710. Accuracy 0.989.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24150388
Train loss (w/o reg) on all data: 0.23657043
Test loss (w/o reg) on all data: 0.15898217
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.561283e-05
Norm of the params: 9.933222
              Random: fixed  56 labels. Loss 0.15898. Accuracy 0.954.
Using normal model
LBFGS training took [314] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2822811
Train loss (w/o reg) on all data: 0.27553803
Test loss (w/o reg) on all data: 0.23027621
Train acc on all data:  0.8615090735434575
Test acc on all data:   0.8893129770992366
Norm of the mean of gradients: 5.959343e-05
Norm of the params: 11.61299
Flipped loss: 0.23028. Accuracy: 0.889
### Flips: 312, rs: 9, checks: 52
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2108271
Train loss (w/o reg) on all data: 0.19926925
Test loss (w/o reg) on all data: 0.20316902
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.8893129770992366
Norm of the mean of gradients: 3.6856974e-05
Norm of the params: 15.203843
     Influence (LOO): fixed  39 labels. Loss 0.20317. Accuracy 0.889.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17279916
Train loss (w/o reg) on all data: 0.15591867
Test loss (w/o reg) on all data: 0.21008907
Train acc on all data:  0.9216809933142311
Test acc on all data:   0.8969465648854962
Norm of the mean of gradients: 1.9967143e-05
Norm of the params: 18.37416
                Loss: fixed  50 labels. Loss 0.21009. Accuracy 0.897.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27613336
Train loss (w/o reg) on all data: 0.2691985
Test loss (w/o reg) on all data: 0.21497123
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.8931297709923665
Norm of the mean of gradients: 2.5810808e-05
Norm of the params: 11.776983
              Random: fixed   7 labels. Loss 0.21497. Accuracy 0.893.
### Flips: 312, rs: 9, checks: 104
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1631542
Train loss (w/o reg) on all data: 0.15146923
Test loss (w/o reg) on all data: 0.1689091
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.3930499e-05
Norm of the params: 15.28723
     Influence (LOO): fixed  68 labels. Loss 0.16891. Accuracy 0.927.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10819319
Train loss (w/o reg) on all data: 0.08791454
Test loss (w/o reg) on all data: 0.15467148
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.538868e-05
Norm of the params: 20.13884
                Loss: fixed  92 labels. Loss 0.15467. Accuracy 0.924.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26536453
Train loss (w/o reg) on all data: 0.25830838
Test loss (w/o reg) on all data: 0.20645297
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 6.208652e-05
Norm of the params: 11.8795185
              Random: fixed  16 labels. Loss 0.20645. Accuracy 0.901.
### Flips: 312, rs: 9, checks: 156
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13093917
Train loss (w/o reg) on all data: 0.11888641
Test loss (w/o reg) on all data: 0.13972646
Train acc on all data:  0.9503342884431709
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.204797e-05
Norm of the params: 15.525956
     Influence (LOO): fixed  92 labels. Loss 0.13973. Accuracy 0.943.
Using normal model
LBFGS training took [184] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061759755
Train loss (w/o reg) on all data: 0.04450769
Test loss (w/o reg) on all data: 0.07123386
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.4985224e-05
Norm of the params: 18.575287
                Loss: fixed 125 labels. Loss 0.07123. Accuracy 0.973.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25182477
Train loss (w/o reg) on all data: 0.24416201
Test loss (w/o reg) on all data: 0.20213696
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.8399347e-05
Norm of the params: 12.379633
              Random: fixed  26 labels. Loss 0.20214. Accuracy 0.908.
### Flips: 312, rs: 9, checks: 208
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0957146
Train loss (w/o reg) on all data: 0.084033646
Test loss (w/o reg) on all data: 0.099123836
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.818621e-06
Norm of the params: 15.284604
     Influence (LOO): fixed 113 labels. Loss 0.09912. Accuracy 0.958.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046104446
Train loss (w/o reg) on all data: 0.02969948
Test loss (w/o reg) on all data: 0.061042402
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8756828e-06
Norm of the params: 18.113514
                Loss: fixed 137 labels. Loss 0.06104. Accuracy 0.985.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2421811
Train loss (w/o reg) on all data: 0.23452093
Test loss (w/o reg) on all data: 0.18610685
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.6345768e-05
Norm of the params: 12.377544
              Random: fixed  37 labels. Loss 0.18611. Accuracy 0.908.
### Flips: 312, rs: 9, checks: 260
Using normal model
LBFGS training took [188] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06656132
Train loss (w/o reg) on all data: 0.054320645
Test loss (w/o reg) on all data: 0.07200838
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.395946e-06
Norm of the params: 15.646517
     Influence (LOO): fixed 129 labels. Loss 0.07201. Accuracy 0.962.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029602788
Train loss (w/o reg) on all data: 0.016887547
Test loss (w/o reg) on all data: 0.02272984
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.583038e-06
Norm of the params: 15.946938
                Loss: fixed 151 labels. Loss 0.02273. Accuracy 0.992.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22599407
Train loss (w/o reg) on all data: 0.21747252
Test loss (w/o reg) on all data: 0.18476057
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 2.4196272e-05
Norm of the params: 13.054916
              Random: fixed  47 labels. Loss 0.18476. Accuracy 0.912.
### Flips: 312, rs: 9, checks: 312
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040414087
Train loss (w/o reg) on all data: 0.030571442
Test loss (w/o reg) on all data: 0.034931246
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.21014245e-05
Norm of the params: 14.030428
     Influence (LOO): fixed 146 labels. Loss 0.03493. Accuracy 0.989.
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022132285
Train loss (w/o reg) on all data: 0.0119545255
Test loss (w/o reg) on all data: 0.022920711
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.5136216e-06
Norm of the params: 14.267278
                Loss: fixed 157 labels. Loss 0.02292. Accuracy 0.989.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22066344
Train loss (w/o reg) on all data: 0.21356173
Test loss (w/o reg) on all data: 0.16185886
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 9.198395e-06
Norm of the params: 11.9178095
              Random: fixed  56 labels. Loss 0.16186. Accuracy 0.939.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.29577163
Train loss (w/o reg) on all data: 0.28958726
Test loss (w/o reg) on all data: 0.23187935
Train acc on all data:  0.8557784145176696
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 8.447465e-05
Norm of the params: 11.121477
Flipped loss: 0.23188. Accuracy: 0.935
### Flips: 312, rs: 10, checks: 52
Using normal model
LBFGS training took [306] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24273065
Train loss (w/o reg) on all data: 0.23393516
Test loss (w/o reg) on all data: 0.19932099
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 5.4292082e-05
Norm of the params: 13.263092
     Influence (LOO): fixed  34 labels. Loss 0.19932. Accuracy 0.916.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20840344
Train loss (w/o reg) on all data: 0.19700155
Test loss (w/o reg) on all data: 0.1852359
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.4691495e-05
Norm of the params: 15.100921
                Loss: fixed  46 labels. Loss 0.18524. Accuracy 0.927.
Using normal model
LBFGS training took [268] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28995574
Train loss (w/o reg) on all data: 0.28377613
Test loss (w/o reg) on all data: 0.22165643
Train acc on all data:  0.8576886341929322
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.8478395e-05
Norm of the params: 11.117208
              Random: fixed   8 labels. Loss 0.22166. Accuracy 0.927.
### Flips: 312, rs: 10, checks: 104
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19551918
Train loss (w/o reg) on all data: 0.18424474
Test loss (w/o reg) on all data: 0.1550518
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 6.1809005e-05
Norm of the params: 15.016289
     Influence (LOO): fixed  65 labels. Loss 0.15505. Accuracy 0.939.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11291413
Train loss (w/o reg) on all data: 0.09392772
Test loss (w/o reg) on all data: 0.13158019
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.5847147e-05
Norm of the params: 19.486618
                Loss: fixed  94 labels. Loss 0.13158. Accuracy 0.947.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28675398
Train loss (w/o reg) on all data: 0.28067216
Test loss (w/o reg) on all data: 0.20461276
Train acc on all data:  0.8634192932187201
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.8418546e-05
Norm of the params: 11.028899
              Random: fixed  15 labels. Loss 0.20461. Accuracy 0.947.
### Flips: 312, rs: 10, checks: 156
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15840983
Train loss (w/o reg) on all data: 0.14474013
Test loss (w/o reg) on all data: 0.12353063
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.5204853e-05
Norm of the params: 16.534632
     Influence (LOO): fixed  91 labels. Loss 0.12353. Accuracy 0.954.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06969209
Train loss (w/o reg) on all data: 0.051216565
Test loss (w/o reg) on all data: 0.06873665
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.37341e-06
Norm of the params: 19.222658
                Loss: fixed 127 labels. Loss 0.06874. Accuracy 0.973.
Using normal model
LBFGS training took [292] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2733572
Train loss (w/o reg) on all data: 0.26653633
Test loss (w/o reg) on all data: 0.19848423
Train acc on all data:  0.8672397325692455
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.5536776e-05
Norm of the params: 11.679795
              Random: fixed  26 labels. Loss 0.19848. Accuracy 0.947.
### Flips: 312, rs: 10, checks: 208
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12495549
Train loss (w/o reg) on all data: 0.112389386
Test loss (w/o reg) on all data: 0.08284356
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.687701e-06
Norm of the params: 15.853144
     Influence (LOO): fixed 119 labels. Loss 0.08284. Accuracy 0.981.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047595274
Train loss (w/o reg) on all data: 0.031759046
Test loss (w/o reg) on all data: 0.04814336
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.058977e-06
Norm of the params: 17.796757
                Loss: fixed 144 labels. Loss 0.04814. Accuracy 0.981.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27078757
Train loss (w/o reg) on all data: 0.2638005
Test loss (w/o reg) on all data: 0.19352031
Train acc on all data:  0.8720152817574021
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.7535565e-05
Norm of the params: 11.821218
              Random: fixed  31 labels. Loss 0.19352. Accuracy 0.950.
### Flips: 312, rs: 10, checks: 260
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.079890534
Train loss (w/o reg) on all data: 0.06662909
Test loss (w/o reg) on all data: 0.047618564
Train acc on all data:  0.9694364851957975
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.1208275e-06
Norm of the params: 16.285852
     Influence (LOO): fixed 144 labels. Loss 0.04762. Accuracy 0.985.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.030553874
Train loss (w/o reg) on all data: 0.016937692
Test loss (w/o reg) on all data: 0.03059801
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.114878e-06
Norm of the params: 16.502232
                Loss: fixed 155 labels. Loss 0.03060. Accuracy 0.989.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26187795
Train loss (w/o reg) on all data: 0.25484857
Test loss (w/o reg) on all data: 0.1792578
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.9555925e-05
Norm of the params: 11.856963
              Random: fixed  42 labels. Loss 0.17926. Accuracy 0.950.
### Flips: 312, rs: 10, checks: 312
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046084743
Train loss (w/o reg) on all data: 0.036031462
Test loss (w/o reg) on all data: 0.026494605
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.5726079e-06
Norm of the params: 14.179761
     Influence (LOO): fixed 163 labels. Loss 0.02649. Accuracy 0.992.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02673994
Train loss (w/o reg) on all data: 0.014193386
Test loss (w/o reg) on all data: 0.020747518
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.199898e-06
Norm of the params: 15.840805
                Loss: fixed 161 labels. Loss 0.02075. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2519658
Train loss (w/o reg) on all data: 0.24448064
Test loss (w/o reg) on all data: 0.17347404
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.7121738e-05
Norm of the params: 12.235327
              Random: fixed  48 labels. Loss 0.17347. Accuracy 0.943.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27137628
Train loss (w/o reg) on all data: 0.26565403
Test loss (w/o reg) on all data: 0.25364387
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 6.693638e-05
Norm of the params: 10.697914
Flipped loss: 0.25364. Accuracy: 0.901
### Flips: 312, rs: 11, checks: 52
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20376018
Train loss (w/o reg) on all data: 0.19244482
Test loss (w/o reg) on all data: 0.21892959
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.1702287e-05
Norm of the params: 15.04351
     Influence (LOO): fixed  36 labels. Loss 0.21893. Accuracy 0.920.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16880414
Train loss (w/o reg) on all data: 0.1529486
Test loss (w/o reg) on all data: 0.2605863
Train acc on all data:  0.9321872015281757
Test acc on all data:   0.8854961832061069
Norm of the mean of gradients: 1.9127527e-05
Norm of the params: 17.807602
                Loss: fixed  46 labels. Loss 0.26059. Accuracy 0.885.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26490808
Train loss (w/o reg) on all data: 0.2590536
Test loss (w/o reg) on all data: 0.25213966
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 2.5934372e-05
Norm of the params: 10.820787
              Random: fixed   7 labels. Loss 0.25214. Accuracy 0.901.
### Flips: 312, rs: 11, checks: 104
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16341488
Train loss (w/o reg) on all data: 0.15069595
Test loss (w/o reg) on all data: 0.17819065
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 4.573946e-05
Norm of the params: 15.949256
     Influence (LOO): fixed  63 labels. Loss 0.17819. Accuracy 0.908.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104952194
Train loss (w/o reg) on all data: 0.08714122
Test loss (w/o reg) on all data: 0.17965864
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.9305355e-06
Norm of the params: 18.873777
                Loss: fixed  86 labels. Loss 0.17966. Accuracy 0.935.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2585671
Train loss (w/o reg) on all data: 0.25273657
Test loss (w/o reg) on all data: 0.22748874
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 4.734554e-05
Norm of the params: 10.798624
              Random: fixed  18 labels. Loss 0.22749. Accuracy 0.908.
### Flips: 312, rs: 11, checks: 156
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11840618
Train loss (w/o reg) on all data: 0.10748434
Test loss (w/o reg) on all data: 0.1331937
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 5.3618455e-06
Norm of the params: 14.779602
     Influence (LOO): fixed  97 labels. Loss 0.13319. Accuracy 0.939.
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.065537915
Train loss (w/o reg) on all data: 0.046575867
Test loss (w/o reg) on all data: 0.11392804
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.4590696e-06
Norm of the params: 19.47411
                Loss: fixed 114 labels. Loss 0.11393. Accuracy 0.958.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24804884
Train loss (w/o reg) on all data: 0.24201296
Test loss (w/o reg) on all data: 0.2100404
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 4.8131682e-05
Norm of the params: 10.98715
              Random: fixed  26 labels. Loss 0.21004. Accuracy 0.920.
### Flips: 312, rs: 11, checks: 208
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07511683
Train loss (w/o reg) on all data: 0.06509551
Test loss (w/o reg) on all data: 0.083444126
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.5855197e-06
Norm of the params: 14.157206
     Influence (LOO): fixed 122 labels. Loss 0.08344. Accuracy 0.969.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031231537
Train loss (w/o reg) on all data: 0.018501366
Test loss (w/o reg) on all data: 0.06842712
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 6.0968714e-06
Norm of the params: 15.956299
                Loss: fixed 136 labels. Loss 0.06843. Accuracy 0.973.
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23916148
Train loss (w/o reg) on all data: 0.23293781
Test loss (w/o reg) on all data: 0.19909602
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 3.0440666e-05
Norm of the params: 11.156755
              Random: fixed  34 labels. Loss 0.19910. Accuracy 0.912.
### Flips: 312, rs: 11, checks: 260
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04488728
Train loss (w/o reg) on all data: 0.035633422
Test loss (w/o reg) on all data: 0.048078157
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.99326e-06
Norm of the params: 13.604305
     Influence (LOO): fixed 141 labels. Loss 0.04808. Accuracy 0.989.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023677621
Train loss (w/o reg) on all data: 0.013219694
Test loss (w/o reg) on all data: 0.047972497
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.9529216e-06
Norm of the params: 14.462315
                Loss: fixed 145 labels. Loss 0.04797. Accuracy 0.989.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22769824
Train loss (w/o reg) on all data: 0.2215616
Test loss (w/o reg) on all data: 0.19505444
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.4411826e-05
Norm of the params: 11.0784855
              Random: fixed  42 labels. Loss 0.19505. Accuracy 0.916.
### Flips: 312, rs: 11, checks: 312
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017065845
Train loss (w/o reg) on all data: 0.009230382
Test loss (w/o reg) on all data: 0.026775671
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.076092e-07
Norm of the params: 12.518358
     Influence (LOO): fixed 152 labels. Loss 0.02678. Accuracy 0.992.
Using normal model
LBFGS training took [71] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01725928
Train loss (w/o reg) on all data: 0.008759874
Test loss (w/o reg) on all data: 0.024389915
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.3638764e-06
Norm of the params: 13.037949
                Loss: fixed 150 labels. Loss 0.02439. Accuracy 0.989.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21807453
Train loss (w/o reg) on all data: 0.21189038
Test loss (w/o reg) on all data: 0.17402045
Train acc on all data:  0.9083094555873925
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.5659574e-05
Norm of the params: 11.121283
              Random: fixed  51 labels. Loss 0.17402. Accuracy 0.931.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27822936
Train loss (w/o reg) on all data: 0.2726637
Test loss (w/o reg) on all data: 0.22673316
Train acc on all data:  0.8567335243553008
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 6.127769e-05
Norm of the params: 10.550487
Flipped loss: 0.22673. Accuracy: 0.905
### Flips: 312, rs: 12, checks: 52
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21037054
Train loss (w/o reg) on all data: 0.20128998
Test loss (w/o reg) on all data: 0.18293104
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 2.5023648e-05
Norm of the params: 13.476324
     Influence (LOO): fixed  39 labels. Loss 0.18293. Accuracy 0.916.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17248453
Train loss (w/o reg) on all data: 0.16094914
Test loss (w/o reg) on all data: 0.20511405
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 1.0189821e-05
Norm of the params: 15.189068
                Loss: fixed  50 labels. Loss 0.20511. Accuracy 0.901.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27204406
Train loss (w/o reg) on all data: 0.26641852
Test loss (w/o reg) on all data: 0.21926355
Train acc on all data:  0.8653295128939829
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.6259031e-05
Norm of the params: 10.607111
              Random: fixed   8 labels. Loss 0.21926. Accuracy 0.920.
### Flips: 312, rs: 12, checks: 104
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1570488
Train loss (w/o reg) on all data: 0.14539754
Test loss (w/o reg) on all data: 0.14559811
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 8.746944e-06
Norm of the params: 15.265167
     Influence (LOO): fixed  72 labels. Loss 0.14560. Accuracy 0.954.
Using normal model
LBFGS training took [204] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09055488
Train loss (w/o reg) on all data: 0.07395743
Test loss (w/o reg) on all data: 0.16289578
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.2731716e-05
Norm of the params: 18.219467
                Loss: fixed  96 labels. Loss 0.16290. Accuracy 0.943.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2643167
Train loss (w/o reg) on all data: 0.25852188
Test loss (w/o reg) on all data: 0.21608384
Train acc on all data:  0.8720152817574021
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 8.765391e-05
Norm of the params: 10.765511
              Random: fixed  14 labels. Loss 0.21608. Accuracy 0.927.
### Flips: 312, rs: 12, checks: 156
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12494219
Train loss (w/o reg) on all data: 0.11187346
Test loss (w/o reg) on all data: 0.12509154
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.1150757e-05
Norm of the params: 16.167084
     Influence (LOO): fixed  92 labels. Loss 0.12509. Accuracy 0.947.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04420971
Train loss (w/o reg) on all data: 0.028614683
Test loss (w/o reg) on all data: 0.07654391
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4820114e-06
Norm of the params: 17.660707
                Loss: fixed 126 labels. Loss 0.07654. Accuracy 0.977.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2604338
Train loss (w/o reg) on all data: 0.25494564
Test loss (w/o reg) on all data: 0.20234337
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.5322446e-05
Norm of the params: 10.47678
              Random: fixed  20 labels. Loss 0.20234. Accuracy 0.931.
### Flips: 312, rs: 12, checks: 208
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07826954
Train loss (w/o reg) on all data: 0.064748354
Test loss (w/o reg) on all data: 0.095821925
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.540312e-06
Norm of the params: 16.444567
     Influence (LOO): fixed 117 labels. Loss 0.09582. Accuracy 0.966.
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032006636
Train loss (w/o reg) on all data: 0.017633062
Test loss (w/o reg) on all data: 0.052802864
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.6930476e-06
Norm of the params: 16.954987
                Loss: fixed 136 labels. Loss 0.05280. Accuracy 0.977.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2554305
Train loss (w/o reg) on all data: 0.25052822
Test loss (w/o reg) on all data: 0.18266709
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 6.0188413e-05
Norm of the params: 9.901804
              Random: fixed  29 labels. Loss 0.18267. Accuracy 0.950.
### Flips: 312, rs: 12, checks: 260
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.047851417
Train loss (w/o reg) on all data: 0.035299018
Test loss (w/o reg) on all data: 0.09596777
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.1507944e-06
Norm of the params: 15.844493
     Influence (LOO): fixed 132 labels. Loss 0.09597. Accuracy 0.947.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027075645
Train loss (w/o reg) on all data: 0.015796661
Test loss (w/o reg) on all data: 0.037221745
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6807633e-06
Norm of the params: 15.019311
                Loss: fixed 144 labels. Loss 0.03722. Accuracy 0.989.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23928158
Train loss (w/o reg) on all data: 0.23444322
Test loss (w/o reg) on all data: 0.17543308
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 9.883604e-06
Norm of the params: 9.837039
              Random: fixed  41 labels. Loss 0.17543. Accuracy 0.939.
### Flips: 312, rs: 12, checks: 312
Using normal model
LBFGS training took [114] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042729393
Train loss (w/o reg) on all data: 0.03120524
Test loss (w/o reg) on all data: 0.0769144
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.7133718e-05
Norm of the params: 15.181667
     Influence (LOO): fixed 141 labels. Loss 0.07691. Accuracy 0.966.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017627906
Train loss (w/o reg) on all data: 0.0083979
Test loss (w/o reg) on all data: 0.027817156
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.6539396e-06
Norm of the params: 13.586763
                Loss: fixed 149 labels. Loss 0.02782. Accuracy 0.989.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22835496
Train loss (w/o reg) on all data: 0.22320312
Test loss (w/o reg) on all data: 0.16988374
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.2551029e-05
Norm of the params: 10.150706
              Random: fixed  48 labels. Loss 0.16988. Accuracy 0.947.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2621229
Train loss (w/o reg) on all data: 0.2559195
Test loss (w/o reg) on all data: 0.25705215
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.8969465648854962
Norm of the mean of gradients: 1.7597898e-05
Norm of the params: 11.138593
Flipped loss: 0.25705. Accuracy: 0.897
### Flips: 312, rs: 13, checks: 52
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20132086
Train loss (w/o reg) on all data: 0.18958916
Test loss (w/o reg) on all data: 0.23176372
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 1.27449675e-05
Norm of the params: 15.317772
     Influence (LOO): fixed  37 labels. Loss 0.23176. Accuracy 0.912.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16493142
Train loss (w/o reg) on all data: 0.1494986
Test loss (w/o reg) on all data: 0.2564595
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.4859617e-05
Norm of the params: 17.568619
                Loss: fixed  45 labels. Loss 0.25646. Accuracy 0.908.
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25277767
Train loss (w/o reg) on all data: 0.24626076
Test loss (w/o reg) on all data: 0.24933164
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.8969465648854962
Norm of the mean of gradients: 3.4819393e-05
Norm of the params: 11.416581
              Random: fixed   7 labels. Loss 0.24933. Accuracy 0.897.
### Flips: 312, rs: 13, checks: 104
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15769169
Train loss (w/o reg) on all data: 0.1450233
Test loss (w/o reg) on all data: 0.19901572
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 9.376939e-06
Norm of the params: 15.917531
     Influence (LOO): fixed  63 labels. Loss 0.19902. Accuracy 0.912.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09291331
Train loss (w/o reg) on all data: 0.07465294
Test loss (w/o reg) on all data: 0.20827481
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.3226275e-05
Norm of the params: 19.1104
                Loss: fixed  84 labels. Loss 0.20827. Accuracy 0.924.
Using normal model
LBFGS training took [303] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24294148
Train loss (w/o reg) on all data: 0.2358873
Test loss (w/o reg) on all data: 0.23223922
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 3.1963897e-05
Norm of the params: 11.87786
              Random: fixed  18 labels. Loss 0.23224. Accuracy 0.924.
### Flips: 312, rs: 13, checks: 156
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12640001
Train loss (w/o reg) on all data: 0.11258465
Test loss (w/o reg) on all data: 0.17211269
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.2691194e-06
Norm of the params: 16.622486
     Influence (LOO): fixed  87 labels. Loss 0.17211. Accuracy 0.935.
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05073574
Train loss (w/o reg) on all data: 0.034436334
Test loss (w/o reg) on all data: 0.12922135
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.4783981e-06
Norm of the params: 18.055143
                Loss: fixed 114 labels. Loss 0.12922. Accuracy 0.954.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23900732
Train loss (w/o reg) on all data: 0.23213564
Test loss (w/o reg) on all data: 0.22767588
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 5.3834476e-05
Norm of the params: 11.723219
              Random: fixed  24 labels. Loss 0.22768. Accuracy 0.908.
### Flips: 312, rs: 13, checks: 208
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08606585
Train loss (w/o reg) on all data: 0.0734472
Test loss (w/o reg) on all data: 0.11199508
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.9744216e-06
Norm of the params: 15.886252
     Influence (LOO): fixed 110 labels. Loss 0.11200. Accuracy 0.962.
Using normal model
LBFGS training took [155] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040125243
Train loss (w/o reg) on all data: 0.025691371
Test loss (w/o reg) on all data: 0.10128319
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 7.850888e-06
Norm of the params: 16.99051
                Loss: fixed 126 labels. Loss 0.10128. Accuracy 0.962.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23723012
Train loss (w/o reg) on all data: 0.230748
Test loss (w/o reg) on all data: 0.19975072
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.2898997e-05
Norm of the params: 11.386068
              Random: fixed  32 labels. Loss 0.19975. Accuracy 0.927.
### Flips: 312, rs: 13, checks: 260
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054762717
Train loss (w/o reg) on all data: 0.043374125
Test loss (w/o reg) on all data: 0.07695022
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.9506328e-06
Norm of the params: 15.092112
     Influence (LOO): fixed 131 labels. Loss 0.07695. Accuracy 0.966.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03204205
Train loss (w/o reg) on all data: 0.019140122
Test loss (w/o reg) on all data: 0.07979205
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.8281804e-06
Norm of the params: 16.063576
                Loss: fixed 134 labels. Loss 0.07979. Accuracy 0.973.
Using normal model
LBFGS training took [263] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22932836
Train loss (w/o reg) on all data: 0.22221449
Test loss (w/o reg) on all data: 0.18421458
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.9674897e-05
Norm of the params: 11.928014
              Random: fixed  39 labels. Loss 0.18421. Accuracy 0.939.
### Flips: 312, rs: 13, checks: 312
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038378183
Train loss (w/o reg) on all data: 0.028618444
Test loss (w/o reg) on all data: 0.044042576
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9175275e-06
Norm of the params: 13.971213
     Influence (LOO): fixed 142 labels. Loss 0.04404. Accuracy 0.981.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023719741
Train loss (w/o reg) on all data: 0.013462859
Test loss (w/o reg) on all data: 0.05965213
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.6615547e-06
Norm of the params: 14.322627
                Loss: fixed 143 labels. Loss 0.05965. Accuracy 0.969.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21912508
Train loss (w/o reg) on all data: 0.21238954
Test loss (w/o reg) on all data: 0.17087974
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.6473087e-05
Norm of the params: 11.606498
              Random: fixed  48 labels. Loss 0.17088. Accuracy 0.943.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2906839
Train loss (w/o reg) on all data: 0.28554115
Test loss (w/o reg) on all data: 0.20574225
Train acc on all data:  0.8586437440305635
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.7056994e-05
Norm of the params: 10.141753
Flipped loss: 0.20574. Accuracy: 0.947
### Flips: 312, rs: 14, checks: 52
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22949484
Train loss (w/o reg) on all data: 0.2198357
Test loss (w/o reg) on all data: 0.18051988
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.5047135e-05
Norm of the params: 13.899018
     Influence (LOO): fixed  36 labels. Loss 0.18052. Accuracy 0.943.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17642793
Train loss (w/o reg) on all data: 0.16238625
Test loss (w/o reg) on all data: 0.17921959
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.6238731e-05
Norm of the params: 16.758091
                Loss: fixed  52 labels. Loss 0.17922. Accuracy 0.935.
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28064603
Train loss (w/o reg) on all data: 0.27516225
Test loss (w/o reg) on all data: 0.18766406
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.1523753e-05
Norm of the params: 10.472619
              Random: fixed   9 labels. Loss 0.18766. Accuracy 0.950.
### Flips: 312, rs: 14, checks: 104
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18576923
Train loss (w/o reg) on all data: 0.17525795
Test loss (w/o reg) on all data: 0.14954416
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.0782684e-05
Norm of the params: 14.499163
     Influence (LOO): fixed  66 labels. Loss 0.14954. Accuracy 0.947.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.092524394
Train loss (w/o reg) on all data: 0.07126381
Test loss (w/o reg) on all data: 0.14629263
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.511651e-06
Norm of the params: 20.620663
                Loss: fixed  96 labels. Loss 0.14629. Accuracy 0.954.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2670409
Train loss (w/o reg) on all data: 0.26150098
Test loss (w/o reg) on all data: 0.17333797
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.7923312e-05
Norm of the params: 10.526093
              Random: fixed  22 labels. Loss 0.17334. Accuracy 0.947.
### Flips: 312, rs: 14, checks: 156
Using normal model
LBFGS training took [208] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14181586
Train loss (w/o reg) on all data: 0.13101695
Test loss (w/o reg) on all data: 0.13279667
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 8.050854e-06
Norm of the params: 14.696187
     Influence (LOO): fixed  92 labels. Loss 0.13280. Accuracy 0.954.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.060470827
Train loss (w/o reg) on all data: 0.04151005
Test loss (w/o reg) on all data: 0.11529132
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.8788959e-06
Norm of the params: 19.47346
                Loss: fixed 124 labels. Loss 0.11529. Accuracy 0.950.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25185594
Train loss (w/o reg) on all data: 0.24563092
Test loss (w/o reg) on all data: 0.1687146
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.2505776e-05
Norm of the params: 11.157971
              Random: fixed  33 labels. Loss 0.16871. Accuracy 0.950.
### Flips: 312, rs: 14, checks: 208
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0944349
Train loss (w/o reg) on all data: 0.08170457
Test loss (w/o reg) on all data: 0.10632666
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 5.129217e-06
Norm of the params: 15.956395
     Influence (LOO): fixed 120 labels. Loss 0.10633. Accuracy 0.954.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037640393
Train loss (w/o reg) on all data: 0.022468893
Test loss (w/o reg) on all data: 0.09180117
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.6225206e-06
Norm of the params: 17.419243
                Loss: fixed 143 labels. Loss 0.09180. Accuracy 0.973.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24580653
Train loss (w/o reg) on all data: 0.23939086
Test loss (w/o reg) on all data: 0.17202757
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.1521529e-05
Norm of the params: 11.32755
              Random: fixed  36 labels. Loss 0.17203. Accuracy 0.947.
### Flips: 312, rs: 14, checks: 260
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056079365
Train loss (w/o reg) on all data: 0.043850746
Test loss (w/o reg) on all data: 0.05086558
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.61115e-06
Norm of the params: 15.638812
     Influence (LOO): fixed 142 labels. Loss 0.05087. Accuracy 0.992.
Using normal model
LBFGS training took [136] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029978648
Train loss (w/o reg) on all data: 0.016784789
Test loss (w/o reg) on all data: 0.055572443
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1773864e-06
Norm of the params: 16.244297
                Loss: fixed 153 labels. Loss 0.05557. Accuracy 0.985.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2329583
Train loss (w/o reg) on all data: 0.22647373
Test loss (w/o reg) on all data: 0.15662403
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 0.00012868877
Norm of the params: 11.388211
              Random: fixed  47 labels. Loss 0.15662. Accuracy 0.943.
### Flips: 312, rs: 14, checks: 312
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032031074
Train loss (w/o reg) on all data: 0.022547327
Test loss (w/o reg) on all data: 0.034304667
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 4.342284e-06
Norm of the params: 13.772253
     Influence (LOO): fixed 156 labels. Loss 0.03430. Accuracy 0.992.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025469672
Train loss (w/o reg) on all data: 0.01357517
Test loss (w/o reg) on all data: 0.039480813
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 8.635936e-07
Norm of the params: 15.423684
                Loss: fixed 157 labels. Loss 0.03948. Accuracy 0.985.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21848695
Train loss (w/o reg) on all data: 0.21143182
Test loss (w/o reg) on all data: 0.14438269
Train acc on all data:  0.9178605539637058
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.459737e-05
Norm of the params: 11.878664
              Random: fixed  56 labels. Loss 0.14438. Accuracy 0.958.
Using normal model
LBFGS training took [298] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26665604
Train loss (w/o reg) on all data: 0.25934926
Test loss (w/o reg) on all data: 0.19291084
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.4758417e-05
Norm of the params: 12.088663
Flipped loss: 0.19291. Accuracy: 0.950
### Flips: 312, rs: 15, checks: 52
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2040054
Train loss (w/o reg) on all data: 0.19337673
Test loss (w/o reg) on all data: 0.15302949
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 9.904633e-06
Norm of the params: 14.579903
     Influence (LOO): fixed  38 labels. Loss 0.15303. Accuracy 0.962.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17230126
Train loss (w/o reg) on all data: 0.15724434
Test loss (w/o reg) on all data: 0.17831713
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 5.7559496e-06
Norm of the params: 17.35334
                Loss: fixed  49 labels. Loss 0.17832. Accuracy 0.943.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25905246
Train loss (w/o reg) on all data: 0.2516912
Test loss (w/o reg) on all data: 0.17492871
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.6798076e-05
Norm of the params: 12.133632
              Random: fixed   8 labels. Loss 0.17493. Accuracy 0.954.
### Flips: 312, rs: 15, checks: 104
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16291508
Train loss (w/o reg) on all data: 0.14986895
Test loss (w/o reg) on all data: 0.13123737
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.0144312e-05
Norm of the params: 16.153101
     Influence (LOO): fixed  67 labels. Loss 0.13124. Accuracy 0.962.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.092132114
Train loss (w/o reg) on all data: 0.07334158
Test loss (w/o reg) on all data: 0.11222153
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.7970747e-06
Norm of the params: 19.385838
                Loss: fixed  96 labels. Loss 0.11222. Accuracy 0.950.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24491416
Train loss (w/o reg) on all data: 0.23677647
Test loss (w/o reg) on all data: 0.1662851
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.564218e-05
Norm of the params: 12.7575
              Random: fixed  22 labels. Loss 0.16629. Accuracy 0.950.
### Flips: 312, rs: 15, checks: 156
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10630226
Train loss (w/o reg) on all data: 0.09457229
Test loss (w/o reg) on all data: 0.08413062
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.366594e-06
Norm of the params: 15.316638
     Influence (LOO): fixed 100 labels. Loss 0.08413. Accuracy 0.973.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.053063076
Train loss (w/o reg) on all data: 0.036595903
Test loss (w/o reg) on all data: 0.053756334
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.0646208e-06
Norm of the params: 18.147821
                Loss: fixed 124 labels. Loss 0.05376. Accuracy 0.981.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23462112
Train loss (w/o reg) on all data: 0.22579856
Test loss (w/o reg) on all data: 0.15912727
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.4181138e-05
Norm of the params: 13.283489
              Random: fixed  30 labels. Loss 0.15913. Accuracy 0.962.
### Flips: 312, rs: 15, checks: 208
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07494497
Train loss (w/o reg) on all data: 0.065100744
Test loss (w/o reg) on all data: 0.0556165
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.068123e-05
Norm of the params: 14.031555
     Influence (LOO): fixed 117 labels. Loss 0.05562. Accuracy 0.989.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033968244
Train loss (w/o reg) on all data: 0.019516444
Test loss (w/o reg) on all data: 0.028409213
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.353305e-06
Norm of the params: 17.001059
                Loss: fixed 134 labels. Loss 0.02841. Accuracy 0.992.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22541168
Train loss (w/o reg) on all data: 0.21648487
Test loss (w/o reg) on all data: 0.15290502
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.8953926e-05
Norm of the params: 13.361745
              Random: fixed  37 labels. Loss 0.15291. Accuracy 0.950.
### Flips: 312, rs: 15, checks: 260
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04853051
Train loss (w/o reg) on all data: 0.038597208
Test loss (w/o reg) on all data: 0.04479304
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.931368e-06
Norm of the params: 14.094894
     Influence (LOO): fixed 130 labels. Loss 0.04479. Accuracy 0.989.
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019896641
Train loss (w/o reg) on all data: 0.010338905
Test loss (w/o reg) on all data: 0.02310024
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.9466994e-06
Norm of the params: 13.825872
                Loss: fixed 143 labels. Loss 0.02310. Accuracy 0.989.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21501552
Train loss (w/o reg) on all data: 0.20586124
Test loss (w/o reg) on all data: 0.1432559
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 7.732626e-06
Norm of the params: 13.530909
              Random: fixed  45 labels. Loss 0.14326. Accuracy 0.954.
### Flips: 312, rs: 15, checks: 312
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023819678
Train loss (w/o reg) on all data: 0.016322136
Test loss (w/o reg) on all data: 0.024112953
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.3404219e-06
Norm of the params: 12.2454405
     Influence (LOO): fixed 143 labels. Loss 0.02411. Accuracy 0.985.
Using normal model
LBFGS training took [81] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0146600325
Train loss (w/o reg) on all data: 0.006981718
Test loss (w/o reg) on all data: 0.023360549
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 7.4056067e-07
Norm of the params: 12.392186
                Loss: fixed 146 labels. Loss 0.02336. Accuracy 0.992.
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2071053
Train loss (w/o reg) on all data: 0.19720566
Test loss (w/o reg) on all data: 0.14246744
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.3413235e-05
Norm of the params: 14.070988
              Random: fixed  49 labels. Loss 0.14247. Accuracy 0.950.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28363842
Train loss (w/o reg) on all data: 0.27738926
Test loss (w/o reg) on all data: 0.28312832
Train acc on all data:  0.8624641833810889
Test acc on all data:   0.8473282442748091
Norm of the mean of gradients: 4.5649547e-05
Norm of the params: 11.179583
Flipped loss: 0.28313. Accuracy: 0.847
### Flips: 312, rs: 16, checks: 52
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21821587
Train loss (w/o reg) on all data: 0.20715654
Test loss (w/o reg) on all data: 0.27679965
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.8473282442748091
Norm of the mean of gradients: 5.371948e-05
Norm of the params: 14.872347
     Influence (LOO): fixed  37 labels. Loss 0.27680. Accuracy 0.847.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18422763
Train loss (w/o reg) on all data: 0.16951784
Test loss (w/o reg) on all data: 0.3046102
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.8587786259541985
Norm of the mean of gradients: 1.138407e-05
Norm of the params: 17.152132
                Loss: fixed  47 labels. Loss 0.30461. Accuracy 0.859.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27594185
Train loss (w/o reg) on all data: 0.26954094
Test loss (w/o reg) on all data: 0.26715028
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.8664122137404581
Norm of the mean of gradients: 1.8184948e-05
Norm of the params: 11.314509
              Random: fixed  10 labels. Loss 0.26715. Accuracy 0.866.
### Flips: 312, rs: 16, checks: 104
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18535978
Train loss (w/o reg) on all data: 0.17378438
Test loss (w/o reg) on all data: 0.23494299
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.8664122137404581
Norm of the mean of gradients: 1.5945976e-05
Norm of the params: 15.215385
     Influence (LOO): fixed  64 labels. Loss 0.23494. Accuracy 0.866.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.116087876
Train loss (w/o reg) on all data: 0.097045
Test loss (w/o reg) on all data: 0.27070442
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.8931297709923665
Norm of the mean of gradients: 6.917887e-06
Norm of the params: 19.515572
                Loss: fixed  89 labels. Loss 0.27070. Accuracy 0.893.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27054933
Train loss (w/o reg) on all data: 0.26413456
Test loss (w/o reg) on all data: 0.24929397
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.8778625954198473
Norm of the mean of gradients: 1.8253686e-05
Norm of the params: 11.326766
              Random: fixed  19 labels. Loss 0.24929. Accuracy 0.878.
### Flips: 312, rs: 16, checks: 156
Using normal model
LBFGS training took [185] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14914924
Train loss (w/o reg) on all data: 0.13711278
Test loss (w/o reg) on all data: 0.17429473
Train acc on all data:  0.938872970391595
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 3.1557407e-05
Norm of the params: 15.515453
     Influence (LOO): fixed  92 labels. Loss 0.17429. Accuracy 0.916.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07877597
Train loss (w/o reg) on all data: 0.060006432
Test loss (w/o reg) on all data: 0.18165368
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.1183331e-05
Norm of the params: 19.375002
                Loss: fixed 119 labels. Loss 0.18165. Accuracy 0.935.
Using normal model
LBFGS training took [307] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26649866
Train loss (w/o reg) on all data: 0.2601319
Test loss (w/o reg) on all data: 0.24315564
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.8816793893129771
Norm of the mean of gradients: 2.0333644e-05
Norm of the params: 11.284288
              Random: fixed  23 labels. Loss 0.24316. Accuracy 0.882.
### Flips: 312, rs: 16, checks: 208
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10841263
Train loss (w/o reg) on all data: 0.098340295
Test loss (w/o reg) on all data: 0.10627014
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.0448248e-05
Norm of the params: 14.193194
     Influence (LOO): fixed 118 labels. Loss 0.10627. Accuracy 0.966.
Using normal model
LBFGS training took [181] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05570046
Train loss (w/o reg) on all data: 0.038773812
Test loss (w/o reg) on all data: 0.13259307
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 2.5344352e-06
Norm of the params: 18.399265
                Loss: fixed 138 labels. Loss 0.13259. Accuracy 0.962.
Using normal model
LBFGS training took [302] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25539702
Train loss (w/o reg) on all data: 0.24860123
Test loss (w/o reg) on all data: 0.23149732
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 1.9984744e-05
Norm of the params: 11.658294
              Random: fixed  31 labels. Loss 0.23150. Accuracy 0.912.
### Flips: 312, rs: 16, checks: 260
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063370615
Train loss (w/o reg) on all data: 0.054143414
Test loss (w/o reg) on all data: 0.09315102
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.6677454e-06
Norm of the params: 13.584701
     Influence (LOO): fixed 141 labels. Loss 0.09315. Accuracy 0.966.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034034587
Train loss (w/o reg) on all data: 0.020011196
Test loss (w/o reg) on all data: 0.08958489
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.496401e-06
Norm of the params: 16.747173
                Loss: fixed 151 labels. Loss 0.08958. Accuracy 0.977.
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2407035
Train loss (w/o reg) on all data: 0.233251
Test loss (w/o reg) on all data: 0.22161752
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 3.1555304e-05
Norm of the params: 12.208589
              Random: fixed  42 labels. Loss 0.22162. Accuracy 0.916.
### Flips: 312, rs: 16, checks: 312
Using normal model
LBFGS training took [142] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028171044
Train loss (w/o reg) on all data: 0.019238202
Test loss (w/o reg) on all data: 0.06869413
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.5312235e-06
Norm of the params: 13.366258
     Influence (LOO): fixed 158 labels. Loss 0.06869. Accuracy 0.977.
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021966778
Train loss (w/o reg) on all data: 0.01125749
Test loss (w/o reg) on all data: 0.054334715
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3976742e-06
Norm of the params: 14.635085
                Loss: fixed 160 labels. Loss 0.05433. Accuracy 0.977.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22768855
Train loss (w/o reg) on all data: 0.2201067
Test loss (w/o reg) on all data: 0.18264067
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.7870637e-05
Norm of the params: 12.3140955
              Random: fixed  56 labels. Loss 0.18264. Accuracy 0.943.
Using normal model
LBFGS training took [296] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27864537
Train loss (w/o reg) on all data: 0.27228925
Test loss (w/o reg) on all data: 0.1968758
Train acc on all data:  0.8576886341929322
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.733493e-05
Norm of the params: 11.274865
Flipped loss: 0.19688. Accuracy: 0.935
### Flips: 312, rs: 17, checks: 52
Using normal model
LBFGS training took [273] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20412052
Train loss (w/o reg) on all data: 0.19119507
Test loss (w/o reg) on all data: 0.14782381
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.3865624e-05
Norm of the params: 16.078213
     Influence (LOO): fixed  40 labels. Loss 0.14782. Accuracy 0.947.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18103772
Train loss (w/o reg) on all data: 0.1674199
Test loss (w/o reg) on all data: 0.14053012
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.0448988e-05
Norm of the params: 16.503225
                Loss: fixed  48 labels. Loss 0.14053. Accuracy 0.939.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27067345
Train loss (w/o reg) on all data: 0.26384324
Test loss (w/o reg) on all data: 0.18787198
Train acc on all data:  0.8634192932187201
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 3.556671e-05
Norm of the params: 11.687778
              Random: fixed   9 labels. Loss 0.18787. Accuracy 0.931.
### Flips: 312, rs: 17, checks: 104
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17722638
Train loss (w/o reg) on all data: 0.16475247
Test loss (w/o reg) on all data: 0.13216265
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.3338287e-05
Norm of the params: 15.794877
     Influence (LOO): fixed  60 labels. Loss 0.13216. Accuracy 0.954.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10935098
Train loss (w/o reg) on all data: 0.0901479
Test loss (w/o reg) on all data: 0.10569264
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.1161617e-05
Norm of the params: 19.597488
                Loss: fixed  87 labels. Loss 0.10569. Accuracy 0.958.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.262136
Train loss (w/o reg) on all data: 0.25498384
Test loss (w/o reg) on all data: 0.17623466
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.9894944e-05
Norm of the params: 11.960079
              Random: fixed  17 labels. Loss 0.17623. Accuracy 0.935.
### Flips: 312, rs: 17, checks: 156
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13651925
Train loss (w/o reg) on all data: 0.12279293
Test loss (w/o reg) on all data: 0.10803926
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.1199776e-05
Norm of the params: 16.568844
     Influence (LOO): fixed  86 labels. Loss 0.10804. Accuracy 0.966.
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0661023
Train loss (w/o reg) on all data: 0.048255604
Test loss (w/o reg) on all data: 0.073623195
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.8522217e-06
Norm of the params: 18.892698
                Loss: fixed 120 labels. Loss 0.07362. Accuracy 0.966.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2534106
Train loss (w/o reg) on all data: 0.24611884
Test loss (w/o reg) on all data: 0.16580734
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.7668726e-05
Norm of the params: 12.076234
              Random: fixed  27 labels. Loss 0.16581. Accuracy 0.935.
### Flips: 312, rs: 17, checks: 208
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09533424
Train loss (w/o reg) on all data: 0.08150104
Test loss (w/o reg) on all data: 0.09046722
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.5077274e-05
Norm of the params: 16.633223
     Influence (LOO): fixed 111 labels. Loss 0.09047. Accuracy 0.966.
Using normal model
LBFGS training took [150] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042515337
Train loss (w/o reg) on all data: 0.027628236
Test loss (w/o reg) on all data: 0.05321474
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.6911422e-06
Norm of the params: 17.255201
                Loss: fixed 137 labels. Loss 0.05321. Accuracy 0.985.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2472253
Train loss (w/o reg) on all data: 0.23976976
Test loss (w/o reg) on all data: 0.15464512
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.2722259e-05
Norm of the params: 12.211094
              Random: fixed  31 labels. Loss 0.15465. Accuracy 0.943.
### Flips: 312, rs: 17, checks: 260
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06953518
Train loss (w/o reg) on all data: 0.057150867
Test loss (w/o reg) on all data: 0.066106185
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.9115216e-05
Norm of the params: 15.738051
     Influence (LOO): fixed 130 labels. Loss 0.06611. Accuracy 0.981.
Using normal model
LBFGS training took [122] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027352516
Train loss (w/o reg) on all data: 0.015848055
Test loss (w/o reg) on all data: 0.033508558
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.5381828e-06
Norm of the params: 15.168692
                Loss: fixed 148 labels. Loss 0.03351. Accuracy 0.985.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23576464
Train loss (w/o reg) on all data: 0.22811575
Test loss (w/o reg) on all data: 0.15373896
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.0984022e-05
Norm of the params: 12.368418
              Random: fixed  41 labels. Loss 0.15374. Accuracy 0.943.
### Flips: 312, rs: 17, checks: 312
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037439045
Train loss (w/o reg) on all data: 0.026414948
Test loss (w/o reg) on all data: 0.047219284
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7247268e-06
Norm of the params: 14.848635
     Influence (LOO): fixed 148 labels. Loss 0.04722. Accuracy 0.981.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022341479
Train loss (w/o reg) on all data: 0.012103422
Test loss (w/o reg) on all data: 0.023520153
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.079985e-06
Norm of the params: 14.309479
                Loss: fixed 155 labels. Loss 0.02352. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22263825
Train loss (w/o reg) on all data: 0.21479276
Test loss (w/o reg) on all data: 0.14174078
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.2242691e-05
Norm of the params: 12.5263605
              Random: fixed  51 labels. Loss 0.14174. Accuracy 0.950.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2683141
Train loss (w/o reg) on all data: 0.26002818
Test loss (w/o reg) on all data: 0.21537337
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.8893129770992366
Norm of the mean of gradients: 1.427372e-05
Norm of the params: 12.873157
Flipped loss: 0.21537. Accuracy: 0.889
### Flips: 312, rs: 18, checks: 52
Using normal model
LBFGS training took [279] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19987291
Train loss (w/o reg) on all data: 0.18669175
Test loss (w/o reg) on all data: 0.18096627
Train acc on all data:  0.9197707736389685
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.1744104e-05
Norm of the params: 16.236479
     Influence (LOO): fixed  37 labels. Loss 0.18097. Accuracy 0.924.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16618448
Train loss (w/o reg) on all data: 0.14968829
Test loss (w/o reg) on all data: 0.17471719
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 7.971308e-06
Norm of the params: 18.163807
                Loss: fixed  47 labels. Loss 0.17472. Accuracy 0.927.
Using normal model
LBFGS training took [205] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25956023
Train loss (w/o reg) on all data: 0.25071397
Test loss (w/o reg) on all data: 0.21840912
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 2.0742264e-05
Norm of the params: 13.301313
              Random: fixed   6 labels. Loss 0.21841. Accuracy 0.905.
### Flips: 312, rs: 18, checks: 104
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1507022
Train loss (w/o reg) on all data: 0.13649859
Test loss (w/o reg) on all data: 0.13643613
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 8.415962e-06
Norm of the params: 16.854439
     Influence (LOO): fixed  68 labels. Loss 0.13644. Accuracy 0.947.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10011399
Train loss (w/o reg) on all data: 0.08029824
Test loss (w/o reg) on all data: 0.13343203
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.9331215e-05
Norm of the params: 19.907663
                Loss: fixed  87 labels. Loss 0.13343. Accuracy 0.954.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25262758
Train loss (w/o reg) on all data: 0.24306881
Test loss (w/o reg) on all data: 0.20066606
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 3.1322696e-05
Norm of the params: 13.826622
              Random: fixed  12 labels. Loss 0.20067. Accuracy 0.901.
### Flips: 312, rs: 18, checks: 156
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10825278
Train loss (w/o reg) on all data: 0.092301175
Test loss (w/o reg) on all data: 0.12485699
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.274267e-06
Norm of the params: 17.86147
     Influence (LOO): fixed  92 labels. Loss 0.12486. Accuracy 0.954.
Using normal model
LBFGS training took [182] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057774298
Train loss (w/o reg) on all data: 0.040213525
Test loss (w/o reg) on all data: 0.097491734
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.490107e-06
Norm of the params: 18.740744
                Loss: fixed 117 labels. Loss 0.09749. Accuracy 0.969.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24830867
Train loss (w/o reg) on all data: 0.23924792
Test loss (w/o reg) on all data: 0.19406724
Train acc on all data:  0.8872970391595033
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 2.1792139e-05
Norm of the params: 13.461619
              Random: fixed  18 labels. Loss 0.19407. Accuracy 0.916.
### Flips: 312, rs: 18, checks: 208
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07301743
Train loss (w/o reg) on all data: 0.055812005
Test loss (w/o reg) on all data: 0.089759186
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.264672e-05
Norm of the params: 18.550163
     Influence (LOO): fixed 114 labels. Loss 0.08976. Accuracy 0.966.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033246923
Train loss (w/o reg) on all data: 0.018723326
Test loss (w/o reg) on all data: 0.09652138
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.4619428e-06
Norm of the params: 17.04324
                Loss: fixed 131 labels. Loss 0.09652. Accuracy 0.989.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2399155
Train loss (w/o reg) on all data: 0.23034032
Test loss (w/o reg) on all data: 0.1928089
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 2.0842477e-05
Norm of the params: 13.838484
              Random: fixed  25 labels. Loss 0.19281. Accuracy 0.912.
### Flips: 312, rs: 18, checks: 260
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.054281615
Train loss (w/o reg) on all data: 0.040612545
Test loss (w/o reg) on all data: 0.065685675
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.9163634e-06
Norm of the params: 16.53425
     Influence (LOO): fixed 130 labels. Loss 0.06569. Accuracy 0.981.
Using normal model
LBFGS training took [127] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025382288
Train loss (w/o reg) on all data: 0.013673045
Test loss (w/o reg) on all data: 0.05848889
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.3663347e-06
Norm of the params: 15.303101
                Loss: fixed 142 labels. Loss 0.05849. Accuracy 0.985.
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22806118
Train loss (w/o reg) on all data: 0.21798393
Test loss (w/o reg) on all data: 0.18172961
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 5.415612e-05
Norm of the params: 14.196655
              Random: fixed  35 labels. Loss 0.18173. Accuracy 0.924.
### Flips: 312, rs: 18, checks: 312
Using normal model
LBFGS training took [133] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.026124533
Train loss (w/o reg) on all data: 0.01563763
Test loss (w/o reg) on all data: 0.026317233
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 5.3097638e-06
Norm of the params: 14.482336
     Influence (LOO): fixed 146 labels. Loss 0.02632. Accuracy 0.996.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015030758
Train loss (w/o reg) on all data: 0.0066133146
Test loss (w/o reg) on all data: 0.037219796
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.313373e-07
Norm of the params: 12.974933
                Loss: fixed 150 labels. Loss 0.03722. Accuracy 0.989.
Using normal model
LBFGS training took [290] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22178721
Train loss (w/o reg) on all data: 0.21277295
Test loss (w/o reg) on all data: 0.15429099
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 9.313036e-06
Norm of the params: 13.427031
              Random: fixed  44 labels. Loss 0.15429. Accuracy 0.947.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28449562
Train loss (w/o reg) on all data: 0.27883482
Test loss (w/o reg) on all data: 0.23581992
Train acc on all data:  0.8605539637058262
Test acc on all data:   0.8778625954198473
Norm of the mean of gradients: 4.7944883e-05
Norm of the params: 10.640291
Flipped loss: 0.23582. Accuracy: 0.878
### Flips: 312, rs: 19, checks: 52
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22467133
Train loss (w/o reg) on all data: 0.21405177
Test loss (w/o reg) on all data: 0.21573813
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.8931297709923665
Norm of the mean of gradients: 2.3880733e-05
Norm of the params: 14.573648
     Influence (LOO): fixed  39 labels. Loss 0.21574. Accuracy 0.893.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18320854
Train loss (w/o reg) on all data: 0.16982532
Test loss (w/o reg) on all data: 0.18990548
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 8.08299e-06
Norm of the params: 16.360456
                Loss: fixed  49 labels. Loss 0.18991. Accuracy 0.908.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27613002
Train loss (w/o reg) on all data: 0.27010927
Test loss (w/o reg) on all data: 0.21218556
Train acc on all data:  0.8643744030563515
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 0.00013871063
Norm of the params: 10.973387
              Random: fixed  10 labels. Loss 0.21219. Accuracy 0.901.
### Flips: 312, rs: 19, checks: 104
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17840524
Train loss (w/o reg) on all data: 0.16460833
Test loss (w/o reg) on all data: 0.18332317
Train acc on all data:  0.9255014326647565
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 4.3367014e-05
Norm of the params: 16.611385
     Influence (LOO): fixed  65 labels. Loss 0.18332. Accuracy 0.916.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.106280886
Train loss (w/o reg) on all data: 0.08667529
Test loss (w/o reg) on all data: 0.15617716
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.9147179e-05
Norm of the params: 19.801819
                Loss: fixed  94 labels. Loss 0.15618. Accuracy 0.935.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2654026
Train loss (w/o reg) on all data: 0.25954124
Test loss (w/o reg) on all data: 0.18859817
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.1615831e-05
Norm of the params: 10.827128
              Random: fixed  23 labels. Loss 0.18860. Accuracy 0.920.
### Flips: 312, rs: 19, checks: 156
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13487548
Train loss (w/o reg) on all data: 0.12008865
Test loss (w/o reg) on all data: 0.13305415
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 6.3394723e-06
Norm of the params: 17.196991
     Influence (LOO): fixed  93 labels. Loss 0.13305. Accuracy 0.943.
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07170128
Train loss (w/o reg) on all data: 0.050111286
Test loss (w/o reg) on all data: 0.10407604
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.344725e-06
Norm of the params: 20.779797
                Loss: fixed 120 labels. Loss 0.10408. Accuracy 0.962.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2569904
Train loss (w/o reg) on all data: 0.25052193
Test loss (w/o reg) on all data: 0.17626531
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.6976956e-05
Norm of the params: 11.37407
              Random: fixed  31 labels. Loss 0.17627. Accuracy 0.927.
### Flips: 312, rs: 19, checks: 208
Using normal model
LBFGS training took [192] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.099709384
Train loss (w/o reg) on all data: 0.084426776
Test loss (w/o reg) on all data: 0.10399225
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.2301323e-05
Norm of the params: 17.482912
     Influence (LOO): fixed 119 labels. Loss 0.10399. Accuracy 0.950.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.055115737
Train loss (w/o reg) on all data: 0.035486847
Test loss (w/o reg) on all data: 0.067950316
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.4965713e-06
Norm of the params: 19.813576
                Loss: fixed 136 labels. Loss 0.06795. Accuracy 0.981.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25115108
Train loss (w/o reg) on all data: 0.24450704
Test loss (w/o reg) on all data: 0.17069916
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 5.217288e-05
Norm of the params: 11.5273905
              Random: fixed  38 labels. Loss 0.17070. Accuracy 0.939.
### Flips: 312, rs: 19, checks: 260
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07811066
Train loss (w/o reg) on all data: 0.06345384
Test loss (w/o reg) on all data: 0.07520403
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 5.2724313e-06
Norm of the params: 17.12123
     Influence (LOO): fixed 136 labels. Loss 0.07520. Accuracy 0.962.
Using normal model
LBFGS training took [156] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04218907
Train loss (w/o reg) on all data: 0.025759097
Test loss (w/o reg) on all data: 0.05760213
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.14887e-06
Norm of the params: 18.127312
                Loss: fixed 147 labels. Loss 0.05760. Accuracy 0.981.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24591805
Train loss (w/o reg) on all data: 0.23951033
Test loss (w/o reg) on all data: 0.15760556
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 6.4318345e-05
Norm of the params: 11.32053
              Random: fixed  45 labels. Loss 0.15761. Accuracy 0.950.
### Flips: 312, rs: 19, checks: 312
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05633995
Train loss (w/o reg) on all data: 0.04355729
Test loss (w/o reg) on all data: 0.049923345
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.8775047e-06
Norm of the params: 15.98916
     Influence (LOO): fixed 149 labels. Loss 0.04992. Accuracy 0.981.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038496062
Train loss (w/o reg) on all data: 0.023263987
Test loss (w/o reg) on all data: 0.052827414
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 9.735413e-06
Norm of the params: 17.453983
                Loss: fixed 151 labels. Loss 0.05283. Accuracy 0.985.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23165585
Train loss (w/o reg) on all data: 0.2249616
Test loss (w/o reg) on all data: 0.1595865
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.7164627e-05
Norm of the params: 11.570881
              Random: fixed  58 labels. Loss 0.15959. Accuracy 0.950.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26683173
Train loss (w/o reg) on all data: 0.26113382
Test loss (w/o reg) on all data: 0.18362196
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.8712133e-05
Norm of the params: 10.675105
Flipped loss: 0.18362. Accuracy: 0.939
### Flips: 312, rs: 20, checks: 52
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19816259
Train loss (w/o reg) on all data: 0.18623775
Test loss (w/o reg) on all data: 0.17144856
Train acc on all data:  0.9111747851002865
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.1096076e-05
Norm of the params: 15.443335
     Influence (LOO): fixed  35 labels. Loss 0.17145. Accuracy 0.935.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16071737
Train loss (w/o reg) on all data: 0.14700218
Test loss (w/o reg) on all data: 0.15398781
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 9.477352e-05
Norm of the params: 16.562126
                Loss: fixed  47 labels. Loss 0.15399. Accuracy 0.943.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2629611
Train loss (w/o reg) on all data: 0.2570503
Test loss (w/o reg) on all data: 0.17727056
Train acc on all data:  0.8720152817574021
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.048634e-05
Norm of the params: 10.872695
              Random: fixed   5 labels. Loss 0.17727. Accuracy 0.935.
### Flips: 312, rs: 20, checks: 104
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15493369
Train loss (w/o reg) on all data: 0.14311479
Test loss (w/o reg) on all data: 0.13057992
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 4.961075e-06
Norm of the params: 15.374585
     Influence (LOO): fixed  69 labels. Loss 0.13058. Accuracy 0.947.
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0973413
Train loss (w/o reg) on all data: 0.080149926
Test loss (w/o reg) on all data: 0.16715832
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 6.9803955e-06
Norm of the params: 18.542587
                Loss: fixed  86 labels. Loss 0.16716. Accuracy 0.947.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25640514
Train loss (w/o reg) on all data: 0.25022438
Test loss (w/o reg) on all data: 0.1688931
Train acc on all data:  0.87774594078319
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.3254859e-05
Norm of the params: 11.118251
              Random: fixed  12 labels. Loss 0.16889. Accuracy 0.947.
### Flips: 312, rs: 20, checks: 156
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10710335
Train loss (w/o reg) on all data: 0.095651254
Test loss (w/o reg) on all data: 0.106985584
Train acc on all data:  0.9570200573065902
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.4443903e-05
Norm of the params: 15.134128
     Influence (LOO): fixed  96 labels. Loss 0.10699. Accuracy 0.958.
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05189254
Train loss (w/o reg) on all data: 0.036542423
Test loss (w/o reg) on all data: 0.12171438
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.2451304e-06
Norm of the params: 17.521484
                Loss: fixed 115 labels. Loss 0.12171. Accuracy 0.954.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24833389
Train loss (w/o reg) on all data: 0.24260388
Test loss (w/o reg) on all data: 0.16204022
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 3.2452823e-05
Norm of the params: 10.705141
              Random: fixed  22 labels. Loss 0.16204. Accuracy 0.939.
### Flips: 312, rs: 20, checks: 208
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0716423
Train loss (w/o reg) on all data: 0.062068522
Test loss (w/o reg) on all data: 0.07611014
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.3553445e-06
Norm of the params: 13.83747
     Influence (LOO): fixed 120 labels. Loss 0.07611. Accuracy 0.969.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024803482
Train loss (w/o reg) on all data: 0.013061505
Test loss (w/o reg) on all data: 0.0845893
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.965618e-06
Norm of the params: 15.324475
                Loss: fixed 134 labels. Loss 0.08459. Accuracy 0.969.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24036694
Train loss (w/o reg) on all data: 0.23377562
Test loss (w/o reg) on all data: 0.16436602
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.7753095e-05
Norm of the params: 11.48157
              Random: fixed  28 labels. Loss 0.16437. Accuracy 0.935.
### Flips: 312, rs: 20, checks: 260
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038430054
Train loss (w/o reg) on all data: 0.02975291
Test loss (w/o reg) on all data: 0.04839716
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.065604e-06
Norm of the params: 13.173568
     Influence (LOO): fixed 136 labels. Loss 0.04840. Accuracy 0.985.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014990939
Train loss (w/o reg) on all data: 0.006659252
Test loss (w/o reg) on all data: 0.067530155
Train acc on all data:  1.0
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.3988305e-07
Norm of the params: 12.9086685
                Loss: fixed 141 labels. Loss 0.06753. Accuracy 0.973.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23452628
Train loss (w/o reg) on all data: 0.22795495
Test loss (w/o reg) on all data: 0.14817087
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.2163326e-05
Norm of the params: 11.464143
              Random: fixed  35 labels. Loss 0.14817. Accuracy 0.954.
### Flips: 312, rs: 20, checks: 312
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01900634
Train loss (w/o reg) on all data: 0.010447347
Test loss (w/o reg) on all data: 0.030658955
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.1024853e-06
Norm of the params: 13.083572
     Influence (LOO): fixed 147 labels. Loss 0.03066. Accuracy 0.985.
Using normal model
LBFGS training took [70] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.010756865
Train loss (w/o reg) on all data: 0.004403062
Test loss (w/o reg) on all data: 0.022274949
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.1461062e-07
Norm of the params: 11.272801
                Loss: fixed 148 labels. Loss 0.02227. Accuracy 0.985.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2182502
Train loss (w/o reg) on all data: 0.21068633
Test loss (w/o reg) on all data: 0.1351703
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8611481e-05
Norm of the params: 12.299492
              Random: fixed  45 labels. Loss 0.13517. Accuracy 0.966.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26907
Train loss (w/o reg) on all data: 0.26276895
Test loss (w/o reg) on all data: 0.21891499
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.7587154e-05
Norm of the params: 11.225894
Flipped loss: 0.21891. Accuracy: 0.947
### Flips: 312, rs: 21, checks: 52
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19004864
Train loss (w/o reg) on all data: 0.17835844
Test loss (w/o reg) on all data: 0.18356638
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.8162567e-05
Norm of the params: 15.290647
     Influence (LOO): fixed  38 labels. Loss 0.18357. Accuracy 0.916.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16000861
Train loss (w/o reg) on all data: 0.14487633
Test loss (w/o reg) on all data: 0.19317926
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 2.1236085e-05
Norm of the params: 17.396713
                Loss: fixed  48 labels. Loss 0.19318. Accuracy 0.931.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26146984
Train loss (w/o reg) on all data: 0.2552058
Test loss (w/o reg) on all data: 0.1888985
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.623016e-05
Norm of the params: 11.192872
              Random: fixed  11 labels. Loss 0.18890. Accuracy 0.954.
### Flips: 312, rs: 21, checks: 104
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15878749
Train loss (w/o reg) on all data: 0.148383
Test loss (w/o reg) on all data: 0.14858738
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.0546693e-05
Norm of the params: 14.425316
     Influence (LOO): fixed  65 labels. Loss 0.14859. Accuracy 0.927.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08671053
Train loss (w/o reg) on all data: 0.06765702
Test loss (w/o reg) on all data: 0.12817024
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.9186107e-06
Norm of the params: 19.521015
                Loss: fixed  95 labels. Loss 0.12817. Accuracy 0.958.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2474784
Train loss (w/o reg) on all data: 0.24051163
Test loss (w/o reg) on all data: 0.16757126
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.6127653e-05
Norm of the params: 11.804048
              Random: fixed  19 labels. Loss 0.16757. Accuracy 0.962.
### Flips: 312, rs: 21, checks: 156
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10610758
Train loss (w/o reg) on all data: 0.0947246
Test loss (w/o reg) on all data: 0.10862172
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.2558592e-05
Norm of the params: 15.088388
     Influence (LOO): fixed  96 labels. Loss 0.10862. Accuracy 0.954.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03957309
Train loss (w/o reg) on all data: 0.024735326
Test loss (w/o reg) on all data: 0.115371495
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.5288298e-06
Norm of the params: 17.226585
                Loss: fixed 125 labels. Loss 0.11537. Accuracy 0.950.
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23699753
Train loss (w/o reg) on all data: 0.22936746
Test loss (w/o reg) on all data: 0.165235
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.022164e-05
Norm of the params: 12.353192
              Random: fixed  25 labels. Loss 0.16523. Accuracy 0.954.
### Flips: 312, rs: 21, checks: 208
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06846613
Train loss (w/o reg) on all data: 0.05718364
Test loss (w/o reg) on all data: 0.07410668
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 6.1231194e-06
Norm of the params: 15.021641
     Influence (LOO): fixed 120 labels. Loss 0.07411. Accuracy 0.977.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.023950456
Train loss (w/o reg) on all data: 0.012261788
Test loss (w/o reg) on all data: 0.089530885
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.7688861e-06
Norm of the params: 15.289648
                Loss: fixed 136 labels. Loss 0.08953. Accuracy 0.966.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22640312
Train loss (w/o reg) on all data: 0.21805707
Test loss (w/o reg) on all data: 0.15823889
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.0895262e-05
Norm of the params: 12.919787
              Random: fixed  36 labels. Loss 0.15824. Accuracy 0.962.
### Flips: 312, rs: 21, checks: 260
Using normal model
LBFGS training took [137] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036909323
Train loss (w/o reg) on all data: 0.027782412
Test loss (w/o reg) on all data: 0.03619535
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.5086706e-06
Norm of the params: 13.510672
     Influence (LOO): fixed 136 labels. Loss 0.03620. Accuracy 0.989.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019177523
Train loss (w/o reg) on all data: 0.009008967
Test loss (w/o reg) on all data: 0.063888766
Train acc on all data:  1.0
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.6856584e-07
Norm of the params: 14.260824
                Loss: fixed 140 labels. Loss 0.06389. Accuracy 0.969.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21615319
Train loss (w/o reg) on all data: 0.207886
Test loss (w/o reg) on all data: 0.13218711
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 4.6125202e-05
Norm of the params: 12.858612
              Random: fixed  45 labels. Loss 0.13219. Accuracy 0.969.
### Flips: 312, rs: 21, checks: 312
Using normal model
LBFGS training took [90] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020962942
Train loss (w/o reg) on all data: 0.013542554
Test loss (w/o reg) on all data: 0.0138752265
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8434982e-06
Norm of the params: 12.182273
     Influence (LOO): fixed 146 labels. Loss 0.01388. Accuracy 0.992.
Using normal model
LBFGS training took [82] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016408004
Train loss (w/o reg) on all data: 0.007652235
Test loss (w/o reg) on all data: 0.040895026
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 8.597363e-07
Norm of the params: 13.233117
                Loss: fixed 144 labels. Loss 0.04090. Accuracy 0.981.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20523614
Train loss (w/o reg) on all data: 0.19663559
Test loss (w/o reg) on all data: 0.12776932
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.4530674e-05
Norm of the params: 13.115295
              Random: fixed  54 labels. Loss 0.12777. Accuracy 0.958.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26922488
Train loss (w/o reg) on all data: 0.263945
Test loss (w/o reg) on all data: 0.20191658
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.804807e-05
Norm of the params: 10.27607
Flipped loss: 0.20192. Accuracy: 0.924
### Flips: 312, rs: 22, checks: 52
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19915576
Train loss (w/o reg) on all data: 0.18941075
Test loss (w/o reg) on all data: 0.1581691
Train acc on all data:  0.9149952244508118
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.1757495e-05
Norm of the params: 13.96067
     Influence (LOO): fixed  39 labels. Loss 0.15817. Accuracy 0.931.
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14843363
Train loss (w/o reg) on all data: 0.1339239
Test loss (w/o reg) on all data: 0.17676081
Train acc on all data:  0.936007640878701
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 1.6323938e-05
Norm of the params: 17.035099
                Loss: fixed  51 labels. Loss 0.17676. Accuracy 0.920.
Using normal model
LBFGS training took [230] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2612114
Train loss (w/o reg) on all data: 0.25560224
Test loss (w/o reg) on all data: 0.18521903
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.0479214e-05
Norm of the params: 10.591644
              Random: fixed   9 labels. Loss 0.18522. Accuracy 0.935.
### Flips: 312, rs: 22, checks: 104
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1374518
Train loss (w/o reg) on all data: 0.12459391
Test loss (w/o reg) on all data: 0.12856804
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 4.6538553e-05
Norm of the params: 16.036135
     Influence (LOO): fixed  74 labels. Loss 0.12857. Accuracy 0.954.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08064734
Train loss (w/o reg) on all data: 0.06305663
Test loss (w/o reg) on all data: 0.10123956
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.1586541e-05
Norm of the params: 18.756712
                Loss: fixed  94 labels. Loss 0.10124. Accuracy 0.943.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25103033
Train loss (w/o reg) on all data: 0.24500786
Test loss (w/o reg) on all data: 0.17236227
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 8.1240825e-05
Norm of the params: 10.974945
              Random: fixed  17 labels. Loss 0.17236. Accuracy 0.947.
### Flips: 312, rs: 22, checks: 156
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10783898
Train loss (w/o reg) on all data: 0.09437299
Test loss (w/o reg) on all data: 0.08785808
Train acc on all data:  0.958930276981853
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 7.6381375e-06
Norm of the params: 16.410969
     Influence (LOO): fixed  94 labels. Loss 0.08786. Accuracy 0.966.
Using normal model
LBFGS training took [141] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037559416
Train loss (w/o reg) on all data: 0.022263225
Test loss (w/o reg) on all data: 0.0563512
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.6631267e-06
Norm of the params: 17.490679
                Loss: fixed 119 labels. Loss 0.05635. Accuracy 0.966.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2441058
Train loss (w/o reg) on all data: 0.2376871
Test loss (w/o reg) on all data: 0.17223935
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 7.232453e-05
Norm of the params: 11.330231
              Random: fixed  23 labels. Loss 0.17224. Accuracy 0.939.
### Flips: 312, rs: 22, checks: 208
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0711176
Train loss (w/o reg) on all data: 0.05906012
Test loss (w/o reg) on all data: 0.061088707
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 7.56828e-06
Norm of the params: 15.528996
     Influence (LOO): fixed 114 labels. Loss 0.06109. Accuracy 0.973.
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.031149454
Train loss (w/o reg) on all data: 0.017503204
Test loss (w/o reg) on all data: 0.031765506
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.702401e-06
Norm of the params: 16.520443
                Loss: fixed 127 labels. Loss 0.03177. Accuracy 0.985.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23771188
Train loss (w/o reg) on all data: 0.23122542
Test loss (w/o reg) on all data: 0.16178434
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.3279834e-05
Norm of the params: 11.389867
              Random: fixed  29 labels. Loss 0.16178. Accuracy 0.947.
### Flips: 312, rs: 22, checks: 260
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04295026
Train loss (w/o reg) on all data: 0.030899215
Test loss (w/o reg) on all data: 0.04517181
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.619467e-06
Norm of the params: 15.52485
     Influence (LOO): fixed 132 labels. Loss 0.04517. Accuracy 0.981.
Using normal model
LBFGS training took [131] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02379303
Train loss (w/o reg) on all data: 0.012367451
Test loss (w/o reg) on all data: 0.014913801
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.8353325e-06
Norm of the params: 15.116601
                Loss: fixed 135 labels. Loss 0.01491. Accuracy 0.992.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22680552
Train loss (w/o reg) on all data: 0.2194403
Test loss (w/o reg) on all data: 0.15989147
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 8.221735e-05
Norm of the params: 12.13691
              Random: fixed  35 labels. Loss 0.15989. Accuracy 0.935.
### Flips: 312, rs: 22, checks: 312
Using normal model
LBFGS training took [112] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01796813
Train loss (w/o reg) on all data: 0.010199892
Test loss (w/o reg) on all data: 0.020040043
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.1547694e-06
Norm of the params: 12.46454
     Influence (LOO): fixed 145 labels. Loss 0.02004. Accuracy 0.992.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020633591
Train loss (w/o reg) on all data: 0.009950971
Test loss (w/o reg) on all data: 0.0191213
Train acc on all data:  1.0
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 1.2020075e-06
Norm of the params: 14.616854
                Loss: fixed 139 labels. Loss 0.01912. Accuracy 0.996.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20928016
Train loss (w/o reg) on all data: 0.20113009
Test loss (w/o reg) on all data: 0.1526722
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.5786921e-05
Norm of the params: 12.767196
              Random: fixed  45 labels. Loss 0.15267. Accuracy 0.947.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27560055
Train loss (w/o reg) on all data: 0.26931363
Test loss (w/o reg) on all data: 0.20519361
Train acc on all data:  0.8615090735434575
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 6.3316955e-05
Norm of the params: 11.213318
Flipped loss: 0.20519. Accuracy: 0.935
### Flips: 312, rs: 23, checks: 52
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20883465
Train loss (w/o reg) on all data: 0.19721064
Test loss (w/o reg) on all data: 0.1756289
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.5919517e-05
Norm of the params: 15.247298
     Influence (LOO): fixed  37 labels. Loss 0.17563. Accuracy 0.931.
Using normal model
LBFGS training took [202] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16930917
Train loss (w/o reg) on all data: 0.15522256
Test loss (w/o reg) on all data: 0.17461671
Train acc on all data:  0.9188156638013372
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 8.127494e-06
Norm of the params: 16.784878
                Loss: fixed  50 labels. Loss 0.17462. Accuracy 0.920.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26373848
Train loss (w/o reg) on all data: 0.25691783
Test loss (w/o reg) on all data: 0.19768539
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.6750437e-05
Norm of the params: 11.679607
              Random: fixed  10 labels. Loss 0.19769. Accuracy 0.935.
### Flips: 312, rs: 23, checks: 104
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16091761
Train loss (w/o reg) on all data: 0.14854673
Test loss (w/o reg) on all data: 0.13536575
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.1618507e-05
Norm of the params: 15.729516
     Influence (LOO): fixed  70 labels. Loss 0.13537. Accuracy 0.943.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10549862
Train loss (w/o reg) on all data: 0.08841542
Test loss (w/o reg) on all data: 0.15539554
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 7.5457187e-06
Norm of the params: 18.484156
                Loss: fixed  88 labels. Loss 0.15540. Accuracy 0.943.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25169817
Train loss (w/o reg) on all data: 0.24395505
Test loss (w/o reg) on all data: 0.19614825
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.4104902e-05
Norm of the params: 12.444373
              Random: fixed  19 labels. Loss 0.19615. Accuracy 0.931.
### Flips: 312, rs: 23, checks: 156
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11981531
Train loss (w/o reg) on all data: 0.10653051
Test loss (w/o reg) on all data: 0.07227696
Train acc on all data:  0.9493791786055397
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.9654072e-06
Norm of the params: 16.300186
     Influence (LOO): fixed  95 labels. Loss 0.07228. Accuracy 0.973.
Using normal model
LBFGS training took [186] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05833813
Train loss (w/o reg) on all data: 0.040800232
Test loss (w/o reg) on all data: 0.11568942
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.2514325e-05
Norm of the params: 18.728535
                Loss: fixed 119 labels. Loss 0.11569. Accuracy 0.950.
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24536909
Train loss (w/o reg) on all data: 0.23813358
Test loss (w/o reg) on all data: 0.17536865
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 0.000112473404
Norm of the params: 12.029553
              Random: fixed  29 labels. Loss 0.17537. Accuracy 0.939.
### Flips: 312, rs: 23, checks: 208
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0914138
Train loss (w/o reg) on all data: 0.079299405
Test loss (w/o reg) on all data: 0.050541356
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.2963227e-05
Norm of the params: 15.565602
     Influence (LOO): fixed 117 labels. Loss 0.05054. Accuracy 0.985.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037115358
Train loss (w/o reg) on all data: 0.020987723
Test loss (w/o reg) on all data: 0.07076753
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3524834e-06
Norm of the params: 17.959753
                Loss: fixed 135 labels. Loss 0.07077. Accuracy 0.977.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23624633
Train loss (w/o reg) on all data: 0.22917333
Test loss (w/o reg) on all data: 0.15798834
Train acc on all data:  0.889207258834766
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.514293e-05
Norm of the params: 11.8937
              Random: fixed  40 labels. Loss 0.15799. Accuracy 0.950.
### Flips: 312, rs: 23, checks: 260
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06275896
Train loss (w/o reg) on all data: 0.051919546
Test loss (w/o reg) on all data: 0.04165434
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0670722e-05
Norm of the params: 14.72373
     Influence (LOO): fixed 134 labels. Loss 0.04165. Accuracy 0.981.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025477605
Train loss (w/o reg) on all data: 0.012660903
Test loss (w/o reg) on all data: 0.040214386
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7018632e-06
Norm of the params: 16.010435
                Loss: fixed 144 labels. Loss 0.04021. Accuracy 0.985.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2249008
Train loss (w/o reg) on all data: 0.21716851
Test loss (w/o reg) on all data: 0.15523182
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.2037874e-05
Norm of the params: 12.435665
              Random: fixed  48 labels. Loss 0.15523. Accuracy 0.950.
### Flips: 312, rs: 23, checks: 312
Using normal model
LBFGS training took [149] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043637607
Train loss (w/o reg) on all data: 0.03408836
Test loss (w/o reg) on all data: 0.027499242
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.5600897e-06
Norm of the params: 13.819731
     Influence (LOO): fixed 145 labels. Loss 0.02750. Accuracy 0.996.
Using normal model
LBFGS training took [119] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.022694074
Train loss (w/o reg) on all data: 0.011186252
Test loss (w/o reg) on all data: 0.032427
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.411469e-06
Norm of the params: 15.170907
                Loss: fixed 148 labels. Loss 0.03243. Accuracy 0.989.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21817505
Train loss (w/o reg) on all data: 0.21019493
Test loss (w/o reg) on all data: 0.15340555
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.348441e-05
Norm of the params: 12.633387
              Random: fixed  53 labels. Loss 0.15341. Accuracy 0.950.
Using normal model
LBFGS training took [229] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25957274
Train loss (w/o reg) on all data: 0.25127637
Test loss (w/o reg) on all data: 0.23913904
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.8778625954198473
Norm of the mean of gradients: 1.6035076e-05
Norm of the params: 12.881271
Flipped loss: 0.23914. Accuracy: 0.878
### Flips: 312, rs: 24, checks: 52
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20469785
Train loss (w/o reg) on all data: 0.19364265
Test loss (w/o reg) on all data: 0.20931347
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 1.5803194e-05
Norm of the params: 14.869566
     Influence (LOO): fixed  33 labels. Loss 0.20931. Accuracy 0.901.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16206287
Train loss (w/o reg) on all data: 0.14536819
Test loss (w/o reg) on all data: 0.20984915
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.8931297709923665
Norm of the mean of gradients: 7.300543e-06
Norm of the params: 18.272757
                Loss: fixed  47 labels. Loss 0.20985. Accuracy 0.893.
Using normal model
LBFGS training took [243] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24698956
Train loss (w/o reg) on all data: 0.23844488
Test loss (w/o reg) on all data: 0.21358877
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 4.717752e-05
Norm of the params: 13.072626
              Random: fixed  11 labels. Loss 0.21359. Accuracy 0.901.
### Flips: 312, rs: 24, checks: 104
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15338227
Train loss (w/o reg) on all data: 0.14091064
Test loss (w/o reg) on all data: 0.16765834
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 2.2343533e-05
Norm of the params: 15.793432
     Influence (LOO): fixed  68 labels. Loss 0.16766. Accuracy 0.924.
Using normal model
LBFGS training took [177] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09003654
Train loss (w/o reg) on all data: 0.07096961
Test loss (w/o reg) on all data: 0.15286121
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 4.5820316e-06
Norm of the params: 19.527895
                Loss: fixed  90 labels. Loss 0.15286. Accuracy 0.935.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24409665
Train loss (w/o reg) on all data: 0.2356179
Test loss (w/o reg) on all data: 0.20516501
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 5.525003e-05
Norm of the params: 13.022096
              Random: fixed  18 labels. Loss 0.20517. Accuracy 0.924.
### Flips: 312, rs: 24, checks: 156
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10970677
Train loss (w/o reg) on all data: 0.09795388
Test loss (w/o reg) on all data: 0.11138686
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 7.554578e-06
Norm of the params: 15.331594
     Influence (LOO): fixed  96 labels. Loss 0.11139. Accuracy 0.943.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06023848
Train loss (w/o reg) on all data: 0.042696457
Test loss (w/o reg) on all data: 0.10315569
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.3922647e-05
Norm of the params: 18.730738
                Loss: fixed 112 labels. Loss 0.10316. Accuracy 0.950.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23995483
Train loss (w/o reg) on all data: 0.23167984
Test loss (w/o reg) on all data: 0.19872046
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 9.978601e-06
Norm of the params: 12.864665
              Random: fixed  24 labels. Loss 0.19872. Accuracy 0.939.
### Flips: 312, rs: 24, checks: 208
Using normal model
LBFGS training took [151] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.057918653
Train loss (w/o reg) on all data: 0.04572667
Test loss (w/o reg) on all data: 0.083567984
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.7440185e-06
Norm of the params: 15.615366
     Influence (LOO): fixed 119 labels. Loss 0.08357. Accuracy 0.966.
Using normal model
LBFGS training took [152] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040935352
Train loss (w/o reg) on all data: 0.026523845
Test loss (w/o reg) on all data: 0.08483375
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 6.7166015e-06
Norm of the params: 16.977343
                Loss: fixed 124 labels. Loss 0.08483. Accuracy 0.969.
Using normal model
LBFGS training took [265] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23375654
Train loss (w/o reg) on all data: 0.22615641
Test loss (w/o reg) on all data: 0.18507986
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 4.86679e-05
Norm of the params: 12.328935
              Random: fixed  33 labels. Loss 0.18508. Accuracy 0.939.
### Flips: 312, rs: 24, checks: 260
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028603237
Train loss (w/o reg) on all data: 0.019363578
Test loss (w/o reg) on all data: 0.05091928
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2678461e-06
Norm of the params: 13.593866
     Influence (LOO): fixed 138 labels. Loss 0.05092. Accuracy 0.981.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02399651
Train loss (w/o reg) on all data: 0.012986388
Test loss (w/o reg) on all data: 0.047033265
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1233873e-06
Norm of the params: 14.83922
                Loss: fixed 137 labels. Loss 0.04703. Accuracy 0.981.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22620219
Train loss (w/o reg) on all data: 0.21850657
Test loss (w/o reg) on all data: 0.17419653
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.8823184e-05
Norm of the params: 12.406138
              Random: fixed  40 labels. Loss 0.17420. Accuracy 0.935.
### Flips: 312, rs: 24, checks: 312
Using normal model
LBFGS training took [78] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.011590781
Train loss (w/o reg) on all data: 0.005504897
Test loss (w/o reg) on all data: 0.018104244
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.2015224e-07
Norm of the params: 11.032575
     Influence (LOO): fixed 147 labels. Loss 0.01810. Accuracy 0.992.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020291585
Train loss (w/o reg) on all data: 0.010445726
Test loss (w/o reg) on all data: 0.050316904
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.4388446e-07
Norm of the params: 14.032719
                Loss: fixed 141 labels. Loss 0.05032. Accuracy 0.981.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21230462
Train loss (w/o reg) on all data: 0.20400429
Test loss (w/o reg) on all data: 0.16522878
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.6831347e-05
Norm of the params: 12.884364
              Random: fixed  48 labels. Loss 0.16523. Accuracy 0.935.
Using normal model
LBFGS training took [289] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27681682
Train loss (w/o reg) on all data: 0.27066344
Test loss (w/o reg) on all data: 0.2234185
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 5.3311946e-05
Norm of the params: 11.093581
Flipped loss: 0.22342. Accuracy: 0.908
### Flips: 312, rs: 25, checks: 52
Using normal model
LBFGS training took [190] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1944524
Train loss (w/o reg) on all data: 0.1807275
Test loss (w/o reg) on all data: 0.21063569
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 6.5430075e-05
Norm of the params: 16.56799
     Influence (LOO): fixed  41 labels. Loss 0.21064. Accuracy 0.912.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15548798
Train loss (w/o reg) on all data: 0.13721128
Test loss (w/o reg) on all data: 0.19672216
Train acc on all data:  0.9283667621776505
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.2759493e-05
Norm of the params: 19.11895
                Loss: fixed  51 labels. Loss 0.19672. Accuracy 0.920.
Using normal model
LBFGS training took [312] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2733419
Train loss (w/o reg) on all data: 0.26718146
Test loss (w/o reg) on all data: 0.22032446
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 2.982776e-05
Norm of the params: 11.099932
              Random: fixed   3 labels. Loss 0.22032. Accuracy 0.905.
### Flips: 312, rs: 25, checks: 104
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14771926
Train loss (w/o reg) on all data: 0.13171701
Test loss (w/o reg) on all data: 0.13512771
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 7.223979e-06
Norm of the params: 17.889803
     Influence (LOO): fixed  68 labels. Loss 0.13513. Accuracy 0.939.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.090762734
Train loss (w/o reg) on all data: 0.06928414
Test loss (w/o reg) on all data: 0.16030933
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 7.083153e-06
Norm of the params: 20.726116
                Loss: fixed  90 labels. Loss 0.16031. Accuracy 0.935.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26491737
Train loss (w/o reg) on all data: 0.25851244
Test loss (w/o reg) on all data: 0.21066655
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 0.000109305416
Norm of the params: 11.318083
              Random: fixed  11 labels. Loss 0.21067. Accuracy 0.916.
### Flips: 312, rs: 25, checks: 156
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110249676
Train loss (w/o reg) on all data: 0.096014634
Test loss (w/o reg) on all data: 0.11275518
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.736417e-06
Norm of the params: 16.873083
     Influence (LOO): fixed  97 labels. Loss 0.11276. Accuracy 0.958.
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05819436
Train loss (w/o reg) on all data: 0.03900221
Test loss (w/o reg) on all data: 0.09668912
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 6.7641495e-06
Norm of the params: 19.591913
                Loss: fixed 117 labels. Loss 0.09669. Accuracy 0.966.
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25251493
Train loss (w/o reg) on all data: 0.24538493
Test loss (w/o reg) on all data: 0.20282763
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 9.018602e-06
Norm of the params: 11.94151
              Random: fixed  19 labels. Loss 0.20283. Accuracy 0.927.
### Flips: 312, rs: 25, checks: 208
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07685815
Train loss (w/o reg) on all data: 0.06520197
Test loss (w/o reg) on all data: 0.07370964
Train acc on all data:  0.9732569245463228
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.2245604e-05
Norm of the params: 15.268387
     Influence (LOO): fixed 117 labels. Loss 0.07371. Accuracy 0.977.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034682445
Train loss (w/o reg) on all data: 0.021545958
Test loss (w/o reg) on all data: 0.04477661
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.0307606e-06
Norm of the params: 16.20894
                Loss: fixed 135 labels. Loss 0.04478. Accuracy 0.989.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24749447
Train loss (w/o reg) on all data: 0.24040498
Test loss (w/o reg) on all data: 0.19509847
Train acc on all data:  0.8958930276981852
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.2728934e-05
Norm of the params: 11.9075575
              Random: fixed  25 labels. Loss 0.19510. Accuracy 0.931.
### Flips: 312, rs: 25, checks: 260
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038349804
Train loss (w/o reg) on all data: 0.02692641
Test loss (w/o reg) on all data: 0.056568224
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.080912e-06
Norm of the params: 15.115154
     Influence (LOO): fixed 136 labels. Loss 0.05657. Accuracy 0.981.
Using normal model
LBFGS training took [117] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020587416
Train loss (w/o reg) on all data: 0.010554676
Test loss (w/o reg) on all data: 0.040573176
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.4729455e-07
Norm of the params: 14.165269
                Loss: fixed 143 labels. Loss 0.04057. Accuracy 0.989.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23592125
Train loss (w/o reg) on all data: 0.22911243
Test loss (w/o reg) on all data: 0.17392106
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 7.044288e-05
Norm of the params: 11.669461
              Random: fixed  36 labels. Loss 0.17392. Accuracy 0.947.
### Flips: 312, rs: 25, checks: 312
Using normal model
LBFGS training took [96] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019603387
Train loss (w/o reg) on all data: 0.011611874
Test loss (w/o reg) on all data: 0.02256203
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.8134963e-06
Norm of the params: 12.642399
     Influence (LOO): fixed 146 labels. Loss 0.02256. Accuracy 0.992.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013763198
Train loss (w/o reg) on all data: 0.006283685
Test loss (w/o reg) on all data: 0.020510904
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.863913e-07
Norm of the params: 12.23071
                Loss: fixed 149 labels. Loss 0.02051. Accuracy 0.992.
Using normal model
LBFGS training took [231] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22882415
Train loss (w/o reg) on all data: 0.22182639
Test loss (w/o reg) on all data: 0.15671979
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.7641927e-05
Norm of the params: 11.830268
              Random: fixed  44 labels. Loss 0.15672. Accuracy 0.958.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26190564
Train loss (w/o reg) on all data: 0.25656903
Test loss (w/o reg) on all data: 0.19368163
Train acc on all data:  0.8653295128939829
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 2.3552899e-05
Norm of the params: 10.33113
Flipped loss: 0.19368. Accuracy: 0.924
### Flips: 312, rs: 26, checks: 52
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19588935
Train loss (w/o reg) on all data: 0.18636328
Test loss (w/o reg) on all data: 0.16888852
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.1042042e-05
Norm of the params: 13.802952
     Influence (LOO): fixed  38 labels. Loss 0.16889. Accuracy 0.935.
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16251491
Train loss (w/o reg) on all data: 0.14982931
Test loss (w/o reg) on all data: 0.18801849
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.1635464e-05
Norm of the params: 15.928339
                Loss: fixed  47 labels. Loss 0.18802. Accuracy 0.931.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25766316
Train loss (w/o reg) on all data: 0.2524269
Test loss (w/o reg) on all data: 0.18811671
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 7.0712515e-05
Norm of the params: 10.23355
              Random: fixed   6 labels. Loss 0.18812. Accuracy 0.924.
### Flips: 312, rs: 26, checks: 104
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14785978
Train loss (w/o reg) on all data: 0.1359869
Test loss (w/o reg) on all data: 0.14904074
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.3027372e-06
Norm of the params: 15.409661
     Influence (LOO): fixed  68 labels. Loss 0.14904. Accuracy 0.950.
Using normal model
LBFGS training took [173] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09027992
Train loss (w/o reg) on all data: 0.07333131
Test loss (w/o reg) on all data: 0.14779355
Train acc on all data:  0.9637058261700095
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.7122422e-05
Norm of the params: 18.4112
                Loss: fixed  90 labels. Loss 0.14779. Accuracy 0.958.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2494353
Train loss (w/o reg) on all data: 0.24389191
Test loss (w/o reg) on all data: 0.18050101
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 6.935566e-05
Norm of the params: 10.529384
              Random: fixed  15 labels. Loss 0.18050. Accuracy 0.935.
### Flips: 312, rs: 26, checks: 156
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11100281
Train loss (w/o reg) on all data: 0.099542566
Test loss (w/o reg) on all data: 0.11092518
Train acc on all data:  0.9522445081184336
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.471581e-05
Norm of the params: 15.139518
     Influence (LOO): fixed  90 labels. Loss 0.11093. Accuracy 0.943.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.051929064
Train loss (w/o reg) on all data: 0.037486795
Test loss (w/o reg) on all data: 0.09736751
Train acc on all data:  0.9847182425978988
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.2838055e-06
Norm of the params: 16.99545
                Loss: fixed 119 labels. Loss 0.09737. Accuracy 0.973.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2417906
Train loss (w/o reg) on all data: 0.23574407
Test loss (w/o reg) on all data: 0.1793336
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 3.0842984e-05
Norm of the params: 10.99684
              Random: fixed  20 labels. Loss 0.17933. Accuracy 0.935.
### Flips: 312, rs: 26, checks: 208
Using normal model
LBFGS training took [170] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06052457
Train loss (w/o reg) on all data: 0.049607597
Test loss (w/o reg) on all data: 0.071705975
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.5933335e-06
Norm of the params: 14.776315
     Influence (LOO): fixed 119 labels. Loss 0.07171. Accuracy 0.969.
Using normal model
LBFGS training took [109] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024896719
Train loss (w/o reg) on all data: 0.014337503
Test loss (w/o reg) on all data: 0.03191443
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 3.173853e-06
Norm of the params: 14.532182
                Loss: fixed 137 labels. Loss 0.03191. Accuracy 0.985.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22786617
Train loss (w/o reg) on all data: 0.22087981
Test loss (w/o reg) on all data: 0.18003504
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 1.74092e-05
Norm of the params: 11.820631
              Random: fixed  30 labels. Loss 0.18004. Accuracy 0.931.
### Flips: 312, rs: 26, checks: 260
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037566505
Train loss (w/o reg) on all data: 0.027334012
Test loss (w/o reg) on all data: 0.048374142
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.7553453e-06
Norm of the params: 14.305588
     Influence (LOO): fixed 130 labels. Loss 0.04837. Accuracy 0.985.
Using normal model
LBFGS training took [95] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01763582
Train loss (w/o reg) on all data: 0.008817817
Test loss (w/o reg) on all data: 0.03239008
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.9634824e-06
Norm of the params: 13.280064
                Loss: fixed 142 labels. Loss 0.03239. Accuracy 0.992.
Using normal model
LBFGS training took [180] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22379416
Train loss (w/o reg) on all data: 0.21702172
Test loss (w/o reg) on all data: 0.16438398
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 4.873988e-05
Norm of the params: 11.638251
              Random: fixed  37 labels. Loss 0.16438. Accuracy 0.931.
### Flips: 312, rs: 26, checks: 312
Using normal model
LBFGS training took [93] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02305121
Train loss (w/o reg) on all data: 0.014764031
Test loss (w/o reg) on all data: 0.041027214
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.1227645e-06
Norm of the params: 12.874145
     Influence (LOO): fixed 140 labels. Loss 0.04103. Accuracy 0.985.
Using normal model
LBFGS training took [91] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01182553
Train loss (w/o reg) on all data: 0.0048334617
Test loss (w/o reg) on all data: 0.027427064
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0738365e-06
Norm of the params: 11.825454
                Loss: fixed 146 labels. Loss 0.02743. Accuracy 0.985.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21334983
Train loss (w/o reg) on all data: 0.20607811
Test loss (w/o reg) on all data: 0.15228485
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.343428e-05
Norm of the params: 12.059621
              Random: fixed  44 labels. Loss 0.15228. Accuracy 0.954.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28263828
Train loss (w/o reg) on all data: 0.27706003
Test loss (w/o reg) on all data: 0.18334274
Train acc on all data:  0.8586437440305635
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 5.7982797e-05
Norm of the params: 10.562429
Flipped loss: 0.18334. Accuracy: 0.939
### Flips: 312, rs: 27, checks: 52
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21877417
Train loss (w/o reg) on all data: 0.2089817
Test loss (w/o reg) on all data: 0.15422712
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.91191e-05
Norm of the params: 13.994623
     Influence (LOO): fixed  37 labels. Loss 0.15423. Accuracy 0.935.
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16944072
Train loss (w/o reg) on all data: 0.15647395
Test loss (w/o reg) on all data: 0.1307942
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.5296831e-05
Norm of the params: 16.10389
                Loss: fixed  50 labels. Loss 0.13079. Accuracy 0.939.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2790047
Train loss (w/o reg) on all data: 0.27363324
Test loss (w/o reg) on all data: 0.17506963
Train acc on all data:  0.8662846227316141
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.9371684e-05
Norm of the params: 10.364795
              Random: fixed   6 labels. Loss 0.17507. Accuracy 0.950.
### Flips: 312, rs: 27, checks: 104
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17360702
Train loss (w/o reg) on all data: 0.16225787
Test loss (w/o reg) on all data: 0.12345059
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.0695777e-05
Norm of the params: 15.065955
     Influence (LOO): fixed  64 labels. Loss 0.12345. Accuracy 0.954.
Using normal model
LBFGS training took [200] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108796105
Train loss (w/o reg) on all data: 0.092511676
Test loss (w/o reg) on all data: 0.11542428
Train acc on all data:  0.9551098376313276
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.19815595e-05
Norm of the params: 18.046844
                Loss: fixed  89 labels. Loss 0.11542. Accuracy 0.958.
Using normal model
LBFGS training took [236] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2706081
Train loss (w/o reg) on all data: 0.2649539
Test loss (w/o reg) on all data: 0.16548473
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.25845745e-05
Norm of the params: 10.634087
              Random: fixed  16 labels. Loss 0.16548. Accuracy 0.954.
### Flips: 312, rs: 27, checks: 156
Using normal model
LBFGS training took [238] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1293916
Train loss (w/o reg) on all data: 0.11592007
Test loss (w/o reg) on all data: 0.093622304
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.3608145e-05
Norm of the params: 16.41434
     Influence (LOO): fixed  91 labels. Loss 0.09362. Accuracy 0.966.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06270418
Train loss (w/o reg) on all data: 0.04537372
Test loss (w/o reg) on all data: 0.06579564
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.4392608e-06
Norm of the params: 18.617443
                Loss: fixed 122 labels. Loss 0.06580. Accuracy 0.966.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26642606
Train loss (w/o reg) on all data: 0.2607901
Test loss (w/o reg) on all data: 0.15838848
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 2.7450711e-05
Norm of the params: 10.616904
              Random: fixed  20 labels. Loss 0.15839. Accuracy 0.966.
### Flips: 312, rs: 27, checks: 208
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08005688
Train loss (w/o reg) on all data: 0.0675638
Test loss (w/o reg) on all data: 0.07095899
Train acc on all data:  0.9723018147086915
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.444687e-06
Norm of the params: 15.807011
     Influence (LOO): fixed 118 labels. Loss 0.07096. Accuracy 0.973.
Using normal model
LBFGS training took [123] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03520958
Train loss (w/o reg) on all data: 0.021579731
Test loss (w/o reg) on all data: 0.050987486
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.1200733e-06
Norm of the params: 16.510513
                Loss: fixed 140 labels. Loss 0.05099. Accuracy 0.985.
Using normal model
LBFGS training took [261] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2580388
Train loss (w/o reg) on all data: 0.25213414
Test loss (w/o reg) on all data: 0.15833133
Train acc on all data:  0.8825214899713467
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 4.8421636e-05
Norm of the params: 10.867068
              Random: fixed  30 labels. Loss 0.15833. Accuracy 0.958.
### Flips: 312, rs: 27, checks: 260
Using normal model
LBFGS training took [134] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04573986
Train loss (w/o reg) on all data: 0.034496367
Test loss (w/o reg) on all data: 0.040469345
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8907044e-06
Norm of the params: 14.99566
     Influence (LOO): fixed 140 labels. Loss 0.04047. Accuracy 0.989.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02160553
Train loss (w/o reg) on all data: 0.0110764075
Test loss (w/o reg) on all data: 0.037709642
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 5.479879e-06
Norm of the params: 14.51146
                Loss: fixed 150 labels. Loss 0.03771. Accuracy 0.977.
Using normal model
LBFGS training took [287] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24986549
Train loss (w/o reg) on all data: 0.24346106
Test loss (w/o reg) on all data: 0.1547093
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 3.4477733e-05
Norm of the params: 11.317626
              Random: fixed  36 labels. Loss 0.15471. Accuracy 0.950.
### Flips: 312, rs: 27, checks: 312
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03495461
Train loss (w/o reg) on all data: 0.02630784
Test loss (w/o reg) on all data: 0.02668265
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3451737e-06
Norm of the params: 13.150491
     Influence (LOO): fixed 151 labels. Loss 0.02668. Accuracy 0.992.
Using normal model
LBFGS training took [89] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.014778685
Train loss (w/o reg) on all data: 0.006636241
Test loss (w/o reg) on all data: 0.02296255
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.4270686e-07
Norm of the params: 12.761226
                Loss: fixed 154 labels. Loss 0.02296. Accuracy 0.992.
Using normal model
LBFGS training took [256] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2382918
Train loss (w/o reg) on all data: 0.23157836
Test loss (w/o reg) on all data: 0.1473852
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.846596e-05
Norm of the params: 11.5874405
              Random: fixed  44 labels. Loss 0.14739. Accuracy 0.947.
Using normal model
LBFGS training took [262] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26544863
Train loss (w/o reg) on all data: 0.25859815
Test loss (w/o reg) on all data: 0.19814168
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.5186041e-05
Norm of the params: 11.705105
Flipped loss: 0.19814. Accuracy: 0.927
### Flips: 312, rs: 28, checks: 52
Using normal model
LBFGS training took [210] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20065127
Train loss (w/o reg) on all data: 0.18977909
Test loss (w/o reg) on all data: 0.183953
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.4360835e-05
Norm of the params: 14.7459755
     Influence (LOO): fixed  37 labels. Loss 0.18395. Accuracy 0.916.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15273334
Train loss (w/o reg) on all data: 0.13732229
Test loss (w/o reg) on all data: 0.17775376
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 3.7128906e-05
Norm of the params: 17.55622
                Loss: fixed  50 labels. Loss 0.17775. Accuracy 0.916.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25340044
Train loss (w/o reg) on all data: 0.24532495
Test loss (w/o reg) on all data: 0.19102895
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 2.3583632e-05
Norm of the params: 12.70864
              Random: fixed  10 labels. Loss 0.19103. Accuracy 0.920.
### Flips: 312, rs: 28, checks: 104
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15096936
Train loss (w/o reg) on all data: 0.1391983
Test loss (w/o reg) on all data: 0.14499484
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.6507108e-05
Norm of the params: 15.343433
     Influence (LOO): fixed  66 labels. Loss 0.14499. Accuracy 0.939.
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08463709
Train loss (w/o reg) on all data: 0.066448696
Test loss (w/o reg) on all data: 0.13299507
Train acc on all data:  0.9713467048710601
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.2163753e-06
Norm of the params: 19.072699
                Loss: fixed  92 labels. Loss 0.13300. Accuracy 0.947.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24463499
Train loss (w/o reg) on all data: 0.23602766
Test loss (w/o reg) on all data: 0.1837315
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.7241147e-05
Norm of the params: 13.120461
              Random: fixed  19 labels. Loss 0.18373. Accuracy 0.924.
### Flips: 312, rs: 28, checks: 156
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.110141486
Train loss (w/o reg) on all data: 0.09843635
Test loss (w/o reg) on all data: 0.10686494
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 6.4724927e-06
Norm of the params: 15.300419
     Influence (LOO): fixed  92 labels. Loss 0.10686. Accuracy 0.954.
Using normal model
LBFGS training took [169] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052178457
Train loss (w/o reg) on all data: 0.036477484
Test loss (w/o reg) on all data: 0.08385843
Train acc on all data:  0.9885386819484241
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 4.1042804e-06
Norm of the params: 17.720594
                Loss: fixed 115 labels. Loss 0.08386. Accuracy 0.962.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24013923
Train loss (w/o reg) on all data: 0.23184767
Test loss (w/o reg) on all data: 0.17368859
Train acc on all data:  0.8939828080229226
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.9902137e-05
Norm of the params: 12.877548
              Random: fixed  25 labels. Loss 0.17369. Accuracy 0.935.
### Flips: 312, rs: 28, checks: 208
Using normal model
LBFGS training took [198] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06483806
Train loss (w/o reg) on all data: 0.05129683
Test loss (w/o reg) on all data: 0.07862767
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 5.127618e-06
Norm of the params: 16.45675
     Influence (LOO): fixed 116 labels. Loss 0.07863. Accuracy 0.966.
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037687667
Train loss (w/o reg) on all data: 0.024858573
Test loss (w/o reg) on all data: 0.059079565
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.1605517e-06
Norm of the params: 16.018175
                Loss: fixed 130 labels. Loss 0.05908. Accuracy 0.969.
Using normal model
LBFGS training took [235] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23647511
Train loss (w/o reg) on all data: 0.22852814
Test loss (w/o reg) on all data: 0.16339032
Train acc on all data:  0.897803247373448
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 9.7729484e-05
Norm of the params: 12.60712
              Random: fixed  30 labels. Loss 0.16339. Accuracy 0.947.
### Flips: 312, rs: 28, checks: 260
Using normal model
LBFGS training took [128] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.042173047
Train loss (w/o reg) on all data: 0.031312067
Test loss (w/o reg) on all data: 0.043762382
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 5.614039e-06
Norm of the params: 14.738371
     Influence (LOO): fixed 131 labels. Loss 0.04376. Accuracy 0.981.
Using normal model
LBFGS training took [111] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021079428
Train loss (w/o reg) on all data: 0.010690908
Test loss (w/o reg) on all data: 0.043017436
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2896999e-06
Norm of the params: 14.414244
                Loss: fixed 141 labels. Loss 0.04302. Accuracy 0.981.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2288582
Train loss (w/o reg) on all data: 0.22109933
Test loss (w/o reg) on all data: 0.15994398
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 1.1859572e-05
Norm of the params: 12.457024
              Random: fixed  36 labels. Loss 0.15994. Accuracy 0.943.
### Flips: 312, rs: 28, checks: 312
Using normal model
LBFGS training took [107] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025348699
Train loss (w/o reg) on all data: 0.016382279
Test loss (w/o reg) on all data: 0.026405497
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 2.8702846e-06
Norm of the params: 13.3913555
     Influence (LOO): fixed 142 labels. Loss 0.02641. Accuracy 0.989.
Using normal model
LBFGS training took [94] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.012983631
Train loss (w/o reg) on all data: 0.0057275346
Test loss (w/o reg) on all data: 0.02975074
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.2395003e-06
Norm of the params: 12.046658
                Loss: fixed 147 labels. Loss 0.02975. Accuracy 0.981.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22677894
Train loss (w/o reg) on all data: 0.21881942
Test loss (w/o reg) on all data: 0.15552548
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.6892168e-05
Norm of the params: 12.617065
              Random: fixed  40 labels. Loss 0.15553. Accuracy 0.943.
Using normal model
LBFGS training took [308] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26369345
Train loss (w/o reg) on all data: 0.2580971
Test loss (w/o reg) on all data: 0.21280043
Train acc on all data:  0.8672397325692455
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 6.847688e-05
Norm of the params: 10.579544
Flipped loss: 0.21280. Accuracy: 0.920
### Flips: 312, rs: 29, checks: 52
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19470565
Train loss (w/o reg) on all data: 0.18335856
Test loss (w/o reg) on all data: 0.19433518
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 4.8345733e-05
Norm of the params: 15.064582
     Influence (LOO): fixed  37 labels. Loss 0.19434. Accuracy 0.927.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14623617
Train loss (w/o reg) on all data: 0.13183485
Test loss (w/o reg) on all data: 0.1995896
Train acc on all data:  0.941738299904489
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 2.8367736e-05
Norm of the params: 16.971338
                Loss: fixed  51 labels. Loss 0.19959. Accuracy 0.916.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26082402
Train loss (w/o reg) on all data: 0.25524944
Test loss (w/o reg) on all data: 0.20611191
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 9.040644e-05
Norm of the params: 10.558952
              Random: fixed   4 labels. Loss 0.20611. Accuracy 0.912.
### Flips: 312, rs: 29, checks: 104
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14612198
Train loss (w/o reg) on all data: 0.1329004
Test loss (w/o reg) on all data: 0.114827916
Train acc on all data:  0.9379178605539638
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.8600658e-05
Norm of the params: 16.261349
     Influence (LOO): fixed  67 labels. Loss 0.11483. Accuracy 0.962.
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06906494
Train loss (w/o reg) on all data: 0.05051717
Test loss (w/o reg) on all data: 0.15268975
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.7844788e-05
Norm of the params: 19.260202
                Loss: fixed  95 labels. Loss 0.15269. Accuracy 0.954.
Using normal model
LBFGS training took [271] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2555117
Train loss (w/o reg) on all data: 0.2498361
Test loss (w/o reg) on all data: 0.20035084
Train acc on all data:  0.8815663801337154
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 2.3511428e-05
Norm of the params: 10.65419
              Random: fixed  13 labels. Loss 0.20035. Accuracy 0.924.
### Flips: 312, rs: 29, checks: 156
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10398833
Train loss (w/o reg) on all data: 0.0914775
Test loss (w/o reg) on all data: 0.077467315
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.5136462e-05
Norm of the params: 15.818238
     Influence (LOO): fixed  93 labels. Loss 0.07747. Accuracy 0.973.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04043091
Train loss (w/o reg) on all data: 0.026707172
Test loss (w/o reg) on all data: 0.114555694
Train acc on all data:  0.994269340974212
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.266205e-06
Norm of the params: 16.56728
                Loss: fixed 119 labels. Loss 0.11456. Accuracy 0.950.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24041346
Train loss (w/o reg) on all data: 0.23345467
Test loss (w/o reg) on all data: 0.20824626
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 4.579306e-05
Norm of the params: 11.797274
              Random: fixed  21 labels. Loss 0.20825. Accuracy 0.931.
### Flips: 312, rs: 29, checks: 208
Using normal model
LBFGS training took [167] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06743367
Train loss (w/o reg) on all data: 0.054520164
Test loss (w/o reg) on all data: 0.052658323
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.1328733e-05
Norm of the params: 16.070784
     Influence (LOO): fixed 112 labels. Loss 0.05266. Accuracy 0.977.
Using normal model
LBFGS training took [120] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024775522
Train loss (w/o reg) on all data: 0.013474289
Test loss (w/o reg) on all data: 0.079096064
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.1467222e-06
Norm of the params: 15.034117
                Loss: fixed 131 labels. Loss 0.07910. Accuracy 0.973.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23145622
Train loss (w/o reg) on all data: 0.22425406
Test loss (w/o reg) on all data: 0.19760716
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 9.033135e-05
Norm of the params: 12.001798
              Random: fixed  29 labels. Loss 0.19761. Accuracy 0.935.
### Flips: 312, rs: 29, checks: 260
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.040721513
Train loss (w/o reg) on all data: 0.029488495
Test loss (w/o reg) on all data: 0.036507472
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.5572946e-06
Norm of the params: 14.988675
     Influence (LOO): fixed 128 labels. Loss 0.03651. Accuracy 0.989.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.01829776
Train loss (w/o reg) on all data: 0.008811408
Test loss (w/o reg) on all data: 0.041576073
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.595956e-07
Norm of the params: 13.774144
                Loss: fixed 135 labels. Loss 0.04158. Accuracy 0.985.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22249256
Train loss (w/o reg) on all data: 0.21490338
Test loss (w/o reg) on all data: 0.19204807
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.4779399e-05
Norm of the params: 12.320046
              Random: fixed  36 labels. Loss 0.19205. Accuracy 0.935.
### Flips: 312, rs: 29, checks: 312
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02393529
Train loss (w/o reg) on all data: 0.01638631
Test loss (w/o reg) on all data: 0.016239798
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.2248488e-06
Norm of the params: 12.287376
     Influence (LOO): fixed 140 labels. Loss 0.01624. Accuracy 0.992.
Using normal model
LBFGS training took [85] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013260808
Train loss (w/o reg) on all data: 0.006058989
Test loss (w/o reg) on all data: 0.039151467
Train acc on all data:  1.0
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.165328e-07
Norm of the params: 12.001517
                Loss: fixed 139 labels. Loss 0.03915. Accuracy 0.969.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21316862
Train loss (w/o reg) on all data: 0.20583892
Test loss (w/o reg) on all data: 0.18510696
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 7.157307e-05
Norm of the params: 12.107606
              Random: fixed  44 labels. Loss 0.18511. Accuracy 0.950.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27944028
Train loss (w/o reg) on all data: 0.2739329
Test loss (w/o reg) on all data: 0.18339932
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.5075722e-05
Norm of the params: 10.495135
Flipped loss: 0.18340. Accuracy: 0.935
### Flips: 312, rs: 30, checks: 52
Using normal model
LBFGS training took [255] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2172485
Train loss (w/o reg) on all data: 0.20768282
Test loss (w/o reg) on all data: 0.15674917
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.3212522e-05
Norm of the params: 13.83162
     Influence (LOO): fixed  36 labels. Loss 0.15675. Accuracy 0.943.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17037685
Train loss (w/o reg) on all data: 0.15509298
Test loss (w/o reg) on all data: 0.1367439
Train acc on all data:  0.9264565425023877
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 1.556571e-05
Norm of the params: 17.483635
                Loss: fixed  48 labels. Loss 0.13674. Accuracy 0.939.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27636582
Train loss (w/o reg) on all data: 0.2709349
Test loss (w/o reg) on all data: 0.18011387
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 5.315875e-05
Norm of the params: 10.422003
              Random: fixed   4 labels. Loss 0.18011. Accuracy 0.931.
### Flips: 312, rs: 30, checks: 104
Using normal model
LBFGS training took [259] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18565524
Train loss (w/o reg) on all data: 0.17572607
Test loss (w/o reg) on all data: 0.13787888
Train acc on all data:  0.9245463228271251
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.0518163e-05
Norm of the params: 14.091953
     Influence (LOO): fixed  63 labels. Loss 0.13788. Accuracy 0.947.
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.115300834
Train loss (w/o reg) on all data: 0.09561255
Test loss (w/o reg) on all data: 0.09183833
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 8.743049e-06
Norm of the params: 19.843533
                Loss: fixed  85 labels. Loss 0.09184. Accuracy 0.966.
Using normal model
LBFGS training took [253] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2734088
Train loss (w/o reg) on all data: 0.2677959
Test loss (w/o reg) on all data: 0.1671597
Train acc on all data:  0.874880611270296
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.6670255e-05
Norm of the params: 10.595199
              Random: fixed  12 labels. Loss 0.16716. Accuracy 0.950.
### Flips: 312, rs: 30, checks: 156
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13886787
Train loss (w/o reg) on all data: 0.12740327
Test loss (w/o reg) on all data: 0.106711976
Train acc on all data:  0.941738299904489
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.3247934e-06
Norm of the params: 15.1423855
     Influence (LOO): fixed  89 labels. Loss 0.10671. Accuracy 0.958.
Using normal model
LBFGS training took [174] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.075991444
Train loss (w/o reg) on all data: 0.057376344
Test loss (w/o reg) on all data: 0.052394487
Train acc on all data:  0.9789875835721108
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.58965e-05
Norm of the params: 19.29513
                Loss: fixed 115 labels. Loss 0.05239. Accuracy 0.977.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2630486
Train loss (w/o reg) on all data: 0.2568313
Test loss (w/o reg) on all data: 0.15979938
Train acc on all data:  0.874880611270296
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.7425196e-05
Norm of the params: 11.151057
              Random: fixed  22 labels. Loss 0.15980. Accuracy 0.950.
### Flips: 312, rs: 30, checks: 208
Using normal model
LBFGS training took [217] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.087761045
Train loss (w/o reg) on all data: 0.07454916
Test loss (w/o reg) on all data: 0.06076313
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.5310862e-06
Norm of the params: 16.255386
     Influence (LOO): fixed 121 labels. Loss 0.06076. Accuracy 0.977.
Using normal model
LBFGS training took [154] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04719117
Train loss (w/o reg) on all data: 0.03057575
Test loss (w/o reg) on all data: 0.034204315
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 4.155438e-06
Norm of the params: 18.229328
                Loss: fixed 138 labels. Loss 0.03420. Accuracy 0.989.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25464666
Train loss (w/o reg) on all data: 0.24844451
Test loss (w/o reg) on all data: 0.14758746
Train acc on all data:  0.8806112702960841
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.346668e-05
Norm of the params: 11.137452
              Random: fixed  30 labels. Loss 0.14759. Accuracy 0.962.
### Flips: 312, rs: 30, checks: 260
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0648053
Train loss (w/o reg) on all data: 0.05229515
Test loss (w/o reg) on all data: 0.048916362
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 9.311858e-06
Norm of the params: 15.817807
     Influence (LOO): fixed 138 labels. Loss 0.04892. Accuracy 0.981.
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03649375
Train loss (w/o reg) on all data: 0.021122295
Test loss (w/o reg) on all data: 0.032207366
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.0283881e-05
Norm of the params: 17.533657
                Loss: fixed 147 labels. Loss 0.03221. Accuracy 0.985.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24757569
Train loss (w/o reg) on all data: 0.24119294
Test loss (w/o reg) on all data: 0.1444028
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 3.2280674e-05
Norm of the params: 11.29845
              Random: fixed  35 labels. Loss 0.14440. Accuracy 0.954.
### Flips: 312, rs: 30, checks: 312
Using normal model
LBFGS training took [124] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.038362302
Train loss (w/o reg) on all data: 0.029012391
Test loss (w/o reg) on all data: 0.03107927
Train acc on all data:  0.9875835721107927
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.0887977e-06
Norm of the params: 13.674729
     Influence (LOO): fixed 154 labels. Loss 0.03108. Accuracy 0.992.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.028937535
Train loss (w/o reg) on all data: 0.01575784
Test loss (w/o reg) on all data: 0.033097573
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 9.421253e-07
Norm of the params: 16.235575
                Loss: fixed 155 labels. Loss 0.03310. Accuracy 0.989.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23737563
Train loss (w/o reg) on all data: 0.23040251
Test loss (w/o reg) on all data: 0.13943492
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.1165013e-05
Norm of the params: 11.809418
              Random: fixed  46 labels. Loss 0.13943. Accuracy 0.947.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27598685
Train loss (w/o reg) on all data: 0.26944545
Test loss (w/o reg) on all data: 0.21472134
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.9085599e-05
Norm of the params: 11.438005
Flipped loss: 0.21472. Accuracy: 0.935
### Flips: 312, rs: 31, checks: 52
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21431756
Train loss (w/o reg) on all data: 0.20403916
Test loss (w/o reg) on all data: 0.18119869
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.70277e-05
Norm of the params: 14.33765
     Influence (LOO): fixed  37 labels. Loss 0.18120. Accuracy 0.935.
Using normal model
LBFGS training took [203] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1676932
Train loss (w/o reg) on all data: 0.15262207
Test loss (w/o reg) on all data: 0.19641778
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 2.6568092e-05
Norm of the params: 17.361526
                Loss: fixed  49 labels. Loss 0.19642. Accuracy 0.905.
Using normal model
LBFGS training took [250] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27495578
Train loss (w/o reg) on all data: 0.26850536
Test loss (w/o reg) on all data: 0.20780426
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 7.774023e-05
Norm of the params: 11.358191
              Random: fixed   4 labels. Loss 0.20780. Accuracy 0.924.
### Flips: 312, rs: 31, checks: 104
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15610811
Train loss (w/o reg) on all data: 0.14364302
Test loss (w/o reg) on all data: 0.16660388
Train acc on all data:  0.933142311365807
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.9797786e-05
Norm of the params: 15.78929
     Influence (LOO): fixed  73 labels. Loss 0.16660. Accuracy 0.927.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.104611956
Train loss (w/o reg) on all data: 0.08529158
Test loss (w/o reg) on all data: 0.13678676
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 4.521602e-05
Norm of the params: 19.657251
                Loss: fixed  90 labels. Loss 0.13679. Accuracy 0.950.
Using normal model
LBFGS training took [286] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2619543
Train loss (w/o reg) on all data: 0.25523975
Test loss (w/o reg) on all data: 0.20030375
Train acc on all data:  0.8701050620821394
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 2.8443621e-05
Norm of the params: 11.588395
              Random: fixed  17 labels. Loss 0.20030. Accuracy 0.935.
### Flips: 312, rs: 31, checks: 156
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.11230641
Train loss (w/o reg) on all data: 0.10062198
Test loss (w/o reg) on all data: 0.09745491
Train acc on all data:  0.9579751671442216
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 1.6510034e-05
Norm of the params: 15.286876
     Influence (LOO): fixed 104 labels. Loss 0.09745. Accuracy 0.973.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06564435
Train loss (w/o reg) on all data: 0.049405914
Test loss (w/o reg) on all data: 0.06232287
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.224978e-06
Norm of the params: 18.021345
                Loss: fixed 120 labels. Loss 0.06232. Accuracy 0.977.
Using normal model
LBFGS training took [241] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.256064
Train loss (w/o reg) on all data: 0.24905413
Test loss (w/o reg) on all data: 0.19826752
Train acc on all data:  0.8758357211079274
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.240362e-05
Norm of the params: 11.840493
              Random: fixed  21 labels. Loss 0.19827. Accuracy 0.943.
### Flips: 312, rs: 31, checks: 208
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07019964
Train loss (w/o reg) on all data: 0.05814765
Test loss (w/o reg) on all data: 0.0657972
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.091722e-06
Norm of the params: 15.525454
     Influence (LOO): fixed 123 labels. Loss 0.06580. Accuracy 0.977.
Using normal model
LBFGS training took [139] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03514273
Train loss (w/o reg) on all data: 0.021020858
Test loss (w/o reg) on all data: 0.027746268
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 7.734445e-06
Norm of the params: 16.805878
                Loss: fixed 137 labels. Loss 0.02775. Accuracy 0.989.
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24297887
Train loss (w/o reg) on all data: 0.23573461
Test loss (w/o reg) on all data: 0.18613487
Train acc on all data:  0.889207258834766
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 3.5826135e-05
Norm of the params: 12.036824
              Random: fixed  30 labels. Loss 0.18613. Accuracy 0.943.
### Flips: 312, rs: 31, checks: 260
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05189765
Train loss (w/o reg) on all data: 0.041108295
Test loss (w/o reg) on all data: 0.05011135
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.4397995e-06
Norm of the params: 14.6896925
     Influence (LOO): fixed 136 labels. Loss 0.05011. Accuracy 0.981.
Using normal model
LBFGS training took [110] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025004517
Train loss (w/o reg) on all data: 0.013638122
Test loss (w/o reg) on all data: 0.01896523
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7559244e-06
Norm of the params: 15.077397
                Loss: fixed 147 labels. Loss 0.01897. Accuracy 0.992.
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2326602
Train loss (w/o reg) on all data: 0.2257121
Test loss (w/o reg) on all data: 0.16979681
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.7142782e-05
Norm of the params: 11.788211
              Random: fixed  41 labels. Loss 0.16980. Accuracy 0.954.
### Flips: 312, rs: 31, checks: 312
Using normal model
LBFGS training took [135] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033241995
Train loss (w/o reg) on all data: 0.023897052
Test loss (w/o reg) on all data: 0.031200202
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.035302e-06
Norm of the params: 13.671096
     Influence (LOO): fixed 147 labels. Loss 0.03120. Accuracy 0.985.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017744347
Train loss (w/o reg) on all data: 0.008933427
Test loss (w/o reg) on all data: 0.013518972
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0258993e-06
Norm of the params: 13.274729
                Loss: fixed 152 labels. Loss 0.01352. Accuracy 0.996.
Using normal model
LBFGS training took [196] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21719405
Train loss (w/o reg) on all data: 0.20928334
Test loss (w/o reg) on all data: 0.1501409
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.483006e-05
Norm of the params: 12.578333
              Random: fixed  52 labels. Loss 0.15014. Accuracy 0.969.
Using normal model
LBFGS training took [304] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27210268
Train loss (w/o reg) on all data: 0.26581782
Test loss (w/o reg) on all data: 0.21899927
Train acc on all data:  0.8615090735434575
Test acc on all data:   0.8969465648854962
Norm of the mean of gradients: 2.5876425e-05
Norm of the params: 11.211477
Flipped loss: 0.21900. Accuracy: 0.897
### Flips: 312, rs: 32, checks: 52
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20419267
Train loss (w/o reg) on all data: 0.19268438
Test loss (w/o reg) on all data: 0.17165485
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.720078e-05
Norm of the params: 15.171219
     Influence (LOO): fixed  37 labels. Loss 0.17165. Accuracy 0.927.
Using normal model
LBFGS training took [240] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17716902
Train loss (w/o reg) on all data: 0.16416146
Test loss (w/o reg) on all data: 0.18979916
Train acc on all data:  0.9121298949379179
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 8.959065e-06
Norm of the params: 16.129208
                Loss: fixed  46 labels. Loss 0.18980. Accuracy 0.901.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2668602
Train loss (w/o reg) on all data: 0.26056352
Test loss (w/o reg) on all data: 0.20490101
Train acc on all data:  0.8681948424068768
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 1.8536975e-05
Norm of the params: 11.2220125
              Random: fixed   7 labels. Loss 0.20490. Accuracy 0.912.
### Flips: 312, rs: 32, checks: 104
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16666265
Train loss (w/o reg) on all data: 0.15427935
Test loss (w/o reg) on all data: 0.13446644
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.2708107e-05
Norm of the params: 15.737404
     Influence (LOO): fixed  66 labels. Loss 0.13447. Accuracy 0.943.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10596282
Train loss (w/o reg) on all data: 0.08811057
Test loss (w/o reg) on all data: 0.14199886
Train acc on all data:  0.9608404966571156
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 9.213439e-06
Norm of the params: 18.895634
                Loss: fixed  89 labels. Loss 0.14200. Accuracy 0.935.
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2648208
Train loss (w/o reg) on all data: 0.25860488
Test loss (w/o reg) on all data: 0.19663842
Train acc on all data:  0.8691499522445081
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.8222125e-05
Norm of the params: 11.149833
              Random: fixed  13 labels. Loss 0.19664. Accuracy 0.924.
### Flips: 312, rs: 32, checks: 156
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12531514
Train loss (w/o reg) on all data: 0.11199105
Test loss (w/o reg) on all data: 0.10428712
Train acc on all data:  0.9474689589302769
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.767286e-06
Norm of the params: 16.324274
     Influence (LOO): fixed  92 labels. Loss 0.10429. Accuracy 0.958.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06504181
Train loss (w/o reg) on all data: 0.047282547
Test loss (w/o reg) on all data: 0.08305525
Train acc on all data:  0.9818529130850048
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5448778e-06
Norm of the params: 18.846363
                Loss: fixed 116 labels. Loss 0.08306. Accuracy 0.973.
Using normal model
LBFGS training took [294] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2598881
Train loss (w/o reg) on all data: 0.25366855
Test loss (w/o reg) on all data: 0.19928744
Train acc on all data:  0.874880611270296
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.1308433e-05
Norm of the params: 11.153083
              Random: fixed  20 labels. Loss 0.19929. Accuracy 0.924.
### Flips: 312, rs: 32, checks: 208
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.094766855
Train loss (w/o reg) on all data: 0.08262454
Test loss (w/o reg) on all data: 0.084886536
Train acc on all data:  0.9646609360076409
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.6572754e-05
Norm of the params: 15.583526
     Influence (LOO): fixed 112 labels. Loss 0.08489. Accuracy 0.958.
Using normal model
LBFGS training took [138] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.046144508
Train loss (w/o reg) on all data: 0.029576384
Test loss (w/o reg) on all data: 0.08682465
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 4.181037e-06
Norm of the params: 18.203367
                Loss: fixed 131 labels. Loss 0.08682. Accuracy 0.973.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25222066
Train loss (w/o reg) on all data: 0.24583848
Test loss (w/o reg) on all data: 0.19060498
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.6512271e-05
Norm of the params: 11.297954
              Random: fixed  28 labels. Loss 0.19060. Accuracy 0.916.
### Flips: 312, rs: 32, checks: 260
Using normal model
LBFGS training took [168] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0663092
Train loss (w/o reg) on all data: 0.05474941
Test loss (w/o reg) on all data: 0.07020412
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 5.3911604e-06
Norm of the params: 15.205125
     Influence (LOO): fixed 129 labels. Loss 0.07020. Accuracy 0.969.
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035291955
Train loss (w/o reg) on all data: 0.021512961
Test loss (w/o reg) on all data: 0.063280426
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.204763e-06
Norm of the params: 16.6006
                Loss: fixed 144 labels. Loss 0.06328. Accuracy 0.981.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24381846
Train loss (w/o reg) on all data: 0.23720603
Test loss (w/o reg) on all data: 0.18704817
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 2.8255647e-05
Norm of the params: 11.499947
              Random: fixed  35 labels. Loss 0.18705. Accuracy 0.924.
### Flips: 312, rs: 32, checks: 312
Using normal model
LBFGS training took [146] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04620559
Train loss (w/o reg) on all data: 0.03458889
Test loss (w/o reg) on all data: 0.05774689
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.714183e-06
Norm of the params: 15.242508
     Influence (LOO): fixed 140 labels. Loss 0.05775. Accuracy 0.969.
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.024730947
Train loss (w/o reg) on all data: 0.012822249
Test loss (w/o reg) on all data: 0.059910964
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 6.8473463e-07
Norm of the params: 15.432887
                Loss: fixed 152 labels. Loss 0.05991. Accuracy 0.981.
Using normal model
LBFGS training took [277] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23546028
Train loss (w/o reg) on all data: 0.22801009
Test loss (w/o reg) on all data: 0.18591855
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 1.1516641e-05
Norm of the params: 12.206712
              Random: fixed  40 labels. Loss 0.18592. Accuracy 0.927.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27211517
Train loss (w/o reg) on all data: 0.26601335
Test loss (w/o reg) on all data: 0.24268551
Train acc on all data:  0.8653295128939829
Test acc on all data:   0.8740458015267175
Norm of the mean of gradients: 1.691685e-05
Norm of the params: 11.04701
Flipped loss: 0.24269. Accuracy: 0.874
### Flips: 312, rs: 33, checks: 52
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20715448
Train loss (w/o reg) on all data: 0.19668002
Test loss (w/o reg) on all data: 0.21495031
Train acc on all data:  0.9006685768863419
Test acc on all data:   0.8893129770992366
Norm of the mean of gradients: 3.8176022e-05
Norm of the params: 14.473745
     Influence (LOO): fixed  38 labels. Loss 0.21495. Accuracy 0.889.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17703785
Train loss (w/o reg) on all data: 0.16310091
Test loss (w/o reg) on all data: 0.21536292
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.8893129770992366
Norm of the mean of gradients: 1.5572958e-05
Norm of the params: 16.695475
                Loss: fixed  45 labels. Loss 0.21536. Accuracy 0.889.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25880596
Train loss (w/o reg) on all data: 0.25166962
Test loss (w/o reg) on all data: 0.24398492
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.8778625954198473
Norm of the mean of gradients: 1.8576713e-05
Norm of the params: 11.94683
              Random: fixed  12 labels. Loss 0.24398. Accuracy 0.878.
### Flips: 312, rs: 33, checks: 104
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16616113
Train loss (w/o reg) on all data: 0.15479657
Test loss (w/o reg) on all data: 0.17334193
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 6.17872e-06
Norm of the params: 15.076181
     Influence (LOO): fixed  64 labels. Loss 0.17334. Accuracy 0.916.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10852942
Train loss (w/o reg) on all data: 0.088707164
Test loss (w/o reg) on all data: 0.2041899
Train acc on all data:  0.9656160458452722
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 5.6628132e-06
Norm of the params: 19.910929
                Loss: fixed  84 labels. Loss 0.20419. Accuracy 0.916.
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2507262
Train loss (w/o reg) on all data: 0.24331321
Test loss (w/o reg) on all data: 0.22514988
Train acc on all data:  0.8844317096466093
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 1.0601617e-05
Norm of the params: 12.176186
              Random: fixed  21 labels. Loss 0.22515. Accuracy 0.908.
### Flips: 312, rs: 33, checks: 156
Using normal model
LBFGS training took [206] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12926199
Train loss (w/o reg) on all data: 0.117928095
Test loss (w/o reg) on all data: 0.11985156
Train acc on all data:  0.9436485195797517
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 5.9610875e-06
Norm of the params: 15.055824
     Influence (LOO): fixed  92 labels. Loss 0.11985. Accuracy 0.935.
Using normal model
LBFGS training took [162] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07861361
Train loss (w/o reg) on all data: 0.06033331
Test loss (w/o reg) on all data: 0.17150651
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 3.040699e-06
Norm of the params: 19.120827
                Loss: fixed 110 labels. Loss 0.17151. Accuracy 0.931.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24055456
Train loss (w/o reg) on all data: 0.23292135
Test loss (w/o reg) on all data: 0.21844934
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 5.5117624e-05
Norm of the params: 12.35573
              Random: fixed  30 labels. Loss 0.21845. Accuracy 0.912.
### Flips: 312, rs: 33, checks: 208
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09535618
Train loss (w/o reg) on all data: 0.08358669
Test loss (w/o reg) on all data: 0.10890787
Train acc on all data:  0.9627507163323782
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.2381422e-05
Norm of the params: 15.34242
     Influence (LOO): fixed 110 labels. Loss 0.10891. Accuracy 0.950.
Using normal model
LBFGS training took [126] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.043895938
Train loss (w/o reg) on all data: 0.027829854
Test loss (w/o reg) on all data: 0.10884534
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 1.8853617e-06
Norm of the params: 17.925447
                Loss: fixed 132 labels. Loss 0.10885. Accuracy 0.962.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23000024
Train loss (w/o reg) on all data: 0.22210489
Test loss (w/o reg) on all data: 0.20778964
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 4.207276e-05
Norm of the params: 12.566101
              Random: fixed  40 labels. Loss 0.20779. Accuracy 0.908.
### Flips: 312, rs: 33, checks: 260
Using normal model
LBFGS training took [191] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.064494036
Train loss (w/o reg) on all data: 0.053842347
Test loss (w/o reg) on all data: 0.06636031
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 8.263247e-06
Norm of the params: 14.595677
     Influence (LOO): fixed 131 labels. Loss 0.06636. Accuracy 0.977.
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.029768791
Train loss (w/o reg) on all data: 0.016700858
Test loss (w/o reg) on all data: 0.07416527
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 1.2810741e-06
Norm of the params: 16.16659
                Loss: fixed 144 labels. Loss 0.07417. Accuracy 0.969.
Using normal model
LBFGS training took [222] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22402994
Train loss (w/o reg) on all data: 0.21538873
Test loss (w/o reg) on all data: 0.2010125
Train acc on all data:  0.9054441260744985
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 1.6670456e-05
Norm of the params: 13.14626
              Random: fixed  45 labels. Loss 0.20101. Accuracy 0.912.
### Flips: 312, rs: 33, checks: 312
Using normal model
LBFGS training took [163] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.037088104
Train loss (w/o reg) on all data: 0.02647615
Test loss (w/o reg) on all data: 0.04633738
Train acc on all data:  0.9904489016236867
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.5151227e-06
Norm of the params: 14.568427
     Influence (LOO): fixed 142 labels. Loss 0.04634. Accuracy 0.981.
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.021615738
Train loss (w/o reg) on all data: 0.011381238
Test loss (w/o reg) on all data: 0.026839769
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 3.7164932e-06
Norm of the params: 14.306991
                Loss: fixed 152 labels. Loss 0.02684. Accuracy 0.992.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21580754
Train loss (w/o reg) on all data: 0.20664138
Test loss (w/o reg) on all data: 0.19061604
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.2656734e-05
Norm of the params: 13.539692
              Random: fixed  52 labels. Loss 0.19062. Accuracy 0.924.
Using normal model
LBFGS training took [291] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.28153232
Train loss (w/o reg) on all data: 0.2758663
Test loss (w/o reg) on all data: 0.19655907
Train acc on all data:  0.8605539637058262
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.5709273e-05
Norm of the params: 10.645195
Flipped loss: 0.19656. Accuracy: 0.935
### Flips: 312, rs: 34, checks: 52
Using normal model
LBFGS training took [266] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22271767
Train loss (w/o reg) on all data: 0.21340166
Test loss (w/o reg) on all data: 0.1601002
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.976093e-05
Norm of the params: 13.649913
     Influence (LOO): fixed  35 labels. Loss 0.16010. Accuracy 0.943.
Using normal model
LBFGS training took [187] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18442884
Train loss (w/o reg) on all data: 0.17127201
Test loss (w/o reg) on all data: 0.15500775
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 5.4523713e-05
Norm of the params: 16.221483
                Loss: fixed  50 labels. Loss 0.15501. Accuracy 0.931.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2777432
Train loss (w/o reg) on all data: 0.27202502
Test loss (w/o reg) on all data: 0.18905886
Train acc on all data:  0.8672397325692455
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 5.1466224e-05
Norm of the params: 10.694077
              Random: fixed   5 labels. Loss 0.18906. Accuracy 0.950.
### Flips: 312, rs: 34, checks: 104
Using normal model
LBFGS training took [223] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18110831
Train loss (w/o reg) on all data: 0.17101097
Test loss (w/o reg) on all data: 0.11719237
Train acc on all data:  0.9140401146131805
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.4616516e-05
Norm of the params: 14.210798
     Influence (LOO): fixed  66 labels. Loss 0.11719. Accuracy 0.966.
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.108686134
Train loss (w/o reg) on all data: 0.08959128
Test loss (w/o reg) on all data: 0.115095176
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 1.9510357e-05
Norm of the params: 19.542189
                Loss: fixed  91 labels. Loss 0.11510. Accuracy 0.947.
Using normal model
LBFGS training took [300] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.27049744
Train loss (w/o reg) on all data: 0.2650212
Test loss (w/o reg) on all data: 0.1778543
Train acc on all data:  0.8729703915950334
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 2.9066116e-05
Norm of the params: 10.465394
              Random: fixed  16 labels. Loss 0.17785. Accuracy 0.947.
### Flips: 312, rs: 34, checks: 156
Using normal model
LBFGS training took [189] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13881175
Train loss (w/o reg) on all data: 0.12658727
Test loss (w/o reg) on all data: 0.089181125
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.397444e-05
Norm of the params: 15.636165
     Influence (LOO): fixed  93 labels. Loss 0.08918. Accuracy 0.966.
Using normal model
LBFGS training took [211] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061833173
Train loss (w/o reg) on all data: 0.04313743
Test loss (w/o reg) on all data: 0.06392622
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.1462837e-06
Norm of the params: 19.336878
                Loss: fixed 120 labels. Loss 0.06393. Accuracy 0.973.
Using normal model
LBFGS training took [276] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26922563
Train loss (w/o reg) on all data: 0.26363724
Test loss (w/o reg) on all data: 0.17701536
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.7353424e-05
Norm of the params: 10.572014
              Random: fixed  17 labels. Loss 0.17702. Accuracy 0.954.
### Flips: 312, rs: 34, checks: 208
Using normal model
LBFGS training took [195] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1013448
Train loss (w/o reg) on all data: 0.090919696
Test loss (w/o reg) on all data: 0.06383032
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.6902684e-05
Norm of the params: 14.439604
     Influence (LOO): fixed 117 labels. Loss 0.06383. Accuracy 0.981.
Using normal model
LBFGS training took [160] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04295329
Train loss (w/o reg) on all data: 0.02688387
Test loss (w/o reg) on all data: 0.054006886
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.0632798e-06
Norm of the params: 17.927309
                Loss: fixed 139 labels. Loss 0.05401. Accuracy 0.981.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2568819
Train loss (w/o reg) on all data: 0.25048253
Test loss (w/o reg) on all data: 0.16304648
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.3104038e-05
Norm of the params: 11.313146
              Random: fixed  27 labels. Loss 0.16305. Accuracy 0.958.
### Flips: 312, rs: 34, checks: 260
Using normal model
LBFGS training took [183] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06334952
Train loss (w/o reg) on all data: 0.05217198
Test loss (w/o reg) on all data: 0.0456472
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 4.737455e-06
Norm of the params: 14.951618
     Influence (LOO): fixed 138 labels. Loss 0.04565. Accuracy 0.985.
Using normal model
LBFGS training took [125] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027665941
Train loss (w/o reg) on all data: 0.016406046
Test loss (w/o reg) on all data: 0.042475186
Train acc on all data:  0.9961795606494747
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 1.8192297e-06
Norm of the params: 15.006596
                Loss: fixed 151 labels. Loss 0.04248. Accuracy 0.985.
Using normal model
LBFGS training took [242] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24489433
Train loss (w/o reg) on all data: 0.23783125
Test loss (w/o reg) on all data: 0.14482999
Train acc on all data:  0.8930276981852913
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.093062e-05
Norm of the params: 11.88535
              Random: fixed  37 labels. Loss 0.14483. Accuracy 0.973.
### Flips: 312, rs: 34, checks: 312
Using normal model
LBFGS training took [140] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.034860395
Train loss (w/o reg) on all data: 0.025403274
Test loss (w/o reg) on all data: 0.021526607
Train acc on all data:  0.9923591212989494
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 2.9900482e-06
Norm of the params: 13.752906
     Influence (LOO): fixed 154 labels. Loss 0.02153. Accuracy 0.985.
Using normal model
LBFGS training took [105] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.018665835
Train loss (w/o reg) on all data: 0.009693571
Test loss (w/o reg) on all data: 0.02810125
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 1.8532176e-06
Norm of the params: 13.39572
                Loss: fixed 158 labels. Loss 0.02810. Accuracy 0.989.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23384836
Train loss (w/o reg) on all data: 0.22667532
Test loss (w/o reg) on all data: 0.13259791
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 3.9538445e-05
Norm of the params: 11.977517
              Random: fixed  47 labels. Loss 0.13260. Accuracy 0.973.
Using normal model
LBFGS training took [285] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26164472
Train loss (w/o reg) on all data: 0.25667822
Test loss (w/o reg) on all data: 0.16444607
Train acc on all data:  0.8739255014326648
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 2.2403348e-05
Norm of the params: 9.966429
Flipped loss: 0.16445. Accuracy: 0.943
### Flips: 312, rs: 35, checks: 52
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17982328
Train loss (w/o reg) on all data: 0.16927162
Test loss (w/o reg) on all data: 0.15402515
Train acc on all data:  0.9207258834765998
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 8.008126e-06
Norm of the params: 14.526982
     Influence (LOO): fixed  41 labels. Loss 0.15403. Accuracy 0.939.
Using normal model
LBFGS training took [199] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1504042
Train loss (w/o reg) on all data: 0.13863765
Test loss (w/o reg) on all data: 0.14474845
Train acc on all data:  0.9302769818529131
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.15004905e-05
Norm of the params: 15.340506
                Loss: fixed  50 labels. Loss 0.14475. Accuracy 0.935.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.25442716
Train loss (w/o reg) on all data: 0.24949403
Test loss (w/o reg) on all data: 0.15096499
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 3.20947e-05
Norm of the params: 9.932896
              Random: fixed  10 labels. Loss 0.15096. Accuracy 0.969.
### Flips: 312, rs: 35, checks: 104
Using normal model
LBFGS training took [193] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13766535
Train loss (w/o reg) on all data: 0.12712461
Test loss (w/o reg) on all data: 0.100695945
Train acc on all data:  0.9455587392550143
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 8.6870095e-06
Norm of the params: 14.519462
     Influence (LOO): fixed  74 labels. Loss 0.10070. Accuracy 0.969.
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07609944
Train loss (w/o reg) on all data: 0.062369462
Test loss (w/o reg) on all data: 0.093964525
Train acc on all data:  0.9751671442215855
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 7.532603e-06
Norm of the params: 16.571045
                Loss: fixed  92 labels. Loss 0.09396. Accuracy 0.958.
Using normal model
LBFGS training took [278] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24981542
Train loss (w/o reg) on all data: 0.24457888
Test loss (w/o reg) on all data: 0.15060993
Train acc on all data:  0.8882521489971347
Test acc on all data:   0.9618320610687023
Norm of the mean of gradients: 3.8035592e-05
Norm of the params: 10.233803
              Random: fixed  15 labels. Loss 0.15061. Accuracy 0.962.
### Flips: 312, rs: 35, checks: 156
Using normal model
LBFGS training took [213] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.10213553
Train loss (w/o reg) on all data: 0.09116168
Test loss (w/o reg) on all data: 0.07850622
Train acc on all data:  0.9675262655205349
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.7172017e-05
Norm of the params: 14.814755
     Influence (LOO): fixed  92 labels. Loss 0.07851. Accuracy 0.977.
Using normal model
LBFGS training took [129] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027968436
Train loss (w/o reg) on all data: 0.015569188
Test loss (w/o reg) on all data: 0.055209186
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.0419448e-06
Norm of the params: 15.7475395
                Loss: fixed 117 labels. Loss 0.05521. Accuracy 0.981.
Using normal model
LBFGS training took [270] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24534686
Train loss (w/o reg) on all data: 0.23995173
Test loss (w/o reg) on all data: 0.14611202
Train acc on all data:  0.89207258834766
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 3.72011e-05
Norm of the params: 10.387618
              Random: fixed  22 labels. Loss 0.14611. Accuracy 0.977.
### Flips: 312, rs: 35, checks: 208
Using normal model
LBFGS training took [172] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.06314232
Train loss (w/o reg) on all data: 0.052174587
Test loss (w/o reg) on all data: 0.057440087
Train acc on all data:  0.9828080229226361
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.7749686e-06
Norm of the params: 14.810628
     Influence (LOO): fixed 113 labels. Loss 0.05744. Accuracy 0.981.
Using normal model
LBFGS training took [116] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.017375238
Train loss (w/o reg) on all data: 0.008028686
Test loss (w/o reg) on all data: 0.031676356
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 6.182737e-07
Norm of the params: 13.672273
                Loss: fixed 128 labels. Loss 0.03168. Accuracy 0.985.
Using normal model
LBFGS training took [226] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23491408
Train loss (w/o reg) on all data: 0.22945692
Test loss (w/o reg) on all data: 0.13268161
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.3064138e-05
Norm of the params: 10.447168
              Random: fixed  30 labels. Loss 0.13268. Accuracy 0.977.
### Flips: 312, rs: 35, checks: 260
Using normal model
LBFGS training took [118] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.027461343
Train loss (w/o reg) on all data: 0.018273704
Test loss (w/o reg) on all data: 0.021080768
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.8281366e-06
Norm of the params: 13.555544
     Influence (LOO): fixed 131 labels. Loss 0.02108. Accuracy 0.989.
Using normal model
LBFGS training took [100] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.016360119
Train loss (w/o reg) on all data: 0.007492417
Test loss (w/o reg) on all data: 0.03849854
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 7.799199e-07
Norm of the params: 13.317434
                Loss: fixed 130 labels. Loss 0.03850. Accuracy 0.985.
Using normal model
LBFGS training took [281] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22963193
Train loss (w/o reg) on all data: 0.22432
Test loss (w/o reg) on all data: 0.12337805
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 2.7156291e-05
Norm of the params: 10.307219
              Random: fixed  36 labels. Loss 0.12338. Accuracy 0.981.
### Flips: 312, rs: 35, checks: 312
Using normal model
LBFGS training took [101] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.019650128
Train loss (w/o reg) on all data: 0.011112698
Test loss (w/o reg) on all data: 0.018103205
Train acc on all data:  0.9952244508118434
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.290889e-07
Norm of the params: 13.0670805
     Influence (LOO): fixed 136 labels. Loss 0.01810. Accuracy 0.992.
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013955925
Train loss (w/o reg) on all data: 0.006091567
Test loss (w/o reg) on all data: 0.021032585
Train acc on all data:  1.0
Test acc on all data:   0.9847328244274809
Norm of the mean of gradients: 5.987665e-07
Norm of the params: 12.541417
                Loss: fixed 135 labels. Loss 0.02103. Accuracy 0.985.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22327156
Train loss (w/o reg) on all data: 0.21799274
Test loss (w/o reg) on all data: 0.11404081
Train acc on all data:  0.9092645654250239
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.128924e-05
Norm of the params: 10.275048
              Random: fixed  41 labels. Loss 0.11404. Accuracy 0.981.
Using normal model
LBFGS training took [246] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2735004
Train loss (w/o reg) on all data: 0.26671758
Test loss (w/o reg) on all data: 0.2637738
Train acc on all data:  0.8672397325692455
Test acc on all data:   0.8816793893129771
Norm of the mean of gradients: 1.6661561e-05
Norm of the params: 11.647164
Flipped loss: 0.26377. Accuracy: 0.882
### Flips: 312, rs: 36, checks: 52
Using normal model
LBFGS training took [237] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21262681
Train loss (w/o reg) on all data: 0.20207228
Test loss (w/o reg) on all data: 0.22910064
Train acc on all data:  0.9016236867239733
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 4.380698e-05
Norm of the params: 14.52896
     Influence (LOO): fixed  38 labels. Loss 0.22910. Accuracy 0.901.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.17725068
Train loss (w/o reg) on all data: 0.16250691
Test loss (w/o reg) on all data: 0.23497504
Train acc on all data:  0.9226361031518625
Test acc on all data:   0.8931297709923665
Norm of the mean of gradients: 3.1844662e-05
Norm of the params: 17.17194
                Loss: fixed  46 labels. Loss 0.23498. Accuracy 0.893.
Using normal model
LBFGS training took [249] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2624659
Train loss (w/o reg) on all data: 0.25551203
Test loss (w/o reg) on all data: 0.24633154
Train acc on all data:  0.87774594078319
Test acc on all data:   0.8931297709923665
Norm of the mean of gradients: 2.0005084e-05
Norm of the params: 11.793109
              Random: fixed  11 labels. Loss 0.24633. Accuracy 0.893.
### Flips: 312, rs: 36, checks: 104
Using normal model
LBFGS training took [299] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1623654
Train loss (w/o reg) on all data: 0.15118931
Test loss (w/o reg) on all data: 0.18726538
Train acc on all data:  0.9274116523400191
Test acc on all data:   0.9122137404580153
Norm of the mean of gradients: 8.064057e-06
Norm of the params: 14.95065
     Influence (LOO): fixed  72 labels. Loss 0.18727. Accuracy 0.912.
Using normal model
LBFGS training took [215] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1213588
Train loss (w/o reg) on all data: 0.10285684
Test loss (w/o reg) on all data: 0.16907775
Train acc on all data:  0.956064947468959
Test acc on all data:   0.9312977099236641
Norm of the mean of gradients: 5.7264815e-06
Norm of the params: 19.236404
                Loss: fixed  83 labels. Loss 0.16908. Accuracy 0.931.
Using normal model
LBFGS training took [221] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2593281
Train loss (w/o reg) on all data: 0.2521556
Test loss (w/o reg) on all data: 0.23102883
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.8969465648854962
Norm of the mean of gradients: 4.5688037e-05
Norm of the params: 11.977045
              Random: fixed  18 labels. Loss 0.23103. Accuracy 0.897.
### Flips: 312, rs: 36, checks: 156
Using normal model
LBFGS training took [218] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13669144
Train loss (w/o reg) on all data: 0.12478374
Test loss (w/o reg) on all data: 0.16255252
Train acc on all data:  0.944603629417383
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 2.4759176e-05
Norm of the params: 15.432233
     Influence (LOO): fixed  93 labels. Loss 0.16255. Accuracy 0.905.
Using normal model
LBFGS training took [209] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.083345324
Train loss (w/o reg) on all data: 0.06286104
Test loss (w/o reg) on all data: 0.13015425
Train acc on all data:  0.9770773638968482
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 8.043718e-06
Norm of the params: 20.240692
                Loss: fixed 109 labels. Loss 0.13015. Accuracy 0.950.
Using normal model
LBFGS training took [239] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2522128
Train loss (w/o reg) on all data: 0.24475957
Test loss (w/o reg) on all data: 0.22740082
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 1.9585908e-05
Norm of the params: 12.209201
              Random: fixed  26 labels. Loss 0.22740. Accuracy 0.901.
### Flips: 312, rs: 36, checks: 208
Using normal model
LBFGS training took [224] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09905216
Train loss (w/o reg) on all data: 0.086368285
Test loss (w/o reg) on all data: 0.12718177
Train acc on all data:  0.9617956064947469
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 1.785637e-05
Norm of the params: 15.927258
     Influence (LOO): fixed 117 labels. Loss 0.12718. Accuracy 0.935.
Using normal model
LBFGS training took [158] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052113924
Train loss (w/o reg) on all data: 0.0340739
Test loss (w/o reg) on all data: 0.079296306
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 3.9320366e-06
Norm of the params: 18.994747
                Loss: fixed 131 labels. Loss 0.07930. Accuracy 0.958.
Using normal model
LBFGS training took [264] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24608865
Train loss (w/o reg) on all data: 0.23897618
Test loss (w/o reg) on all data: 0.22012378
Train acc on all data:  0.8911174785100286
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 0.00010910828
Norm of the params: 11.926839
              Random: fixed  34 labels. Loss 0.22012. Accuracy 0.901.
### Flips: 312, rs: 36, checks: 260
Using normal model
LBFGS training took [159] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.061384413
Train loss (w/o reg) on all data: 0.05149094
Test loss (w/o reg) on all data: 0.07085674
Train acc on all data:  0.9780324737344794
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 7.785195e-06
Norm of the params: 14.066609
     Influence (LOO): fixed 143 labels. Loss 0.07086. Accuracy 0.977.
Using normal model
LBFGS training took [145] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03997685
Train loss (w/o reg) on all data: 0.024237644
Test loss (w/o reg) on all data: 0.060268536
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9694656488549618
Norm of the mean of gradients: 2.3048715e-06
Norm of the params: 17.742157
                Loss: fixed 145 labels. Loss 0.06027. Accuracy 0.969.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23918718
Train loss (w/o reg) on all data: 0.23230992
Test loss (w/o reg) on all data: 0.20827852
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 6.815688e-05
Norm of the params: 11.72797
              Random: fixed  40 labels. Loss 0.20828. Accuracy 0.901.
### Flips: 312, rs: 36, checks: 312
Using normal model
LBFGS training took [143] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03872881
Train loss (w/o reg) on all data: 0.030788349
Test loss (w/o reg) on all data: 0.04418514
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 3.1066324e-06
Norm of the params: 12.601956
     Influence (LOO): fixed 159 labels. Loss 0.04419. Accuracy 0.989.
Using normal model
LBFGS training took [104] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025515754
Train loss (w/o reg) on all data: 0.013338302
Test loss (w/o reg) on all data: 0.03775946
Train acc on all data:  1.0
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.880097e-07
Norm of the params: 15.606058
                Loss: fixed 155 labels. Loss 0.03776. Accuracy 0.981.
Using normal model
LBFGS training took [227] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22546706
Train loss (w/o reg) on all data: 0.21876495
Test loss (w/o reg) on all data: 0.1956694
Train acc on all data:  0.9025787965616046
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 3.6348818e-05
Norm of the params: 11.577661
              Random: fixed  54 labels. Loss 0.19567. Accuracy 0.901.
Using normal model
LBFGS training took [297] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26806995
Train loss (w/o reg) on all data: 0.2618867
Test loss (w/o reg) on all data: 0.16999948
Train acc on all data:  0.8710601719197708
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.8169725e-05
Norm of the params: 11.120498
Flipped loss: 0.17000. Accuracy: 0.950
### Flips: 312, rs: 37, checks: 52
Using normal model
LBFGS training took [282] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.19641572
Train loss (w/o reg) on all data: 0.18543163
Test loss (w/o reg) on all data: 0.14688756
Train acc on all data:  0.9130850047755492
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.9188473e-05
Norm of the params: 14.82167
     Influence (LOO): fixed  35 labels. Loss 0.14689. Accuracy 0.954.
Using normal model
LBFGS training took [274] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.15634204
Train loss (w/o reg) on all data: 0.14206177
Test loss (w/o reg) on all data: 0.117037505
Train acc on all data:  0.9340974212034384
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.4079938e-05
Norm of the params: 16.89987
                Loss: fixed  48 labels. Loss 0.11704. Accuracy 0.966.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26217428
Train loss (w/o reg) on all data: 0.2561503
Test loss (w/o reg) on all data: 0.16652317
Train acc on all data:  0.8834765998089781
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.3855625e-05
Norm of the params: 10.976311
              Random: fixed   8 labels. Loss 0.16652. Accuracy 0.950.
### Flips: 312, rs: 37, checks: 104
Using normal model
LBFGS training took [216] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1553812
Train loss (w/o reg) on all data: 0.14182383
Test loss (w/o reg) on all data: 0.115956664
Train acc on all data:  0.9350525310410697
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 3.1422584e-05
Norm of the params: 16.466555
     Influence (LOO): fixed  60 labels. Loss 0.11596. Accuracy 0.966.
Using normal model
LBFGS training took [176] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.086899325
Train loss (w/o reg) on all data: 0.06752333
Test loss (w/o reg) on all data: 0.08879871
Train acc on all data:  0.9742120343839542
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.5623924e-05
Norm of the params: 19.685526
                Loss: fixed  87 labels. Loss 0.08880. Accuracy 0.973.
Using normal model
LBFGS training took [283] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2544508
Train loss (w/o reg) on all data: 0.24774465
Test loss (w/o reg) on all data: 0.15867552
Train acc on all data:  0.8863419293218721
Test acc on all data:   0.9656488549618321
Norm of the mean of gradients: 1.8829187e-05
Norm of the params: 11.581147
              Random: fixed  15 labels. Loss 0.15868. Accuracy 0.966.
### Flips: 312, rs: 37, checks: 156
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.12090303
Train loss (w/o reg) on all data: 0.10663041
Test loss (w/o reg) on all data: 0.08723616
Train acc on all data:  0.9512893982808023
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 9.0107715e-06
Norm of the params: 16.895338
     Influence (LOO): fixed  82 labels. Loss 0.08724. Accuracy 0.973.
Using normal model
LBFGS training took [166] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.056177244
Train loss (w/o reg) on all data: 0.040223725
Test loss (w/o reg) on all data: 0.06565778
Train acc on all data:  0.9866284622731614
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 9.896254e-06
Norm of the params: 17.862541
                Loss: fixed 111 labels. Loss 0.06566. Accuracy 0.977.
Using normal model
LBFGS training took [254] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24136455
Train loss (w/o reg) on all data: 0.23396534
Test loss (w/o reg) on all data: 0.15516281
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 5.7746845e-05
Norm of the params: 12.164883
              Random: fixed  24 labels. Loss 0.15516. Accuracy 0.958.
### Flips: 312, rs: 37, checks: 208
Using normal model
LBFGS training took [164] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.080747366
Train loss (w/o reg) on all data: 0.06809902
Test loss (w/o reg) on all data: 0.06650835
Train acc on all data:  0.9684813753581661
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.099399e-06
Norm of the params: 15.904933
     Influence (LOO): fixed 109 labels. Loss 0.06651. Accuracy 0.977.
Using normal model
LBFGS training took [157] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.035746075
Train loss (w/o reg) on all data: 0.022563322
Test loss (w/o reg) on all data: 0.045535583
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 3.1466266e-06
Norm of the params: 16.23746
                Loss: fixed 130 labels. Loss 0.04554. Accuracy 0.981.
Using normal model
LBFGS training took [275] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2349673
Train loss (w/o reg) on all data: 0.22769013
Test loss (w/o reg) on all data: 0.14981602
Train acc on all data:  0.8968481375358166
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.4227638e-05
Norm of the params: 12.064142
              Random: fixed  33 labels. Loss 0.14982. Accuracy 0.950.
### Flips: 312, rs: 37, checks: 260
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.04450597
Train loss (w/o reg) on all data: 0.034535725
Test loss (w/o reg) on all data: 0.031182587
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.3243499e-06
Norm of the params: 14.12108
     Influence (LOO): fixed 131 labels. Loss 0.03118. Accuracy 0.992.
Using normal model
LBFGS training took [132] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.033400588
Train loss (w/o reg) on all data: 0.021623535
Test loss (w/o reg) on all data: 0.036838103
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.1650839e-06
Norm of the params: 15.347347
                Loss: fixed 139 labels. Loss 0.03684. Accuracy 0.981.
Using normal model
LBFGS training took [260] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23460388
Train loss (w/o reg) on all data: 0.2275289
Test loss (w/o reg) on all data: 0.1488665
Train acc on all data:  0.8997134670487106
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.5231384e-05
Norm of the params: 11.895369
              Random: fixed  35 labels. Loss 0.14887. Accuracy 0.950.
### Flips: 312, rs: 37, checks: 312
Using normal model
LBFGS training took [147] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.032035075
Train loss (w/o reg) on all data: 0.022532053
Test loss (w/o reg) on all data: 0.024081305
Train acc on all data:  0.9914040114613181
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 9.3975814e-07
Norm of the params: 13.7862425
     Influence (LOO): fixed 142 labels. Loss 0.02408. Accuracy 0.996.
Using normal model
LBFGS training took [102] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.020570636
Train loss (w/o reg) on all data: 0.011655663
Test loss (w/o reg) on all data: 0.04188797
Train acc on all data:  0.9980897803247374
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 1.7273507e-06
Norm of the params: 13.352881
                Loss: fixed 147 labels. Loss 0.04189. Accuracy 0.981.
Using normal model
LBFGS training took [252] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2237641
Train loss (w/o reg) on all data: 0.21673432
Test loss (w/o reg) on all data: 0.13732138
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9465648854961832
Norm of the mean of gradients: 3.851879e-05
Norm of the params: 11.8573065
              Random: fixed  47 labels. Loss 0.13732. Accuracy 0.947.
Using normal model
LBFGS training took [225] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26091123
Train loss (w/o reg) on all data: 0.25565937
Test loss (w/o reg) on all data: 0.19956161
Train acc on all data:  0.8720152817574021
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 5.0352875e-05
Norm of the params: 10.24876
Flipped loss: 0.19956. Accuracy: 0.924
### Flips: 312, rs: 38, checks: 52
Using normal model
LBFGS training took [280] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.18708915
Train loss (w/o reg) on all data: 0.17796257
Test loss (w/o reg) on all data: 0.15399356
Train acc on all data:  0.9159503342884432
Test acc on all data:   0.9351145038167938
Norm of the mean of gradients: 4.656544e-05
Norm of the params: 13.510426
     Influence (LOO): fixed  42 labels. Loss 0.15399. Accuracy 0.935.
Using normal model
LBFGS training took [214] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1501158
Train loss (w/o reg) on all data: 0.13795933
Test loss (w/o reg) on all data: 0.18015026
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9083969465648855
Norm of the mean of gradients: 2.4185605e-05
Norm of the params: 15.59261
                Loss: fixed  50 labels. Loss 0.18015. Accuracy 0.908.
Using normal model
LBFGS training took [248] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2529725
Train loss (w/o reg) on all data: 0.24747826
Test loss (w/o reg) on all data: 0.19294883
Train acc on all data:  0.8787010506208214
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 4.136113e-05
Norm of the params: 10.482619
              Random: fixed   8 labels. Loss 0.19295. Accuracy 0.927.
### Flips: 312, rs: 38, checks: 104
Using normal model
LBFGS training took [171] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.14082609
Train loss (w/o reg) on all data: 0.13007076
Test loss (w/o reg) on all data: 0.124180116
Train acc on all data:  0.9369627507163324
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.4829014e-05
Norm of the params: 14.666514
     Influence (LOO): fixed  69 labels. Loss 0.12418. Accuracy 0.939.
Using normal model
LBFGS training took [207] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.08854846
Train loss (w/o reg) on all data: 0.072734006
Test loss (w/o reg) on all data: 0.10831715
Train acc on all data:  0.9703915950334289
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 2.3625209e-05
Norm of the params: 17.78452
                Loss: fixed  89 labels. Loss 0.10832. Accuracy 0.950.
Using normal model
LBFGS training took [234] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24288835
Train loss (w/o reg) on all data: 0.23732433
Test loss (w/o reg) on all data: 0.17773171
Train acc on all data:  0.8853868194842407
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.8296317e-05
Norm of the params: 10.548945
              Random: fixed  17 labels. Loss 0.17773. Accuracy 0.927.
### Flips: 312, rs: 38, checks: 156
Using normal model
LBFGS training took [228] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.09474518
Train loss (w/o reg) on all data: 0.084608555
Test loss (w/o reg) on all data: 0.08966209
Train acc on all data:  0.9598853868194842
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.1234963e-05
Norm of the params: 14.238419
     Influence (LOO): fixed  98 labels. Loss 0.08966. Accuracy 0.958.
Using normal model
LBFGS training took [165] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0527926
Train loss (w/o reg) on all data: 0.03683927
Test loss (w/o reg) on all data: 0.06928827
Train acc on all data:  0.9837631327602674
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 5.9474996e-06
Norm of the params: 17.862438
                Loss: fixed 112 labels. Loss 0.06929. Accuracy 0.973.
Using normal model
LBFGS training took [247] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23582798
Train loss (w/o reg) on all data: 0.23011409
Test loss (w/o reg) on all data: 0.17673928
Train acc on all data:  0.894937917860554
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 7.2690884e-05
Norm of the params: 10.690081
              Random: fixed  24 labels. Loss 0.17674. Accuracy 0.927.
### Flips: 312, rs: 38, checks: 208
Using normal model
LBFGS training took [197] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.05679574
Train loss (w/o reg) on all data: 0.046367876
Test loss (w/o reg) on all data: 0.062155236
Train acc on all data:  0.9799426934097422
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 2.6585706e-06
Norm of the params: 14.441512
     Influence (LOO): fixed 118 labels. Loss 0.06216. Accuracy 0.973.
Using normal model
LBFGS training took [115] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025126647
Train loss (w/o reg) on all data: 0.015024372
Test loss (w/o reg) on all data: 0.022390733
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 1.273813e-06
Norm of the params: 14.21427
                Loss: fixed 132 labels. Loss 0.02239. Accuracy 0.992.
Using normal model
LBFGS training took [251] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22773986
Train loss (w/o reg) on all data: 0.22191566
Test loss (w/o reg) on all data: 0.16483055
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9427480916030534
Norm of the mean of gradients: 4.0095132e-05
Norm of the params: 10.792769
              Random: fixed  31 labels. Loss 0.16483. Accuracy 0.943.
### Flips: 312, rs: 38, checks: 260
Using normal model
LBFGS training took [73] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.03191526
Train loss (w/o reg) on all data: 0.023873448
Test loss (w/o reg) on all data: 0.045150496
Train acc on all data:  0.9894937917860553
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 1.4672279e-06
Norm of the params: 12.682121
     Influence (LOO): fixed 130 labels. Loss 0.04515. Accuracy 0.977.
Using normal model
LBFGS training took [92] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.015981019
Train loss (w/o reg) on all data: 0.007771104
Test loss (w/o reg) on all data: 0.022634739
Train acc on all data:  1.0
Test acc on all data:   0.9885496183206107
Norm of the mean of gradients: 5.7484544e-07
Norm of the params: 12.81399
                Loss: fixed 138 labels. Loss 0.02263. Accuracy 0.989.
Using normal model
LBFGS training took [212] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.22374529
Train loss (w/o reg) on all data: 0.21797833
Test loss (w/o reg) on all data: 0.15540135
Train acc on all data:  0.9044890162368673
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 1.5970907e-05
Norm of the params: 10.739601
              Random: fixed  35 labels. Loss 0.15540. Accuracy 0.954.
### Flips: 312, rs: 38, checks: 312
Using normal model
LBFGS training took [83] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02412798
Train loss (w/o reg) on all data: 0.016619816
Test loss (w/o reg) on all data: 0.025282854
Train acc on all data:  0.994269340974212
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.0907966e-06
Norm of the params: 12.254113
     Influence (LOO): fixed 135 labels. Loss 0.02528. Accuracy 0.996.
Using normal model
LBFGS training took [59] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.013434714
Train loss (w/o reg) on all data: 0.0067810933
Test loss (w/o reg) on all data: 0.014618576
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 2.9488936e-06
Norm of the params: 11.535702
                Loss: fixed 142 labels. Loss 0.01462. Accuracy 0.992.
Using normal model
LBFGS training took [219] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21909428
Train loss (w/o reg) on all data: 0.21341439
Test loss (w/o reg) on all data: 0.15425311
Train acc on all data:  0.9073543457497613
Test acc on all data:   0.9541984732824428
Norm of the mean of gradients: 2.1370144e-05
Norm of the params: 10.658233
              Random: fixed  39 labels. Loss 0.15425. Accuracy 0.954.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2674427
Train loss (w/o reg) on all data: 0.26033384
Test loss (w/o reg) on all data: 0.23224026
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.8816793893129771
Norm of the mean of gradients: 1.9626104e-05
Norm of the params: 11.923819
Flipped loss: 0.23224. Accuracy: 0.882
### Flips: 312, rs: 39, checks: 52
Using normal model
LBFGS training took [258] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.20443165
Train loss (w/o reg) on all data: 0.19286254
Test loss (w/o reg) on all data: 0.1924189
Train acc on all data:  0.9063992359121299
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 4.6758065e-05
Norm of the params: 15.211256
     Influence (LOO): fixed  35 labels. Loss 0.19242. Accuracy 0.901.
Using normal model
LBFGS training took [233] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.16243804
Train loss (w/o reg) on all data: 0.1467989
Test loss (w/o reg) on all data: 0.21050106
Train acc on all data:  0.9235912129894938
Test acc on all data:   0.9045801526717557
Norm of the mean of gradients: 9.903731e-06
Norm of the params: 17.685669
                Loss: fixed  48 labels. Loss 0.21050. Accuracy 0.905.
Using normal model
LBFGS training took [272] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.26480913
Train loss (w/o reg) on all data: 0.2580864
Test loss (w/o reg) on all data: 0.22490726
Train acc on all data:  0.8767908309455588
Test acc on all data:   0.9007633587786259
Norm of the mean of gradients: 1.2817912e-05
Norm of the params: 11.595455
              Random: fixed   5 labels. Loss 0.22491. Accuracy 0.901.
### Flips: 312, rs: 39, checks: 104
Using normal model
LBFGS training took [244] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1699467
Train loss (w/o reg) on all data: 0.15893155
Test loss (w/o reg) on all data: 0.15616627
Train acc on all data:  0.9293218720152817
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 1.9480782e-05
Norm of the params: 14.842609
     Influence (LOO): fixed  59 labels. Loss 0.15617. Accuracy 0.916.
Using normal model
LBFGS training took [201] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.1004796
Train loss (w/o reg) on all data: 0.081915
Test loss (w/o reg) on all data: 0.1895729
Train acc on all data:  0.9665711556829035
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 2.632593e-05
Norm of the params: 19.268944
                Loss: fixed  88 labels. Loss 0.18957. Accuracy 0.927.
Using normal model
LBFGS training took [267] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2534831
Train loss (w/o reg) on all data: 0.24678802
Test loss (w/o reg) on all data: 0.20016234
Train acc on all data:  0.8796561604584527
Test acc on all data:   0.916030534351145
Norm of the mean of gradients: 4.9151862e-05
Norm of the params: 11.571562
              Random: fixed  17 labels. Loss 0.20016. Accuracy 0.916.
### Flips: 312, rs: 39, checks: 156
Using normal model
LBFGS training took [161] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.13291128
Train loss (w/o reg) on all data: 0.12227145
Test loss (w/o reg) on all data: 0.10796472
Train acc on all data:  0.9484240687679083
Test acc on all data:   0.9580152671755725
Norm of the mean of gradients: 1.62206e-05
Norm of the params: 14.587548
     Influence (LOO): fixed  91 labels. Loss 0.10796. Accuracy 0.958.
Using normal model
LBFGS training took [220] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.063968346
Train loss (w/o reg) on all data: 0.04593448
Test loss (w/o reg) on all data: 0.12985599
Train acc on all data:  0.9808978032473734
Test acc on all data:   0.950381679389313
Norm of the mean of gradients: 1.5390995e-05
Norm of the params: 18.991508
                Loss: fixed 117 labels. Loss 0.12986. Accuracy 0.950.
Using normal model
LBFGS training took [284] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.24637534
Train loss (w/o reg) on all data: 0.23942915
Test loss (w/o reg) on all data: 0.19530684
Train acc on all data:  0.8901623686723973
Test acc on all data:   0.9198473282442748
Norm of the mean of gradients: 3.135725e-05
Norm of the params: 11.78659
              Random: fixed  23 labels. Loss 0.19531. Accuracy 0.920.
### Flips: 312, rs: 39, checks: 208
Using normal model
LBFGS training took [178] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.07887866
Train loss (w/o reg) on all data: 0.06654628
Test loss (w/o reg) on all data: 0.06383854
Train acc on all data:  0.9761222540592168
Test acc on all data:   0.9770992366412213
Norm of the mean of gradients: 4.6992627e-06
Norm of the params: 15.705017
     Influence (LOO): fixed 119 labels. Loss 0.06384. Accuracy 0.977.
Using normal model
LBFGS training took [106] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.036940146
Train loss (w/o reg) on all data: 0.022467075
Test loss (w/o reg) on all data: 0.07594264
Train acc on all data:  0.997134670487106
Test acc on all data:   0.9732824427480916
Norm of the mean of gradients: 8.820896e-06
Norm of the params: 17.013565
                Loss: fixed 136 labels. Loss 0.07594. Accuracy 0.973.
Using normal model
LBFGS training took [232] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.23457935
Train loss (w/o reg) on all data: 0.22781004
Test loss (w/o reg) on all data: 0.17735721
Train acc on all data:  0.8987583572110793
Test acc on all data:   0.9274809160305344
Norm of the mean of gradients: 9.028849e-05
Norm of the params: 11.635555
              Random: fixed  36 labels. Loss 0.17736. Accuracy 0.927.
### Flips: 312, rs: 39, checks: 260
Using normal model
LBFGS training took [153] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.052644067
Train loss (w/o reg) on all data: 0.040444102
Test loss (w/o reg) on all data: 0.04288301
Train acc on all data:  0.9856733524355301
Test acc on all data:   0.9809160305343512
Norm of the mean of gradients: 7.514867e-06
Norm of the params: 15.620476
     Influence (LOO): fixed 136 labels. Loss 0.04288. Accuracy 0.981.
Using normal model
LBFGS training took [121] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.025801843
Train loss (w/o reg) on all data: 0.0136948
Test loss (w/o reg) on all data: 0.025016988
Train acc on all data:  0.9990448901623686
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 5.6372314e-06
Norm of the params: 15.560876
                Loss: fixed 145 labels. Loss 0.02502. Accuracy 0.992.
Using normal model
LBFGS training took [245] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.2233899
Train loss (w/o reg) on all data: 0.21633145
Test loss (w/o reg) on all data: 0.17015655
Train acc on all data:  0.9035339063992359
Test acc on all data:   0.9389312977099237
Norm of the mean of gradients: 2.5560536e-05
Norm of the params: 11.881447
              Random: fixed  46 labels. Loss 0.17016. Accuracy 0.939.
### Flips: 312, rs: 39, checks: 312
Using normal model
LBFGS training took [108] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.02872615
Train loss (w/o reg) on all data: 0.019452102
Test loss (w/o reg) on all data: 0.0267335
Train acc on all data:  0.9933142311365807
Test acc on all data:   0.9961832061068703
Norm of the mean of gradients: 3.997525e-06
Norm of the params: 13.61914
     Influence (LOO): fixed 149 labels. Loss 0.02673. Accuracy 0.996.
Using normal model
LBFGS training took [103] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.0155161815
Train loss (w/o reg) on all data: 0.0071860915
Test loss (w/o reg) on all data: 0.023060506
Train acc on all data:  1.0
Test acc on all data:   0.9923664122137404
Norm of the mean of gradients: 6.7131583e-07
Norm of the params: 12.907433
                Loss: fixed 155 labels. Loss 0.02306. Accuracy 0.992.
Using normal model
LBFGS training took [257] iter.
After training with LBFGS: 
Train loss (w reg) on all data: 0.21484725
Train loss (w/o reg) on all data: 0.20745842
Test loss (w/o reg) on all data: 0.17294928
Train acc on all data:  0.9102196752626552
Test acc on all data:   0.9236641221374046
Norm of the mean of gradients: 1.0912304e-05
Norm of the params: 12.156339
              Random: fixed  51 labels. Loss 0.17295. Accuracy 0.924.
